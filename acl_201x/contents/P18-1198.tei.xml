<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What you can cram into a single $&amp;!#* vector: Probing sentence embeddings for linguistic properties</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 2126</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
							<email>aconneau@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="department" key="dep3">Facebook AI Research Sorbonne Universités</orgName>
								<orgName type="department" key="dep4">Facebook AI Research</orgName>
								<orgName type="institution" key="instit1">Université Le Mans</orgName>
								<orgName type="institution" key="instit2">Université Le Mans</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Kruszewski</surname></persName>
							<email>germank@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="department" key="dep3">Facebook AI Research Sorbonne Universités</orgName>
								<orgName type="department" key="dep4">Facebook AI Research</orgName>
								<orgName type="institution" key="instit1">Université Le Mans</orgName>
								<orgName type="institution" key="instit2">Université Le Mans</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
							<email>glample@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="department" key="dep3">Facebook AI Research Sorbonne Universités</orgName>
								<orgName type="department" key="dep4">Facebook AI Research</orgName>
								<orgName type="institution" key="instit1">Université Le Mans</orgName>
								<orgName type="institution" key="instit2">Université Le Mans</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
							<email>loic.barrault@univ-lemans.fr</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="department" key="dep3">Facebook AI Research Sorbonne Universités</orgName>
								<orgName type="department" key="dep4">Facebook AI Research</orgName>
								<orgName type="institution" key="instit1">Université Le Mans</orgName>
								<orgName type="institution" key="instit2">Université Le Mans</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
							<email>mbaroni@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="department" key="dep3">Facebook AI Research Sorbonne Universités</orgName>
								<orgName type="department" key="dep4">Facebook AI Research</orgName>
								<orgName type="institution" key="instit1">Université Le Mans</orgName>
								<orgName type="institution" key="instit2">Université Le Mans</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">What you can cram into a single $&amp;!#* vector: Probing sentence embeddings for linguistic properties</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2126" to="2136"/>
							<date type="published">July 15-20, 2018. 2018. 2126</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. &quot;Downstream&quot; tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite Ray Mooney's quip that you cannot cram the meaning of a whole %&amp;!$# sentence into a single $&amp;!#* vector, sentence embedding meth- ods have achieved impressive results in tasks rang- ing from machine translation ( <ref type="bibr" target="#b37">Sutskever et al., 2014;</ref><ref type="bibr" target="#b6">Cho et al., 2014</ref>) to entailment detection ( <ref type="bibr" target="#b41">Williams et al., 2018)</ref>, spurring the quest for "uni- versal embeddings" trained once and used in a va- riety of applications (e.g., <ref type="bibr" target="#b8">Conneau et al., 2017;</ref><ref type="bibr" target="#b35">Subramanian et al., 2018)</ref>. Posi- tive results on concrete problems suggest that em- beddings capture important linguistic properties of sentences. However, real-life "downstream" tasks require complex forms of inference, making it dif- ficult to pinpoint the information a model is rely- ing upon. Impressive as it might be that a system can tell that the sentence "A movie that doesn't aim too high, but it doesn't need to" <ref type="bibr" target="#b28">(Pang and Lee, 2004</ref>) expresses a subjective viewpoint, it is hard to tell how the system (or even a human) comes to this conclusion. Complex tasks can also carry hidden biases that models might lock onto <ref type="bibr" target="#b13">(Jabri et al., 2016)</ref>. For example, <ref type="bibr" target="#b19">Lai and Hockenmaier (2014)</ref> show that the simple heuristic of checking for explicit negation words leads to good accuracy in the SICK sentence entailment task.</p><p>Model introspection techniques have been ap- plied to sentence encoders in order to gain a bet- ter understanding of which properties of the in- put sentences their embeddings retain (see Sec- tion 5). However, these techniques often depend on the specifics of an encoder architecture, and consequently cannot be used to compare different methods. <ref type="bibr" target="#b33">Shi et al. (2016)</ref> and <ref type="bibr" target="#b0">Adi et al. (2017)</ref> introduced a more general approach, relying on the notion of what we will call probing tasks. A probing task is a classification problem that fo- cuses on simple linguistic properties of sentences. For example, one such task might require to cat- egorize sentences by the tense of their main verb. Given an encoder (e.g., an LSTM) pre-trained on a certain task (e.g., machine translation), we use the sentence embeddings it produces to train the tense classifier (without further embedding tun- ing). If the classifier succeeds, it means that the pre-trained encoder is storing readable tense infor- mation into the embeddings it creates. Note that: (i) The probing task asks a simple question, min- imizing interpretability problems. (ii) Because of their simplicity, it is easier to control for biases in probing tasks than in downstream tasks. (iii) The probing task methodology is agnostic with respect to the encoder architecture, as long as it produces a vector representation of sentences.</p><p>We greatly extend earlier work on probing tasks as follows. First, we introduce a larger set of prob- ing tasks (10 in total), organized by the type of lin- guistic properties they probe. Second, we system- atize the probing task methodology, controlling for a number of possible nuisance factors, and fram- ing all tasks so that they only require single sen- tence representations as input, for maximum gen- erality and to ease result interpretation. Third, we use our probing tasks to explore a wide range of state-of-the-art encoding architectures and train- ing methods, and further relate probing and down- stream task performance. Finally, we are publicly releasing our probing data sets and tools, hoping they will become a standard way to study the lin- guistic properties of sentence embeddings. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Probing tasks</head><p>In constructing our probing benchmarks, we adopted the following criteria. First, for general- ity and interpretability, the task classification prob- lem should only require single sentence embed- dings as input (as opposed to, e.g., sentence and word embeddings, or multiple sentence represen- tations). Second, it should be possible to construct large training sets in order to train parameter-rich multi-layer classifiers, in case the relevant proper- ties are non-linearly encoded in the sentence vec- tors. Third, nuisance variables such as lexical cues or sentence length should be controlled for. Fi- nally, and most importantly, we want tasks that address an interesting set of linguistic properties. We thus strove to come up with a set of tasks that, while respecting the previous constraints, probe a wide range of phenomena, from superficial prop- erties of sentences such as which words they con- tain to their hierarchical structure to subtle facets of semantic acceptability. We think the current task set is reasonably representative of different linguistic domains, but we are not claiming that it is exhaustive. We expect future work to extend it.</p><p>The sentences for all our tasks are extracted from the Toronto Book Corpus ( , more specifically from the random pre-processed portion made available by <ref type="bibr" target="#b29">Paperno et al. (2016)</ref>. We only sample sentences in the 5-to-28 word range. We parse them with the Stanford Parser (2017-06-09 version), using the pre-trained PCFG model ( <ref type="bibr" target="#b16">Klein and Manning, 2003)</ref>, and we rely on the part-of-speech, constituency and dependency parsing information provided by this tool where needed. For each task, we construct training sets containing 100k sentences, and 10k-sentence val-idation and test sets. All sets are balanced, having an equal number of instances of each target class.</p><p>Surface information These tasks test the extent to which sentence embeddings are preserving sur- face properties of the sentences they encode. One can solve the surface tasks by simply looking at tokens in the input sentences: no linguistic knowl- edge is called for. The first task is to predict the length of sentences in terms of number of words (SentLen). Following <ref type="bibr" target="#b0">Adi et al. (2017)</ref>, we group sentences into 6 equal-width bins by length, and treat SentLen as a 6-way classification task. The word content (WC) task tests whether it is possible to recover information about the original words in the sentence from its embedding. We picked 1000 mid-frequency words from the source corpus vo- cabulary (the words with ranks between 2k and 3k when sorted by frequency), and sampled equal numbers of sentences that contain one and only one of these words. The task is to tell which of the 1k words a sentence contains (1k-way classifi- cation). This setup allows us to probe a sentence embedding for word content without requiring an auxiliary word embedding (as in the setup of Adi and colleagues).</p><p>Syntactic information The next batch of tasks test whether sentence embeddings are sensitive to syntactic properties of the sentences they encode. The bigram shift (BShift) task tests whether an encoder is sensitive to legal word orders. In this binary classification problem, models must distin- guish intact sentences sampled from the corpus from sentences where we inverted two random ad- jacent words ("What you are doing out there?").</p><p>The tree depth (TreeDepth) task checks whether an encoder infers the hierarchical struc- ture of sentences, and in particular whether it can group sentences by the depth of the longest path from root to any leaf. Since tree depth is naturally correlated with sentence length, we de-correlate these variables through a structured sampling pro- cedure. In the resulting data set, tree depth val- ues range from 5 to 12, and the task is to catego- rize sentences into the class corresponding to their depth (8 classes). As an example, the following is a long <ref type="bibr">(</ref> In the top constituent task (TopConst), sen- tences must be classified in terms of the sequence of top constituents immediately below the sen- tence (S) node. An encoder that successfully ad- dresses this challenge is not only capturing latent syntactic structures, but clustering them by con- stituent types. TopConst was introduced by <ref type="bibr" target="#b33">Shi et al. (2016)</ref>. Following them, we frame it as a 20-way classification problem: 19 classes for the most frequent top constructions, and one for all other constructions. As an example, " <ref type="bibr">[Then]</ref> [very dark gray letters on a black screen] [appeared] <ref type="bibr">[.]</ref>" has top constituent sequence: "ADVP NP VP .".</p><p>Note that, while we would not expect an un- trained human subject to be explicitly aware of tree depth or top constituency, similar information must be implicitly computed to correctly parse sentences, and there is suggestive evidence that the brain tracks something akin to tree depth during sentence processing ( <ref type="bibr">Nelson et al., 2017)</ref>.</p><p>Semantic information These tasks also rely on syntactic structure, but they further require some understanding of what a sentence denotes. The Tense task asks for the tense of the main-clause verb (VBP/VBZ forms are labeled as present, VBD as past). No target form occurs across the train/dev/test split, so that classifiers cannot rely on specific words (it is not clear that Shi and col- leagues, who introduced this task, controlled for this factor). The subject number (SubjNum) task focuses on the number of the subject of the main clause (number in English is more often explic- itly marked on nouns than verbs). Again, there is no target overlap across partitions. Similarly, object number (ObjNum) tests for the number of the direct object of the main clause (again, avoid- ing lexical overlap). To solve the previous tasks correctly, an encoder must not only capture tense and number, but also extract structural informa- tion (about the main clause and its arguments). We grouped Tense, SubjNum and ObjNum with the semantic tasks, since, at least for models that treat words as unanalyzed input units (without access to morphology), they must rely on what a sentence denotes (e.g., whether the described event took place in the past), rather than on struc- tural/syntactic information. We recognize, how- ever, that the boundary between syntactic and se- mantic tasks is somewhat arbitrary.</p><p>In the semantic odd man out (SOMO) task, we modified sentences by replacing a random noun or verb o with another noun or verb r. To make the task more challenging, the bigrams formed by the replacement with the previous and following words in the sentence have frequencies that are comparable (on a log-scale) with those of the orig- inal bigrams. That is, if the original sentence con- tains bigrams w n−1 o and ow n+1 , the correspond- ing bigrams w n−1 r and rw n+1 in the modified sentence will have comparable corpus frequencies.</p><p>No sentence is included in both original and modi- fied format, and no replacement is repeated across train/dev/test sets. The task of the classifier is to tell whether a sentence has been modified or not. An example modified sentence is: " No one could see this Hayes and I wanted to know if it was real or a spoonful (orig.: ploy)." Note that judg- ing plausibility of a syntactically well-formed sen- tence of this sort will often require grasping rather subtle semantic factors, ranging from selectional preference to topical coherence.</p><p>The coordination inversion (CoordInv) bench- mark contains sentences made of two coordinate clauses. In half of the sentences, we inverted the order of the clauses. The task is to tell whether a sentence is intact or modified. Sentences are balanced in terms of clause length, and no sentence appears in both original and inverted versions. As an example, original "They might be only memories, but I can still feel each one" becomes: "I can still feel each one, but they might be only memories." Often, addressing CoordInv requires an understanding of broad discourse and pragmatic factors.</p><p>Row Hum. Eval. of <ref type="table" target="#tab_2">Table 2</ref> reports human- validated "reasonable" upper bounds for all the tasks, estimated in different ways, depending on the tasks. For the surface ones, there is always a straightforward correct answer that a human an- notator with enough time and patience could find. The upper bound is thus estimated at 100%. The TreeDepth, TopConst, Tense, SubjNum and Ob- jNum tasks depend on automated PoS and pars- ing annotation. In these cases, the upper bound is given by the proportion of sentences correctly annotated by the automated procedure. To esti- mate this quantity, one linguistically-trained au- thor checked the annotation of 200 randomly sam- pled test sentences from each task. Finally, the BShift, SOMO and CoordInv manipulations can accidentally generate acceptable sentences. For example, one modified SOMO sentence is: "He pulled out the large round onion (orig.: cork) and saw the amber balm inside.", that is arguably not more anomalous than the original. For these tasks, we ran Amazon Mechanical Turk experiments in which subjects were asked to judge whether 1k randomly sampled test sentences were acceptable or not. Reported human accuracies are based on majority voting. See Appendix for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sentence embedding models</head><p>In this section, we present the three sentence en- coders that we consider and the seven tasks on which we train them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentence encoder architectures</head><p>A wide variety of neural networks encoding sen- tences into fixed-size representations exist. We fo- cus here on three that have been shown to perform well on standard NLP tasks.</p><p>BiLSTM-last/max For a sequence of T words {w t } t=1,...,T , a bidirectional LSTM computes a set of T vectors {h t } t . For t ∈ [1, . . . , T ], h t is the concatenation of a forward LSTM and a back- ward LSTM that read the sentences in two op- posite directions. We experiment with two ways of combining the varying number of (h 1 , . . . , h T ) to form a fixed-size vector, either by selecting the last hidden state of h T or by selecting the maximum value over each dimension of the hid- den units. The choice of these models are moti- vated by their demonstrated efficiency in seq2seq <ref type="bibr" target="#b37">(Sutskever et al., 2014</ref>) and universal sentence rep- resentation learning ( <ref type="bibr" target="#b8">Conneau et al., 2017)</ref>, re- spectively. <ref type="bibr">2</ref> Gated ConvNet We also consider the non- recurrent convolutional equivalent of LSTMs, based on stacked gated temporal convolutions. Gated convolutional networks were shown to per- form well as neural machine translation encoders <ref type="bibr" target="#b11">(Gehring et al., 2017</ref>) and language modeling de- coders ( ). The encoder is com- posed of an input word embedding table that is augmented with positional encodings (Sukhbaatar et al., 2015), followed by a stack of temporal con- volutions with small kernel size. The output of each convolutional layer is filtered by a gating mechanism, similar to the one of LSTMs. Finally, max-pooling along the temporal dimension is per- formed on the output feature maps of the last con- volution <ref type="bibr" target="#b7">(Collobert and Weston, 2008</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training tasks</head><p>Seq2seq systems have shown strong results in ma- chine translation ( <ref type="bibr" target="#b42">Zhou et al., 2016</ref>). They con- sist of an encoder that encodes a source sen- tence into a fixed-size representation, and a de- coder which acts as a conditional language model and that generates the target sentence. We train Neural Machine Translation systems on three language pairs using about 2M sentences from the Europarl corpora ( <ref type="bibr" target="#b17">Koehn, 2005</ref>). We pick English-French, which involves two similar lan- guages, English-German, involving larger syn- tactic differences, and English-Finnish, a distant pair. We also train with an AutoEncoder objec- tive <ref type="bibr" target="#b34">(Socher et al., 2011</ref>) on Europarl source En- glish sentences. Following <ref type="bibr" target="#b40">Vinyals et al. (2015)</ref>, we train a seq2seq architecture to generate lin- earized grammatical parse trees (see <ref type="table">Table 1</ref>) from source sentences (Seq2Tree). We use the Stan- ford parser to generate trees for Europarl source English sentences. We train SkipThought vectors ( ) by predicting the next sentence given the current one ( <ref type="bibr" target="#b38">Tang et al., 2017</ref>), on 30M sentences from the Toronto Book Corpus, exclud- ing those in the probing sets. Finally, following <ref type="bibr" target="#b8">Conneau et al. (2017)</ref>, we train sentence encoders on Natural Language Inference using the con- catenation of the SNLI (Bowman et al., 2015) and MultiNLI ( <ref type="bibr" target="#b5">Bowman et al., 2015</ref>) data sets (about 1M sentence pairs). In this task, a sentence en- coder is trained to encode two sentences, which are fed to a classifier and whose role is to dis- tinguish whether the sentences are contradictory, neutral or entailed. Finally, as in <ref type="bibr" target="#b8">Conneau et al. (2017)</ref>, we also include Untrained encoders with random weights, which act as random projections of pre-trained word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training details</head><p>BiLSTM encoders use 2 layers of 512 hidden units (∼4M parameters), Gated ConvNet has 8 convo- lutional layers of 512 hidden units, kernel size 3 (∼12M parameters). We use pre-trained fast- Text word embeddings of size 300 ( <ref type="bibr" target="#b24">Mikolov et al., 2018</ref>) without fine-tuning, to isolate the impact of encoder architectures and to handle words outside the training sets. Training task performance and further details are in Appendix. AutoEncoder I myself was out on an island in the Swedish archipelago , at Sandhamn .</p><p>I myself was out on an island in the Swedish archipelago , at Sand@ ham@ n .</p><p>NMT En-Fr I myself was out on an island in the Swedish archipelago , at Sandhamn .</p><p>Je me trouvais ce jour là sur une île de l' archipel sué- dois , à Sand@ ham@ n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NMT En-De</head><p>We really need to up our particular con- tribution in that regard .</p><p>Wir müssen wirklich unsere spezielle Hilfs@ leistung in dieser Hinsicht aufstocken .</p><p>NMT En-Fi It is too early to see one system as a uni- versal panacea and dismiss another .</p><p>Nyt on liian aikaista nostaa yksi järjestelmä jal@ usta@ lle ja antaa jollekin toiselle huono arvo@ sana .</p><p>SkipThought the old sami was gone , and he was a different person now .</p><p>the new sami didn 't mind standing barefoot in dirty white , sans ra@ y-@ bans and without beautiful women following his every move .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seq2Tree</head><p>Dikoya is a village in Sri Lanka . <ref type="table">Table 1</ref>: Source and target examples for seq2seq training tasks.</p><formula xml:id="formula_0">( ROOT ( S ( NP NNP ) NP ( VP VBZ ( NP ( NP DT NN ) NP ( PP IN ( NP NNP NNP ) NP ) PP ) NP ) VP . ) S ) ROOT</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Probing task experiments</head><p>Baselines Baseline and human-bound perfor- mance are reported in the top block of <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Length is a linear classifier with sentence length as sole feature. NB-uni-tfidf is a Naive Bayes classifier using words' tfidf scores as features, NB- bi-tfidf its extension to bigrams. Finally, BoV- fastText derives sentence representations by aver- aging the fastText embeddings of the words they contain (same embeddings used as input to the en- coders). 3 Except, trivially, for Length on SentLen and the NB baselines on WC, there is a healthy gap be- tween top baseline performance and human up- per bounds. NB-uni-tfidf evaluates to what extent our tasks can be addressed solely based on knowl- edge about the distribution of words in the train- ing sentences. Words are of course to some extent informative for most tasks, leading to relatively high performance in Tense, SubjNum and Ob- jNum. Recall that the words containing the probed features are disjoint between train and test parti- tions, so we are not observing a confound here, but rather the effect of the redundancies one expects in natural language data. For example, for Tense, since sentences often contain more than one verb in the same tense, NB-uni-tfidf can exploit non- target verbs as cues: the NB features most associ- ated to the past class are verbs in the past tense (e.g "sensed", "lied", "announced"), and similarly for present (e.g "uses", "chuckles", "frowns"). Us- ing bigram features (NB-bi-tfidf) brings in gen- eral little or no improvement with respect to the unigram baseline, except, trivially, for the BShift <ref type="bibr">3</ref> Similar results are obtained summing embeddings, and using GloVe embeddings ( <ref type="bibr" target="#b30">Pennington et al., 2014</ref>). task, where NB-bi-tfidf can easily detect unlikely bigrams. NB-bi-tfidf has below-random perfor- mance on SOMO, confirming that the semantic intruder is not given away by superficial bigram cues.</p><p>Our first striking result is the good overall per- formance of Bag-of-Vectors, confirming early in- sights that aggregated word embeddings capture surprising amounts of sentence information <ref type="bibr" target="#b31">(Pham et al., 2015;</ref><ref type="bibr" target="#b1">Arora et al., 2017;</ref><ref type="bibr" target="#b0">Adi et al., 2017</ref>). BoV's good WC and SentLen performance was al- ready established by <ref type="bibr" target="#b0">Adi et al. (2017)</ref>. Not sur- prisingly, word-order-unaware BoV performs ran- domly in BShift and in the more sophisticated se- mantic tasks SOMO and CoordInv. More interest- ingly, BoV is very good at the Tense, SubjNum, ObjNum, and TopConst tasks (much better than the word-based baselines), and well above chance in TreeDepth. The good performance on Tense, SubjNum and ObjNum has a straightforward ex- planation we have already hinted at above. Many sentences are naturally "redundant", in the sense that most tensed verbs in a sentence are in the same tense, and similarly for number in nouns. In 95.2% Tense, 75.9% SubjNum and 78.7% Ob- jNum test sentences, the target tense/number fea- ture is also the majority one for the whole sen- tence. Word embeddings capture features such as number and tense <ref type="bibr" target="#b25">(Mikolov et al., 2013</ref>  by the embeddings (such as the part of speech of a word) might signal different syntactic structures. For example, sentences in the "WHNP SQ ." top constituent class (e.g., "How long before you leave us again?") must contain a wh word, and will of- ten feature an auxiliary or modal verb. BoV can rely on this information to noisily predict the cor- rect class.</p><p>Encoding architectures Comfortingly, proper encoding architectures clearly outperform BoV. An interesting observation in gests that the prior imposed by the encoder ar- chitecture strongly preconditions the nature of the embeddings. Complementing recent evidence that convolutional architectures are on a par with recur- rent ones in seq2seq tasks <ref type="bibr" target="#b11">(Gehring et al., 2017)</ref>, we find that Gated ConvNet's overall probing task performance is comparable to that of the best LSTM architecture (although, as shown in Ap- pendix, the LSTM has a slight edge on down- stream tasks). We also replicate the finding of <ref type="bibr" target="#b8">Conneau et al. (2017)</ref> that BiLSTM-max outper- forms BiLSTM-last both in the downstream tasks (see Appendix) and in the probing tasks <ref type="table" target="#tab_2">(Table 2)</ref>. Interestingly, the latter only outperforms the for- mer in SentLen, a task that captures a superficial aspect of sentences (how many words they con- tain), that could get in the way of inducing more useful linguistic knowledge.</p><p>Training tasks We focus next on how different training tasks affect BiLSTM-max, but the pat- terns are generally representative across architec- tures. NMT training leads to encoders that are more linguistically aware than those trained on the NLI data set, despite the fact that we confirm the finding of Conneau and colleagues that NLI is best for downstream tasks (Appendix). Perhaps, NMT captures richer linguistic features useful for the probing tasks, whereas shallower or more ad- hoc features might help more in our current down- stream tasks. Suggestively, the one task where NLI clearly outperforms NMT is WC. Thus, NLI training is better at preserving shallower word fea- tures that might be more useful in downstream tasks (cf. <ref type="figure" target="#fig_3">Figure 2</ref> and discussion there). Unsupervised training (SkipThought and Au- toEncoder) is not on a par with supervised tasks, but still effective. AutoEncoder training leads, un- surprisingly, to a model excelling at SentLen, but it attains low performance in the WC prediction task. This curious result might indicate that the latter information is stored in the embeddings in a complex way, not easily readable by our MLP. At the other end, Seq2Tree is trained to predict an- notation from the same parser we used to create some of the probing tasks. Thus, its high perfor- mance on TopConst, Tense, SubjNum, ObjNum and TreeDepth is probably an artifact. Indeed, for most of these tasks, Seq2Tree performance is above the human bound, that is, Seq2Tree learned to mimic the parser errors in our benchmarks. For the more challenging SOMO and CoordInv tasks, that only indirectly rely on tagging/parsing infor- mation, Seq2Tree is comparable to NMT, that does not use explicit syntactic information.</p><p>Perhaps most interestingly, BiLSTM-max al- ready achieves very good performance without any training (Untrained row in <ref type="table" target="#tab_2">Table 2</ref>). Un- trained BiLSTM-max also performs quite well in the downstream tasks (Appendix). This ar- chitecture must encode priors that are intrinsi- cally good for sentence representations. Untrained BiLSTM-max exploits the input fastText embed- dings, and multiplying the latter by a random re- current matrix provides a form of positional en- coding. However, good performance in a task such as SOMO, where BoV fails and positional infor- mation alone should not help (the intruder is ran- domly distributed across the sentence), suggests that other architectural biases are at work. In- triguingly, a preliminary comparison of untrained BiLSTM-max and human subjects on the SOMO sentences evaluated by both reveals that, whereas humans have a bias towards finding sentences ac- ceptable (62% sentences are rated as untampered with, vs. 48% ground-truth proportion), the model has a strong bias in the opposite direction (it rates 83% of the sentences as modified). A cursory look at contrasting errors confirms, unsurprisingly, that those made by humans are perfectly justi- fied, while model errors are opaque. For exam- ple, the sentence "I didn't come here to reunite (orig. undermine) you" seems perfectly acceptable in its modified form, and indeed subjects judged it as such, whereas untrained BiLSTM-max "cor- rectly" rated it as a modified item. Conversely, it is difficult to see any clear reason for the latter tendency to rate perfectly acceptable originals as modified. We leave a more thorough investigation to further work. See similar observations on the effectiveness of untrained ConvNets in vision by <ref type="bibr" target="#b39">Ulyanov et al. (2017)</ref>.</p><p>Probing task comparison A good encoder, such as NMT-trained BiLSTM-max, shows gen- erally good performance across probing tasks. At one extreme, performance is not particularly high on the surface tasks, which might be an indirect sign of the encoder extracting "deeper" linguistic properties. At the other end, performance is still far from the human bounds on TreeDepth, BShift, SOMO and CoordInv. The last 3 tasks ask if a sentence is syntactically or semantically anoma- lous. This is a daunting job for an encoder that has not been explicitly trained on acceptability, and it is interesting that the best models are, at least to a certain extent, able to produce reasonable anomaly judgments. The asymmetry between the difficult TreeDepth and easier TopConst is also interesting. Intuitively, TreeDepth requires more nuanced syn- tactic information (down to the deepest leaf of the tree) than TopConst, that only requires identifying broad chunks. <ref type="figure" target="#fig_2">Figure 1</ref> reports how probing task accuracy changes in function of encoder training epochs. The figure shows that NMT probing performance is largely independent of target language, with strikingly similar development patterns across French, German and Finnish. Note in particular the similar probing accuracy curves in French and Finnish, while the corresponding BLEU scores (in lavender) are consistently higher in the former lan- NMT En-Fr -BiLSTM-max NMT En-De -BiLSTM-max guage. For both NMT and SkipThought, WC performance keeps increasing with epochs. For the other tasks, we observe instead an early flat- tening of the NMT probing curves, while BLEU performance keeps increasing. Most strikingly, SentLen performance is actually decreasing, sug- gesting again that, as a model captures deeper lin- guistic properties, it will tend to forget about this superficial feature. Finally, for the challenging SOMO task, the curves are mostly flat, suggesting that what BiLSTM-max is able to capture about this task is already encoded in its architecture, and further training doesn't help much.</p><p>Probing vs. downstream tasks <ref type="figure" target="#fig_3">Figure 2</ref> reports correlation between performance on our probing tasks and the downstream tasks available in the SentEval 5 suite, which consists of classification (MR, CR, SUBJ, MPQA, SST2, SST5, TREC), natural language inference (SICK-E), semantic relatedness (SICK-R, STSB), paraphrase detec- tion (MRPC) and semantic textual similarity (STS 2012 to 2017) tasks. Strikingly, WC is signifi- cantly positively correlated with all downstream tasks. This suggests that, at least for current mod- els, the latter do not require extracting particu- larly abstract knowledge from the data. Just rely- ing on the words contained in the input sentences can get you a long way. Conversely, there is a significant negative correlation between SentLen and most downstream tasks. The number of words in a sentence is not informative about its linguis- tic contents. The more models abstract away from such information, the more likely it is they will use their capacity to capture more interest- ing features, as the decrease of the SentLen curve along training (see <ref type="figure" target="#fig_2">Figure 1</ref>) also suggests. Co- ordInv and, especially, SOMO, the tasks requir- ing the most sophisticated semantic knowledge, are those that positively correlate with the largest number of downstream tasks after WC. We ob- serve intriguing asymmetries: SOMO correlates with the SICK-E sentence entailment test, but not with SICK-R, which is about modeling sentence relatedness intuitions. Indeed, logical entailment requires deeper semantic analysis than modeling similarity judgments. TopConst and the num- ber tasks negatively correlate with various similar- ity and sentiment data sets (SST, STS, SICK-R). This might expose biases in these tasks: SICK-R, for example, deliberately contains sentence pairs with opposite voice, that will have different con- stituent structure but equal meaning <ref type="bibr" target="#b23">(Marelli et al., 2014)</ref>. It might also mirrors genuine factors af- fecting similarity judgments (e.g., two sentences differing only in object number are very similar). Remarkably, TREC question type classification is the downstream task correlating with most prob- ing tasks. Question classification is certainly an outlier among our downstream tasks, but we must leave a full understanding of this behaviour to fu- ture work (this is exactly the sort of analysis our probing tasks should stimulate).   <ref type="bibr" target="#b9">Dalvi et al. (2017)</ref> are also interested in un- derstanding the type of linguistic knowledge en- coded in sentence and word embeddings, but their focus is on word-level morphosyntax and lexical semantics, and specifically on NMT encoders and decoders. Sennrich (2017) also focuses on NMT systems, and proposes a contrastive test to as- sess how they handle various linguistic phenom- ena. Other work explores the linguistic behaviour of recurrent networks and related models by using visualization, input/hidden representation deletion techniques or by looking at the word-by-word be- haviour of the network (e.g., <ref type="bibr" target="#b26">Nagamine et al., 2015;</ref><ref type="bibr" target="#b12">Hupkes et al., 2017;</ref><ref type="bibr" target="#b22">Linzen et al., 2016;</ref><ref type="bibr" target="#b14">Kàdàr et al., 2017;</ref><ref type="bibr" target="#b21">Li et al., 2017)</ref>. These methods, complementary to ours, are not agnostic to encoder architecture, and cannot be used for general-purpose cross-model evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Finally, <ref type="bibr" target="#b8">Conneau et al. (2017)</ref> propose a large- scale, multi-task evaluation of sentence embed- dings, focusing entirely on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced a set of tasks probing the linguis- tic knowledge of sentence embedding methods. Their purpose is not to encourage the development of ad-hoc models that attain top performance on them, but to help exploring what information is captured by different pre-trained encoders.</p><p>We performed an extensive linguistic evaluation of modern sentence encoders. Our results suggest that the encoders are capturing a wide range of properties, well above those captured by a set of strong baselines. We further uncovered interesting patterns of correlation between the probing tasks and more complex "downstream" tasks, and pre- sented a set of intriguing findings about the lin- guistic properties of various embedding methods. For example, we found that Bag-of-Vectors is sur- prisingly good at capturing sentence-level proper- ties, thanks to redundancies in natural linguistic input. We showed that different encoder architec- tures trained with the same objective with similar performance can result in different embeddings, pointing out the importance of the architecture prior for sentence embeddings. In particular, we found that BiLSTM-max embeddings are already capturing interesting linguistic knowledge before training, and that, after training, they detect se- mantic acceptability without having been exposed to anomalous sentences before. We hope that our publicly available probing task set will become a standard benchmarking tool of the linguistic prop- erties of new encoders, and that it will stir research towards a better understanding of what they learn.</p><p>In future work, we would like to extend the probing tasks to other languages (which should be relatively easy, given that they are automati- cally generated), investigate how multi-task train- ing affects probing task performance and leverage our probing tasks to find more linguistically-aware universal encoders.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Probing task scores after each training epoch, for NMT and SkipThought. We also report training score evolution: BLEU for NMT; perplexity (PPL) for SkipThought.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Spearman correlation matrix between probing and downstream tasks. Correlations based on all sentence embeddings we investigated (more than 40). Cells in gray denote task pairs that are not significantly correlated (after correcting for multiple comparisons).</figDesc><graphic url="image-1.png" coords="9,72.00,62.81,218.26,139.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>), so aggre- gated word embeddings will naturally track these features' majority values in a sentence. BoV's TopConst and TreeDepth performance is more sur- prising. Accuracy is well above NB, showing that BoV is exploiting cues beyond specific words strongly associated to the target classes. We con- jecture that more abstract word features captured Task SentLen WC TreeDepth TopConst BShift Tense SubjNum ObjNum SOMO CoordInv</figDesc><table>Baseline representations 
Majority vote 
20.0 
0.5 
17.9 
5.0 
50.0 
50.0 
50.0 
50.0 
50.0 
50.0 
Hum. Eval. 
100 
100 
84.0 
84.0 
98.0 
85.0 
88.0 
86.5 
81.2 
85.0 
Length 
100 
0.2 
18.1 
9.3 
50.6 
56.5 
50.3 
50.1 
50.2 
50.0 
NB-uni-tfidf 
22.7 
97.8 
24.1 
41.9 
49.5 
77.7 
68.9 
64.0 
38.0 
50.5 
NB-bi-tfidf 
23.0 
95.0 
24.6 
53.0 
63.8 
75.9 
69.1 
65.4 
39.9 
55.7 
BoV-fastText 
66.6 
91.6 
37.1 
68.1 
50.8 
89.1 
82.1 
79.8 
54.2 
54.8 

BiLSTM-last encoder 
Untrained 
36.7 
43.8 
28.5 
76.3 
49.8 
84.9 
84.7 
74.7 
51.1 
64.3 
AutoEncoder 
99.3 
23.3 
35.6 
78.2 
62.0 
84.3 
84.7 
82.1 
49.9 
65.1 
NMT En-Fr 
83.5 
55.6 
42.4 
81.6 
62.3 
88.1 
89.7 
89.5 
52.0 
71.2 
NMT En-De 
83.8 
53.1 
42.1 
81.8 
60.6 
88.6 
89.3 
87.3 
51.5 
71.3 
NMT En-Fi 
82.4 
52.6 
40.8 
81.3 
58.8 
88.4 
86.8 
85.3 
52.1 
71.0 
Seq2Tree 
94.0 
14.0 
59.6 
89.4 
78.6 
89.9 
94.4 
94.7 
49.6 
67.8 
SkipThought 
68.1 
35.9 
33.5 
75.4 
60.1 
89.1 
80.5 
77.1 
55.6 
67.7 
NLI 
75.9 
47.3 
32.7 
70.5 
54.5 
79.7 
79.3 
71.3 
53.3 
66.5 

BiLSTM-max encoder 
Untrained 
73.3 
88.8 
46.2 
71.8 
70.6 
89.2 
85.8 
81.9 
73.3 
68.3 
AutoEncoder 
99.1 
17.5 
45.5 
74.9 
71.9 
86.4 
87.0 
83.5 
73.4 
71.7 
NMT En-Fr 
80.1 
58.3 
51.7 
81.9 
73.7 
89.5 
90.3 
89.1 
73.2 
75.4 
NMT En-De 
79.9 
56.0 
52.3 
82.2 
72.1 
90.5 
90.9 
89.5 
73.4 
76.2 
NMT En-Fi 
78.5 
58.3 
50.9 
82.5 
71.7 
90.0 
90.3 
88.0 
73.2 
75.4 
Seq2Tree 
93.3 
10.3 
63.8 
89.6 
82.1 
90.9 
95.1 
95.1 
73.2 
71.9 
SkipThought 
66.0 
35.7 
44.6 
72.5 
73.8 
90.3 
85.0 
80.6 
73.6 
71.0 
NLI 
71.7 
87.3 
41.6 
70.5 
65.1 
86.7 
80.7 
80.3 
62.1 
66.8 

GatedConvNet encoder 
Untrained 
90.3 
17.1 
30.3 
47.5 
62.0 
78.2 
72.2 
70.9 
61.4 
59.6 
AutoEncoder 
99.4 
16.8 
46.3 
75.2 
71.9 
87.7 
88.5 
86.5 
73.5 
72.4 
NMT En-Fr 
84.8 
41.3 
44.6 
77.6 
67.9 
87.9 
88.8 
86.6 
66.1 
72.0 
NMT En-De 
89.6 
49.0 
50.5 
81.7 
72.3 
90.4 
91.4 
89.7 
72.8 
75.1 
NMT En-Fi 
89.3 
51.5 
49.6 
81.8 
70.9 
90.4 
90.9 
89.4 
72.4 
75.1 
Seq2Tree 
96.5 
8.7 
62.0 
88.9 
83.6 
91.5 
94.5 
94.3 
73.5 
73.8 
SkipThought 
79.1 
48.4 
45.7 
79.2 
73.4 
90.7 
86.6 
81.7 
72.4 
72.3 
NLI 
73.8 
29.2 
43.2 
63.9 
70.7 
81.3 
77.5 
74.4 
73.3 
71.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Probing task accuracies. Classification performed by a MLP with sigmoid nonlinearity, taking 
pre-learned sentence embeddings as input (see Appendix for details and logistic regression results). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 is</head><label>2</label><figDesc></figDesc><table>that dif-
ferent encoder architectures trained with the same 
objective, and achieving similar performance on 
the training task, 4 can lead to linguistically dif-
ferent embeddings, as indicated by the probing 
tasks. Coherently with the findings of Conneau 
et al. (2017) for the downstream tasks, this sug-

4 See Appendix for details on training task performance. 

</table></figure>

			<note place="foot" n="1"> https://github.com/facebookresearch/ SentEval/tree/master/data/probing</note>

			<note place="foot" n="2"> We also experimented with a unidirectional LSTM, with consistently poorer results.</note>

			<note place="foot" n="5"> https://github.com/facebookresearch/ SentEval</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank David Lopez-Paz, Holger Schwenk, Hervé Jégou, Marc'Aurelio Ranzato and Douwe Kiela for useful comments and discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fine-grained analysis of sentence embeddings using auxiliary prediction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Kermany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<ptr target="https://openreview.net/group?id=ICLR.cc/2017/conference" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR Conference Track</title>
		<meeting>ICLR Conference Track<address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Published online</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="https://openreview.net/group?id=ICLR.cc/2017/conference" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR Conference Track</title>
		<meeting>ICLR Conference Track<address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Published online</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What do neural machine translation models learn about morphology?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="861" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating layers of representation in neural machine translation on part-of-speech and semantic tagging tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP. Taipei</title>
		<meeting>IJCNLP. Taipei<address><addrLine>Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine learning</title>
		<meeting>the 25th International Conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding and improving morphological learning in the neural machine translation decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="142" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Visualisation and diagnostic classifiers reveal how recurrent and recursive neural networks process hierarchical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieuwke</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Veldhoen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1711.10203" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Revisiting visual question answering baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV<address><addrLine>Amsterdam, the Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="727" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Representation of linguistic form and function in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Àkos</forename><surname>Kàdàr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afra</forename><surname>Alishahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="761" to="780" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL. Sapporo</title>
		<meeting>ACL. Sapporo<address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MT summit</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions. Association for Computational Linguistics</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Illinois-LH: A denotational and distributional approach to semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="329" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing and understanding neural models in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="681" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monroe</forename><surname>Will</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1612" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Assessing the ability of LSTMs to learn syntax-sensitive dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC. Rekjavik</title>
		<meeting>LREC. Rekjavik<address><addrLine>Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Advances in pre-training distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC. Miyazaki</title>
		<meeting>LREC. Miyazaki<address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring how deep neural networks form phonemic categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tasha</forename><surname>Nagamine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Christophe Pallier, and Stanislas Dehaene. 2017. Neurophysiological dynamics of phrase-structure building during sentence processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imen</forename><forename type="middle">El</forename><surname>Karoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristof</forename><surname>Giber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilda</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sydney</forename><surname>Cash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Naccache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3669" to="3678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1525" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Jointly optimizing word representations for lexical and sentential tasks with the C-PHRASE model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Nghia The Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="971" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">How grammatical is characterlevel neural machine translation? assessing MT quality with contrastive translation pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="376" to="382" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Does string-based neural MT learn source syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1526" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS. Granada, Spain</title>
		<meeting>NIPS. Granada, Spain</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning general purpose distributed sentence representations via large scale multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher J</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Trimming and improving skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia R De</forename><surname>Sa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1711.10925" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deep recurrent models with fast-forward connections for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04199</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
