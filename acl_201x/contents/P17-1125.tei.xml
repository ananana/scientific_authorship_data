<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Flexible and Creative Chinese Poetry Generation Using Neural Memory</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyuan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Abel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyue</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andi</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">Flexible and Creative Chinese Poetry Generation Using Neural Memory</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1364" to="1373"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1125</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>It has been shown that Chinese poems can be successfully generated by sequence-to-sequence neural models, particularly with the attention mechanism. A potential problem of this approach, however , is that neural models can only learn abstract rules, while poem generation is a highly creative process that involves not only rules but also innovations for which pure statistical models are not appropriate in principle. This work proposes a memory-augmented neural model for Chi-nese poem generation, where the neural model and the augmented memory work together to balance the requirements of linguistic accordance and aesthetic innovation , leading to innovative generations that are still rule-compliant. In addition, it is found that the memory mechanism provides interesting flexibility that can be used to generate poems with different styles.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Classical Chinese poetry is a special cultural her- itage with over 2,000 years of history and is stil- l fascinating us today. Among the various gen- res, perhaps the most popular one is the quatrain, a special style with a strict structure (four lines with five or seven characters per line), a regulat- ed rhythmical form (the last characters in the sec- ond and fourth lines must follow the same rhyth- m), and a required tonal pattern (tones of charac- ters in some positions should satisfy a predefined regulation) <ref type="bibr" target="#b9">(Wang, 2002</ref>). This genre flourished mostly in the Tang Dynasty, and so are often called <ref type="bibr">1</ref> Corresponding author: Dong Wang; RM 1-303, FIT BLDG, Tsinghua University, Beijing (100084), P.R. China.</p><p>'Tang poems'. An example of a quatrain written by Wei Wang, a famous poet in the Tang Dynasty, is shown in <ref type="table">Table 1</ref>.</p><p>Due to the stringent restrictions in both rhyth- m and tone, it is not trivial to create a ful- ly rule-compliant quatrain. More importantly, besides such strict regulations, a good quatrain should also read fluently, hold a consistent theme, and express a unique affection. Therefore, poem generation is widely recognized as a very intelli- gent activity and can be performed only by knowl- edgeable people with a lot of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wi</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Climbing the Paradise Mound</head><p>•¿Ø· § (* Z Z P Z) As I was not in a good mood this evening round, °•"(P P P Z P) I went by cart to climb the Ancient Paradise Mound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IÃ•Ð § (* P P Z Z)</head><p>It is now nearing dusk, •´C'³"(* Z Z P P) When the setting sun is infinitely fine, which is a must. <ref type="table">Table 1</ref>: An example of a 5-char quatrain. The tonal pattern is shown at the end of each line, where 'P' indicates a level tone, 'Z' indicates a downward tone, and '*' indicates the tone can be either. The translation is from <ref type="bibr" target="#b7">(Tang, 2005</ref>).</p><p>In this paper we are interested in machine poet- ry generation. Several approaches have been stud- ied by researchers. For example, rule-based meth- ods ( <ref type="bibr" target="#b19">Zhou et al., 2010)</ref>, statistical machine trans- lation (SMT) models <ref type="bibr" target="#b4">(Jiang and Zhou, 2008;</ref><ref type="bibr" target="#b3">He et al., 2012</ref>) and neural models ( <ref type="bibr" target="#b18">Zhang and Lapata, 2014;</ref><ref type="bibr">Wang et al., 2016a,c)</ref>. Compared to previ-ous approaches (e.g., rule-based or SMT), the neu- ral model approach tends to generate more fluen- t poems and some generations are so natural that even professional poets can not tell they are the work of machines ( <ref type="bibr" target="#b10">Wang et al., 2016a</ref>).</p><p>In spite of these promising results, neural mod- els suffer from a particular problem in poem gen- eration, a lack of innovation. Due to the statistical nature of neural models, they pay much more at- tention to high-frequency patterns, whereas they ignore low-frequency ones. In other words, the more regular and common the patterns, the bet- ter the neural model is good at learning them and tends to use them more frequently at run-time. This property certainly helps to generate fluen- t sentences, but it is not always useful: the major value of poetry is not fluency, but the aesthetic in- novation that can stimulate some unique feelings. This is particularly true for Chinese quatrains that are highly compact and expressive: it is nearly im- possible to find two similar works in the thousands of years of history in this genre, demonstrating the importance of uniqueness or innovation. Ironical- ly, the most important thing, innovation, is largely treated as trivial, if not noise, by present neural models.</p><p>Actually this problem is shared by all gener- ation models based on statistics (although it is more serious for neural models) and has aroused a long-standing criticism for machine poem genera- tion: it can generate, and sometimes generate well, but the generation tends to be unsurprising and not particularly interesting. More seriously, this prob- lem exists not only in poem generation, but also in all generation tasks that require innovation. This paper tries to solve this extremely chal- lenging problem. We argue that the essential prob- lem is that statistical models are good at learn- ing general rules (usage of regular words and their combinations) but are less capable of remember- ing special instances that are difficult to cover with general rules. In other words, there is only rule-based reasoning, no instance-based memory. We therefore present a memory-augmented neu- ral model which involves a neural memory so that special instances can be saved and referred to at run-time. This is like a human poet who creates poems by not only referring to common rules and patterns, but also recalls poems that he has read before. It is hard to say whether this combination of rules and instances produces true innovation (which often requires real-life motivation rather than simple word reordering), but it indeed offers interesting flexibility to generate new outputs that look creative and are still rule-compliant. More- over, this flexibility can be used in other ways, e.g., generating poems with different styles.</p><p>In this paper, we use the memory-augmented neural model to generate flexible and creative Chi- nese poems. We investigate three scenarios where adding a memory may contribute: the first scenari- o involves a well trained neural model where we aim to promote innovation by adding a memory, the second scenario involves an over-fitted neural model where we hope the memory can regularize the innovation, and in the third scenario, the mem- ory is used to encourage generation of poems of different styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A multitude of methods have been proposed for automatic poem generation. The first approach is based on rules and/or templates. For example, phrase search ( <ref type="bibr" target="#b14">Wu et al., 2009)</ref>, word association norm ( <ref type="bibr" target="#b5">Netzer et al., 2009)</ref>, tem- plate search <ref type="bibr" target="#b6">(Oliveira, 2012)</ref>, genetic search ( <ref type="bibr" target="#b19">Zhou et al., 2010</ref>), text summarization ( <ref type="bibr">Yan et al., 2013)</ref>. Another approach involves various SMT methods, e.g., <ref type="bibr" target="#b4">(Jiang and Zhou, 2008;</ref><ref type="bibr" target="#b3">He et al., 2012</ref>). A disadvantage shared by the above methods is that they are based on the surface forms of words or characters, having no deep understanding of the meaning of a poem.</p><p>More recently, neural models have been the sub- ject of much attention. A clear advantage of the neural-based methods is that they can 'discover' the meaning of words or characters, and can there- fore more deeply understand the meaning of a po- em. Here we only review studies on Chinese po- etry generation that are mostly related to our re- search. The first study we have found in this di- rection is the work by <ref type="bibr" target="#b18">Zhang and Lapata (2014)</ref>, which proposed an RNN-based approach that pro- duces each new line character-by-character us- ing a recurrent neural network (RNN), with al- l the lines generated already (in the form of a vector) as a contextual input. This model can generate quatrains of reasonable quality. <ref type="bibr" target="#b11">Wang et al. (2016b)</ref> proposed a much simpler neural model that treats a poem as an entire charac- ter sequence, and poem generation is conduct- ed character-by-character. This approach can be easily extended to various genres such as Song Iambics. To avoid theme drift caused by this long-sequence generation, <ref type="bibr" target="#b11">Wang et al. (2016b)</ref> u- tilized the neural attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) by which human intention is encod- ed by an RNN to guide the generation. The same model was used by <ref type="bibr" target="#b10">Wang et al. (2016a)</ref> for Chi- nese quatrain generation. <ref type="bibr">Yan (2016)</ref> proposed a hierarchical RNN model that conducts iterative generation. Recently, <ref type="bibr" target="#b12">Wang et al. (2016c)</ref> pro- posed a similar sequence generation model, but with the difference that attention is placed not on- ly on the human input, but also on all the char- acters that have been generated so far. They also proposed a topic planning scheme to encourage a smooth and consistent theme.</p><p>All the neural models mentioned above try to generate fluent and meaningful poems, but none of them consider innovation.</p><p>The memory-augmented neural model proposed in this study intends to address this issue. Our system was built following the model structure and train- ing strategy proposed by <ref type="bibr" target="#b10">Wang et al. (2016a)</ref> due to its simplicity and demonstrated quality, but the memory mechanism is general and can be applied to any of the models presented above.</p><p>The idea of memory argumentation was in- spired by the recent advance in neural Turing ma- chine ( <ref type="bibr" target="#b1">Graves et al., 2014</ref><ref type="bibr" target="#b2">Graves et al., , 2016</ref> and memory net- work ( <ref type="bibr" target="#b13">Weston et al., 2014</ref>). These new model- s equip neural networks with an external memo- ry that can be accessed and manipulated via some trainable operations. In comparison, the memory in our work plays a simple role of knowledge stor- age, and the only operation is simple pre-defined READ. In this sense, our model can be regarded as a simplified neural Turing machine that omits training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Memory-augmented neural model</head><p>In this section, we first present the idea of memory augmentation, and then describe the model struc- ture and training method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Memory augmentation</head><p>The idea of memory augmentation is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. It contains two components, the neu- ral model component on the left, and the mem- ory component on the right. In this work, the attention-based RNN generation model presented by ( <ref type="bibr" target="#b10">Wang et al., 2016a</ref>) is used as the neural mod- el component, although any neural model is suit- able. The memory component involves a set of 'direct' mappings from input to output, and there- fore can be used to memorize some special cases of the generation that can not be represented by the neural model. For poem generation, the memory stores the information regarding which character should be generated in a particular context. The output from the two components are then integrat- ed, leading to a consolidated output.</p><p>There are several ways to understand the memory-augmented neural model. Firstly, it can be regarded as a way of combining reasoning (neu- ral model) and knowledge (memory). Secondly, it can be regarded as a way of combining rule-based inference (neural model) and instance-based re- trieval (memory). Thirdly, it can be regarded as a way of combining predictions from comple- mentary systems, where the neural model is con- tinuous and parameter-shared, while the memo- ry is discrete and contains no parameter sharing. Finally, the memory can be regarded as an ef- fective regularization that constrains and modi- fies the behavior of the neural model, resulting in generations with desired properties. Note that this memory-augmented neural model is inspired by and related to the memory network proposed by <ref type="bibr" target="#b13">Weston et al.(2014)</ref> and <ref type="bibr" target="#b2">Graves et al.(2016)</ref>, but we more focus on an accompanying memory that plays the role of assistance and regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model structure</head><p>Using the Chinese poetry generation model shown in <ref type="figure" target="#fig_0">Fig. 1</ref> as an example, this section discuss- es the creation of a memory-augmented neu- ral model. Firstly, the neural model part is an attention-based sequence-to-sequence mod- el ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>). The encoder is a bi-directional RNN (with GRU units) that converts the input topic words, denoted by the embeddings of the compositional characters (x 1 , x 2 , ..., x N ), into a sequence of hidden states (h 1 , h 2 , ..., h N ). The decoder then generates the whole quatrain character-by-character, denoted by the corre- sponding embeddings (y 1 , y 2 , ...). At each step t, the prediction for the state s t is based on the last generation y t−1 , the previous status s t−1 of the de- coder, as well as all the hidden states (h 1 , h 2 , ...) of the encoder. Each hidden state h i contributes to the generation according to a relevance factor α t that measures the similarity between s t−1 and h i . This is written as:</p><formula xml:id="formula_0">s t = f d (y t−1 , s t−1 , N i=1 α t,i h i )</formula><p>where α t,i represents the contribution of h i to the present generation, and can be implemented as any function. The output of the model is a posterior probability over the whole set of characters, writ- ten by</p><formula xml:id="formula_1">z t = σ(s t W )</formula><p>where W is the projection parameter.</p><p>The memory consists of a set of elements</p><formula xml:id="formula_2">{m i } K i=1</formula><p>, where K is the size of the memory. Each element m i involves two parts, the source part m i (s), that encodes the context, i.e. when this ele- ment should be selected, and the target part m i (g), that encodes what should be output if this element is selected. In our study, the neural model is firstly trained, and then the memory is created by running f d (the decoder of the neural model). Specifically, for the k-th poem selected to be in the memory, the character sequence is input to the decoder one by one, with the contribution from the encoder set to zero. Denoting the starting position of this po- em in the memory is p k , the status of the decoder at the j-th step is used as the source part of the (p k + j)-th element of the memory, and the em- bedding of the corresponding character, x j , is set to be the target part. this is formally written as:</p><formula xml:id="formula_3">m i (s) = f d (x j−1 , s j−1 , 0)<label>(1)</label></formula><p>and</p><formula xml:id="formula_4">m i (g) = x j where i = p k + j.</formula><p>At run-time, the memory elements are selected according to their fit to the present decoder status s t , and then the outputs of the selected elements are averaged as the output of the memory compo- nent. We choose cosine distance to measure the fitting degree, and have 1 :</p><formula xml:id="formula_5">v t = K i=1 cos(s t , m i (s))m i (g).<label>(2)</label></formula><p>The output of the neural model and the memory can be combined in various ways. Here, a simple linear combination before the softmax is used, i.e.,</p><formula xml:id="formula_6">z t = σ(s t W + βv t E)<label>(3)</label></formula><p>where β is a pre-defined weighting factor, and E contains word embeddings of all the characters. Although it is possible to train β from the data, we found that the learned β is not better than the manually-selected one. This is probably because β is a factor to trade-off the contribution from the model and the memory, and how to make the trade-off should be a 'prior knowledge' rather than a tunable parameter. In fact, if it is trained, than it will be immediately adapted to match the training data, which will nullify our effort to encourage in- novative generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Training</head><p>In our implementation, only the neural model component is required to be trained. The training algorithm follows the scheme defined in ( <ref type="bibr" target="#b10">Wang et al., 2016a)</ref>, where the cross entropy between the distributions over Chinese characters given by the decoder and the ground truth is used as the ob- jective function. The optimization uses the SGD algorithm together with AdaDelta to adjust the learning rate <ref type="bibr" target="#b17">(Zeiler, 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Memory augmentation for Chinese poetry generation</head><p>This section describes how the memory mechanis- m can be used to trade-off between the require- ments for rule-compliant generation and aesthetic innovation, and how it can also be used to do more interesting things, for example style transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Memory for innovative generation</head><p>In this section, we describe how the memory mechanism promotes innovation. Monitoring the training process for the attention-based model, we found that the cost on the training set will keep decreasing until approaching zero, but on the val- idation set, the degradation stops after only one iteration. This can be explained by the fact that Chinese quatrains are highly unique, so the com- mon patterns can be fully learned in one iteration, resulting in overfitting with additional iterations. Due to the overfitting, we observe that with the one-iteration model, reasonable poems can be gen- erated, and with the over-fitted model, the gener- ated poems are meaningless, in that they do not resemble feasible character sequences.</p><p>The energy model perspective helps to explain this difference. For the one-iteration model, the energy surface is smooth and the energy of the training data is not very low, as illustrated in plot (a) in <ref type="figure" target="#fig_1">Fig. 2</ref>, where the x-axis represents the input and y-axis represents the output, and the z-axis represents the energy. With this model, inputs with small variance will be attracted to the same low-energy area, leading to similar generations. These generations are trivial, but at least reason- able. If the model is overfitted, however, the ener- gy at the locations of the training data becomes much lower than their surrounding areas, lead- ing to a bumpy energy surface as shown in plot (b) in <ref type="figure" target="#fig_1">Fig. 2</ref>. With this model, inputs with a s- mall variation may be attracted to very different low-energy areas, leading to significantly differ- ent generations. Since many of the low-energy ar- eas are nothing to do with good generations but are simply caused by the complex energy func- tion, the generations can be highly surprising for human readers, and the quality is not guaranteed. In some sense, these generations can be regarded as 'innovative' , but based on observations made in our experiments, most of them are meaningless.</p><p>The augmented memory introduces a new en- ergy function, which is combined with the energy function of the neural model to change the energy surface of the generation system. This can be seen in Eq. (3), where s t W and βv t E can be regarded as the energy function of the neural model com- ponent and the memory component, respectively, and the energy function of the memory-augmented system is the sum of the energy functions of these two components. For this reason, the effect of the memory mechanism can be regarded as a regular- ization of the neural model that will adjust its gen- eration behavior. This regularization effect is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, where the energy function of the memory shown in plot (c) is added to the energy function of the one- iteration model and the overfitted model, as shown in plot (e) and plot (f) respectively. It can be seen that with the memory involved, the energy sur- face becomes more bumpy with the one-iteration model, and more smooth with the overfitted mod- el. In the former case, the effect of the memory is to encourage innovation, while still focusing on rule-compliance, and in the latter case, the effec- t is to encourage rule compliance, while keeping the capability for innovation.</p><p>It is important to notice that the energy function of the memory component is a linear combination of the energy functions of the compositional ele- ments (see Eq. <ref type="formula" target="#formula_5">(2)</ref>), each of which is convex and is minimized at the location represented by the element. This means that the energy surface of the memory is rather 'healthy', in the sense that low-energy locations mostly correspond to good generations. For this reason, the regularization provided by the memory is safe and helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Memory for style transfer</head><p>The effect of the memory is easy to control. For example, the complexity of the behavior can be controlled by the memory size, the featured bias can be controlled by memory selection, and the strength of the impact can be controlled by the weighting parameter β. This means that the mem- ory mechanism is very flexible and can be used to produce poems with desired properties.</p><p>In this work, we use these capabilities to gen- erate poems with different styles. This has been illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, where the energy function of the style memory shown in plot (d) is biased to- wards a particular style, and once it is added to energy function of the one-iteration model, the re- sulting energy function shown in plot (g) obtains lower values at locations corresponding to the lo- cations of the memory, which encourages genera- tion of poems with similar styles as those poems in the memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>This section describes the experiments and results carried out in this paper. Here, The baseline sys- tem was a reproduction of the Attention-based sys- tem presented in ( <ref type="bibr" target="#b10">Wang et al., 2016a</ref>). the model in This system has been shown to be rather flexi- ble and powerful: it can generate different genres of Chinese poems, and when generating quatrains it has been shown to be able to fool human ex- perts in many cases ( <ref type="bibr" target="#b10">Wang et al., 2016a</ref>) and the authors had did a thorough comparison with com- petitive methods mentioned in the related work of this paper. We obtained the database and the source code (in theano), and reproduced their sys- tem using Tensorflow from Google 2 . We didn't make comparisons with some previous methods such as NNLM, SMT, RNNPG as they had been fully compared in ( <ref type="bibr" target="#b10">Wang et al., 2016a</ref>) and all of them were much worse than the attention-based system. Another reason was that the experts were not happy to evaluate poems with clearly bad qual- ity. We also reproduced the model in ( <ref type="bibr" target="#b12">Wang et al., 2016c</ref>) with the help of the first author. Howev- er, since their implementation did not involve any restrictions on rhythm and tone, the experts were reluctant to recognize them as good poems. With a larger dataset (e.g., 1 Million poems), it is as- sumed that the rhythm and tone can be learned and their system would be good in both fluency and rule compliance. It should be also emphasized that the memory approach proposed in this paper is a general technique and is complementary to other efforts such as the planning approach ( <ref type="bibr" target="#b12">Wang et al., 2016c</ref>) and the recursive approach <ref type="bibr">(Yan, 2016)</ref>.</p><p>Based on the baseline system, we built the memory-augmented model, and conducted two experiments to demonstrate its power. The first is an innovation experiment which employs memory to promote or regularize the generation of innova- tive poems, and the second is a style-transfer ex- periment which employs memory to generate flex- ible poems in different styles.</p><p>We invited 34 experts to participate in the ex- periments, and all of them have rich experience not only evaluating poems, but also in writing them. Most of the experts are from prestigious in- stitutes, including Peking university and the Chi- nese Academy of Social Science (CASS). Follow- ing the suggestions of the experts, we use five met- rics to evaluate the generation, as listed below:</p><p>• Compliance: if regulations on tones and rhymes are satisfied;</p><p>• Fluency: if the sentences read fluently and convey reasonable meaning;</p><p>• Theme consistency: if the entire poem ad- heres to a single theme;</p><p>• Aesthetic innovation: if the quatrain stimu- lates any aesthetic feeling with elaborate in- novation;</p><p>• Scenario consistency: if the scenario remains consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>The baseline system was built with two cus- tomized datasets. The first dataset is a Chinese po-em corpus (CPC), which we used in this work to train the embeddings of Chinese characters. Our CPC dataset contains 284,899 traditional Chinese poems in various genres, including Tang quatrain- s, Song Iambics, Yuan Songs, and Ming and Qing poems. This large quantity of data ensures reliable learning for the semantic content of most Chinese characters.</p><p>Our second dataset is a Chinese quatrain cor- pus (CQC) that we have collected from the in- ternet, which consists of 13, 299 5-char quatrain- s and 65, 560 7-char quatrains. This corpus was used to train the attention-based RNN baseline. We filtered out the poems whose characters are all low-frequency (less than 100 counts in the database). After the filtering, the remaining corpus contains 9,195 5-char quatrains and 49,162 7-char quatrains. We used 9,000 5-char and 49,000 7-char quatrains to train the attention model, and the rest for validation.</p><p>Another two datasets were created for use in the memory-augmented system. Our first dataset, MEM-I, contains 500 quatrains randomly select- ed from our CQC corpus. This dataset was used to produce the memory in the innovation experi- ment; the second dataset, MEM-S, contains 300 quatrains with clear styles, including 100 pastoral, 100 battlefield and 100 romantic quatrains. It was used to generate memory with different styles in the style-transfer experiment. All the datasets will be released online 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Process</head><p>We invited 34 experts to evaluate the quality of the poem generation. In the innovation experi- ment, the evaluation consisted of a comparison between different systems and configurations in terms of the five metrics. The innovation question- s presented the expert with two poems, and asked them to judge which of the poems was better in terms of the five metrics; in the style-transfer ex- periment, the evaluation was performed by iden- tifying the style of a generated poem. The eval- uation was conducted online, with each question- naire containing 11 questions focusing on innova- tion and 4 questions concerned with style-transfer. Each of the style-transfer questions presented the expert with a single poem and asked them to score it between 1 to 5, with a larger score being bet- ter, in terms of compliance, aesthetic innovation, scenario consistency, and fluency. They were also asked to specify the style of the poem.</p><p>Using the poems generated by our systems, we generated many different questions of both types, and then created a number of online questionnaires that randomly selected from these questions. This meant that as discussed above, each questionnaire had 11 randomly selected innovation questions, and 4 randomly selected style transfer questions. Each question was only used once, meaning that it was not duplicated on multiple questionnaires, and so each questionnaire was different.</p><p>Experts could choose to answer multiple ques- tionnaires if they wished, as each one was differ- ent. From the 34 experts, we collected 69 complet- ed questionnaires, which equals to 759 innovation questions and 276 style-transfer questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Innovation experiment</head><p>This experiment focuses on the contribution of memory for innovative poem generation. We ex- perimented with two configurations: one is with a one-iteration model (C 1 ) and the other is with an overfitted model (C ∞ ). The memory was gen- erated from the 500 quatrains in MEM-I, and the weighting factor was defined empirically as 16 for C 1 and 49 for C ∞ .</p><p>The topics of the generation were 160 key- words randomly selected from Shixuhanyinge (Li- u, 1735). Given a pair of poems generated by t- wo different configurations using the same topic, the experts were asked to choose which one they preferred. The evaluation is therefore pair-wised, and each pair of configurations contains at least 180 evaluations. The results are shown in <ref type="table">Table 2</ref>, where the preference ratio for each pair of config- urations was tested in terms of the 5 metrics.</p><p>From the first row of <ref type="table">Table 2</ref>, we observe that the experts have a clear preference for the poem- s generated by the C 1 model, the one that can produce fluent yet uninteresting poems. In par- ticular, the 'aesthetic innovation' score for C ∞ is not better than C 1 , which was different from what we expected. Informal offline discussions with the poetry experts found that the experts identi- fied some innovative expression in the C ∞ con- dition, but most of the them was regarded as being nonsense in the opinion of many of the experts. In comparison to sparking innovation, fluency and being meaningful is more important not only for non-expert readers, but also for professional poet-  <ref type="table">Table 2</ref>: Preference ratios for systems with or without overfitting and with or without memory augmen- tation.</p><p>s. In other words, only meaningful innovation is regarded as innovation, and irrational innovation is simply treated as junk.</p><p>From the second and third rows of <ref type="table">Table 2</ref>, it can be seen that involving memory significantly improves both C 1 and C ∞ , particularly for C ∞ . For C 1 , the most substantial improvement is ob- served in terms of 'Aesthetic innovation', which is consistent with our argument that memory can help encourage innovation for this model. For C ∞ , 'Fluency' seems to be the most improved metric. This is also consistent with our argument that involving memory constrains over-innovation for over-fitted models.</p><p>The last row of <ref type="table">Table 2</ref> is an extra experiment that investigates if C ∞ is regularized well enough after introducing the memory. It seems that with the regularization, the overfitting problem is large- ly solved, and the generation is nearly as fluent and consistent as the C 1 condition. Interestingly, the score for aesthetic innovation is also signifi- cantly improved. Since the regularization is not supposed to boost innovation, this seems confus- ing at first glance (in comparison to the result on the same metric in the first row), but this is proba- bly because the increased fluency and consistency makes the innovation more appreciated, therefore doubly confirming our argument that true innova- tion should be reasonable and meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Style-transfer experiment</head><p>In the second experiment, the memory mechanism is used to generate poems in different styles. We chose three styles: pastoral, battlefield, and ro- mantic. A style-specific memory, which we cal- l style memory, was constructed for each style by the corresponding quatrains in the MEM-S dataset. The system with one-iteration model C 1 was used as the baseline. Two sets of topics were used in the experiment, one is general and the oth- er is style-biased. The experiments then investi- gate if the memory mechanism can produce a clear style if the topic is general, and can transfer to a different style if the topic is style-biased already. The experts were asked to specify the style from four options including the three defined above and a 'unclear style' option. In addition, the experts were asked to score the poems in terms of compli- ance, fluency, aesthetic innovation, and scenario consistency, which we can use to check if the style transfer impacts the quality of the poem genera- tion. Note that we did not ask for the theme con- sistency to be scored in this experiment because the topic words were not presented to the experts, in order to prevent the topic affecting their judg- ment regarding the style. The score ranges from 1 to 5, with a larger score being better. <ref type="table" target="#tab_2">Table 3</ref> presents the results with the general top- ics. The numbers show the probabilities that the poems generated by a particular system were la- beled as having various styles. Since the topics are unbiased in types, the generation of the base- line system is assumed to be with unclear styles. For other systems, the style of the generation is assumed to be the same as the style of their mem- ories. The results in <ref type="table" target="#tab_2">Table 3</ref> clearly demonstrates these assumptions. The tendency that romantic poems are recognized as pastoral poems is a lit- tle surprising. Further analysis shows that experts tend to recognize romantic poems as pastoral po- ems only if there are any related symbols such as trees, mountain, river. These words are very gen- eral in Chinese quatrains. The indicator words of romantic poems such as skirt, rouge, and singing are not as popular and their indication power is not as strong, leading to less labeling of romantic po- ems, as shown in the results.  We also tested transferring from one style to another. This was achieved by generating poem- s with some style-biased topics, and then using a style memory to force the generation to change the style. Our experiments show that in 73% cases the style can be successfully transferred. Finally, the scores of the poems generated with and without the style memories are shown in <ref type="table">Ta- ble 4</ref>, where the poems generated with both gener- al and style-biased topics are accounted for. It can be seen that overall, the style transfer may degrade fluency a little. This is understandable, as enforc- ing a particular style has to break the optimal gen- eration with the baseline, which is assumed to be good at generating fluent poems. Nevertheless the sacrifice is not significant.  <ref type="table">Table 4</ref>: Averaged scores for systems with or with- out style memory. <ref type="table" target="#tab_4">Table 5</ref> to <ref type="table">Table 7</ref> shows example poems gener- ated by the system C 1 , C 1 +Mem and C 1 +Style Mem where the style in this case is set to be ro- mantic. The three poems were generated with the same, very general, topic ('g(oneself)'). More examples are given in the supporting material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probability</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Examples</head><p>gld¿Ã%Ô § Nothing in my heart, ˜FÀºØOEî" Spring wind is not a pity.</p><p>#&lt;mÛ¤3 § Don't ask where it is, ·8®k½ƒD" I've noticed that and tell others. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we proposed a memory mechanis- m to support innovative Chinese poem genera- tion by neural models augmented with a memo- ry. Experimental results demonstrated that mem- ory can boost innovation from two opposite di- ˜ìgkÃ&lt;Š § Nobody speaking in the mountain, Ø´"\Y&gt;" Also no green cloud stepping into the river.</p><p>#rSºNá" § Spring wind does not stir leaves, smÉä÷ôE" But flowers blooming in trees and flying to boats. }EùfSÚe § Green sleeves and red flowers in cold spring, 7ñ"Vë•" Willow leaves gone in fragrant mist. <ref type="table">Table 7:</ref> Example poems generated by the C 1 +Style Mem system where the style is roman- tic.</p><p>rections: either by encouraging creative genera- tion for regularly-trained models, or by encourag- ing rule-compliance for overfitted models. Both s- trategies work well, although the former generated poetry that was preferred by experts in our exper- iments. Furthermore, we found that the memory can be used to modify the style of the generat- ed poems in a flexible way. The experts we col- laborated with feel that the present generation is comparable to today's experienced amateur poets. Future work involves investigating a better memo- ry selection scheme. Other regularization method- s (e.g., norm or drop out) are also interesting and may alleviate the over-fitting problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The memory-augmented neural model used for Chinese poetry generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The energy surface for (a) one-iteration model (b) overfitted model (c) memory (d) style memory (e) one-iteration model augmented with memory (f) overfitted model augmented with memory (g) one-iteration model augmented with style memory.</figDesc><graphic url="image-3.png" coords="6,246.03,183.28,108.42,81.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Probability that poems generated by each 
configuration with general topics are labeled as 
various styles. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Example poems generated by the C 1 
system. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Example poems generated by the 
C 1 +Mem system. 

s †®òD•/ § 
Beautiful face addressed by rouge, 
ðKÉTD" 
Mandarin duck outside the curtain. 
</table></figure>

			<note place="foot" n="1"> In fact, we run a parallel decoder to provide st in Eq.(2). This decoder does not accept input from the encoder and so is consistent with the memory construction process as Eq.(1).</note>

			<note place="foot" n="2"> https://www.tensorflow.org/</note>

			<note place="foot" n="3"> http://vivi.cslt.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This paper was supported by the National Natural Science Foundation of China (NSFC) under the project NO.61371136, NO.61633013, NO.61472428.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabskabarwi´nskabarwi´nska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating Chinese classical poems with statistical machine translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Sixth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating Chinese couplets using a statistical mt approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gaiku: Generating haiku with word associations norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gabay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Approaches to Linguistic Creativity. Association for Computational Linguistics</title>
		<meeting>the Workshop on Computational Approaches to Linguistic Creativity. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Poetryme: a versatile platform for poetry generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ECAI 2012 Workshop on Computational Creativity, Concept Invention, and General Intelligence</title>
		<meeting>the ECAI 2012 Workshop on Computational Creativity, Concept Invention, and General Intelligence</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">English Translation for Tang Poems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Tang</surname></persName>
		</author>
		<editor>Ying Yi Tang Shi San Bai Shou</editor>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Tianjin People Publisher</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Hitch haiku: An interactive supporting system for composing haiku poem. Entertainment Computing-ICEC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoko</forename><surname>Tosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Obara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiko</forename><surname>Minoh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A Summary of Rhyming Constraints of Chinese Poems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<editor>Shi Ci Ge Lv Gai Yao</editor>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Beijin Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Can machine generate traditional Chinese poetry? a feigenbaum test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BICS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Chinese song iambics generation with neural attention-based model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 16</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with planning based neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">New hitch haiku: An interactive renku poem composition supporting tool applied for sightseeing navigation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoko</forename><surname>Tosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Nakatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Entertainment Computing-ICEC</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">2016. i, Poet: Automatic poetry composition through recurrent neural networks with iterative polishing schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">2013. i, Poet: automatic Chinese poetry composition through a generative summarization framework under constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqiang</forename><surname>Shou-De Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third international joint conference on Artificial Intelligence</title>
		<meeting>the Twenty-Third international joint conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<biblScope unit="page" from="2197" to="2203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Genetic algorithm and its implementation of automatic generation of Chinese Songci</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Le</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Software</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="427" to="437" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
