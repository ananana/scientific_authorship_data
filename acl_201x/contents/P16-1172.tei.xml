<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimizing an Approximation of ROUGE-a Problem-Reduction Approach to Extractive Multi-Document Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Peyrard</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Training Group AIPHES and UKP Lab Computer Science Department</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>Eckle-Kohler</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Training Group AIPHES and UKP Lab Computer Science Department</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Optimizing an Approximation of ROUGE-a Problem-Reduction Approach to Extractive Multi-Document Summarization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1825" to="1836"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a problem-reduction approach to extractive multi-document summarization: we propose a reduction to the problem of scoring individual sentences with their ROUGE scores based on supervised learning. For the summariza-tion, we solve an optimization problem where the ROUGE score of the selected summary sentences is maximized. To this end, we derive an approximation of the ROUGE-N score of a set of sentences, and define a principled discrete optimization problem for sentence selection. Mathematical and empirical evidence suggests that the sentence selection step is solved almost exactly, thus reducing the problem to the sentence scoring task. We perform a detailed experimental evaluation on two DUC datasets to demonstrate the validity of our approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-document summarization (MDS) is the task of constructing a summary from a topically re- lated document collection. This paper focuses on the variant of extractive and generic MDS, which has been studied in detail for the news domain us- ing available benchmark datasets from the Docu- ment Understanding Conference (DUC) <ref type="bibr">(Over et al., 2007)</ref>.</p><p>Extractive MDS can be cast as a budgeted sub- set selection problem <ref type="bibr" target="#b19">(McDonald, 2007;</ref><ref type="bibr" target="#b14">Lin and Bilmes, 2011</ref>) where the document collection is considered as a set of sentences and the task is to select a subset of the sentences under a length constraint. State-of-the-art and recent works in extractive MDS solve this discrete optimization problem using integer linear programming <ref type="bibr">(ILP)</ref> or submodular function maximization <ref type="bibr" target="#b4">(Gillick and Favre, 2009;</ref><ref type="bibr" target="#b21">Mogren et al., 2015;</ref><ref type="bibr" target="#b12">Li et al., 2013b;</ref><ref type="bibr" target="#b9">Kulesza and Taskar, 2012;</ref>). The objective function that is maximized in the optimization step varies considerably in pre- vious work. For instance, <ref type="bibr">Yih et al. (2007)</ref> maxi- mize the number of informative words, <ref type="bibr" target="#b4">Gillick and Favre (2009)</ref> the coverage of particular concepts, and others maximize a notion of "summary wor- thiness", while minimizing summary redundancy ( <ref type="bibr" target="#b14">Lin and Bilmes, 2011;</ref><ref type="bibr" target="#b8">Kågebäck et al., 2014)</ref>.</p><p>There are also multiple approaches which max- imize the evaluation metric for system sum- maries itself based on supervised Machine Learn- ing (ML). System summaries are commonly eval- uated using ROUGE <ref type="bibr" target="#b15">(Lin, 2004</ref>), a recall oriented metric that measures the n-gram overlap between a system summary and a set of human-written ref- erence summaries.</p><p>The benchmark datasets for MDS can be em- ployed in two different ways for supervised learn- ing of ROUGE scores: either by training a model that assigns ROUGE scores to individual tex- tual units (e.g., sentences), or by performing structured output learning and directly maximiz- ing the ROUGE scores of the created summaries ( <ref type="bibr" target="#b25">Nishikawa et al., 2014;</ref><ref type="bibr">Takamura and Okumura, 2010;</ref><ref type="bibr">Sipos et al., 2012</ref>). The latter approach suf- fers both from the limited amount of training data and from the higher complexity of the machine learning models.</p><p>In contrast, supervised learning of ROUGE scores for individual sentences can be performed with simple regression models using hundreds of sentences as training instances, taken from a sin- gle pair of documents and reference summaries. Extractive MDS can leverage the ROUGE scores of individual sentences in various ways, in partic- ular, as part of an optimization step. In our work, we follow the previously successful approaches to extractive MDS using discrete optimization, and make the following contributions:</p><p>We provide a theoretical justification and em- pirical validation for using ROUGE scores of individual sentences as an optimization objec- tive. Assuming that ROUGE scores of individ- ual sentences have been estimated by a super- vised learner, we derive an approximation of the ROUGE-N score for a set of sentences from the ROUGE-N scores of the individual sentences in the general case of N &gt;= 1.</p><p>We use our approximation to define a math- ematically principled discrete optimization prob- lem for sentence selection. We empirically evalu- ate our framework on two DUC datasets, demon- strating the validity of our approximation, as well as its ability to achieve competitive ROUGE scores in comparison to several strong baselines.</p><p>Most importantly, the resulting framework re- duces the MDS task to the problem of scoring in- dividual sentences with their ROUGE scores. The overall summarization task is converted to two se- quential tasks: (i) scoring single sentences, and (ii) selecting summary sentences by solving an opti- mization problem where the ROUGE score of the selected sentences is maximized.</p><p>The optimization objective we propose almost exactly solves (ii), which we justify by provid- ing both mathematical and empirical evidence. Hence, solving the whole problem of MDS is re- duced to solving (i).</p><p>The rest of this paper is structured as follows: in Section 2, we discuss related work. Section 3 presents our subset selection framework consist- ing of an approximation of the ROUGE score of a set of sentences, and a mathematically principled discrete optimization problem for sentence selec- tion. We evaluate our framework in Section 4 and discuss the results in Section 5. Section 6 con- cludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Related to our approach is previous work in ex- tractive MDS that (i) casts the summarization problem as budgeted subset selection, and (ii) employs supervised learning on MDS datasets to learn a scoring function for textual units.</p><p>Budgeted Subset Selection Extractive MDS can be formulated as the problem of selecting a subset of textual units from a document collection such that the overall score of the created summary is maximal and a given length constraint is ob- served. The selection of textual units for the sum- mary relies on their individual scores, assigned by a scoring function which represents aspects of their relevance for a summary. Often, sentences are considered as textual units.</p><p>Simultaneously maximizing the relevance scores of the selected units and minimizing their pairwise redundancy given a length constraint is a global inference problem which can be solved using ILP <ref type="bibr" target="#b19">(McDonald, 2007)</ref>. Several state-of-the-art results in MDS have been obtained by using ILP to maximize the number of relevant concepts in the created summary while minimiz- ing the pairwise similarity between the selected sentences ( <ref type="bibr" target="#b4">Gillick and Favre, 2009;</ref><ref type="bibr" target="#b2">Boudin et al., 2015;</ref><ref type="bibr">Woodsend and Lapata, 2012)</ref>.</p><p>Another way to formulate the problem of find- ing the best subset of textual units is to maxi- mize a submodular function. Maximizing sub- modular functions is a general technique that uses a greedy optimization algorithm with a mathe- matical guarantee on optimality . Performing summarization in the framework of submodularity is natural because summaries try to maximize the coverage of rele- vant units while minimizing redundancy ( <ref type="bibr" target="#b14">Lin and Bilmes, 2011)</ref>. However, several different cover- age and redundancy functions have been proposed ( <ref type="bibr" target="#b14">Lin and Bilmes, 2011;</ref><ref type="bibr" target="#b8">Kågebäck et al., 2014;</ref><ref type="bibr">Yin and Pei, 2015)</ref> recently, and there is not yet a clear consensus on which coverage function to maxi- mize.</p><p>Supervised Learning Supervised learning us- ing datasets with reference summaries has already been employed in early work on summarization to classify sentences as summary-worthy or not ( <ref type="bibr" target="#b10">Kupiec et al., 1995;</ref><ref type="bibr" target="#b0">Aone et al., 1995)</ref>.</p><p>Learning a scoring function for various kinds of textual units has become especially popular in the context of global optimization: scores of tex- tual units, learned from data, are fed into an ILP problem solver to find the subset of sentences with maximal overall score. For example, <ref type="bibr">Yih et al. (2007)</ref> score each word in the document cluster based on frequency and position, <ref type="bibr" target="#b12">Li et al. (2013b)</ref> learn bigram frequency in the reference summaries, and  learn word importance from a rich set of features.</p><p>Closely related to our work are summarization approaches that include a supervised component which assigns ROUGE scores to individual sen- tences. For example, <ref type="bibr" target="#b24">Ng et al. (2012)</ref>, <ref type="bibr" target="#b11">Li et al. (2013a)</ref> and <ref type="bibr" target="#b13">Li et al. (2015)</ref> all use a regres- sion model to learn ROUGE-2 scores for indi- vidual sentences, but use it in different ways for the summarization. While <ref type="bibr" target="#b24">Ng et al. (2012)</ref> use the ROUGE scores of sentences in combination with the Maximal Marginal Relevance algorithm as a baseline approach, <ref type="bibr" target="#b11">Li et al. (2013a)</ref> use the scores to select the top-ranked sentences for sen- tence compression and subsequent summarization. <ref type="bibr" target="#b13">Li et al. (2015)</ref>, in contrast, use the ROUGE scores to re-rank a set of sentences that are output by an optimization step.</p><p>While learning ROUGE scores of textual units is widely used in summarization systems, the the- oretical background on why this is useful has not been well studied yet. In our work, we present the mathematical and empirical justification for this common practice. In the next section, we start with the mathematical justification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Content Selection Framework</head><p>3.1 Approximation of ROUGE-N Notation: Let S = {s i |i ≤ m} be a set of m sentences which constitute a system summary. We use ρ N (S) or simply ρ(S) to denote the ROUGE- N score of S. ROUGE-N evaluates the n-gram overlap between S and a set of reference sum- maries <ref type="bibr" target="#b15">(Lin, 2004</ref>). Let S * denote the reference summary and R N the number of n-gram tokens in S * . R N is a function of the summary length in words, in particular, R 1 is the target size of the summary in words. Finally, let F S (g) denote the number of times the n-gram type g occurs in S. For a single reference summary, ROUGE-N is computed as follows:</p><formula xml:id="formula_0">ρ(S) = 1 R N g∈S * min(F S (g), F S * (g))<label>(1)</label></formula><p>For compactness, we use the following notation for any set of sentences X:</p><formula xml:id="formula_1">C X,S * (g) = min(F X (g), F S * (g))<label>(2)</label></formula><p>C X,S * (g) can be understood as the contribution of the n-gram g.</p><p>ROUGE-N for a Pair of Sentences: Using this notation, the ROUGE-N score of a set of two sen- tences a and b can be written as:</p><formula xml:id="formula_2">ρ(a ∪ b) = 1 R N g∈S * min(C a∪b,S * (g), F S * (g))<label>(</label></formula><p>3) We observe that ρ(a ∪ b) can be expressed as a function of the individual scores ρ(a) and ρ(b):</p><formula xml:id="formula_3">ρ(a ∪ b) = ρ(a) + ρ(b) − (a ∩ b)<label>(4)</label></formula><p>where (a ∩ b) is an error correction term that dis- cards overcounted n-grams from the sum of ρ(a) and ρ(b):</p><formula xml:id="formula_4">(a ∩ b) = 1 R N g∈S * max(C a,S * (g)+C b,S * (g)−F S * (g), 0)<label>(5)</label></formula><p>A proof that this error correction is correct is given in appendix A.1.</p><p>General Formulation of ROUGE-N: We can extend the previous formulation of ρ to sets of ar- bitrary cardinality using recursion. If ρ(S) is given for a set of sentences S, and a is a sentence then:</p><formula xml:id="formula_5">ρ(S ∪ a) = ρ(S) + ρ(a) − (S ∩ a)<label>(6)</label></formula><p>We prove in appendix A.1 that this formula is the ROUGE-N score of S ∪ a. Another way to obtain ρ for an arbitrary set S is to adapt the principle of inclusion-exclusion:</p><formula xml:id="formula_6">ρ(S) = m i=1 ρ(s i )+ m k=2 (−1) k+1 ( 1≤i 1 ≤···≤i k ≤m (k) (s i 1 ∩ · · · ∩ s i k ))<label>(7)</label></formula><p>This formula can be understood as adding up scores of individual sentences, but n-grams ap- pearing in the intersection of two sentences might be overcounted. <ref type="bibr">(2)</ref> is used to account for these n-grams. But now, n-grams in the intersection of three sentences might be undercounted and <ref type="bibr">(3)</ref> is used to correct this. Each (k) contributes to im- proving the accuracy by refining the errors made by (k−1) for the n-grams appearing in the inter- section of k sentences. When k = |S|, ρ(S) is exactly the ROUGE-N of S. A rigorous proof and details about (k) are provided in appendix A.2.</p><p>Approximation of ROUGE-N for a Pair of Sen- tences: To find a valid approximation of ρ as de- fined in <ref type="formula" target="#formula_2">(7)</ref>, we first consider the ρ(a ∪ b) from equation (3) and then extend it to the general case. When maximizing ρ, scores for sentences are as- sumed to be given (e.g., estimated by a ML com- ponent). We still need to estimate (a ∩ b), which means, according to (5), to estimate:</p><formula xml:id="formula_7">g∈S * max(C a,S * (g) + C b,S * (g) − F S * (g), 0) (8)</formula><p>At inference time, neither S * (the reference sum- mary) nor F S * (number of occurrences of n-grams in the reference summary) is known.</p><p>At this point, we can observe that, similar as for sentence scoring, can be estimated via a su- pervised ML component. Such an ML model can easily be trained on the intersections of all sen- tence pairs in a given training dataset. Hence, we can assume that both the scores for individual sen- tences and the are learned empirically from data using ML. As a result, we have pushed all estima- tion steps into supervised ML components, which leaves the subset selection step fully principled.</p><p>However, we found in our experiments that even a simple heuristic yields a decent approximation of . The heuristic uses the frequency f req(g) of an n-gram g observed in the source documents:</p><formula xml:id="formula_8">g∈S * max(C a,S * (g) + C b,S * (g) − F S * (g), 0) ≈ g∈a∩b 1[freq(g) ≥ α] (9)</formula><p>The threshold α tells us which n-grams are likely to appear in the reference summary, and it is de- termined by grid-search on the training set. This is penalizing n-grams which appear twice and are likely to occur in the summary. It can be under- stood as a way of limiting redundancy. In prac- tice, we used α = 0.3. However, we experimented with various values of the hyper-parameter α and found that its value has no significant impact as long as it is fairly small (&lt; 0.5). Higher values will ignore too many redundant n-grams and the summary will have a high redundancy.</p><p>R N is known since it is simply the number of n-gram tokens in the summaries. We end up with the following approximation for the pairwise case:</p><formula xml:id="formula_9">˜ ρ(a ∪ b) = ρ(a) + ρ(b) − ˜ (a ∪ b), where˜( where˜ where˜(a ∪ b) = 1 R N g∈a∩b 1[freq(g) ≥ α] (10)</formula><p>General Approximation of ROUGE-N: Now, we can approximate ρ(S) for the general case de- fined by equation <ref type="formula" target="#formula_2">(7)</ref>. We recall that ρ(S) con- tains the sum of ρ(s i ), the pairwise error terms <ref type="bibr">(2)</ref> (s i ∩ s j ), the error terms of three sentences <ref type="bibr">(3)</ref> and so on. We can restrict ourselves to the individual sen- tences and the pairwise error corrections. Indeed, the intersection between more than two sentences is often empty, and accounting for it does not im- prove the accuracy significantly, but greatly in- creases the computational cost.</p><p>A formulation of in the case of two sentences has already been defined in (10). Thus, we have an approximation of the ROUGE-N function for any set of sentences that can be computed at inference time:</p><formula xml:id="formula_10">˜ ρ(S) = n i=1 ρ(s i ) − a,b∈S,a =b˜( =b˜ =b˜(a ∩ b)<label>(11)</label></formula><p>We empirically checked the validity of this ap- proximation. For this, we sampled 1000 sets of sentences from source documents of DUC-2003 (sets of 2 to 5 sentences) and compared their˜ρtheir˜ their˜ρ score to the real ROUGE-N. We observe a pear- son's r correlation ≥ 0.97, which validates˜ρvalidates˜ validates˜ρ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discrete Optimizatioñ</head><p>Optimizatioñ ρ from equation (11) defines a set function that scores a set of sentences. The task of summariza- tion is now to select the set S * with maximal˜ρmaximal˜ maximal˜ρ(S * ) under a length constraint.</p><p>Submodularity: A submodular function is a set function obeying the diminishing returns property: ∀S ⊆ T and a sentence a:</p><formula xml:id="formula_11">F (S ∪ a) − F (S) ≥ F (T ∪ a) − F (T )</formula><p>. Submodular functions are con- venient because maximization under constraints can be done greedily with a guarantee of the op- timality of the solution ( .</p><p>It has been shown that ROUGE-N is submodu- lar (Lin and Bilmes, 2011) and it is easy to verify that˜ρthat˜ that˜ρ is submodular as well (the proof is given in the supplemental material).</p><p>We can therefore apply the greedy maximiza- tion algorithm to find a good set of sentences. This has the advantage of being straightforward and fast, however it does not necessarily find the op- timal solution.</p><p>ILP: A common way to solve a discrete opti- mization problem is to formulate it as an ILP. It maximizes (or minimizes) a linear objective func- tion with some linear constraints where the vari- ables are integers. ILP has been well studied and existing tools can efficiently retrieve the exact so- lution of an ILP problem. We observe that it is possible to formulate the maximization of˜ρof˜ of˜ρ(S) as an ILP. Let x be the binary vector whose i-th entry indicates whether sentence i is in the summary or not, ˜ ρ(s i ) the scores of sentences, and K the length constraint. We pre-compute the symmetric matrix˜Pmatrix˜ matrix˜P where˜P where˜ where˜P i,j = ˜ (s i ∩ s j ) and solve the following ILP:</p><formula xml:id="formula_12">max( n i=1 x i * ˜ ρ(s i ) − d 1 R i≥j α i,j * ˜ P i, j) n i=1 x i * len(s i ) ≤ K ∀(i, j), α i,j − x i ≤ 0 ∀(i, j), α i,j − x j ≤ 0 ∀(i, j), x i + x j − α i,j ≤ 1</formula><p>d is a damping factor that allows to account for approximation errors. When d = 0, the problem becomes the maximization of "summary worthi- ness" under a length constraint, with "summary worthiness" being defined by ρ(s i ).</p><p>In practice, we used a value d = 0.9 because we observed that the learner tends to slightly over- estimate the ROUGE-N scores of sentences. The mathematical derivation implies d = 1, however we can easily adjust for shifts in average scores of sentences from the estimation step by adjust- ing d. Another option would be to post-process the scores after the estimation step to fix the av- erage and let d = 1 in the optimization step. In- deed, if d moves away from 1, we move away from the mathematical framework of ROUGE-N maxi- mization.</p><p>If d = 0, it seems intuitive to interpret the sec- ond term as minimizing the summary redundancy, which is in accordance to previous works.</p><p>However, in our framework, this term has a pre- cise interpretation:</p><p>it maximizes ROUGE-N scores up to the second order of precision, and the ROUGE-N formula it- self already induces a notion of "summary worthi- ness" and redundancy, which we can empirically infer from data via supervised ML for sentence scoring, and a simple heuristic for sentence inter- sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We perform three kinds of experiments in order to empirically evaluate our framework: first, we show that our proposed approximation is valid, then we analyze a basic supervised sentence scor- ing component, and finally we perform an extrin- sic evaluation on end-to-end extractive MDS.</p><p>In our experiments, we use the DUC datasets from <ref type="bibr">2002 and</ref><ref type="bibr">2003 (DUC-02 and</ref><ref type="bibr">DUC-03)</ref>. We use the variants of ROUGE identified by <ref type="bibr">Owczarzak et al. (2012)</ref> as strongly correlating with human evaluation methods: ROUGE-2 re- call with stemming and stopwords not removed (giving the best agreement with human evalua- tion), and ROUGE-1 recall (as the measure with the highest ability to identify the better summary in a pair of system summaries). For DUC-03, summaries are truncated to 100 words, and to 200 words for DUC-02. <ref type="bibr">1</ref> The truncation is done auto- matically by ROUGE. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Framework Validity</head><p>Given that sentences receive scores close to their individual ROUGE-N, we presented a function that approximates the ROUGE-N of sets of these sentences and proposed an optimization to find the best scoring set under a length constraint.</p><p>To validate our framework empirically, we con- sider its upper-bound, which is obtained when our ILP/submodular optimizations use the real ROUGE-N scores of the individual sentences, cal- culated based on the reference summaries. We compare this upper bound to a greedy approach, which simply adds the best scoring sentences one by one to the subset until the length limit is reached, and to the real upper bound for extractive summarization which is determined by solving a maximum coverage problem for n-grams from the reference summary (as it was done by <ref type="bibr">Takamura and Okumura (2010)</ref>). <ref type="table" target="#tab_0">Table 1</ref> shows the results. We observe that ILP- R produces scores close to the reference, thus re- ducing the problem of extractive summarization to the task of sentence scoring, because the perfect scores induced near perfect extracted summaries in this framework. SBL-R seems less promising than ILP-R because it greedily maximizes a func- tion which ILP-R exactly maximizes. Therefore, we continue our experiments in the following sec-tions with ILP-R only. However, SBL-R offers a nice trade-off between performance and computa- tion cost. The greedy optimization of SBL-R is noticeably faster than ILP-R.  In practice, the learner will not produce per- fect scores. We experimentally validated that with learned scores converging to true scores, the ex- tracted summary converges to the best extrac- tive summary (w.r.t to ROUGE-N). To this end, we simulated approximate learners by artificially randomizing the true scores to end up with lists having various correlations with the true scores. We fed these scores to ILP-R and computed the ROUGE-1 of the generated summaries for an ex- ample topic from DUC-2003. <ref type="figure">Figure 1</ref> displays the expected ROUGE-1 versus the performance of the artificial learner (correlation with true scores of sentences). We observe that, as the learner im- proves, the generated summaries approach the best ROUGE scoring summary. <ref type="figure">Figure 1</ref>: ROUGE-1 of summary against sentence scores correlation with true ROUGE-1 scores of sentences (d30003t from DUC-2003).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sentence Scoring</head><p>Now we look at the supervised learning compo- nent which learns ROUGE-N scores for individ- ual sentences. We know that we can achieve an overall summary ROUGE-N score close to the up- per bound, if a learner would be able to learn the scores perfectly. For better understanding the dif- ficulty of the task of sentence scoring, we look at the correlation of the scores produced by a basic learner and the true scores given in a reference dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model and Features</head><p>From an existing summa- rization dataset (e.g. a DUC dataset), a training set can straightforwardly be extracted by annotat- ing each sentence in the source documents with its ROUGE-N score. For each topic in the dataset, this yields a list of sentences and their target score.</p><p>To support the claim that learning ROUGE scores for individual sentences is easier than solv- ing the whole summarization task, it is suffi- cient to choose a basic learner with simple fea- tures and little in-domain training data (models are trained on one DUC dataset and evaluated on another). Specifically, we employ a support vector regression (SVR). <ref type="bibr">3</ref> We use only classical surface-level features to represent sentences (po- sition, length, overlap with title) and combine them with frequency features. The latter include TF*IDF weighting of the terms (similar to <ref type="bibr" target="#b17">Luhn (1958)</ref>), the sum of the frequency of the bi-grams in the sentence, as well as the sum of the document frequency (number of source documents in which the n-grams appear) of the terms and bi-grams in a sentence.</p><p>We trained two models, R1 and R2 on DUC- 02 and DUC-03. For R1, the target score is the ROUGE-1 recall, while R2 learns ROUGE-2 re- call.</p><p>Correlation Analysis We evaluated our sen- tence scoring models R1 and R2 by calculating the correlation of the scores produced by R1 and R2 and the true scores given in the DUC-03 data. We compare both models to the true ROUGE-1 and ROUGE-2 scores. In addition, we calculated the correlation of the TF*IDF and LexRank scores, in order to understand how well they would fit into our framework (TF*IDF and LexRank are de- scribed in section 4.3).</p><p>The results are displayed in <ref type="table" target="#tab_2">Table 2</ref>. Even with a basic learner it is possible to learn scores that correlate well with the true ROUGE-N scores, which supports the claim that it is easier to learn scores for individual sentences than to solve the whole problem of summarization. This finding strongly supports our proposed reduction of the extractive MDS problem to the task of learning <ref type="bibr">with</ref>   scores for individual sentences, which correlate well with their true ROUGE-N scores. We observe that TF*IDF correlates surprisingly well with the ROUGE-1 score, which indicates that we can expect a significant performance gain when feeding TF*IDF scores to our optimization framework. LexRank, on the other hand, orders sentences according to their centrality and does not look at individual sentences. Accordingly, we observe a low correlation with the true ROUGE- N scores, and thus LexRank may not benefit from the optimization (which we confirmed in our ex- periments).</p><p>Finally, we observe that there is significant room for improvement regarding ROUGE-2, as well as for Kendall's tau in ROUGE-1 where a more sophisticated learner could produce scores that correlate better with the true scores. The higher the correlation of the sentence scores as- signed by a learner and the true scores, the better the summary produced by the subsequent subset selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">End-to-End Evaluation</head><p>In our end-to-end evaluation on extractive MDS, we use the following baselines for comparison:</p><p>• TF*IDF weighting: This simple heuristic was introduced by <ref type="bibr" target="#b17">Luhn (1958)</ref>. Each sen- tence receives a score from the TF*IDF of its terms. We trained IDFs (Inverse Document Frequencies) on a background corpus 4 to im- prove the original algorithm.</p><p>• LexRank: Among other graph-based ap- proaches to summarization (Mani and Bloe- dorn, 1997; <ref type="bibr">Radev et al., 2000;</ref><ref type="bibr" target="#b20">Mihalcea, 2004)</ref>, <ref type="bibr">LexRank (Erkan and Radev, 2004</ref>) has become the most popular one. A similar- ity graph G(V, E) is constructed where V is the set of sentences and an edge e ij is drawn between sentences v i and v j if and only if <ref type="bibr">4</ref> We used DBpedia long abstract: http://wiki.dbpedia.org/Downloads2015-04. the cosine similarity between them is above a given threshold. Sentences are scored ac- cording to their PageRank score in G. For our experiments, we use the implementation available in the sumy package. <ref type="bibr">5</ref> • ICSI: ICSI is a recent system that has been identified as one of the state-of-the-art sys- tems by . It is a global linear optimization framework that extracts a summary by solving a maximum coverage problem considering the most important con- cepts in the source documents. Concepts are identified as bi-grams and their importance is estimated via their frequency in the source documents. <ref type="bibr" target="#b2">Boudin et al. (2015)</ref> released a Python implementation (ICSI sume) that we use in our experiments.</p><p>• SFOUR: SFOUR is a structured prediction approach that trains an end-to-end system with a large-margin method to optimize a convex relaxation of ROUGE ( <ref type="bibr">Sipos et al., 2012</ref>). We use the publicly available imple- mentation. <ref type="bibr">6</ref> As described in the previous section, two models are trained: R1 and R2. We evaluate both of them in the end-to-end setup with and without our op- timization. In the greedy version, sentences are added as long as the summary length is valid. We apply the optimization for sentence scor- ing models trained on ROUGE-1 and ROUGE-2 as well. The scoring models are trained on one dataset and evaluated on the other. For the ILP op- timization, the damping factor can vary and leads to different performance. We report the best re- sults among few variations. In order to speed-up the ILP step, we propose to limit the search space by only looking at the top K sentences 7 (hence the importance of learning a correct ordering as well, like Kendall's tau). This results in a mas- sive speed-up and can even lead to better results as it prunes parts of the noise. Finally, we perform significance testing with the t-test to compare dif- ferences between two means. 8  <ref type="table">Table 3</ref>: Impact of the optimization step on sen- tence subset selection.</p><p>Results <ref type="table">Table 3</ref> shows the results. The proposed optimization significantly and systematically im- proves TF*IDF performance as we expected from our analysis in the previous section. This re- sult suggests that using only a frequency signal in source documents is enough to get high scor- ing summaries, which supports the common belief that frequency is one of the most useful features for generic news summarization. It also aligns well with the strong performance of ICSI, which combines an ILP step with frequency information as well.</p><p>The optimization also significantly and system- atically improves upon the greedy approach com- bined with our scoring models. Combining a SVR learner (SVR-1 and SVR-2) and our ILP-R pro- duces results on par with ICSI and sometimes sig- nificantly better. SFOUR maximizes ROUGE in an end-to-end fashion, but is outperformed by our framework when using the same training data. The framework is able to reach a competitive perfor- mance even with a basic learner. These results again suggest that investigating better learners for sentence scoring might be promising in order to improve the quality of the summaries.</p><p>We observe that the model trained on ROUGE- 2 is performing better than the model trained on ROUGE-1, although learning the ROUGE-2 scores seems to be harder than learning ROUGE-1 <ref type="bibr">8</ref> The symbol * indicates that the difference compared to the previous best baseline is significant with p ≤ 0.05. scores (as shown in table 2). However, errors and approximations propagate less easily in ROUGE- 2, because the number of bi-grams in the intersec- tion of two given sentences is far less. Hence we conclude that learning ROUGE-2 scores should be put into the focus of future work on improving sentence scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>This section discusses our contributions in a broader context. ROUGE Our subset selection framework per- forms the task of content selection, selecting an unordered set of textual units (sentences for now) for a system summary. The re-ordering of the sentences is left to a subsequent processing step, which accounts for aspects of discourse coherence and readability.</p><p>While we justified our choice of ROUGE-1 re- call and ROUGE-2 recall as optimization objec- tives by their strong correlation with human evalu- ation methods, ROUGE-N has also various draw- backs. In particular, it does not take into account the overall discourse coherence of a system sum- mary (see the supplemental material for examples of summaries generated by our framework).</p><p>From a broader perspective, systems that have high ROUGE scores can only be as good as ROUGE is, as a proxy for summary quality. However, as long as systems are evaluated with ROUGE, a natural approach is to develop systems that maximize it.</p><p>Should novel automatic evaluation metrics be developed, our approach can still be applied, pro- vided that the new metrics can be expressed as a function of the scores of individual sentences.</p><p>Structured Learning Compared to MDS ap- proaches using structured learning, our problem- reduction has the important advantage that it con- siderably scales-up the available training data by working on sentences instead of docu- ments/summaries pairs. Moreover, the task of sen- tence scoring is not dependent on arbitrary param- eters such as the summary length which are inher- ently abstracted from the "summary worthiness" of individual textual units.</p><p>Error Propagation The first step of the frame- work is left to a ML component which can only produce approximate scores. Empirical results (in <ref type="figure">Figure 1</ref> and Table 2) suggest that even with an imperfect first step, the subsequent optimization is able to produce high scoring summaries. How- ever, it might be insightful to study rigorously and in greater detail the propagation of errors induced by the first step.</p><p>Other Metrics This work focused on maxi- mizing ROUGE-N recall because it is a widely acknowledged automatic evaluation metric. ROUGE-N relies on reference summaries which forces us to perform an estimation step. In our framework, we use ML to estimate the individual scores of sentences without using reference summaries.</p><p>However, <ref type="bibr" target="#b16">Louis and Nenkova (2013)</ref> proposed several alternative evaluation metrics for system summaries which do not need reference sum- maries. They are based on the properties of the system summary and the source documents alone, and correlate well with human evaluation. Some of them can even reach a correlation with human evaluation similar to the ROUGE-2 recall.</p><p>An example of such a metric is the Jensen- Shannon Divergence (JSD) which is a symmet- ric smoothed version of the Kullback-Leibler di- vergence. Maximizing JSD can not be solved ex- actly with an ILP because it can not be factorized into individual sentences. However, applying an efficient greedy algorithm or maximizing a fac- torizable relaxation might produce strong results as well (for example, a simple greedy maximiza- tion of Kullback-Leibler divergence already yields good results <ref type="bibr" target="#b5">(Haghighi and Vanderwende, 2009)</ref>).</p><p>Future Work In this work, we developed a prin- cipled subset selection framework and empirically justified it. We focused on solving the second step of the framework while keeping the machine learning component as simple as possible. Essen- tially, our framework performs a modularization of the task of MDS, where all characteristics of the data and feature representations are pushed into a separate machine learning module -they should not affect the subsequent optimization step which remains fixed.</p><p>The promising results we obtained for sum- marization with a basic learner (see Section 4.3) encourage future work on plugging in more so- phisticated supervised learners in our framework. For example, we plan to incorporate lexical- semantic information in the feature representa- tion and leverage large-scale unsupervised pre- training. This direction is particularly promising because we have shown that we can expect sig- nificant performance gains for end-to-end MDS as the sentence scoring component improves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a problem-reduction approach to ex- tractive MDS, which performs a reduction to the problem of scoring individual sentences with their ROUGE scores based on supervised learning. We defined a principled discrete optimization prob- lem for sentence selection which relies on an ap- proximation of ROUGE. We empirically checked the validity of the approach on standard datasets and observed that even with a basic learner the framework produces promising results. The code for our optimizers is available at github.com/ UKPLab/acl2016-optimizing-rouge. <ref type="bibr">Paul Over, Hoa Dang, and Donna Harman. 2007. DUC in Context. Information Processing and Management, 43(6)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplemental Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Recursive Expression of ROUGE-N</head><p>Let S = {s i |i ≤ m} and T = {t i |i ≤ l} be two sets of sentences, S * the reference summary, and ρ(X) denote the ROUGE-N score of the set of sentences X. Assuming that ρ(S) and ρ(T ) are given, we prove the following recursive formula:</p><formula xml:id="formula_13">ρ(S ∪ T ) = ρ(S) + ρ(T ) − (S ∩ T )<label>(12)</label></formula><p>For compactness, we use the following notation as well:</p><formula xml:id="formula_14">C X,S * (g) = min(F X (g), F S * (g))<label>(13)</label></formula><p>Proof: We have the following definitions:</p><formula xml:id="formula_15">ρ(S) = 1 R N g∈S * ˜ F S,S * (g)<label>(14)</label></formula><formula xml:id="formula_16">ρ(T ) = 1 R N g∈S * ˜ F T,S * (g)<label>(15)</label></formula><formula xml:id="formula_17">(S ∩ T ) = 1 R N g∈S * max(C S,S * (g)+C T,S * (g)−F S * (g), 0)<label>(16)</label></formula><p>And by definition of ROUGE, the formula of S ∪ T :</p><formula xml:id="formula_18">ρ(S ∪ T ) = 1 R N g∈S * min(F S∪T (g), F S * (g))<label>(17)</label></formula><p>In order to prove equation <ref type="formula" target="#formula_0">(12)</ref>, we have to show that the following equation holds:</p><formula xml:id="formula_19">g∈S * C S,S * (g) + g∈S * C T,S * (g) − g∈S * max(C S,S * (g) + C T,S * (g) − F S * (g), 0) = g∈S * min(F S∪T (g), F S * (g)) (18)</formula><p>It is sufficient to show:</p><formula xml:id="formula_20">∀g ∈ S * , C S,S * (g) + C T,S * (g)− max(C S,S * (g) + C T,S * (g) − F S * (g), 0) = min(F S∪T (g), F S * (g)) (19)</formula><p>Let g ∈ S * be a n-gram. There are two possi- bilities:</p><p>• F S (g) + F T (g) ≤ F S * (g): g appears less times in S ∪ T than in the reference sum- mary. It implies: min(F S∪T (g), F S * (g)) = F S∪T (g) = F S (g) + F T (g). Moreover, all F X (g) are positive numbers by defini- tion, and F S (g) ≤ F S * (g) is equivalent to: C S,S * (g) = min(F S (g), F S * (g)) = F S (g). Similarly, we have: C T,S * (g) = min(F T (g), F S * (g)) = F T (g). Since max(C S,S * (g) + C T,S * (g) − F S * (g), 0) = 0, the equation <ref type="formula" target="#formula_0">(19)</ref> holds in this case.</p><p>• F S (g) + F T (g) ≥ F S * (g): g appears more frequently in S ∪T than in the reference sum- mary. It implies: min(F S∪T (g), F S * (g)) = F S * (g). Here we have: max(C S,S * (g) + C T,S * (g) − F S * (g), 0) = C S,S * (g) + C T,S * (g) − F S * (g), and it directly follows that equation <ref type="formula" target="#formula_0">(19)</ref> holds in this case as well.</p><p>Equation <ref type="formula" target="#formula_0">(19)</ref> has been proved, which proves <ref type="formula" target="#formula_0">(12)</ref> as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Expanded Expression of ROUGE-N</head><p>Let S = {s i |i ≤ m} be a set of sentences and ρ(S) its ROUGE-N score. We prove the following formula:</p><formula xml:id="formula_21">ρ(S) = m i=1 ρ(s i )+ m k=2 (−1) k+1 ( 1≤i 1 ≤···≤i k ≤m (k) (s i 1 ∩ · · · ∩ s i k ))<label>(20)</label></formula><p>Proof: Let g ∈ S * be a n-gram in the ref- erence summary, and k ∈ [1, m] the number of sentences in which it appears. Specifically, ∃{s i 1 , · · · , s i k }, ∀s i j ∈ {s i 1 , . . . , s i k }, g ∈ s i j . In order to prove the formula (20), we have to find an expression for the (k) that gives to g the correct contribution to the formula:</p><formula xml:id="formula_22">1 R N min(F S (g), F S * (g))<label>(21)</label></formula><p>First, we observe that g does not appear in the terms that contain the intersection of more than k sentences. Specifically, (t) is not affected by g if t ≥ k. However, g is affected by all the (t) for which t ≤ k.</p><p>Given that g appears in the sentences {s i 1 , . . . , s i j }, we can determine the score attributed to g by the previous (t) (t ≤ k):</p><formula xml:id="formula_23">S (k−1) (g) = s∈{s i 1 ,...,s i k } ρ(s)+ k l=2 (−1) (l+1) 1≤i 1 ≤···≤i l ≤k (l) (s i 1 ∩ · · · ∩ s i l ))<label>(22)</label></formula><p>Now, g receives the correct contribution to the overall scores if (k) is defined as follows: Indeed, with this expression for (k) , the score of g is:</p><formula xml:id="formula_24">S (k−1) (g) + 1 R N min(C {s i 1 ,...,s i k } (g), F S * (g)) − S (k−1) (g)<label>(24)</label></formula><p>Which can be simplified to:</p><formula xml:id="formula_25">1 R N min(C {s i 1 ,...,s i k } (g), F S * (g))<label>(25)</label></formula><p>Since g appears only in the sentences {s i 1 , . . . , s i k }, ˜ F {s i 1 ,...,s i k } (g) = F S (g) and it follows that:</p><formula xml:id="formula_26">1 R N min(C {s i 1 ,...,s i k } (g), F S * (g)) = 1 R N min(F S (g), F S * (g))<label>(26)</label></formula><p>This proves equation <ref type="formula" target="#formula_1">(20)</ref> because we observe that g will not be affected by any other terms. Ev- ery (t) for t ≤ k including g is counted by S (k−1) , and no other terms from (k) will affect g because all the other terms (k) should contain at least one sentence that is not in {s i 1 , . . . , s i k } and g would not belong to this intersection by definition.</p><p>Finally, it has been proved in the appendix A.1 that for k = 2, (2) has a reduced form:</p><formula xml:id="formula_27">(2) (s a ∩ s b ) = 1 R N g∈S * max(C sa,S * (g)+C s b ,S * (g)−F S * (g), 0)<label>(27)</label></formula><p>In the paper, we ignore the terms for k ≥ 2, there- fore we do not search for a reduced form for these terms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>k) (s i 1 ∩ · · · ∩ s i j ) = 1 R g∈s i 1 ∩···∩s i j min(C {s i 1 ,...,s i k } (g), F S * (g)) − S (k−1) (g) (23)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Upper bound of our framework compared 
to extractive upper bound. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Correlation of different kinds of sentence scores and their true ROUGE-1 and ROUGE-2 scores. 

</table></figure>

			<note place="foot" n="1"> In the official DUC-03 competitions, summaries of length 665 bytes were expected. Systems could produce different numbers of words. The variation in length has a noticeable impact on ROUGE recall scores. 2 ROUGE-1.5.5 with the parameters:-n 2-m-a-l 100-x-c 95-r 1000-f A-p 0.5-t 0. The length parameter becomes-l 200 for DUC-02.</note>

			<note place="foot" n="3"> We use the implementation in scikit-learn (Pedregosa et al., 2011).</note>

			<note place="foot" n="5"> https://github.com/miso-belica/sumy 6 http://www.cs.cornell.edu/˜rs/sfour/ 7 We used K=50 and observed that a range from K=25 to K=70 yields a good trade-off between computation cost and performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the German Re-search Foundation as part of the Research Training Group "Adaptive Preparation of Information from Heterogeneous Sources" (AIPHES) under grant No. GRK 1994/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A Trainable Summarizer with Knowledge Acquired from Robust NLP Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Gorlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjornar</forename><surname>Larsen</surname></persName>
		</author>
		<editor>Inderjeet Mani and Mark T</editor>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maybury</surname></persName>
		</author>
		<title level="m">Advances in Automatic Text Summarization</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="68" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Concept-based Summarization using Integer Linear Programming: From Concept Pruning to Multiple Optimal Solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Boudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mougard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benot</forename><surname>Favre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<editor>Llus Mrquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton</editor>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1914" to="1918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LexRank: Graph-based Lexical Centrality As Salience in Text Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Scalable Global Model for Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Favre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, ILP &apos;09</title>
		<meeting>the Workshop on Integer Linear Programming for Natural Langauge Processing, ILP &apos;09<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploring Content Models for Multi-document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving the Estimation of Word Importance for News MultiDocument Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="712" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Repository of State of the Art and Competitive Baseline Summaries for Generic News Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Benoit Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1608" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extractive Summarization using Continuous Vector Space Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Kågebäck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olof</forename><surname>Mogren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Tahmasebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devdatt</forename><surname>Dubhashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)</title>
		<meeting>the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="31" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<title level="m">Determinantal Point Processes for Machine Learning. Foundations and Trends in Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="123" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Trainable Document Summarizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Kupiec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francine</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="68" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Document Summarization via Guided Sentence Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="490" to="500" />
		</imprint>
	</monogr>
	<note>Seattle</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using Supervised Bigram-based ILP for Extractive Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1004" to="1013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving Update Summarization via Supervised ILP and Sentence Reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1317" to="1322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Class of Submodular Functions for Document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="510" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ROUGE: A Package for Automatic Evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Text Summarization Branches Out at ACL</title>
		<meeting>the Workshop on Text Summarization Branches Out at ACL<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatically Assessing Machine Summary Content Without a Gold Standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistic</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="300" />
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Automatic Creation of Literature Abstracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Peter Luhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research Development</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="159" to="165" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multidocument Summarization by Graph Search and Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjeet</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bloedorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth National Conference on Artificial Intelligence and Ninth Conference on Innovative Applications of Artificial Intelligence</title>
		<meeting>the Fourteenth National Conference on Artificial Intelligence and Ninth Conference on Innovative Applications of Artificial Intelligence<address><addrLine>Providence, Rhode Island</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="622" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Study of Global Inference Algorithms in Multi-document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th European Conference on IR Research</title>
		<meeting>the 29th European Conference on IR Research<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>SpringerVerlag</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="557" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph-based Ranking Algorithms for Sentence Extraction, Applied to Text Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2004 on Interactive Poster and Demonstration Sessions, ACLdemo &apos;04</title>
		<meeting>the ACL 2004 on Interactive Poster and Demonstration Sessions, ACLdemo &apos;04<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extractive Summarization by Aggregating Multiple Similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Olof Mogren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devdatt</forename><surname>Kågebäck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dubhashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Natural Language Processing</title>
		<meeting><address><addrLine>Hissar, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="451" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Best Algorithms for Approximating the Maximum of a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><forename type="middle">A</forename><surname>Nemhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Submodular Set Function. Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="177" to="188" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An Analysis of Approximations for Maximizing Submodular Set FunctionsI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">L</forename><surname>Nemhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><forename type="middle">A</forename><surname>Wolsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marschall</forename><forename type="middle">L</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="265" to="294" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploiting Category-Specific Information for Multi-Document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ping</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Bysani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew-Lim</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012)</title>
		<meeting>the 24th International Conference on Computational Linguistics (COLING 2012)<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2093" to="2108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to Generate Coherent Summary with Discriminative Hidden Semi-Markov Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Nishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuho</forename><surname>Arita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsumi</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiro</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1648" to="1659" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
