<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two Discourse Driven Language Models for Semantics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Illinois</orgName>
								<orgName type="institution" key="instit2">Urbana-Champaign Urbana</orgName>
								<address>
									<postCode>61801</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Illinois</orgName>
								<orgName type="institution" key="instit2">Urbana-Champaign Urbana</orgName>
								<address>
									<postCode>61801</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Two Discourse Driven Language Models for Semantics</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="290" to="300"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Natural language understanding often requires deep semantic knowledge. Expanding on previous proposals, we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction. We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities. For each model, we investigate four implementations: a &quot;stan-dard&quot; N-gram language model and three discriminatively trained &quot;neural&quot; language models that generate embeddings for semantic frames. The quality of the semantic language models (SemLM) is evaluated both intrinsically, using perplexity and a narrative cloze test and extrinsically-we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language understanding often necessitates deep semantic knowledge. This knowledge needs to be captured at multiple levels, from words to phrases, to sentences, to larger units of dis- course. At each level, capturing meaning fre- quently requires context sensitive abstraction and disambiguation, as shown in the following exam- ple <ref type="bibr" target="#b44">(Winograd, 1972)</ref>:</p><p>Ex.1 <ref type="bibr">[Kevin]</ref> was robbed by <ref type="bibr">[Robert]</ref>. <ref type="bibr">[He]</ref> was arrested by the police.</p><p>Ex.2 <ref type="bibr">[Kevin]</ref> was robbed by <ref type="bibr">[Robert]</ref>. <ref type="bibr">[He]</ref> was rescued by the police.</p><p>In both cases, one needs to resolve the pronoun "he" to either "Robert" or "Kevin". To make the correct decisions, one needs to know that the subject of "rob" is more likely than the object of "rob" to be the object of "arrest" while the object of "rob" is more likely to be the object of "rescue". Thus, beyond understanding individual predicates (e.g., at the semantic role labeling level), there is a need to place them and their arguments in a global context.</p><p>However, just modeling semantic frames is not sufficient; consider a variation of Ex.1:</p><p>Ex.3 Kevin was robbed by Robert, but the police mistakenly arrested him.</p><p>In this case, "him" should refer to "Kevin" as the discourse marker "but" reverses the meaning, illustrating that it is necessary to take discourse markers into account when modeling semantics.</p><p>In this paper we propose that these aspects of semantic knowledge can be modeled as a Seman- tic Language Model (SemLM). Just like the "stan- dard" syntactic language models (LM), we de- fine a basic vocabulary, a finite representation lan- guage, and a prediction task, which allows us to model the distribution over the occurrence of el- ements in the vocabulary as a function of their (well-defined) context. In difference from syn- tactic LMs, we represent natural language at a higher level of semantic abstraction, thus facilitat- ing modeling deep semantic knowledge.</p><p>We propose two distinct discourse driven lan- guage models to capture semantics. In our first se- mantic language model, the Frame-Chain SemLM, we model all semantic frames and discourse mark- ers in the text. Each document is viewed as a sin- gle chain of semantic frames and discourse mark- ers. Moreover, while the vocabulary of discourse markers is rather small, the number of different surface form semantic frames that could appear in the text is very large. To achieve a better level of abstraction, we disambiguate semantic frames and map them to their PropBank/FrameNet represen-tation. <ref type="bibr">Thus, in Ex.3</ref>, the resulting frame chain is "rob.01 -but -arrest.01" ("01" indicates the predicate sense).</p><p>Our second semantic language model is called Entity-Centered SemLM. Here, we model a se- quence of semantic frames and discourse mark- ers involved in a specific co-reference chain. For each co-reference chain in a document, we first extract semantic frames corresponding to each co-referent mention, disambiguate them as be- fore, and then determine the discourse markers between these frames. Thus, each unique frame contains both the disambiguated predicate and the argument label of the mention. In Ex.3, the re- sulting sequence is "rob.01#obj -but -ar- rest.01#obj" (here "obj" indicates the argument la- bel for "Kevin" and "him" respectively). While these two models capture somewhat different se- mantic knowledge, we argue later in the paper that both models can be induced at high quality, and that they are suitable for different NLP tasks.</p><p>For both models of SemLM, we study four language model implementations: N-gram, skip- gram ( <ref type="bibr" target="#b20">Mikolov et al., 2013b</ref>), continuous bag- of-words ( <ref type="bibr" target="#b19">Mikolov et al., 2013a</ref>) and log-bilinear language model <ref type="bibr" target="#b21">(Mnih and Hinton, 2007)</ref>. Each model defines its own prediction task. In total, we produce eight different SemLMs. Except for N- gram model, others yield embeddings for semantic frames as they are neural language models.</p><p>In our empirical study, we evaluate both the quality of all SemLMs and their application to co- reference resolution and shallow discourse parsing tasks. Following the traditional evaluation stan- dard of language models, we first use perplexity as our metric. We also follow the script learning literature <ref type="bibr" target="#b6">(Chambers and Jurafsky, 2008b;</ref><ref type="bibr" target="#b7">Chambers and Jurafsky, 2009;</ref><ref type="bibr" target="#b37">Rudinger et al., 2015)</ref> and evaluate on the narrative cloze test, i.e. randomly removing a token from a sequence and test the sys- tem's ability to recover it. We conduct both eval- uations on two test sets: a hold-out dataset from the New York Times Corpus and gold sequence data (for frame-chain SemLMs, we use Prop- Bank ( <ref type="bibr" target="#b17">Kingsbury and Palmer, 2002</ref>); for entity- centered SemLMs, we use Ontonotes ( <ref type="bibr" target="#b14">Hovy et al., 2006)</ref> ). By comparing the results on these test sets, we show that we do not incur noticeable degradation when building SemLMs using prepro- cessing tools. Moreover, we show that SemLMs improves the performance of co-reference resolu- tion, as well as that of predicting the sense of dis- course connectives for both explicit and implicit ones.</p><p>The main contributions of our work can be summarized as follows: 1) The design of two novel discourse driven Semantic Language mod- els, building on text abstraction and neural em- beddings; 2) The implementation of high quality SemLMs that are shown to improve state-of-the- art NLP systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is related to script learning. Early works ( <ref type="bibr" target="#b38">Schank and Abelson, 1977;</ref><ref type="bibr" target="#b24">Mooney and DeJong, 1985)</ref> tried to construct knowledge bases from documents to learn scripts. Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data <ref type="bibr" target="#b5">(Chambers and Jurafsky, 2008a;</ref><ref type="bibr" target="#b3">Bejan, 2008;</ref><ref type="bibr" target="#b16">Jans et al., 2012;</ref><ref type="bibr" target="#b30">Pichotta and Mooney, 2014;</ref><ref type="bibr" target="#b12">Granroth-Wilding et al., 2015;</ref><ref type="bibr" target="#b31">Pichotta and Mooney, 2016)</ref>. Other works aimed at learning a collection of structured events <ref type="bibr" target="#b8">(Chambers, 2013;</ref><ref type="bibr" target="#b9">Cheung et al., 2013;</ref><ref type="bibr" target="#b9">Cheung et al., 2013;</ref><ref type="bibr" target="#b1">Balasubramanian et al., 2013;</ref><ref type="bibr" target="#b2">Bamman and Smith, 2014;</ref><ref type="bibr" target="#b25">Nguyen et al., 2015)</ref>, and several works have employed neural embeddings ( <ref type="bibr" target="#b23">Modi and Titov, 2014b;</ref><ref type="bibr" target="#b22">Modi and Titov, 2014a;</ref><ref type="bibr" target="#b11">Frermann et al., 2014;</ref><ref type="bibr" target="#b42">Titov and Khoddam, 2015)</ref>.</p><p>In our work, the semantic sequences in the entity-centered SemLMs are similar to narrative schemas ( <ref type="bibr" target="#b7">Chambers and Jurafsky, 2009)</ref>. How- ever, we differ from them in the following aspects: 1) script learning does not generate a probabilis- tic model on semantic frames 1 ; 2) script learning models semantic frame sequences incompletely as they do not consider discourse information; 3) works in script learning rarely show applications to real NLP tasks.</p><p>Some prior works have used scripts-related ideas to help improve NLP tasks <ref type="bibr" target="#b15">(Irwin et al., 2011;</ref><ref type="bibr" target="#b35">Rahman and Ng, 2011;</ref><ref type="bibr" target="#b29">Peng et al., 2015b)</ref>. However, since they use explicit script schemas either as features or constraints, these works suf- fer from data sparsity problems. In our work, the SemLM abstract vocabulary ensures a good cov- erage of frame semantics. <ref type="table">Table 1</ref>: Comparison of vocabularies between frame-chain (FC) and entity-centered (EC) SemLMs. "F-Sen" stands for frames with pred- icate sense information while "F-Arg" stands for frames with argument role label information; "Conn" means discourse marker and "Per" means period. "Seq/Doc" represents the number of se- quence per document. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Two Models for SemLM</head><p>In this section, we describe how we capture se- quential semantic information consisted of seman- tic frames and discourse markers as semantic units (i.e. the vocabulary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semantic Frames and Discourse Markers</head><p>Semantic Frames A semantic frame is composed of a predicate and its corresponding argument par- ticipants. Here we require the predicate to be dis- ambiguated to a specific sense, and we need a cer- tain level of abstraction of arguments so that we can assign abstract labels. The design of Prop- Bank frames <ref type="bibr" target="#b17">(Kingsbury and Palmer, 2002</ref>) and FrameNet frames ( <ref type="bibr" target="#b0">Baker et al., 1998</ref>) perfectly fits our needs. They both have a limited set of frames (in the scale of thousands) and each frame can be uniquely represented by its predicate sense. These frames provide a good level of generalization as each frame can be instantiated into various surface forms in natural texts. We use these frames as part of our vocabulary for SemLMs. Formally, we use the notation f to represent a frame. Also, we de- note fa f#Arg when referring to an argument role label (Arg) inside a frame (f). Discourse Markers We use discourse markers (connectives) to model discourse relationships be- tween frames. There is only a limited number of unique discourse markers, such as and, but, how- ever, etc. We get the full list from the Penn Dis- course Treebank ( <ref type="bibr" target="#b33">Prasad et al., 2008)</ref> and include them as part of our vocabulary for SemLMs. For- mally, we use dis to denote the discourse marker. Note that discourse relationships can exist with- out an explicit discourse marker, which is also a challenge for discourse parsing. Since we cannot reliably identify implicit discourse relationships, we only consider explicit ones here. More impor- tantly, discourse markers are associated with ar- guments <ref type="bibr" target="#b43">(Wellner and Pustejovsky, 2007</ref>) in text (usually two sentences/clauses, sometimes one). We only add a discourse marker in the semantic sequence when its corresponding arguments con- tain semantic frames which belong to the same se- mantic sequence. We call them frame-related dis- course markers. Details on generating semantic frames and discourse markers to form semantic se- quences are discussed in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Frame-Chain SemLM</head><p>For frame-chain SemLM, we model all seman- tic frames and discourse markers in a document.</p><p>We form the semantic sequence by first includ- ing all semantic frames in the order they appear in the text:</p><formula xml:id="formula_0">[f 1 , f 2 , f 3 , . . .].</formula><p>Then we add frame- related discourse markers into the sequence by placing them in their order of appearance. Thus we get a sequence like</p><formula xml:id="formula_1">[f 1 , dis 1 , f 2 , f 3 , dis 2 , . . .].</formula><p>Note that discourse markers do not necessarily exist between all semantic frames. Additionally, we treat the period symbol as a special discourse marker, denoted by "o". As some sentences con- tain more than one semantic frame (situations like clauses), we get the final semantic sequence like this:</p><formula xml:id="formula_2">[f 1 , dis 1 , f 2 , o, f 3 , o, dis 2 , . . . , o]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Entity-Centered SemLM</head><p>We generate semantic sequences according to co-reference chains for entity-centered SemLM. From co-reference resolution, we can get a se- quence like [m 1 , m 2 , m 3 , . . .], where mentions ap- pear in the order they occur in the text. Each mention can be matched to an argument inside a semantic frame. Thus, we replace each mention with its argument label inside a semantic frame, and get [fa 1 , fa 2 , fa 3 , . . .]. We then add discourse markers exactly in they way we do for frame-chain SemLM, and get the following sequence:</p><formula xml:id="formula_3">[fa 1 , dis 1 , fa 2 , fa 3 , dis 2 , . . .]</formula><p>The comparison of vocabularies between frame-chain and entity-centered SemLMs is sum- marized in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementations of SemLM</head><p>In this work, we experiment with four language model implementations: N-gram (NG), Skip- Gram (SG), Continuous Bag-of-Words (CBOW) and Log-bilinear (LB) language model. For ease of explanation, we assume that a semantic unit se- quence is s = [w 1 , w 2 , w 3 , . . . , w k ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">N-gram Model</head><p>For an n-gram model, we predict each token based on its n−1 previous tokens, i.e. we directly model the following conditional probability (in practice, we choose n = 3, Tri-gram (TRI) ):</p><formula xml:id="formula_4">p(w t+2 |w t , w t+1 ).</formula><p>Then, the probability of the sequence is</p><formula xml:id="formula_5">p(s) = p(w 1 )p(w 2 |w 1 ) k−2 t=1 p(w t+2 |w t , w t+1 ).</formula><p>To compute p(w 2 |w 1 ) and p(w 1 ), we need to back off from Tri-gram to Bi-gram and Uni-gram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Skip-Gram Model</head><p>The SG model was proposed in <ref type="bibr" target="#b20">Mikolov et al. (2013b)</ref>. It uses a token to predict its context, i.e. we model the following conditional probability:</p><formula xml:id="formula_6">p(c ∈ c(w t )|w t , θ).</formula><p>Here, c(w t ) is the context for w t and θ denotes the learned parameters which include neural network states and embeddings. Then the probability of the sequence is computed as k t=1 c∈c(wt) p(c|w t , θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Continuous Bag-of-Words Model</head><p>In contrast to skip-gram, CBOW ( <ref type="bibr" target="#b19">Mikolov et al., 2013a</ref>) uses context to predict each token, i.e. we model the following conditional probability:</p><formula xml:id="formula_7">p(w t |c(w t ), θ).</formula><p>In this case, the probability of the sequence is k t=1 p(w t |c(w t ), θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Log-bilinear Model</head><p>LB was introduced in <ref type="bibr" target="#b21">Mnih and Hinton (2007)</ref>. Similar to CBOW, it also uses context to predict each token. However, LB associates a token with three components instead of just one vector: a tar- get vector v(w), a context vector v'(w) and a bias b(w). So, the conditional probability becomes:</p><formula xml:id="formula_8">p(w t |c(w t )) = exp(v(w t ) u(c(w t )) + b(w t )) w∈V exp(v(w) u(c(w t )) + b(w)) .</formula><p>Here, V denotes the vocabulary and we define u(c(w t )) = c i ∈c(wt) q i v (c i ). Note that represents element-wise multiplication and q i is a vector that depends only on the position of a token in the context, which is a also a model parameter.</p><p>So, the overall sequence probability is k t=1 p(w t |c(w t )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Building SemLMs from Scratch</head><p>In this section, we explain how we build SemLMs from un-annotated plain text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset and Preprocessing</head><p>Dataset We use the New York Times Corpus 2 (from year 1987 to 2007) for training. It contains a bit more than 1.8M documents in total.</p><p>Preprocessing We pre-process all documents with semantic role labeling ( <ref type="bibr" target="#b34">Punyakanok et al., 2004</ref>) and part-of-speech tagger <ref type="bibr" target="#b36">(Roth and Zelenko, 1998</ref>). We also implement the explicit dis- course connective identification module in shal- low discourse parsing ( <ref type="bibr" target="#b40">Song et al., 2015)</ref>. Ad- ditionally, we utilize within document entity co- reference ( <ref type="bibr" target="#b28">Peng et al., 2015a</ref>) to produce co- reference chains. To obtain all annotations, we employ the Illinois NLP tools 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Semantic Unit Generation</head><p>FrameNet Mapping We first directly derive se- mantic frames from semantic role labeling anno- tations. As the Illinois SRL package is built upon PropBank frames, we do a mapping to FrameNet frames via VerbNet senses ( <ref type="bibr" target="#b39">Schuler, 2005</ref>), thus achieving a higher level of abstraction. The map- ping file 4 defines deterministic mappings. How- ever, the mapping is not complete and there are remaining PropBank frames. Thus, the generated vocabulary for SemLMs contains both PropBank and FrameNet frames. For example, "place" and "put" with the VerbNet sense id "9.1-2" are con- verted to the same FrameNet frame "Placing". Augmenting to Verb Phrases We apply three heuristic modifications to augment semantic frames defined in Sec. 3.1: 1) if a preposition immediately follows a predicate, we append the preposition to the predicate e.g. "take over"; 2) if we encounter the semantic role label AM-PRD which indicates a secondary predicate, we also ap- pend this secondary predicate to the main predi- cate e.g. "be happy"; 3) if we see the semantic role label AM-NEG which indicates negation, we ap- pend "not" to the predicate e.g. "not like". These three augmentations can co-exist and they allow us to model more fine-grained semantic frames. Verb Compounds We have observed that if two predicates appear very close to each other, e.g. "eat and drink", "decide to buy", they actually rep- resent a unified semantic meaning. Thus, we con- struct compound verbs to connect them together.</p><p>We apply the rule that if the gap between two pred- icates is less than two tokens, we treat them as a unified semantic frame defined by the conjunc- tion of the two (augmented) semantic frames, e.g. "eat.01-drink.01" and "decide.01-buy.01". Argument Labels for Co-referent Mentions To get the argument role label information for co- referent mentions, we need to match each mention to its corresponding semantic role labeling argu- ment. If a mention head is inside an argument, we regard it as a match. We do not consider singleton mentions. Vocabulary Construction After generating all se- mantic units for (augmented and compounded) se- mantic frames and discourse markers, we merge them together as a tentative vocabulary. In order to generate a sensible SemLM, we filter out rare tokens which appear less than 20 times in the data. We add the Unknown token (UNK) and End-of- Sequence token (EOS) to the eventual vocabulary. Statistics on the eventual SemLM vocabular- ies and semantic sequences are shown in <ref type="table">Table 2</ref>. We also compare frame-chain and entity-centered SemLMs to the usual syntactic language model setting. The statistics in <ref type="table">Table 2</ref> shows that they are comparable both in vocabulary size and in the total number of tokens for training. Moreover, entity-centered SemLMs have shorter sequences then frame-chain SemLMs. We also provide sev- eral examples of high-frequency augmented com- pound semantic frames in our generated SemLM <ref type="table">Table 2</ref>: Statistics on SemLM vocabularies and sequences. "F-s" stands for single frame while "F-c" stands for compound frame; "Conn" means discourse marker. "#seq" is the number of se- quences, and "#token" is the total number of to- kens (semantic units). We also compute the av- erage token in a sequence i.e. "#t/s". We com- pare frame-chain (FC) and entity-centered (EC) SemLMs to the usual syntactic language model setting i.e. "LM". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vocabulary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Language Model Training</head><p>NG We implement the N-gram model using the SRILM toolkit <ref type="bibr" target="#b41">(Stolcke, 2002</ref>). We also employ the well-known KneserNey Smoothing ( <ref type="bibr" target="#b18">Kneser and Ney, 1995)</ref> technique. SG &amp; CBOW We utilize the word2vec package to implement both SG and CBOW. In practice, we set the context window size to be 10 for SG while set the number as 5 for CBOW (both are usual settings for syntactic language models). We generate 300- dimension embeddings for both models. LB We use the OxLM toolkit ( <ref type="bibr" target="#b27">Paul et al., 2014</ref>) with Noise-Constrastive Estimation ( <ref type="bibr" target="#b13">Gutmann and Hyvarinen, 2010</ref>) for the LB model. We set the context window size to 5 and produce 150- dimension embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>In this section, we first evaluate the quality of SemLMs through perplexity and a narrative cloze test. More importantly, we show that the proposed SemLMs can help improve the performance of co- reference resolution and shallow discourse pars- ing. This further proves that we successfully cap- ture semantic sequence information which can po- tentially benefit a wide range of semantic related NLP tasks.</p><p>We have designed two models for SemLM: frame-chain (FC) and entity-centered (EC). By training on both types of sequences respectively, we implement four different language models: TRI, SG, CBOW, LB. We focus the evaluation efforts on these eight SemLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Quality Evaluation of SemLMs</head><p>Datasets We use three datasets. We first randomly sample 10% of the New York Times Corpus doc- uments (roughly two years of data), denoted the NYT Hold-out Data. All our SemLMs are trained on the remaining NYT data and tested on this hold-out data. We generate semantic sequences for the training and test data using the methodol- ogy described in Sec. 5.</p><p>We use PropBank data with gold frame annota- tions as another test set. In this case, we only gen- erate frame-chain SemLM sequences by apply- ing semantic unit generation techniques on gold frames, as described in Sec 5.2. When we test on Gold PropBank Data with Frame Chains, we use frame-chain SemLMs trained from all NYT data.</p><p>Similarly, we use Ontonotes data ( <ref type="bibr" target="#b14">Hovy et al., 2006</ref>) with gold frame and co-reference annota- tions as the third test set, Gold Ontonotes Data with Coref Chains. We only generate entity- centered SemLMs by applying semantic unit gen- eration techniques on gold frames and gold co- reference chains, as described in Sec 5.2. Baselines We use Uni-gram (UNI) and Bi-gram (BG) as two language model baselines. In ad- dition, we use the point-wise mutual informa- tion (PMI) for token prediction. Essentially, PMI scores each pair of tokens according to their co- occurrences. It predicts a token in the sequence by choosing the one with the highest total PMI with all other tokens in the sequence. We use the or- dered PMI (OP) as our baseline, which is a vari- ation of PMI by considering asymmetric count- ing ( <ref type="bibr" target="#b16">Jans et al., 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Perplexity</head><p>As SemLMs are language models, it is natural to evaluate the perplexity, which is a measurement of how well a language model can predict sequences.</p><p>Results for SemLM perplexities are presented in <ref type="table">Table 3</ref>. They are computed without consider- ing end token (EOS). We apply tri-gram Kneser- Ney Smoothing to CBOW, SG and LB. LB con- sistently shows the lowest perplexities for both frame-chain and entity-centered SemLMs across all test sets. Similar to syntactic language mod- els, perplexities are fast decreasing from UNI, BI to TRI. Also, CBOW and SG have very close per- plexity results which indicate that their language <ref type="table">Table 3</ref>: Perplexities for SemLMs. UNI, BG, TRI, CBOW, SG, LB are different language model implementations while "FC" and "EC" stand for the two SemLM models studied, respectively. "FC-FM" and "EC-FM" indicate that we removed the "FrameNet Mapping" step (Sec. 5.2). LB con- sistently produces the lowest perplexities for both frame-chain and entity-centered SemLMs. modeling abilities are at the same level. We can compare the results of our frame-chain SemLM on NYT Hold-out Data and Gold Prop- Bank Data with Frame Chains, and our entity- centered SemLM on NYT Hold-out Data and Gold Ontonotes Data with Coref Chains. While we see differences in the results, the gap is narrow and the relative ranking of different SemLMs does not change. This indicates that the automatic SRL and Co-reference annotations added some noise but, more importantly, that the resulting SemLMs are robust to this noise as we still retain the language modeling ability for all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>Additionally, our ablation study removes the "FrameNet Mapping" step in Sec. 5.2 ("FC-FM" and "EC-FM" rows), resulting in only using Prop- Bank frames in the vocabulary. The increase in perplexities shows that "FrameNet Mapping" does produce a higher level of abstraction, which is use- ful for language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Narrative Cloze Test</head><p>We follow the Narrative Cloze Test idea used in script learning <ref type="bibr" target="#b6">(Chambers and Jurafsky, 2008b;</ref><ref type="bibr" target="#b7">Chambers and Jurafsky, 2009)</ref>. As <ref type="bibr" target="#b37">Rudinger et al. (2015)</ref> points out, the narrative cloze test can be regarded as a language modeling evaluation. In the narrative cloze test, we randomly choose and remove one token from each semantic sequence in the test set. We then use language models to predict the missing token and evaluate the correct- ness. For all SemLMs, we use the conditional probabilities defined in Sec. 4 to get token predic- tions. We also use ordered PMI as an additional baseline. The narrative cloze test is conducted on <ref type="table">Table 4</ref>: Narrative cloze test results for SemLMs. UNI, BG, TRI, CBOW, SG, LB are different lan- guage model implementations while "FC" and "EC" stand for our two SemLM models, respectively. "FC-FM" and "EC-FM" mean that we remove the FrameNet mappings. "w/o DIS" indicates the removal of discourse makers in SemLMs. "Rel-Impr" indicates the relative improvement of the best performing SemLM over the strongest baseline. We evaluate on two metrics: mean reciprocal rank (MRR)/recall at 30 (Recall@30). LB outperforms other methods for both frame-chain and entity-centered SemLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>SemLMs Results are provided in <ref type="table">Table 4</ref>. Consistent with the results in the perplexity evaluation, LB out- performs other methods for both frame-chain and entity-centered SemLMs across all test sets. It is interesting to see that UNI performs better than BG in this prediction task. This finding is also reflected in the results reported in <ref type="bibr" target="#b37">Rudinger et al. (2015)</ref>. Though CBOW and SG have similar per- plexity results, SG appears to be stronger in the narrative cloze test. With respect to the strongest baseline (UNI), LB achieves close to 20% rela- tive improvement for Recall@30 metric on NYT hold-out data. On gold data, the frame-chain SemLMs get a relative improvement of 36.5% for Recall@30 while entity-centered SemLMs get 22.3%. For MRR metric, the relative improvement is around half that of the Recall@30 metric.</p><p>In the narrative cloze test, we also carry out an ablation study to remove the "FrameNet Mapping" step in Sec. 5.2 ("FC-FM" and "EC-FM" rows). The decrease in MRR and Recall@30 metrics further strengthens the argument that "FrameNet Mapping" is important for language modeling as it improves the generalization on frames.</p><p>We cannot directly compare with other re- lated works <ref type="bibr" target="#b37">(Rudinger et al., 2015;</ref><ref type="bibr" target="#b31">Pichotta and Mooney, 2016)</ref> because of the differences in data and evaluation metrics. <ref type="bibr" target="#b37">Rudinger et al. (2015)</ref> also use the NYT portion of the Gigaword corpus, but with Concrete annotations; <ref type="bibr" target="#b31">Pichotta and Mooney (2016)</ref> use the English Wikipedia as their data, and Stanford NLP tools for pre-processing while we use the Illinois NLP tools. Consequently, the even- tual chain statistics are different, which leads to different test instances. <ref type="bibr">5</ref> We counter this difficulty <ref type="table">Table 5</ref>: Co-reference resolution results with entity-centered SemLM features. "EC" stands for the entity-centered SemLM. "TRI" is the tri- gram model while "LB" is the log-bilinear model. "p c " means conditional probability features and "em" represents frame embedding features. "w/o DIS" indicates the ablation study by removing all discourse makers for SemLMs. We conduct the experiments by adding SemLM features into the base system. We outperform the state-of-art sys- tem ( <ref type="bibr" target="#b45">Wiseman et al., 2015)</ref>, which reports the best results on CoNLL12 dataset. The improvement achieved by "EC LB (p c +em)" over the base sys- tem is statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACE04 CoNLL12</head><p>Wiseman et al. <ref type="formula">(2015)</ref> - by reporting results on "Gold PropBank Data" and "Gold Ontonotes Data". We hope that these two gold annotation datasets can become standard test sets. <ref type="bibr" target="#b37">Rudinger et al. (2015)</ref> does share a common evaluation metric with us: MRR. If we ignore the data difference and make a rough comparison, we find that the absolute values of our results are bet- ter while <ref type="bibr" target="#b37">Rudinger et al. (2015)</ref> have higher rela- tive improvement ("Rel-Impr" in <ref type="table">Table 4</ref>). This means that 1) the discourse information is very likely to help better model semantics 2) the dis- course information may boost the baseline (UNI) more than it does for the LB model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation of SemLM Applications</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Co-reference Resolution</head><p>Co-reference resolution is the task of identifying mentions that refer to the same entity. To help im- prove its performance, we incorporate SemLM in- formation as features into an existing co-reference resolution system. We choose the state-of-art Illi- nois Co-reference Resolution system (Peng et al., 2015a) as our base system. It employs a su- pervised joint mention detection and co-reference framework. We add additional features into the mention-pair feature set. Given a pair of mentions (m 1 , m 2 ) where m 1 make a rough comparison between them.</p><p>appears before m 2 , we first extract the correspond- ing semantic frame and the argument role label of each mention. We do this by following the proce- dures in Sec. 5. Thus, we can get a pair of semantic frames with argument information (fa 1 , fa 2 ). We may also get an additional discourse marker be- tween these two frames, e.g. (fa 1 , dis, fa 2 ). Now, we add the following conditional probability as the feature from SemLMs:</p><formula xml:id="formula_9">p c = p(fa 2 |fa 1 , dis).</formula><p>We also add p 2 c , √ p c and 1/p c as features. To get the value of p c , we follow the definitions in Sec. 4, and we only use the entity-centered SemLM here as its vocabulary covers frames with argument la- bels. For the neural language model implementa- tions (CBOW, SG and LB), we also include frame embeddings as additional features.</p><p>We evaluate the effect of the added SemLM features on two co-reference benchmark datasets: ACE04 (NIST, 2004) and CoNLL12 ( <ref type="bibr" target="#b32">Pradhan et al., 2012</ref>). We use the standard split of 268 train- ing documents, 68 development documents, and 106 testing documents for ACE04 data <ref type="bibr" target="#b10">(Culotta et al., 2007;</ref><ref type="bibr" target="#b4">Bengtson and Roth, 2008)</ref>. For CoNLL12 data, we follow the train and test doc- ument split from CoNLL-2012 Shared Task. We report CoNLL AVG for results (average of MUC, B 3 , and CEAF e metrics), using the v7.0 scorer provided by the CoNLL-2012 Shared Task.</p><p>Co-reference resolution results with entity- centered SemLM features are shown in <ref type="table">Table 5</ref>. Tri-grams with conditional probability features improve the performance by a small margin, while the log-bilinear model achieves a 0.4-0.5 F1 points improvement. By employing log-bilinear model embeddings, we further improve the numbers and we outperform the best reported results on the CoNLL12 dataset ( <ref type="bibr" target="#b45">Wiseman et al., 2015)</ref>.</p><p>In addition, we carry out ablation studies to re- move all discourse makers during the language modeling process. We re-train our models and study their effects on the generated features. Ta- ble 5 ("w/o DIS" rows) shows that without dis- course information, the SemLM features would hurt the overall performance, thus proving the ne- cessity of considering discourse for semantic lan- guage models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Shallow Discourse Parsing</head><p>Shallow discourse parsing is the task of identi- fying explicit and implicit discourse connectives, <ref type="table">Table 6</ref>: Shallow discourse parsing results with frame-chain SemLM features. "FC" stands for the frame-chain SemLM. "TRI" is the tri-gram model while "LB" is the log-bilinear model. "p c ", "em" are conditional probability and frame embedding features, resp. "w/o DIS" indicates the case where we remove all discourse makers for SemLMs. We do the experiments by adding SemLM features to the base system. The improvement achieved by "FC-LB (p c + em)" over the baseline is statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CoNLL16 Test</head><p>CoNLL16 Blind Explicit Implicit Overall Explicit Implicit Overall Base ( <ref type="bibr" target="#b40">Song et al., 2015)</ref> 89 determine their senses and their discourse argu- ments. In order to show that SemLM can help im- prove shallow discourse parsing, we evaluate on identifying the correct sense of discourse connec- tives (both explicit and implicit ones). We choose <ref type="bibr" target="#b40">Song et al. (2015)</ref>, which uses a su- pervised pipeline approach, as our base system. The system extracts context features for potential discourse connectives and applies the discourse connective sense classifier. Consider an explicit connective "dis"; we extract the semantic frames that are closest to it (left and right), resulting in the sequence [f 1 , dis, f 2 ] by following the procedures described in Sec. 5. We then add the following conditional probabilities as features. Compute q c = p(dis|f 1 , f 2 ). and, similar to what we do for co-reference resolu- tion, we add q c , q 2 c , √ q c , 1/q c as conditional prob- ability features, which can be computed following the definitions in Sec. 4. We also include frame embeddings as additional features. We only use frame-chain SemLMs here. We evaluate on CoNLL16 (Xue et al., 2015) test and blind sets, following the train and devel- opment document split from the Shared Task, and report F1 using the official shared task scorer. <ref type="table">Table 6</ref> shows the results for shallow discourse parsing with SemLM features. Tri-gram with con- ditional probability features improve the perfor- mance for both explicit and implicit connective sense classifiers. Log-bilinear model with condi- tional probability features achieves even better re- sults, and frame embeddings further improve the numbers. SemLMs improve relatively more on ex- plicit connectives than on implicit ones.</p><p>We also show an ablation study in the same setting as we did for co-reference, i.e. removing discourse information ("w/o DIS" rows). While our LB model can still exhibit improvement over the base system, its performance is lower than the proposed discourse driven version, which means that discourse information improves the expres- siveness of semantic language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The paper builds two types of discourse driven se- mantic language models with four different lan- guage model implementations that make use of neural embeddings for semantic frames. We use perplexity and a narrative cloze test to prove that the proposed SemLMs have a good level of ab- straction and are of high quality, and then ap- ply them successfully to the two challenging tasks of co-reference resolution and shallow discourse parsing, exhibiting improvements over state-of- the-art systems. In future work, we plan to apply SemLMs to other semantic related NLP tasks e.g. machine translation and question answering.</p></div>
			<note place="foot" n="1"> Some works may utilize a certain probabilistic framework, but they mainly focus on generating high-quality frames by filtering.</note>

			<note place="foot" n="2"> https://catalog.ldc.upenn.edu/LDC2008T19 3 http://cogcomp.cs.illinois.edu/page/software/ 4 http://verbs.colorado.edu/verb-index/fn/vn-fn.xml</note>

			<note place="foot" n="5"> Rudinger et al. (2015) is similar to our entity-centered SemLM without discourse information. So, in Table 4, we</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Christos Christodoulopoulos and Eric Horn for comments that helped to improve this work. This work is supported by Contract HR0011-15-2-0025 with the US Defense Advanced Research Projects Agency (DARPA). Approved for Public Release, Distribution Unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. This material is also based upon work supported by the U.S. Department of Homeland Security under Award Number 2009-ST-061-CCI002-07.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The berkeley framenet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING/ACL</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generating coherent event schemas at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1721" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of biographical structure from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="363" to="376" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of event scenarios from texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Bejan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS Conference</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="124" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the value of features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengtson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Jointly combining implicit constraints improves temporal ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of narrative event chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">94305</biblScope>
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised learning of narrative schemas and their participants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Event schema induction with a probabilistic entity-driven model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1797" to="1807" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C K</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4813</idno>
		<title level="m">Probabilistic frame induction</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">First-order probabilistic models for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A hierarchical bayesian model for unsupervised induction of script knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Frermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinkal</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">What happens next? event prediction using a compositional neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Granroth-Wilding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Llano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hepworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Colton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charnley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lavrač</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Znidaršič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perovšek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ontonotes: The 90% solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/NAACL</title>
		<meeting>HLT/NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Narrative schema as world knowledge for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL Shared Task</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="86" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Skip n-grams and ranking functions for predicting script events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="336" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">From Treebank to PropBank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC-2002</title>
		<meeting>LREC-2002</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Inducing neural models of script knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning semantic script knowledge with event embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning schemata for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dejong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generative event schema induction with entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Besançon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The ace evaluation plan. US National Institute for Standards and Technology (NIST)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">S</forename><surname>Nist</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Oxlm: A neural language modelling framework for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Phil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="92" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A joint framework for coreference resolution and mention head detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Solving hard coreference problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Statistical script learning with multi-argument events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pichotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="220" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning statistical scripts with lstm recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pichotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In AAAI</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The penn discourse treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC</title>
		<meeting>the 6th International Conference on Language Resources and Evaluation (LREC</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic role labeling via integer linear programming inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zimak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Coreference resolution with world knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Part of speech tagging using a network of linear separators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zelenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLINGACL</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Script induction as language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scripts, plans, goals, and understanding: An inquiry into human knowledge structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Abelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMZ</title>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Verbnet: A broad-coverage, comprehensive verb lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Schuler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving a pipeline architecture for shallow discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kordjamshidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL Shared Task</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">2002</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised induction of semantic roles within a reconstruction-error minimization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Khoddam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Automatically identifying the arguments of discourse connectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wellner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference of EMNLP-CoNLL</title>
		<meeting>the 2007 Joint Conference of EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Understanding natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="191" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning anaphoricity and antecedent ranking features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P C</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Rutherford</surname></persName>
		</author>
		<title level="m">The conll-2015 shared task on shallow discourse parsing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>CoNLL</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
