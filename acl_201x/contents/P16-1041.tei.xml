<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Québec</roleName><forename type="first">Maluuba</forename><surname>Research Montreal</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canada</forename><forename type="middle">Kaheer</forename><surname>Suleman</surname></persName>
							<email>k.suleman@maluuba.com</email>
						</author>
						<title level="a" type="main">A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="432" to="441"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Understanding unstructured text is a major goal within natural language processing. Comprehension tests pose questions based on short text passages to evaluate such understanding. In this work, we investigate machine comprehension on the challenging MCTest benchmark. Partly because of its limited size, prior work on MCTest has focused mainly on engineering better features. We tackle the dataset with a neural approach, harnessing simple neural networks arranged in a parallel hierarchy. The parallel hierarchy enables our model to compare the passage , question, and answer from a variety of trainable perspectives, as opposed to using a manually designed, rigid feature set. Perspectives range from the word level to sentence fragments to sequences of sentences; the networks operate only on word-embedding representations of text. When trained with a methodology designed to help cope with limited training data, our Parallel-Hierarchical model sets a new state of the art for MCTest, outper-forming previous feature-engineered approaches slightly and previous neural approaches by a significant margin (over 15 percentage points).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Humans learn in a variety of ways-by communi- cation with each other and by study, the reading of text. Comprehension of unstructured text by machines, at a near-human level, is a major goal for natural language processing. It has garnered * A. Trischler and Z. Ye contributed equally to this work. significant attention from the machine learning re- search community in recent years.</p><p>Machine comprehension (MC) is evaluated by posing a set of questions based on a text pas- sage (akin to the reading tests we all took in school). Such tests are objectively gradable and can be used to assess a range of abilities, from basic understanding to causal reasoning to infer- ence ( <ref type="bibr" target="#b17">Richardson et al., 2013)</ref>. Given a text pas- sage and a question about its content, a system is tested on its ability to determine the correct an- swer ( <ref type="bibr" target="#b18">Sachan et al., 2015)</ref>. In this work, we focus on MCTest, a complex but data-limited compre- hension benchmark, whose multiple-choice ques- tions require not only extraction but also infer- ence and limited reasoning ( <ref type="bibr" target="#b17">Richardson et al., 2013)</ref>. Inference and reasoning are important hu- man skills that apply broadly, beyond language.</p><p>We present a parallel-hierarchical approach to machine comprehension designed to work well in a data-limited setting. There are many use-cases in which comprehension over limited data would be handy: for example, user manuals, internal doc- umentation, legal contracts, and so on. More- over, work towards more efficient learning from any quantity of data is important in its own right, for bringing machines more in line with the way humans learn. Typically, artificial neural networks require numerous parameters to capture complex patterns, and the more parameters, the more train- ing data is required to tune them. Likewise, deep models learn to extract their own features, but this is a data-intensive process. Our model learns to comprehend at a high level even when data is sparse.</p><p>The key to our model is that it compares the question and answer candidates to the text using several distinct perspectives. We refer to a ques- tion combined with one of its answer candidates as a hypothesis (to be detailed below). The seman-tic perspective compares the hypothesis to sen- tences in the text viewed as single, self-contained thoughts; these are represented using a sum and transformation of word embedding vectors, sim- ilarly to <ref type="bibr">Weston et al. (2014)</ref>. The word-by-word perspective focuses on similarity matches between individual words from hypothesis and text, at var- ious scales. As in the semantic perspective, we consider matches over complete sentences. We also use a sliding window acting on a subsentential scale (inspired by the work of <ref type="bibr" target="#b9">Hill et al. (2015)</ref>), which implicitly considers the linear distance be- tween matched words. Finally, this word-level sliding window operates on two different views of story sentences: the sequential view, where words appear in their natural order, and the depen- dency view, where words are reordered based on a linearization of the sentence's dependency graph. Words are represented throughout by embedding vectors ( <ref type="bibr" target="#b1">Bengio et al., 2000;</ref><ref type="bibr" target="#b14">Mikolov et al., 2013</ref>). These distinct perspectives naturally form a hierar- chy that we depict in <ref type="figure" target="#fig_0">Figure 1</ref>. Language is hierar- chical, so it makes sense that comprehension relies on hierarchical levels of understanding.</p><p>The perspectives of our model can be consid- ered a type of feature. However, they are im- plemented by parametric differentiable functions. This is in contrast to most previous efforts on MCTest, whose numerous hand-engineered fea- tures cannot be trained. Our model, significantly, can be trained end-to-end with backpropagation. To facilitate learning with limited data, we also develop a unique training scheme. We initialize the model's neural networks to perform specific heuristic functions that yield decent (though not impressive) performance on the dataset. Thus, the training scheme gives the model a safe, reasonable baseline from which to start learning. We call this technique training wheels.</p><p>Computational models that comprehend (inso- far as they perform well on MC datasets) have been developed contemporaneously in several re- search groups <ref type="bibr">(Weston et al., 2014;</ref><ref type="bibr" target="#b22">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b9">Hill et al., 2015;</ref><ref type="bibr" target="#b8">Hermann et al., 2015;</ref><ref type="bibr" target="#b11">Kumar et al., 2015)</ref>. Models designed specifi- cally for MCTest include those of <ref type="bibr" target="#b17">Richardson et al. (2013)</ref>, and more recently <ref type="bibr" target="#b18">Sachan et al. (2015)</ref>, , and <ref type="bibr" target="#b26">Yin et al. (2016)</ref>. In exper- iments, our Parallel-Hierarchical model achieves state-of-the-art accuracy on MCTest, outperform- ing these existing methods.</p><p>Below we describe related work, the mathemat- ical details of our model, and our experiments, then analyze our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Problem</head><p>In this section, we borrow from <ref type="bibr" target="#b18">Sachan et al. (2015)</ref>, who laid out the MC problem nicely. Ma- chine comprehension requires machines to answer questions based on unstructured text. This can be viewed as selecting the best answer from a set of candidates. In the multiple-choice case, can- didate answers are predefined, but candidate an- swers may also be undefined yet restricted (e.g., to yes, no, or any noun phrase in the text) <ref type="bibr" target="#b18">(Sachan et al., 2015)</ref>.</p><p>For each question q, let T be the unstructured text and A = {a i } the set of candidate answers to q. The machine comprehension task reduces to selecting the answer that has the highest evidence given T . As in <ref type="bibr" target="#b18">Sachan et al. (2015)</ref>, we combine an answer and a question into a hypothesis, h i = f (q, a i ). To facilitate comparisons of the text with the hypotheses, we also break down the passage into sentences t j , T = {t j }. In our setting, q, a i , and t j each represent a sequence of embedding vectors, one for each word and punctuation mark in the respective item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Machine comprehension is currently a hot topic within the machine learning community. In this section we will focus on the best-performing mod- els applied specifically to MCTest, since it is some- what unique among MC datasets (see Section 5). Generally, models can be divided into two cate- gories: those that use fixed, engineered features, and neural models. The bulk of the work on MCTest falls into the former category.</p><p>Manually engineered features often require sig- nificant effort on the part of a designer, and/or various auxiliary tools to extract them, and they cannot be modified by training. On the other hand, neural models can be trained end-to-end and typically harness only a single feature: vector- representations of words. Word embeddings are fed into a complex and possibly deep neural net- work which processes and compares text to ques- tion and answer. Among deep models, mecha- nisms of attention and working memory are com- mon, as in <ref type="bibr">Weston et al. (2014)</ref> and <ref type="bibr" target="#b8">Hermann et al. (2015)</ref>. <ref type="bibr" target="#b18">Sachan et al. (2015)</ref> treated MCTest as a structured prediction problem, searching for a latent answer- entailing structure connecting question, answer, and text. This structure corresponds to the best latent alignment of a hypothesis with appropri- ate snippets of the text. The process of (latently) selecting text snippets is related to the attention mechanisms typically used in deep networks de- signed for MC and machine translation ( <ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr">Weston et al., 2014;</ref><ref type="bibr" target="#b9">Hill et al., 2015;</ref><ref type="bibr" target="#b8">Hermann et al., 2015</ref>). The model uses event and entity coreference links across sentences along with a host of other features. These include specifically trained word vectors for synonymy; antonymy and class-inclusion relations from ex- ternal database sources; dependencies and seman- tic role labels. The model is trained using a latent structural SVM extended to a multitask setting, so that questions are first classified using a pretrained top-level classifier. This enables the system to use different processing strategies for different ques- tion categories. The model also combines question and answer into a well-formed statement using the rules of <ref type="bibr" target="#b5">Cucerzan and Agichtein (2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature-engineering models</head><p>Our model is simpler than that of <ref type="bibr" target="#b18">Sachan et al. (2015)</ref> in terms of the features it takes in, the training procedure (stochastic gradient descent vs. alternating minimization), question classification (we use none), and question-answer combination (simple concatenation or mean vs. a set of rules).  augmented the baseline fea- ture set from <ref type="bibr" target="#b17">Richardson et al. (2013)</ref> with fea- tures for syntax, frame semantics, coreference chains, and word embeddings. They combined features using a linear latent-variable classifier trained to minimize a max-margin loss function. As in <ref type="bibr" target="#b18">Sachan et al. (2015)</ref>, questions and answers are combined using a set of manually written rules. The method of  achieved the previous state of the art, but has significant com- plexity in terms of the feature set.</p><p>Space does not permit a full description of all models in this category, but we refer the reader to the contributions of <ref type="bibr" target="#b19">Smith et al. (2015)</ref> and <ref type="bibr" target="#b15">Narasimhan and Barzilay (2015)</ref> as well.</p><p>Despite its relative lack of features, the Parallel- Hierarchical model improves upon the feature- engineered state of the art for MCTest by a small amount (about 1% absolute) as detailed in Sec- tion 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neural models</head><p>Neural models have, to date, performed relatively poorly on MCTest. This is because the dataset is sparse and complex. <ref type="bibr" target="#b26">Yin et al. (2016)</ref> investigated deep-learning approaches concurrently with the present work. They measured the performance of the Attentive Reader ( <ref type="bibr" target="#b8">Hermann et al., 2015</ref>) and the Neural Rea- soner ( <ref type="bibr" target="#b16">Peng et al., 2015)</ref>, both deep, end-to-end recurrent models with attention mechanisms, and also developed an attention-based convolutional network, the HABCNN. Their network operates on a hierarchy similar to our own, providing fur- ther evidence of the promise of hierarchical per- spectives. Specifically, the HABCNN processes text at the sentence level and the snippet level, where the latter combines adjacent sentences (as we do through an n-gram input). Embedding vec- tors for the question and the answer candidates are combined and encoded by a convolutional net- work. This encoding modulates attention over sen- tence and snippet encodings, followed by max- pooling to determine the best matches between question, answer, and text. As in the present work, matching scores are given by cosine similarity. The HABCNN also makes use of a question clas- sifier.</p><p>Despite the conceptual overlap between the HABCNN and our approach, the Parallel- Hierarchical model performs significantly better on MCTest (more than 15% absolute) as detailed in Section 5. Other neural models tested in <ref type="bibr" target="#b26">Yin et al. (2016)</ref> fare even worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Parallel-Hierarchical Model</head><p>Let us now define our machine comprehension model in full. We first describe each of the per- spectives separately, then describe how they are combined. Below, we use subscripts to index el- ements of sequences, like word vectors, and su- perscripts to indicate whether elements come from the text, question, or answer. In particular, we use the subscripts k, m, n, p to index sequences from the text, question, answer, and hypothesis, respec- tively, and superscripts t, q, a, h. We depict the model schematically in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Semantic Perspective</head><p>The semantic perspective is similar to the Mem- ory Networks approach for embedding inputs into memory space <ref type="bibr">(Weston et al., 2014</ref>). Each sen- tence of the text is a sequence of d-dimensional word vectors:</p><formula xml:id="formula_0">t j = {t k }, t k ∈ R d .</formula><p>The semantic vector s t is computed by embedding the word vec- tors into a D-dimensional space using a two-layer network that implements weighted sum followed by an affine tranformation and a nonlinearity; i.e.,</p><formula xml:id="formula_1">s t = f A t k ω k t k + b t A .<label>(1)</label></formula><p>The matrix A t ∈ R D×d , the bias vector b t A ∈ R D , and for f we use the leaky ReLU function. The scalar ω k is a trainable weight associated with each word in the vocabulary. These scalar weights implement a kind of exogenous or bottom- up attention that depends only on the input stimu- lus ( <ref type="bibr" target="#b13">Mayer et al., 2004</ref>). They can, for example, learn to perform the function of stopword lists in a soft, trainable way, to nullify the contribution of unimportant filler words.</p><p>The semantic representation of a hypothesis is formed analogously, except that we concatenate the question word vectors q m and answer word vectors a n as a single sequence {h p } = {q m , a n }. For semantic vector s h of the hypothesis, we use a unique transformation matrix A h ∈ R D×d and bias vector b h A ∈ R D . These transformations map a text sentence and a hypothesis into a common space where they can be compared. We compute the semantic match be- tween text sentence and hypothesis using the co- sine similarity,</p><formula xml:id="formula_2">M sem = cos(s t , s h ).<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word-by-Word Perspective</head><p>The first step in building the word-by-word per- spective is to transform word vectors from a text sentence, question, and answer through re- spective neural functions. For the text,</p><formula xml:id="formula_3">˜ t k = f B t t k + b t B</formula><p>, where B t ∈ R D×d , b t B ∈ R D and f is again the leaky ReLU. We transform the question and the answer tõ q m andãandã n analogously using distinct matrices and bias vectors. In con- trast to the semantic perspective, we keep the ques- tion and answer candidates separate in the word- by-word perspective. This is because matches to answer words are inherently more important than matches to question words, and we want our model to learn to use this property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Sentential</head><p>Inspired by the work of <ref type="bibr" target="#b23">Wang and Jiang (2015)</ref> in paraphrase detection, we compute matches be- tween hypotheses and text sentences at the word level. This computation uses the cosine similarity as before:</p><formula xml:id="formula_4">c q km = cos( ˜ t k , ˜ q m ),<label>(3)</label></formula><p>c a kn = cos( ˜ t k , ˜ a n ).</p><p>The word-by-word match between a text sen- tence and question is determined by taking the maximum over k (finding the text word that best matches each question word) and then taking a weighted mean over m (finding the average match over the full question):</p><formula xml:id="formula_6">M q = 1 Z m ω m max k c q km .<label>(5)</label></formula><p>Here, ω m is the word weight for the question word and Z normalizes these weights to sum to one over the question. We define the match between a sen- tence and answer candidate, M a , analogously. Fi- nally, we combine the matches to question and an- swer according to</p><formula xml:id="formula_7">M word = α 1 M q + α 2 M a + α 3 M q M a .<label>(6)</label></formula><p>Here, the α are trainable parameters that control the relative importance of the terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Sequential Sliding Window</head><p>The sequential sliding window is related to the original MCTest baseline by <ref type="bibr" target="#b17">Richardson et al. (2013)</ref>. Our sliding window decays from its focus word according to a Gaussian distribution, which we extend by assigning a trainable weight to each location in the window. This modification en- ables the window to use information about the dis- tance between word matches; the original base- line ( <ref type="bibr" target="#b17">Richardson et al., 2013</ref>) used distance infor- mation through a predefined function. The sliding window scans over the words of the text as one continuous sequence, without sen- tence breaks. Each window is treated like a sen- tence in the previous subsection, but we include a location-based weight λ(k). This weight is based on a word's position in the window, which, given a window, depends on its global position k. The cosine similarity is adapted as</p><formula xml:id="formula_8">s q km = λ(k) cos( ˜ t k , ˜ q m ),<label>(7)</label></formula><p>for the question and analogously for the answer. We initialize the location weights with a Gaus- sian and fine-tune them during training. The final matching score, denoted as M sws , is computed as in (5) and (6) with s q km replacing c q km .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Dependency Sliding Window</head><p>The dependency sliding window operates identi- cally to the linear sliding window, but on a differ- ent view of the text passage. The output of this component is M swd and is formed analogously to M sws . The dependency perspective uses the Stanford Dependency Parser <ref type="bibr" target="#b3">(Chen and Manning, 2014</ref>) as an auxiliary tool. Thus, the dependency graph can be considered a fixed feature. Moreover, lineariza- tion of the dependency graph, because it relies on an eigendecomposition, is not differentiable. However, we handle the linearization in data pre- processing so that the model sees only reordered word-vector inputs.</p><p>Specifically, we run the Stanford Dependency Parser on each text sentence to build a dependency graph. This graph has n w vertices, one for each word in the sentence. From the dependency graph we form the Laplacian matrix L ∈ R nw×nw and determine its eigenvectors. The second eigenvec- tor u 2 of the Laplacian is known as the Fiedler</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>vector. It is the solution to the minimization</head><formula xml:id="formula_9">minimize g N i,j=1 η ij (g(v i ) − g(v j )) 2 ,<label>(8)</label></formula><p>where v i are the vertices of the graph and η ij is the weight of the edge from vertex i to vertex j (Golub and Van Loan, 2012). The Fiedler vector maps a weighted graph onto a line such that con- nected nodes stay close, modulated by the connec- tion weights. 1 This enables us to reorder the words of a sentence based on their proximity in the de- pendency graph. The reordering of the words is given by the ordered index set</p><formula xml:id="formula_10">I = arg sort(u 2 ).<label>(9)</label></formula><p>To give an example of how this works, con- sider the following sentence from MCTest and its dependency-based reordering:</p><p>Jenny, Mrs. Mustard 's helper, called the police. the police, called Jenny helper, Mrs. 's Mustard.</p><p>Sliding-window-based matching on the original sentence will answer the question Who called the police? with Mrs. Mustard. The dependency re- ordering enables the window to determine the cor- rect answer, Jenny.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Combining Distributed Evidence</head><p>It is important in comprehension to synthesize in- formation found throughout a document. MCTest was explicitly designed to ensure that it could not be solved by lexical techniques alone, but would instead require some form of inference or limited reasoning ( <ref type="bibr" target="#b17">Richardson et al., 2013)</ref>. It therefore includes questions where the evidence for an an- swer spans several sentences.</p><p>To perform synthesis, our model also takes in n- grams of sentences, i.e., sentence pairs and triples strung together. The model treats these exactly as it treats single sentences, applying all func- tions detailed above. A later pooling operation combines scores across all n-grams (including the single-sentence input). This is described in the next subsection.</p><p>With n-grams, the model can combine infor- mation distributed across contiguous sentences. In some cases, however, the required evidence is spread across distant sentences. To give our model some capacity to deal with this scenario, we take the top N sentences as scored by all the preced- ing functions, and then repeat the scoring compu- tations, viewing these top N as a single sentence.</p><p>The reasoning behind these approaches can be explained well in a probabilistic setting. If we con- sider our similarity scores to model the likelihood of a text sentence given a hypothesis, p(t j | h i ), then the n-gram and top N approaches model a joint probability p(t j 1 , t j 2 , . . . , t j k | h i ). We can- not model the joint probability as a product of in- dividual terms (score values) because distributed pieces of evidence are likely not independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Combining Perspectives</head><p>We use a multilayer perceptron (MLP) to combine M sem , M word , M swd , and M sws , as well as the scores for separate n-grams, as a final matching score M i for each answer candidate. This MLP has multiple layers of staged input, because the distinct scores have different dimensionality: there is one M sem and one M word for each story sen- tence, and one M swd and one M sws for each appli- cation of the sliding window. The MLP's activa- tion function is linear.</p><p>Our overall training objective is to minimize the ranking loss</p><formula xml:id="formula_11">L(T, q, A) = max(0, µ + max i M i =i * − M i * ),<label>(10)</label></formula><p>where µ is a constant margin, i * indexes the cor- rect answer. We take the maximum over i so that we are ranking the correct answer over the best- ranked incorrect answer (of which there are three). This approach worked better than comparing the correct answer to the incorrect answers individu- ally as in .</p><p>Our implementation of the Parallel-Hierarchical model, built in Theano ( <ref type="bibr" target="#b2">Bergstra et al., 2010</ref>) us- ing the Keras framework <ref type="bibr" target="#b4">(Chollet, 2015)</ref>, is avail- able on Github. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training Wheels</head><p>Before training, we initialized the neural-network components of our model to perform sensible heuristic functions. Training did not converge on the small MCTest without this vital approach.</p><p>Empirically, we found that we could achieve above 50% accuracy on MCTest using a simple sum of word vectors followed by a dot product be- tween the story-sentence sum and the hypothesis sum. Therefore, we initialized the network for the semantic perspective to perform this sum, by ini- tializing A x as the identity matrix and b x A as the zero vector, x ∈ {t, h}. Recall that the activation function is a ReLU so that positive outputs are un- changed.</p><p>We also found basic word-matching scores to be helpful, so we initialized the word-by-word networks likewise. The network for perspective- combination was initialized to perform a sum of individual scores, using a zero bias-vector and a weight matrix of ones, since we found that each perspective contributed positively to the overall re- sult.</p><p>This training wheels approach is related to other techniques from the literature. For in- stance, <ref type="bibr" target="#b20">Socher et al. (2013)</ref> proposed the identity- matrix initialization in the context of parsing, and <ref type="bibr" target="#b12">Le et al. (2015)</ref> proposed it in the context of recurrent neural networks (to preserve the er- ror signal through backpropagation). In residual networks ( <ref type="bibr" target="#b7">He et al., 2015)</ref>, shortcut connections bypass certain layers in the network so that a sim- pler function can be trained in conjunction with the full model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The Dataset</head><p>MCTest is a collection of 660 elementary-level children's stories and associated questions, writ- ten by human subjects. The stories are fictional, ensuring that the answer must be found in the text itself, and carefully limited to what a young child can understand ( <ref type="bibr" target="#b17">Richardson et al., 2013)</ref>.</p><p>The more challenging variant consists of 500 stories with four multiple-choice questions each. Despite the elementary level, stories and questions are more natural and more complex than those found in synthetic MC datasets like bAbI ( <ref type="bibr">Weston et al., 2014</ref>) and CNN ( <ref type="bibr" target="#b8">Hermann et al., 2015)</ref>.</p><p>MCTest is challenging because it is both com- plicated and small. As per <ref type="bibr" target="#b9">Hill et al. (2015)</ref>, "it is very difficult to train statistical models only on MCTest." Its size limits the number of parame- ters that can be trained, and prevents learning any complex language modeling simultaneously with the capacity to answer questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training and Model Details</head><p>In this section we describe important details of the training procedure and model setup. For a com- plete list of hyperparameter settings, our stopword list, and other minutiae, we refer interested readers to our Github repository.</p><p>For word vectors we use Google's publicly available embeddings, trained with word2vec on the 100-billion-word News corpus ( <ref type="bibr" target="#b14">Mikolov et al., 2013</ref>). These vectors are kept fixed throughout training, since we found that training them was not helpful (likely because of MCTest's size). The vectors are 300-dimensional (d = 300).</p><p>We do not use a stopword list for the text pas- sage, instead relying on the trainable word weights to ascribe global importance ratings to words. These weights are initialized with the inverse doc- ument frequency (IDF) statistic computed over the MCTest corpus. <ref type="bibr">3</ref> However, we do use a short stop- word list for questions. This list nullifies query words such as {who, what, when, where, how}, along with conjugations of the verbs to do and to be.</p><p>Following earlier methods, we use a heuris- tic to improve performance on negation ques- tions ( <ref type="bibr" target="#b18">Sachan et al., 2015;</ref>. When a question contains the words which and not, we negate the hypothesis ranking scores so that the minimum becomes the maximum. This heuristic leads to an improvement around 6% on the validation set.</p><p>The most important technique for training the model was the training wheels approach. With- out this, training was not effective at all (see the ablation study in <ref type="table">Table 2</ref>). The identity initializa- tion requires that the network weight matrices are square (d = D).</p><p>We found dropout ( <ref type="bibr" target="#b21">Srivastava et al., 2014</ref>) to be particularly effective at improving generalization from the training to the test set, and used 0.5 as the dropout probability. Dropout occurs after all neural-network transformations, if those transfor- mations are allowed to change with training. Our best performing model held networks at the word- by-word level fixed.</p><p>For combining distributed evidence, we used up to trigrams over sentences and our best- performing model reiterated over the top two sen- tences (N = 2).</p><p>We used the Adam optimizer with the standard settings ( <ref type="bibr" target="#b10">Kingma and Ba, 2014</ref>) and a learning rate of 0.003. To determine the best hyperpa- rameters we performed a search over 150 settings based on validation-set accuracy. MCTest's orig- inal validation set is too small for reliable hy- perparameter tuning, so, following , we merged the training and validation sets of MCTest-160 and MCTest-500, then split them randomly into a 250-story training set and a 200- story validation set. This repartition of the data did not affect overall performance per se; rather, the larger validation set made it easier to choose hyperparameters because validation results were more consistent. <ref type="table" target="#tab_1">Table 1</ref> presents the performance of feature- engineered and neural methods on the MCTest test set. Accuracy scores are divided among questions whose evidence lies in a single sentence (single) and across multiple sentences (multi), and among the two variants. Clearly, MCTest-160 is easier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>The first three rows represent feature- engineered methods. <ref type="bibr" target="#b17">Richardson et al. (2013)</ref> + RTE is the best-performing variant of the original baseline published along with MCTest. It uses a lexical sliding window and distance-based mea- sure, augmented with rules for recognizing textual entailment. We described the methods of <ref type="bibr" target="#b18">Sachan et al. (2015)</ref> and  in Section 3. On MCTest-500, the Parallel Hierarchical model significantly outperforms these methods on single questions (&gt; 2%) and slightly outperforms the latter two on multi questions (≈ 0.3%) and overall (≈ 1%). The method of  achieves the best overall result on MCTest-160. We suspect this is because our neural method suffered from the relative lack of training data.</p><p>The last four rows in <ref type="table" target="#tab_1">Table 1</ref> are neural methods that we discussed in Section 3. Performance mea- sures are taken from <ref type="bibr" target="#b26">Yin et al. (2016)</ref>. Here we see our model outperforming the alternatives by a large margin across the board (&gt; 15%). The Neu- ral Reasoner and the Attentive Reader are large, deep models with hundreds of thousands of pa- rameters, so it is unsurprising that they performed poorly on MCTest. The specifically-designed HABCNN fared better, its convolutional architec- ture cutting down on the parameter count. Because there are similarities between our model and the Method MCTest-160 accuracy (%) MCTest-500 accuracy (%) Single <ref type="formula" target="#formula_1">(112)</ref>    <ref type="table">Table 2</ref>: Ablation study on MCTest-500 (all).</p><p>HABCNN, we hypothesize that the performance difference is attributable to the greater simplicity of our model and our training wheels methodol- ogy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis and Discussion</head><p>We measure the contribution of each component of the model by ablating it. Results on the vali- dation set are given in <ref type="table">Table 2</ref>. Not surprisingly, the n-gram functionality is important, contribut- ing almost 4% accuracy improvement. Without this, the model has almost no means for synthe- sizing distributed evidence. The top N function contributes similarly to the overall performance, suggesting that there is a nonnegligible number of multi questions that have their evidence dis- tributed across noncontiguous sentences. Ablating the sentential component made a significant differ- ence, reducing performance by about 5%. Sim- ple word-by-word matching is obviously useful on MCTest. The sequential sliding window con- tributes about 1.3%, suggesting that word-distance measures are not overly important. Similarly, the dependency-based sliding window makes a very minor contribution. We found this surprising. It may be that linearization of the dependency graph removes too much of its information. The ex- ogenous word weights make a significant contri- bution of over 3%. Allowing the embeddings to change with training reduced performance fairly significantly, almost 8%. As discussed, this is a case of having too many parameters for the avail- able training data. Finally, we see that the training wheels methodology had enormous impact. With- out heuristic-based initialization of the model's various weight matrices, accuracy goes down to about 35%, which is only ten points over random chance.</p><p>Analysis reveals that most of our system's test failures occur on questions about quantity (e.g., How many...? ) and temporal order (e.g., Who was invited last? ). Quantity questions make up 9.5% of our errors on the validation set, while or- der questions make up 10.3%. This weakness is not unexpected, since our architecture lacks any capacity for counting or tracking temporal order. Incorporating mechanisms for these forms of rea- soning is a priority for future work (in contrast, the Memory Network model ( <ref type="bibr">Weston et al., 2014)</ref> is quite good at temporal reasoning).</p><p>The Parallel-Hierarchical model is simple. It does no complex language or sequence modeling. Its simplicity is a response to the limited data of MCTest. Nevertheless, the model achieves state- of-the-art results on the multi questions, which (putatively) require some limited reasoning. Our model is able to handle them reasonably well just by stringing important sentences together. Thus, the model imitates reasoning with a heuristic. This suggests that, to learn true reasoning abilities, MCTest is too simple a dataset-and it is almost certainly too small for this goal.</p><p>However, it may be that human language pro- cessing can be factored into separate processes of comprehension and reasoning. If so, the Parallel- Hierarchical model is a good start on the former. Indeed, if we train the method exclusively on sin- gle questions then its results become even more impressive: we can achieve a test accuracy of 79.1% on MCTest-500. Note that this boost in performance comes from training on only about half the data. The 'single' questions can be con-sidered a close analogue of the RTE task, at which our model becomes very adept even with less data.</p><p>Incorporating the various views of our model amounts to encoding prior knowledge about the problem structure. This is similar to the purpose of feature engineering, except that the views can be fully trained. Encoding problem structure into the structure of neural networks is not new: as an- other example, the convolutional architecture has led to large gains in vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented the novel Parallel-Hierarchical model for machine comprehension, and evalu- ated it on the small but complex MCTest. Our model achieves state-of-the-art results, outper- forming several feature-engineered and neural ap- proaches.</p><p>Working with our model has emphasized to us the following (not necessarily novel) concepts, which we record here to promote further empirical validation.</p><p>• Good comprehension of language is sup- ported by hierarchical levels of understand- ing (cf. <ref type="bibr" target="#b9">Hill et al. (2015)</ref>).</p><p>• Exogenous attention (the trainable word weights) may be broadly helpful for NLP.</p><p>• The training wheels approach, that is, ini- tializing neural networks to perform sensible heuristics, appears helpful for small datasets.</p><p>• Reasoning over language is challenging, but easily simulated in some cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schematic of the Parallel-Hierarchical model. SW stands for "sliding window." MLP represents a fully connected neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 : Experimental results on MCTest.</head><label>1</label><figDesc></figDesc><table>Ablated component 
Validation accuracy (%) 
-
70.13 
n-gram 
66.25 
Top N 
66.63 
Sentential 
65.00 
SW-sequential 
68.88 
SW-dependency 
69.75 
Word weights 
66.88 
Trainable embeddings 
63.50 
Training wheels 
34.75 

</table></figure>

			<note place="foot" n="1"> We experimented with assigning unique edge weights to unique relation types in the dependency graph. However, this had negligible effect. We hypothesize that this is because dependency graphs are trees, which do not have cycles.</note>

			<note place="foot" n="2"> https://github.com/Maluuba/mctest-model</note>

			<note place="foot" n="3"> We override the IDF initialization for words like not, which are frequent but highly informative.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="932" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SciPy</title>
		<meeting>of SciPy</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Factoid question answering over unstructured and structured web content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silviu</forename><surname>Cucerzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page">90</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles F Van</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loan</surname></persName>
		</author>
		<title level="m">Matrix computations</title>
		<imprint>
			<publisher>JHU Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07285</idno>
		<title level="m">Ask me anything: Dynamic memory networks for natural language processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural networks underlying endogenous and exogenous visual-spatial orienting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><forename type="middle">M</forename><surname>Andrew R Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dorflinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stephen M Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seidenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="534" to="541" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Machine comprehension with discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">53rd Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05508</idno>
		<title level="m">Towards neural network-based reasoning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mctest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning answerentailing structures for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A strong lexical matching method for the machine comprehension test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellery</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Greco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matko</forename><surname>Bosnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1693" to="1698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2431" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.08849</idno>
		<title level="m">Learning natural language inference with lstm</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Machine comprehension with syntax, frames, and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">700</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint/>
	</monogr>
<note type="report_type">Bordes. 2014. Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Attention-based convolutional neural network for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.04341</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
