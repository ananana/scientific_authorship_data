<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Natural Language Descriptions for Semantic Representations of Human Brain Activity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eri</forename><surname>Matsuo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Information and Neural Networks</orgName>
								<orgName type="department" key="dep2">National Institute of Information and Communications Technology</orgName>
								<orgName type="department" key="dep3">Artificial Intelligence Research Center</orgName>
								<orgName type="department" key="dep4">National Institute of Advanced Industrial Science and Technology</orgName>
								<orgName type="institution">Ochanomizu University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichiro</forename><surname>Kobayashi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Information and Neural Networks</orgName>
								<orgName type="department" key="dep2">National Institute of Information and Communications Technology</orgName>
								<orgName type="department" key="dep3">Artificial Intelligence Research Center</orgName>
								<orgName type="department" key="dep4">National Institute of Advanced Industrial Science and Technology</orgName>
								<orgName type="institution">Ochanomizu University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Nishimoto</surname></persName>
							<email>nishimoto@nict.go.jp s-nishida@nict.go.jp</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Information and Neural Networks</orgName>
								<orgName type="department" key="dep2">National Institute of Information and Communications Technology</orgName>
								<orgName type="department" key="dep3">Artificial Intelligence Research Center</orgName>
								<orgName type="department" key="dep4">National Institute of Advanced Industrial Science and Technology</orgName>
								<orgName type="institution">Ochanomizu University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nishida</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Information and Neural Networks</orgName>
								<orgName type="department" key="dep2">National Institute of Information and Communications Technology</orgName>
								<orgName type="department" key="dep3">Artificial Intelligence Research Center</orgName>
								<orgName type="department" key="dep4">National Institute of Advanced Industrial Science and Technology</orgName>
								<orgName type="institution">Ochanomizu University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Asoh</surname></persName>
							<email>h.asoh@aist.go.jp</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Information and Neural Networks</orgName>
								<orgName type="department" key="dep2">National Institute of Information and Communications Technology</orgName>
								<orgName type="department" key="dep3">Artificial Intelligence Research Center</orgName>
								<orgName type="department" key="dep4">National Institute of Advanced Industrial Science and Technology</orgName>
								<orgName type="institution">Ochanomizu University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generating Natural Language Descriptions for Semantic Representations of Human Brain Activity</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics-Student Research Workshop</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics-Student Research Workshop <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="22" to="29"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Quantitative analysis of human brain activity based on language representations, such as the semantic categories of words, have been actively studied in the field of brain and neuroscience. Our study aims to generate natural language descriptions for human brain activation phenomena caused by visual stimulus by employing deep learning methods, which have gained interest as an effective approach to automatically describe natural language expressions for various type of multi-modal information , such as images. We employed an image-captioning system based on a deep learning framework as the basis for our method by learning the relationship between the brain activity data and the features of an intermediate expression of the deep neural network owing to lack of training brain data. We conducted three experiments and were able to generate natural language sentences which enabled us to quantitatively interpret brain activity.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the field of brain and neuroscience, analyzing semantic activities occurring in the human brain is an area of active study. Meanwhile, in the field of computational linguistics, the recent evolution of deep learning methods has allowed methods of generating captions for images to be actively stud- ied. Combining these backgrounds, we propose a method to quantitatively interpret the states of the human brain with natural language descriptions, referring to prior methods developed in the fields of both brain and neuroscience and computational linguistics. Because it is difficult to prepare a large-scale brain activity dataset to train a deep neural model of generating captions for brain ac- tivity from scratch, therefore, to handle this prob- lem, we instead reuse a model trained to generate captions for images as the basis for our method. We apply brain activity data, instead of images, to the image caption-generation frameworks pro- posed by <ref type="bibr" target="#b31">Vinyals et al.(2015)</ref> and <ref type="bibr" target="#b32">Xu et al.(2015)</ref> to generate natural language descriptions express- ing the contents of the brain activity. In this way, we aim to achieve a quantitative analysis of brain activities through language representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Language representation estimated from brain activity</head><p>In recent years, in the field of brain and neuro- science, the quantitative analysis of what a human recalls using brain activity data observed via func- tional magnetic resonance imaging (fMRI) while he or she watches motion pictures has been ac- tively studied ( <ref type="bibr" target="#b21">Mitchell et al., 2008;</ref><ref type="bibr" target="#b24">Nishimoto et al., 2011;</ref><ref type="bibr">Pereira et al., 2013;</ref><ref type="bibr" target="#b11">Huth et al., 2012;</ref><ref type="bibr" target="#b28">Stansbury et al., 2013;</ref><ref type="bibr" target="#b10">Horikawa et al., 2013</ref>  <ref type="bibr" target="#b1">Blei et al., 2003)</ref> to assign semantic labels to still pictures using nat- ural language descriptions synchronized with the pictures and discussed the resulting relationship between the visual stimulus evoked by the still pic- tures and brain activity. Based on these relation- ships, they have built a model that classifies brain activity into semantic categories, revealing the ar- eas of the brain that deal with particular categories. <ref type="bibr" target="#b5">Cukur et al. (2013)</ref> estimated how a human being semantically changes his or her recognition of ob- jects from the brain activity data in cases where he or she pays attention to objects in a motion pic- ture. As mentioned above, Statistical models ana- lyzing semantic representation in human brain ac- tivity have attracted considerable attention as ap- propriate models to explain higher order cognitive representations based on human sensory or con- textual information. Furthermore, <ref type="bibr" target="#b23">Nishida et al.(2015)</ref> demonstrated that skip-gram, employed in the framework of word2vec proposed by <ref type="bibr" target="#b19">Mikolov (2013)</ref>, is a more appropriate model than the conventional statisti- cal models used for the quantitative analysis of se- mantic representation in human brain activity un- der the same experimental settings as the prior studies. Moreover, they showed that there is a correlation between the distributed semantics, ob- tained by employing skip-gram to build distributed semantic vectors in the framework of word2vec with the Japanese Wikipedia corpus, and brain ac- tivity observed through blood oxygen level depen- dent (BOLD) contrast imaging via fMRI.</p><p>Prior studies have attempted to quantitatively analyze the relationship between semantic cate- gories and human brain activity from the perspec- tive of language representation, especially, the se- mantic categories of words. In this study, we aim to take a step further toward quantitatively analyz- ing this relationship by expressing brain activity with natural language descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Caption generation from images</head><p>Many previous studies on image caption genera- tion have been based on two principal approaches. The first approach is to retrieve existing captions from a large database for a given image by ranking the captions ( <ref type="bibr" target="#b16">Kuznetsova et al., 2012;</ref><ref type="bibr" target="#b17">Kuznetsova et al., 2014;</ref><ref type="bibr" target="#b30">Vendrov et al., 2016;</ref><ref type="bibr" target="#b33">Yagcioglu et al., 2015)</ref>. The second approach is to fill sen- tence templates based on the features extracted from a given image, such as objects and spatial relationships <ref type="bibr" target="#b7">(Elliott and Keller, 2013;</ref><ref type="bibr" target="#b8">Elliott and Vries, 2015;</ref><ref type="bibr" target="#b15">Kulkarni et al., 2013;</ref><ref type="bibr" target="#b22">Mitchell et al., 2012)</ref>. Although these approaches can produce ac- curate descriptions, they are neither flexible nor natural descriptions such as the ones written by humans. Recently, multiple methods proposed for generating captions for images have been de- veloped based on the encoder-decoder (enc-dec) framework ( <ref type="bibr" target="#b2">Cho et al., 2014;</ref>, which is typically used for media transforma- tion <ref type="bibr" target="#b4">(Chorowski, 2015)</ref>, e.g., machine transla- tion ( <ref type="bibr" target="#b29">Sutskever et al., 2014;</ref><ref type="bibr" target="#b2">Cho et al., 2014;</ref><ref type="bibr" target="#b13">Kiros et al., 2014;</ref>, to generate captions for images ( <ref type="bibr" target="#b6">Donahue et al., 2015;</ref><ref type="bibr" target="#b18">Mao et al., 2014;</ref><ref type="bibr" target="#b31">Vinyals et al., 2015)</ref>.</p><p>In the enc-dec framework, by combining two deep neural network models functioning as an en- coder and a decoder, the enc-dec model first en- codes input information into an intermediate ex- pression and then decodes it into an expression in a different modality than that of the input informa- tion. <ref type="bibr" target="#b31">Vinyals et al. (2015)</ref> achieved caption gen- eration for images by building a enc-dec network employing GoogLeNet ( <ref type="bibr" target="#b12">Ioffe and Szegedy, 2015)</ref>, which works effectively to extract the features of images, as the encoder, and Long Short-Term Memory Language Model (LSTM-LM) <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b29">Sutskever et al., 2014</ref>), which is a deep neural language model, as the decoder. <ref type="bibr" target="#b32">Xu et al. (2015)</ref> proposed a model using the Attention Mechanism (  and demonstrated that the model achieved high precision when generating captions. Attention Mechanism is a system that automatically learns to pay attention to different parts of the input for each element of the output ( <ref type="bibr" target="#b34">Yao et al., 2015)</ref>.</p><p>In our study, we provide an enc-dec network with brain activity data as input, instead of an im- age, and attempt to generate natural language de- scriptions for this data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>First, the process to generate captions for images using deep neural networks, employed in <ref type="bibr" target="#b31">Vinyals et al.(2015)</ref> and <ref type="bibr" target="#b32">Xu et al.(2015)</ref>, works as follows:</p><p>Step 1. Encoder: Extraction of features using VGGNet</p><p>The encoder VGGNet (Simonyan and Zisserman, 2015), a pre-trained deep convolutional network, ex- tracts the features from the input image. In the case with Attention Mechanism, the output of the encoder is 512×14×14 dimensional data from the intermediate convolutional layer of VGGNet. In the case without Attention Mechanism, the output is 4,096 dimensional data from the last fully-connected layer of VGGNet.</p><p>Step 2. Process for intermediate expression In the case with Attention Mechanism, the weighted sum of the set of the intermediate expressions calcu- lated in Step 1 is computed as the input for the decoder.</p><p>The weighted coefficients are learned by means of a multi-layered perceptron based on the hidden states of the decoder at the previous time step and 512 interme- diate expressions. In the case without Attention Mech- anism, the output of the encoder from Step 1 is just the input of the decoder in Step 3.</p><p>Step 3. Decoder: Word estimation by LSTM-LM The LSTM-LM decoder predicts the next word from the intermediate expression produced in Step 2 and the hidden states of LSTM at the previous time step.</p><p>Step 4. Caption generation by iterative word estimation A caption is generated by estimating the words one-by- one repeating Steps 2 and 3 until either the length of the sentence exceeds the predefined maximum or the terminal symbol of a sentence is output.</p><p>This study aims to generate natural language sentences that describe the events a human be- ing calls to mind from the human brain activ- ity input data observed by fMRI via the above caption-generation process. <ref type="figure" target="#fig_0">Figures 1 and 2</ref> show overviews of our methods with and without Atten- tion Mechanism, respectively. In essence, we train a simple model, a 3-layered perceptron (multi- layered perceptron; MLP) or ridge regression model, to learn the corresponding relationships between the cerebral nerve activity data stimulated by the input images and the features of the same image extracted by VGGNet, namely, the interme- diate expression as for the image caption genera- tor. The model replaces VGGNet as the encoder when brain activity data are used as input infor- mation instead of images. Then, the rest of the process to generate captions is the same as that of the above image caption generator. The process of the proposed method is as follows:</p><p>Step 1. Encode brain activity to an intermediate expression</p><p>The model, which is pre-trained to learn the mapping from the brain activity data stimulated by an image to the features extracted from the same image by VG- GNet, maps the input brain data to an intermediate ex- pression.</p><p>Step 2 ∼ 4. The rest of the process is the same as above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this study, we conducted three experiments, un- der the conditions shown in <ref type="table" target="#tab_1">Table 2</ref>, using the model of caption generation for images and the model to learn the corresponding relationships be- tween the brain activity data and the features ob- tained from VGGNet. The model for Exp.1 is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, and the models for both Exps.2 and 3 are illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental settings</head><p>We employed Chainer 1 as the deep-learning framework. We used Microsoft COCO 2 , which contains 414,113 pairs of data with still pictures and natural language descriptions of their con- tents, as the training data for the building caption- generation model. In this study, we have so far been able to train the network with 168,000 pairs of the total dataset in the below experiments. We employed the brain activity data of a sub- ject being stimulated by motion pictures (Nishi- moto et al., 2011) as the data for training and eval- uation. In the experiments, we used BOLD sig- nals observed every 2s via fMRI while the sub- ject was watching motion pictures as the brain ac- tivity data, and the still pictures extracted from the motion pictures were synchronized with the brain data. The brain activity data were ob- served throughout the brain and were recorded in 100(x)×100(y)×32(z) voxels. We employed 30,662 voxels corresponding to only the cerebral cortex region, which is the area of the whole brain, in the above observed voxels as input brain data (see, <ref type="figure" target="#fig_2">Figure 3</ref>). In the Exp.1, the multi-layered perceptron learns the corresponding relationships between the input 30,662 dimensional brain data and the 14×14×512=100,352 dimensional data of the intermediate layer of VGGNet. In Exps.2 and 3, the 4,096 dimensional feature vector output by VGGNet is the target that needs to be correlated with the brain activity data. We have only 3,600 training brain activity data which are too small to train deep neural networks, so we have applied a pre-trained deep neural image caption generator to the task for describing brain activity caused by vi- sual stimulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Exp.1: With Attention Mechanism and 3-layered MLP</head><p>First, we confirmed that our caption-generation model with the Attention Mechanism was well trained and had learned "attention" by generat- ing captions and visualizing the attention for two pictures randomly selected from the COCO test dataset, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. <ref type="figure" target="#fig_4">Figure 5</ref> shows two sets of still pictures and the generated descrip- tions for the brain activity data selected from the test dataset. <ref type="table" target="#tab_3">Table 3</ref> shows the perplexity of the generated captions for the COCO images and the    mean square error in the training process of the 3- layered MLP; the decreasing values of both quan- tities indicates the training progress. The generated sentences for Exp.1 are not grammatically correct -they primarily comprise unsuitable meaningless prepositions and do not explain the contexts of the images. As for the eval- uation of learning the model, the mean square er- ror did not decrease sufficiently. This is proba- bly caused by the fact that the output dimensions (100,352) are too large compared with the input dimensions <ref type="bibr">(30,</ref><ref type="bibr">662)</ref> to learn the corresponding re- lationships between the brain activity data and the set of intermediate expressions.   Using the same procedure as Exp.1, we confirmed that our caption-generation model without Atten- tion Mechanism was well trained by generating captions for two pictures. <ref type="figure" target="#fig_5">Figure 6</ref> shows two sets of still pictures and the generated descriptions for the brain activity data. <ref type="table" target="#tab_4">Table 4</ref> shows the perplexity of the generated im- age captions and the mean square error of MLP.</p><p>Relative to the result of Exp.1, the meaningless prepositions disappear and the generated words seem to depict the image, that is, our model ac- quires more appropriate expressions, both syntac- tically and semantically. This is probably because MLP could learn the relationship better by reduc- ing the output dimension from 100,352 to 4,096; we can confirm this by looking at the decrease in the mean square error.   <ref type="figure" target="#fig_6">Figure 7</ref> shows two sets of pictures and descrip- tions for the brain activity data selected from the test data. The perplexity of the generated captions for the images is the same as in Exp.2, and the mean square error using ridge regression is 8.675. The generated sentences are syntactically estab- lished, that is, prepositions, e.g., "in" and "on," and articles, e.g., "an," are precisely used. Com- pared with the results of Exps.1 and 2, we can see that the grammar of the descriptions has consid- erably improved. Except for the subjects in the descriptions, the contents of the images are cor- rectly described. In particular, in the descriptions of the second image, an appropriate description of the image is generated, as we see that the person and umbrella are both recognized and their rela- tionship is correctly described. In addition, Exp.3 had the lowest mean square error.</p><p>In these three experiments, we confirmed that the methods without Attention Mechanism per- form better than that with Attention Mechanism and that ridge regression produces better results than 3-layered perceptron. Therefore, we can con- clude that a simple method that can avoid over- fitting the data is more appropriate for noisy and small data, such as brain activity data. However, in Exp.2, if we trained the network with more datasets, this result might be changed because we have observed that the mean square error of MLP has been decreasing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We proposed a method to generate descriptions of brain activity data by employing a framework to generate captions for still pictures using deep neu- ral networks and by learning the corresponding re- lationships between the brain activity data and the features of the images extracted by VGGNet. We conducted three experiments to confirm our pro- posed method. We found that the model without Attention Mechanism using ridge regression per- formed the best in our experimental settings. In the future, we aim to increase the accuracy of our method to generate captions by revising the pa- rameter settings, using additional training data and introducing evaluation measures, such as BLEU and METEOR. Moreover, we will further consider ways to learn the relationship between brain activ- ity data and the intermediate expression and will introduce Bayesian optimization to optimize the parameter settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our approach (Exp.1: With Attention Mechanism and 3-Layered MLP).</figDesc><graphic url="image-1.png" coords="4,72.00,66.00,453.57,156.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of our approach (Exps.2 and 3: Without Attention Mechanism and with 3-Layered MLP or Ridge regression).</figDesc><graphic url="image-2.png" coords="4,72.00,246.42,453.57,156.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: 30,662 voxels observed as the cerebral cortex region.</figDesc><graphic url="image-3.png" coords="4,98.98,569.75,396.77,187.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Caption generation with Attention Mechanism: target picture (left), generated caption (center), and attention (right).</figDesc><graphic url="image-4.png" coords="5,80.56,264.85,198.40,111.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Exp.1:Presented stimuli and the descriptions generated from the evoked brain activity.</figDesc><graphic url="image-5.png" coords="5,80.56,433.65,198.53,112.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Exp.2:Presented stimuli and the descriptions generated from the evoked brain activity.</figDesc><graphic url="image-6.png" coords="5,315.83,249.59,198.53,112.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Exp.3:Presented stimuli and the descriptions generated from the evoked brain activity.</figDesc><graphic url="image-7.png" coords="6,80.56,237.02,198.47,102.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : The experimental setting.</head><label>2</label><figDesc></figDesc><table>Exp. 
Image to Caption 
Brain to Intermediate 
Exp.1 Attention Mechanism 
3-layered MLP 
Exp.2 
Without 
(Neural Network) 
Exp.3 Attention Mechanism 
Ridge regression 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Details of the experimental settings. 

Exp.1: Image to Caption Exps.2 and 3: Image to Caption Exp.1: Brain to Intermediates Exp.2: Brain to Intermediate Exp.3: Brain to Intermediate 3 
with Attention 
without Attention 
3-Layered MLP 
3-Layered MLP 
Ridge regression 
Dataset 
Microsoft COCO 
brain activity data 
learning rate : 1.0 ( 0.999) 
learning rate : 0.01 
Hyper-parameters 
gradient norm threshold : 5 
gradient norm threshold : 5 
L2-norm : 0.5 
L2-norm : 0.005 
L2-norm : 0.005 
Learned parameters 
Attention &amp; LSTM 
LSTM 
weight in 3-Layered MLP 
parameters in Ridge reg. 
initialized in [-0.1,0.1] 
initialized in [-0.1,0.1] 
initialized in [-0.2,0.2] 
initialized to 0 
Units per layer 
196 units 
1,000 units 
30,662 -1,000 -100,352 
30,662 -1,000 -4,096 
-
Vocabulary 
Frequent 3,469 words (512-dim vector) 
-
Algorithm 
stochastic gradient descent 
stochastic gradient descent 
ridge regression 
Loss function 
cross entropy 
mean squared error 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Exp.1: Training evaluation.</head><label>3</label><figDesc></figDesc><table>Num. of data Perplexity Iteration MSE 
14000 
88.67 
1 
118.32 
42000 
66.24 
5 
116.44 
84000 
60.40 
10 
114.31 
126000 
60.10 
15 
112.36 
168000 
60.32 
16 
112.01 

4.3 Exp.2: Without Attention Mechanism 
and with 3-Layered MLP 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Exp.2: Training evaluation. 

Num. of data Perplexity Iteration MSE 
14000 
96.50 
1 
28.95 
42000 
47.87 
5 
22.70 
84000 
47.22 
10 
17.19 
126000 
47.37 
15 
13.37 
168000 
46.30 
20 
10.76 

4.4 Exp.3: Without Attention Mechanism 
and with Ridge regression 

</table></figure>

			<note place="foot" n="1"> http://chainer.org/ 2 http://mscoco.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Describing Multimedia Content using Attention based Encoder Decoder Networks. Multimedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1875" to="1886" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1506.07503</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention during natural vision warps semantic representation across the human brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cukur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nishimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="763" to="770" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term Recurrent Convolutional Networks for Visual Recognition and Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image description using visual dependency representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;13</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Describing Images using Inferred Visual Dependency Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long ShortTerm Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Horikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tamaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miyawaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kamitani</surname></persName>
		</author>
		<title level="m">Neural Decoding of Visual Imagery During Sleep. SCIENCE</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">340</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A continuous semantic space describes the representation of thousands of object and action categories across the human brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Huth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nishimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1210" to="1224" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;15 Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Babytalk: Understanding and generating simple image descriptions. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2891" to="2903" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Collective generation of natural image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;12</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TREETALK: Composition and compression of trees for image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<title level="m">Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN). In ICLR&apos;14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;13</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">WordNet: a lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Predicting Human Brain Activity Associated with the Meanings of Nouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Shinkareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">L</forename><surname>Malave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Just</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">320</biblScope>
			<biblScope unit="page">1191</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Midge: Generating image descriptions from computer vision detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>ACL&apos;12</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Word statistics in large-scale texts explain the human cortical semantic representation of objects, actions, and impressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Huth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nishimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Society for Neuroscience Annual Meeting</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reconstructing visual experiences from brain activity evoked by natural movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nishimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Naselaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="1641" to="1646" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generating text from functional brain images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Detre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Human Neuroscience</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Article 72</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using Wikipedia to learn semantic feature representations of concrete concepts in neuroimaging experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereiraa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinicka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Detre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="240" to="252" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Natural Scene Statistics Account for the Representation of Scene Categories in Human Visual Cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Stansbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Naselaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="1025" to="1034" />
			<date type="published" when="2013-09-04" />
			<publisher>Elsevier Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In NIPS&apos;14</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Order-Embeddings of Images and Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR&apos;16</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Distributed Representation Based Query Expansion Approach for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yagcioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cakici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
