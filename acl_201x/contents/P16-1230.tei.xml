<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Milica</roleName><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaši´</forename><surname>Gaši´c</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><forename type="middle">Mrkši´</forename><surname>Mrkši´c</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Rojas-Barahona</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2431" to="2441"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The ability to compute an accurate reward function is essential for optimising a dialogue policy via reinforcement learning. In real-world applications, using explicit user feedback as the reward signal is often unreliable and costly to collect. This problem can be mitigated if the user&apos;s intent is known in advance or data is available to pre-train a task success predictor off-line. In practice neither of these apply for most real world applications. Here we propose an on-line learning framework whereby the dialogue policy is jointly trained alongside the reward model via active learning with a Gaussian process model. This Gaussian process operates on a continuous space dialogue representation generated in an unsupervised fashion using a recurrent neural network encoder-decoder. The experimental results demonstrate that the proposed framework is able to significantly reduce data annotation costs and mitigate noisy user feedback in dialogue policy learning.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Spoken Dialogue Systems (SDS) allow human- computer interaction using natural speech. They can be broadly divided into two categories: chat- oriented systems which aim to converse with users and provide reasonable contextually relevant re- sponses ( <ref type="bibr">Vinyals and Le, 2015;</ref><ref type="bibr" target="#b27">Serban et al., 2015)</ref>, and task-oriented systems designed to as- sist users to achieve specific goals (e.g. find ho- tels, movies or bus schedules) ( <ref type="bibr">Daubigney et al., 2014;</ref>). The latter are typi- cally designed according to a structured ontology (or a database schema), which defines the domain <ref type="figure">Figure 1</ref>: An example of a task-oriented dialogue with a pre-defined task and the evaluation results.</p><p>that the system can talk about. Teaching a system how to respond appropriately in a task-oriented SDS is non-trivial. This dialogue management task is often formulated as a manually defined di- alogue flow that directly determines the quality of interaction. More recently, dialogue management has been formulated as a reinforcement learning (RL) problem which can be automatically opti- mised ( <ref type="bibr" target="#b17">Levin and Pieraccini, 1997;</ref><ref type="bibr">Roy et al., 2000;</ref><ref type="bibr" target="#b37">Williams and Young, 2007;</ref>. In this framework, the system learns by a trial and error process governed by a poten- tially delayed learning objective defined by a re- ward function.</p><p>A typical approach to defining the reward func- tion in a task-oriented dialogue system is to ap- ply a small per-turn penalty to encourage short dialogues and to give a large positive reward at the end of each successful interaction. <ref type="figure">Figure 1</ref> is an example of a dialogue task which is typi- cally set for users who are being paid to converse with the system. When users are primed with a specific task to complete, dialogue success can be determined from subjective user ratings (Subj), or an objective measure (Obj) based on whether or not the pre-specified task was completed ( <ref type="bibr" target="#b36">Walker et al., 1997;</ref><ref type="bibr" target="#b11">Gaši´Gaši´c et al., 2013)</ref>. However, prior knowledge of the user's goal is not normally avail- able in real situations, making the objective reward estimation approach impractical.</p><p>Furthermore, objective ratings are inflexible and often fail as can be seen from <ref type="figure">Figure 1</ref>, if the user does not strictly follow the task. This re- sults in a mismatch between the Obj and Subj rat- ings. However, relying on subjective ratings alone is also problematic since crowd-sourced subjects frequently give inaccurate responses and real users are often unwilling to extend the interaction in or- der to give feedback, resulting in unstable learning ( <ref type="bibr" target="#b41">Zhao et al., 2011;</ref><ref type="bibr" target="#b10">Gaši´Gaši´c et al., 2011</ref>). In order to filter out incorrect user feedback, Gaši´ <ref type="bibr" target="#b11">Gaši´c et al. (2013)</ref> used only dialogues for which Obj = Subj. Nonetheless, this is inefficient and not feasible anyway in most real-world tasks where the user's goal is generally unknown and difficult to infer.</p><p>In light of the above, <ref type="bibr" target="#b29">Su et al. (2015a)</ref> pro- posed learning a neural network-based Obj esti- mator from off-line simulated dialogue data. This removes the need for the Obj check during on- line policy learning and the resulting policy is as effective as one trained with dialogues using the Obj = Subj check. However, a user simulator will only provide a rough approximation of real user statistics and developing a user simulator is a costly process ( <ref type="bibr" target="#b26">Schatzmann et al., 2006</ref>).</p><p>To deal with the above issues, this paper de- scribes an on-line active learning method in which users are asked to provide feedback on whether the dialogue was successful or not. However, active learning is used to limit requests for feedback to only those cases where the feedback would be use- ful, and also a noise model is introduced to com- pensate for cases where the user feedback is inac- curate. A Gaussian process classification (GPC) model is utilised to robustly model the uncertainty presented by the noisy user feedback. Since GPC operates on a fixed-length observation space and dialogues are of variable-length, a recurrent neu- ral network (RNN)-based embedding function is used to provide fixed-length dialogue representa- tions. In essence, the proposed method learns a di- alogue policy and a reward estimator on-line from scratch, and is directly applicable to real-world ap- plications.</p><p>The rest of the paper is organised as follows.</p><p>The next section gives an overview of related work. The proposed framework is then described in §3. This consists of the policy learning al- gorithm, the creation of the dialogue embedding function and the active reward model trained from real user ratings. In §4, the proposed approach is evaluated in the context of an application provid- ing restaurant information in Cambridge, UK. We first give an in-depth analysis of the dialogue em- bedding space. The results of the active reward model when it is trained together with a dialogue policy on-line with real users are then presented. Finally, our conclusions are presented in §5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Dialogue evaluation has been an active research area since late 90s. <ref type="bibr" target="#b36">Walker et al. (1997)</ref> proposed the PARADISE framework, where a linear func- tion of task completion and various dialogue fea- tures such as dialogue duration were used to in- fer user satisfaction. This measure was later used as a reward function for learning a dialogue pol- icy ( <ref type="bibr" target="#b22">Rieser and Lemon, 2011</ref>). However, as noted, task completion is rarely available when the sys- tem is interacting with real users and also concerns have been raised regarding the theoretical validity of the model <ref type="bibr" target="#b16">(Larsen, 2003)</ref>.</p><p>Several approaches have been adopted for learn- ing a dialogue reward model given a corpus of an- notated dialogues. <ref type="bibr" target="#b38">Yang et al. (2012)</ref> used col- laborative filtering to infer user preferences. The use of reward shaping has also been investigated in <ref type="bibr" target="#b8">(El Asri et al., 2014;</ref><ref type="bibr" target="#b29">Su et al., 2015b</ref>) to en- rich the reward function in order to speed up di- alogue policy learning. Also, <ref type="bibr">Ultes and Minker (2015)</ref> demonstrated that there is a strong correla- tion between expert's user satisfaction ratings and dialogue success. However, all these methods as- sume the availability of reliable dialogue annota- tions such as expert ratings, which in practice are hard to obtain.</p><p>One effective way to mitigate the effects of an- notator error is to obtain multiple ratings for the same data and several methods have been devel- oped to guide the annotation process with uncer- tainty models <ref type="bibr" target="#b7">(Dai et al., 2013;</ref>). Active learning is particularly useful for determin- ing when an annotation is needed <ref type="bibr" target="#b28">(Settles, 2010;</ref><ref type="bibr" target="#b40">Zhang and Chaudhuri, 2015)</ref>. It is often utilised using Bayesian optimisation approaches ( <ref type="bibr" target="#b5">Brochu et al., 2010</ref>). Based on this,  exploited a pool-based active learning method for a robotics application. They queried the user for feedback on the most informative sample collected so far and showed the effectiveness of this method.</p><p>Rather than explicitly defining a reward func- tion, inverse RL (IRL) aims to recover the un- derlying reward from demonstrations of good be- haviour and then learn a policy which maximises the recovered reward <ref type="bibr" target="#b25">(Russell, 1998)</ref>. IRL was first introduced to SDS in <ref type="bibr" target="#b21">(Paek and Pieraccini, 2008)</ref>, where the reward was inferred from human-human dialogues to mimic the behaviour observed in a corpus. IRL has also been studied in a Wizard-of-Oz (WoZ) setting ( <ref type="bibr" target="#b4">Boularias et al., 2010;</ref><ref type="bibr">Rojas Barahona and Cerisara, 2014)</ref>, where typically a human expert served as the dialogue manager to select each system reply based on the speech understanding output at different noise lev- els. However, this approach is costly and there is no reason to suppose that a human wizard is acting optimally, especially at high noise levels.</p><p>Since humans are better at giving relative judge- ments than absolute scores, another related line of research has focused on preference-based ap- proaches to RL <ref type="bibr" target="#b7">(Cheng et al., 2011</ref>). In ( <ref type="bibr">Sugiyama et al., 2012)</ref>, users were asked to provide rankings between pairs of dialogues. However, this is also costly and does not scale well in real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Framework</head><p>The proposed system framework is depicted in <ref type="figure">Figure 2</ref>. It is divided into three main parts: a dia- logue policy, a dialogue embedding function, and an active reward model of user feedback. When each dialogue ends, a set of turn-level features f t is extracted and fed into an embedding function σ to obtain a fixed-dimension dialogue representa- tion d that serves as the input space of the reward model R. This reward is modelled as a Gaussian process which for every input point provides an es- timate of task success along with a measure of the estimate uncertainty. Based on this uncertainty, R decides whether to query the user for feedback or not. It then returns a reinforcement signal to up- date the dialogue policy π, which is trained us- ing the GP-SARSA algorithm <ref type="bibr" target="#b9">(Gaši´Gaši´c and Young, 2014</ref>). GP-SARSA also deploys Gaussian process estimation to provide an on-line sample-efficient reinforcement learning algorithm capable of boot- strapping estimates of sparse value functions from minimal numbers of samples (dialogues). The quality of each dialogue is defined by its cumu- lative reward, where each dialogue turn incurs a small negative reward (-1) and the final reward of either 0 or 20 depending on the estimate of task success are provided by the reward model. Note that the key contribution here is to learn the noise robust reward model and the dialogue policy simultaneously on-line, using the user as a 'supervisor'. Active learning is not an essential component of the framework but highly desirable in practice to minimise the impact of the supervi- sion burden on users. The use of a pre-trained em- bedding function is a sub-component of the pro- posed approach and is trained off-line on corpus data rather than manually designed here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unsupervised Dialogue Embeddings</head><p>In order to model user feedback over dialogues of varying length, an embedding function is used to map each dialogue into a fixed-dimensional continuous-space. The use of embedding func- tions has recently gained attention especially for word representations, and has boosted perfor- mance on several natural language processing tasks ( <ref type="bibr" target="#b19">Mikolov et al., 2013;</ref><ref type="bibr">Levy and Goldberg, 2014</ref>). Embedding has also been successfully applied to machine translation (MT) where it enables varying-length phrases to be mapped to fixed-length vectors using an RNN Encoder-Decoder ( <ref type="bibr" target="#b7">Cho et al., 2014</ref>). Similar to MT, dialogue embedding enables variable length sequences of utterances to be mapped into an ap- propriate fixed-length vector. Although embed- ding is used here to create a fixed-dimension input space for the GPC-based task success classifier, it should be noted that it potentially facilitates a va- riety of other downstream tasks which depend on classification or clustering.</p><p>The model structure of the embedding func- tion is described on the left of <ref type="figure">Figure 2</ref>, where the episodic turn-level features f t are extracted from a dialogue and serve as the input features to the encoder. In our proposed model, the en- coder is a Bi-directional Long Short-Term Mem- ory network (BLSTM) (Hochreiter and Schmid- huber, 1997; <ref type="bibr" target="#b12">Graves et al., 2013</ref>). The LSTM is a Recurrent Neural Network (RNN) with gated re- current units introduced to alleviate the vanishing gradient problem. The BLSTM encoder takes into account the sequential information from both di- rections of the input data, computing the forward <ref type="figure">Figure 2</ref>: Schematic of the system framework. The three main system components dialogue policy, dialogue embedding creation, and reward modelling based on user feedback, are described in §3.</p><p>hidden sequences − → h 1:T and the backward hidden sequences ← − h T :1 while iterating over all input fea- tures f t , t = 1, ..., T :</p><formula xml:id="formula_0">− → h t = LST M (f t , − → h t−1 ) ← − h t = LST M (f t , ← − h t+1 )</formula><p>where LST M denotes the activation function. The dialogue representation d is then calculated as the average over all hidden sequences:</p><formula xml:id="formula_1">d = 1 T T t=1 h t<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">h t = [ − → h t ; ← − h t ]</formula><p>is the concatenation of the two directional hidden sequences.</p><p>Given the dialogue representation d output by the encoder, the decoder is a forward LSTM that takes d as its input for each turn t to produce the sequence of features f 1:T .</p><p>The training objective of the encoder-decoder minimises the mean-square-error (MSE) between the prediction f 1:T and the output f 1:T (which is also the input):</p><formula xml:id="formula_3">M SE = 1 N N i=1 T t=1 ||f t − f t || 2 (2)</formula><p>where N is the number of training dialogues and || · || 2 denotes the l 2 -norm. Since all the functions used in the encoder and decoder are differentiable, stochastic gradient decent (SGD) can be used to train the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Active Reward Learning</head><p>A Gaussian process is a Bayesian non-parametric model that can be used for regression or classifi- cation ( <ref type="bibr">Rasmussen and Williams, 2006</ref>). It is par- ticularly appealing since it can learn from a small number of observations by exploiting the correla- tions defined by a kernel function and it provides a measure of uncertainty of its estimates. In the con- text of spoken dialogue systems it has been suc- cessfully used for RL policy optimisation <ref type="bibr" target="#b9">(Gaši´Gaši´c and Young, 2014;</ref><ref type="bibr">Casanueva et al., 2015)</ref> and IRL reward function regression ( <ref type="bibr" target="#b15">Kim et al., 2014</ref>).</p><p>Here we propose modelling dialogue success as a Gaussian process (GP). This involves estimating the probability p(y|d, D) that the task was suc- cessful given the current dialogue representation d and the pool D containing previously classi- fied dialogues. We pose this as a classification problem where the rating is a binary observation y ∈ {−1, 1} that defines failure or success. The observations y are considered to be drawn from a Bernoulli distribution with a success probabil- ity p(y = 1|d, D). The probability is related to a latent function f (d|D) : R dim(d) → R that is mapped to a unit interval by a probit function p(y = 1|d, D) = φ(f (d|D)), where φ denotes the cumulative density function of the standard Gaus-sian distribution.</p><p>The latent function is given a GP prior:</p><formula xml:id="formula_4">f (d) ∼ GP(m(d), k(d, d ))</formula><p>, where m(·) is the mean function and k(·, ·) the covariance function (ker- nel). Here the stationary squared exponential ker- nel k SE is used. It is also combined with a white noise kernel k W N in order to account for the "noise" in users' ratings:</p><formula xml:id="formula_5">k(d, d ) = p 2 exp(− ||d − d || 2 2l 2 ) + σ 2 n<label>(3)</label></formula><p>where the first term denotes k SE and the second term k W N .</p><p>The hyper-parameters p, l, σ n can be ade- quately optimised by maximising the marginal likelihood using a gradient-based method <ref type="bibr" target="#b6">(Chen et al., 2015)</ref>. Since φ(·) is not Gaussian, the resulting posterior probability p(y = 1|d, D) is analytically intractable. So instead an approximation method, expectation propagation (EP), was used ( <ref type="bibr" target="#b20">Nickisch and Rasmussen, 2008</ref>).</p><p>Querying the user for feedback is costly and may impact negatively on the user experience. This impact can be reduced by using active learn- ing informed by the uncertainty estimate of the GP model ( <ref type="bibr" target="#b14">Kapoor et al., 2007)</ref>. This ensures that user feedback is only sought when the model is uncer- tain about its current prediction. For the current application, an on-line (stream-based) version of active learning is required.</p><p>An illustration of a 1-dimensional example is shown in <ref type="figure">Figure 3</ref>. Given the labelled data D, the predictive posterior mean µ * and posterior vari- ance σ 2 * of the latent value f (d * ) for the current di- alogue representation d * can be calculated. Then a threshold interval <ref type="bibr">[1 − λ, λ]</ref> is set on the pre- dictive success probability p(</p><formula xml:id="formula_6">y * = 1|d * , D) = φ(µ * / 1 + σ 2 * )</formula><p>to decide whether this dialogue should be labelled or not. The decision bound- ary implicitly considers both the posterior mean as well as the variance.</p><p>When deploying this reward model in the pro- posed framework, a GP with a zero-mean prior for f is initialised and D = {}. After the dialogue policy π completes each episode with the user, the generated dialogue turns are transformed into the dialogue representation d = σ(f 1:T ) using the dia- logue embedding function σ. Given d, the predic- tive mean and variance of f (d|D) are determined, and the reward model decides whether or not it should seek user feedback based on the threshold λ on φ(f (d|D)). If the model is uncertain, the user's feedback on the current episode d is used to update the GP model and to generate the rein- forcement signal for training the policy π; other- wise the predictive success rating from the reward model is used directly to update the policy. This process takes place after each dialogue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results</head><p>The target application is a live telephone-based spoken dialogue system providing restaurant in- formation for the Cambridge (UK) area. The do- main consists of approximately 150 venues each having 6 slots (attributes) of which 3 can be used by the system to constrain the search (food-type, area and price-range) and the remaining 3 are in- formable properties (phone-number, address and postcode) available once a required database en- tity has been found.</p><p>The shared core components of the SDS com- mon to all experiments comprise a HMM-based recogniser, a confusion network (CNet) semantic input decoder <ref type="figure">(Henderson et al., 2012)</ref>, the BUDS belief state tracker <ref type="bibr" target="#b31">(Thomson and Young, 2010</ref>) that factorises the dialogue state using a dynamic Bayesian network, and a template based natural language generator to map system semantic ac- tions into natural language responses to the user. All policies were trained using the GP-SARSA al- gorithm and the summary action space of the RL policy contains 20 actions.</p><p>The reward given to each dialogue was set to 20 × 1 success − N , where N is the dialogue turn number and 1 is the indicator function for dia- logue success, which is determined by different methods as described in the following section. These rewards constitute the reinforcement signal used for policy learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dialogue representations</head><p>The LSTM Encoder-Decoder model described in §3.1 was used to generate an embedding d for each dialogue. For each dialogue turn that con- tains a user's utterance and a system's response, a feature vector f of size 74 was extracted ( <ref type="bibr" target="#b34">Vandyke et al., 2015)</ref>. This vector consists of the concate- nation of the most likely user intention determined by the semantic decoder, the distribution over each concept of interest defined in the ontology, a one- hot encoding of the system's reply action, and the turn number normalised by the maximum number of turns (here 30). This feature vector was used as the input and the target for the LSTM Encoder- Decoder model, where the training objective was to minimise the MSE of the reconstruction loss.</p><p>The model was implemented using the Theano library ( <ref type="bibr" target="#b3">Bergstra et al., 2010;</ref><ref type="bibr" target="#b2">Bastien et al., 2012)</ref>. A corpus consisting of 8565, 1199 and 650 real user dialogues in the Cambridge restaurant do- main was used for training, validation and test- ing respectively. This corpus was collected via the Amazon Mechanical Turk (AMT) service, where paid subjects interacted with the dialogue system. The sizes of − → h t and ← − h t in the encoder and the hid- den layer in the decoder were all 32, resulting in dim(h t ) = dim(d) = 64. SGD per dialogue was used during backpropagation to train each model. In order to prevent over-fitting, early stopping was applied based on the held-out validation set.</p><p>In order to visualise the impact of the embed- dings, the dialogue representations of all the 650 test dialogues were transformed by the embedding function in <ref type="figure" target="#fig_2">Figure 4</ref> and reduced to two dimen- sions using t-SNE (Van der Maaten and Hinton, 2008). For each dialogue sample, the shape indi- cates whether or not the dialogue was successful, and the colour indicates the length of the dialogue (maximum 30 turns).</p><p>From the figure we can clearly see the colour gradient from the top left (shorter dialogues) to the bottom right (longer dialogues) for the positive Subj labels. This shows that dialogue length was one of the prominent features in the dialogue rep- resentation d. It can also be seen that the longer On the other hand, there are other failed dialogues which are spread throughout the cluster. We can also see that the successful dialogues were on av- erage shorter than 10 turns, which is consistent with the claim that users do not engage in longer dialogues with well-trained task-oriented systems.</p><p>This visualisation shows the potential of the un- supervised dialogue embedding since the trans- formed dialogue representations appear to be cor- related with dialogue success in the majority of cases. For the purpose of GP reward modelling, this LSTM Encoder-Decoder embedding function appears therefore to be suitable for extracting an adequate fixed-dimension dialogue representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dialogue Policy Learning</head><p>Given the well-trained dialogue embedding func- tion, the proposed GP reward model operates on this input space. The system was implemented us- ing the GPy library <ref type="figure">(Hensman et al., 2012</ref>). Given the predictive success probability of each newly seen dialogue, the threshold λ for the uncertainty region was initially set to 1 to encourage label querying and annealed to 0.85 for the first 50 col- lected dialogues and then set to 0.85 thereafter.</p><p>Initially, as each new dialogue was added to the training set, the hyper-parameters that defined the structure of the kernels mentioned in Eqn. 3 were optimised to minimise the negative log marginal likelihood using conjugate gradient as- cent <ref type="bibr">(Rasmussen and Williams, 2006</ref>). To pre- vent overfitting, after the first 40 dialogues, these hyper-parameters were only re-optimised after ev- ery batch of 20 dialogues.</p><p>To investigate the performance of the proposed on-line GP policy learning, three other contrast- ing systems were also tested. Note that the hand- crafted system is not compared since it does not scale to larger domains and is sensitive to speech recognition errors. In each case, the only differ- ence was the method used to compute the reward:</p><p>• the Obj=Subj system which uses prior knowl- edge of the task to only use training dialogues for which the user's subjective assessment of success is consistent with the objective as- sessment of success as in <ref type="bibr" target="#b11">(Gaši´Gaši´c et al., 2013</ref>).</p><p>• the Subj system which directly optimises the policy using only the user assessment of suc- cess whether accurate or not.</p><p>• the off-line RNN system that uses 1K simu- lated data and the corresponding Obj labels to train an RNN success estimator as in ( <ref type="bibr" target="#b29">Su et al., 2015a</ref>).</p><p>For the Subj system rating, in order to focus solely on the performance of the policy rather than other aspects of the system such as the fluency of the reply sentence, users were asked to rate dia- logue success by answering the following ques- tion: Did you find all the information you were looking for? <ref type="figure">Figure 6</ref>: The number of times each system queries the user for feedback during on-line policy optimisation as a function of the number of train- ing dialogues. The orange line represents both the Obj=Subj and Subj systems, and the black line rep- resents the on-line GP system.</p><p>All four of the above systems were trained with a total of 500 dialogues on-line by users recruited via the AMT service. <ref type="figure" target="#fig_3">Figure 5</ref> shows the on- line learning curve of the subjective success rat- ing when during training. For each system, the moving average was calculated using a window of 150 dialogues. In each case, three distinct poli- cies were trained and the results were averaged to reduce noise.</p><p>As can be seen, all four systems perform better than 80 % subjective success rate after approxi- mately 500 training dialogues. The Obj=Subj sys- tem is relatively poor compared to the others. This might be because users often report success even though the objective evaluation indicates failure. In such cases, the dialogue is discarded and not used for training. As a consequence, the Obj=Subj system required approximately 700 dialogues in order to obtain 500 which were useful, whereas all other systems made use of every dialogue.</p><p>To investigate learning behaviour over longer spans, training for the on-line GP and the Subj sys- tems was extended to 850 dialogues. As can be seen, performance in both cases is broadly flat.</p><p>Similar to the conclusions drawn in <ref type="bibr" target="#b10">(Gaši´Gaši´c et al., 2011</ref>), the Subj system suffers from unreliable user feedback. Firstly, as in the Obj=Subj system, users forget the full requirements of the task and in particular, forget to ask for all required infor- mation. Secondly, users give inconsistent feed- back due to a lack of proper care and attention. From <ref type="figure" target="#fig_3">Figure 5</ref> it can be clearly seen that the on- line GP system consistently performed better than Subj system, presumably, because its noise model mitigates the effect of inconsistency in user feed- back. Of course, unlike crowd-sourced subjects, real users might provide more consistent feedback, but nevertheless, some inconsistency is inevitable and the noise model offers the needed robustness.</p><p>The advantage of the on-line GP system in re- ducing the number of times that the system re- quests user feedback (i.e. the label cost) can be seen in <ref type="figure">Figure 6</ref>. The black curve shows the num- ber of active learning queries triggered in the on- line GP system averaged across the three policies. This system required only 150 user feedback re- quests to train a robust reward model. On the other hand, the Obj=Subj and Subj systems require user feedback for every training dialogue as shown by the dashed orange line.</p><p>Of course, the off-line RNN system required no user feedback at all when training the system on- line since it had the benefit of prior access to a user simulator. Its performance during training af- ter the first 300 dialogues was, however, inferior to the on-line GP system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dialogue Policy Evaluation</head><p>In order to compare performance, the averaged re- sults obtained between 400-500 training dialogues are shown in the first section of <ref type="table">Table 1</ref> along with one standard error. For the 400-500 inter- val, the Subj, off-line RNN and on-line GP sys- tems achieved comparable results without statisti- cal differences. The results of continuing training on the Subj and on-line GP systems from 500 to 850 training dialogues are also shown. As can be seen, the on-line GP system was significantly bet- ter presumably because it is more robust to erro- neous user feedback compared to the Subj system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Reward Model Evaluation</head><p>The above results verify the effectiveness of the proposed reward model for policy learning. Here we investigate further the accuracy of the model in predicting the subjective success rate. An eval- uation of the on-line GP reward model between 1 and 850 training dialogues is presented in <ref type="table" target="#tab_0">Table 2</ref>.</p><p>Since three reward models were learnt each with 850 dialogues, there were a total of 2550 training dialogues. Of these, the models queried the user for feedback a total of 454 times, leaving 2096 dialogues for which learning relied on the re- ward model's prediction. The results shown in the table are thus the average over 2096 dialogues. <ref type="table">Table 1</ref>: Subjective evaluation of the Obj=Subj, off-line RNN, Subj and on-line GP system during different stages of on-line policy learning. Sub- jective: user binary rating on dialogue success. Statistical significance was calculated using a two- tailed Students t-test with p-value of 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dialogues Reward Model Subjective (%)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>400-500</head><p>Obj=Subj 85.0 ± 2.1 off-line RNN 89.0 ± 1.8 Subj 90.7 ± 1.7 on-line GP 91.7 ± 1.6 500-850 Subj 87.1 ± 1.0 on-line GP 90.9 ± 0.9* * p &lt; 0.05</p><p>As can be seen, there was a significant imbal- ance between success and fail labels since the pol- icy was improving along with the training dia- logues. This lowered the recall on failed dialogue prediction as the model was biased to data with positive labels. Nevertheless, its precision scores well. On the other hand, the successful dialogues were accurately predicted by the proposed model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Example Dialogues</head><p>The key benefits of the on-line GP reward model compared to other models are its robustness to noise and efficient use of user supervision. Since the four systems compared above differ only in the design of reward model (learning objective), their on-line behaviours were broadly similar. Two example dialogues between users and the on-line GP system are listed in <ref type="table">Table 3</ref> to illustrate how the system operates under different noise con- ditions. The user's subjective rating and the rat- ing determined by the on-line GP reward model are also shown. The labels 'n-th ASR' and 'n- th SEM' indicate the n-th most likely hypotheses from speech recogniser and semantic decoder re- spectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we have proposed an active reward learning model using Gaussian process classifica- tion and an unsupervised neural network-based di- alogue embedding to enable truly on-line policy learning in spoken dialogue systems. The sys- tem enables stable policy optimisation by robustly modelling the inherent noise in real user feedback and uses active learning to minimise the number of feedback requests to the user. We found that the proposed model achieved efficient policy learning and better performance compared to other state- of-the-art methods in the Cambridge restaurant domain. A key advantage of this Bayesian model is that its uncertainty estimate allows active learn- ing and noise handling in a natural way. The unsu- pervised dialogue embedding function required no labelled data to train whilst providing a compact and useful input to the reward predictor. Overall, the techniques developed in this paper enable for the first time a viable approach to on-line learning in deployed real-world dialogue systems which does not need a large corpus of manually anno- tated data or the construction of a user simulator.</p><p>Consistent with all of our previous work, the re- ward function studied here is focused primarily on task success. This may be too simplistic for many commercial applications and further work will be needed in conjunction with human interaction ex- perts to identify and incorporate the extra dimen- sions of dialogue quality that will be needed to achieve the highest levels of user satisfaction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The dialogue representations generated by this LSTM-based unsupervised embedding function are then used as the observations for the reward model described in the next section 3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 : 1 -</head><label>31</label><figDesc>Figure 3: 1-dimensional example of the proposed GP active reward learning model.</figDesc><graphic url="image-4.png" coords="5,311.05,62.81,210.72,206.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: t-SNE visualisation on the unsupervised dialogue representation of the real user data in the Cambridge restaurant domain. Labels are the subjective ratings from the users.</figDesc><graphic url="image-5.png" coords="6,292.57,62.80,247.68,185.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Learning curves showing subjective success as a function of the number of training dialogues used during on-line policy optimisation. The on-line GP, Subj, off-line RNN and Obj=Subj systems are shown as black, yellow, blue, and red lines. The light-coloured areas are one standard error intervals.</figDesc><graphic url="image-6.png" coords="7,64.02,62.81,234.24,135.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistical evaluation of the prediction of 
the on-line GP systems with respect to Subj rating. 

Subj Prec. Recall F-measure Number 
Fail 
1.00 
0.52 
0.68 
204 
Suc. 0.95 
1.00 
0.97 
1892 
Total 0.96 
0.95 
0.95 
2096 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Machine : Thank you for using this system.-Higher noise condition-(User Subjective Evaluation: Success, on-line GP Evaluation: Success) Machine : Hello, welcome to the Cambridge dialogue system</title>
		<idno>2nd SEM] reqalts(</idno>
	</analytic>
	<monogr>
		<title level="m">Example dialogues between on-line users and the proposed on-line GP system.-Low noise condition-(User Subjective Evaluation: Success, on-line GP Evaluation: Success) Machine : Hello, welcome to the Cambridge dialogue system, What kind of food would you like? Human : [Top ASR] Yeah I want to find a restaurant that serves European food</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>Machine : Frankie and Benny&apos;s is a nice place. it is in the south part of town and it is in the expensive price range. Human : [Top ASR] Okay can I have the uh address phone number called for any. [Top SEM] request(addr) [2nd SEM] request(phone,addr</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human : [Top ASR] What is the pub called for free you any</title>
	</analytic>
	<monogr>
		<title level="m">Machine : Frankie and Benny&apos;s is a nice place. Their address is Clifton Way and their phone number is 01223 412430</title>
		<meeting><address><addrLine>B 1, 7 D.Y. Human</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>) Machine : Frankie and Benny&apos;s is a nice place. Top ASR] Thank you goodbye. [Top SEM] bye() Machine : Thank you for using this system</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bergstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference</title>
		<meeting>the Python for Scientific Computing Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning the reward model of dialogue pomdps from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boularias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Machine Learning for Assistive Techniques</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brochu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Cora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1012.2599</idno>
	</analytic>
	<monogr>
		<title level="m">Proc of SigDial</title>
		<editor>Casanueva, Thomas Hain, Heidi Christensen, Ricard Marxer, and Phil Green</editor>
		<meeting>of SigDial</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Knowledge transfer between speakers for personalised dialogue management</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hyper-parameter optimisation of gaussian process reinforcement learning for statistical dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gašic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of SigDial</title>
		<meeting>of SigDial</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kyunghyun Cho, Bart van Merriënboer Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares Holger Schwenk, and Yoshua Bengio</title>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<idno>Dai et al.2013</idno>
	</analytic>
	<monogr>
		<title level="m">Machine learning and knowledge discovery in databases</title>
		<meeting><address><addrLine>Daniel, Malte Viering, Jan Metz, Oliver Kroemer</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Task completion transfer learning for reward inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Laroche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of MLIS</title>
		<meeting>of MLIS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gaussian processes for pomdp-based dialogue manager optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TASLP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="40" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online policy optimisation of spoken dialogue systems via live interaction with human subjects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ASRU</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On-line policy optimisation of bayesian spoken dialogue systems via human interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative spoken language understanding using word confusion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alax</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE SLT</title>
		<editor>IEEE ASRU. [Henderson et al.2012] Matthew Henderson, Milica Gaši´Gaši´c, Blaise Thomson, Pirros Tsiakoulis, Kai Yu, and Steve Young</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Hybrid speech recognition with deep bidirectional lstm</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GPy: A gaussian process framework in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolo</forename><surname>Fusi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Durrande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Zwiessele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<ptr target="http://github.com/SheffieldML/GPy" />
	</analytic>
	<monogr>
		<title level="m">Sepp Hochreiter and Jürgen Schmidhuber</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Active learning with gaussian processes for object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inverse reinforcement learning for micro-turn management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE SLT</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Issues in the evaluation of spoken dialogue systems using objective and subjective measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ASRU</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A stochastic model of computerhuman interaction for learning dialogue strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieraccini1997] Esther</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pieraccini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>Eurospeech. [Levy and Goldberg2014] Omer Levy and Yoav Goldberg</editor>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Neural word embedding as implicit matrix factorization</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">To re (label), or not to re (label)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Christopher H Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second AAAI Conference on Human Computation and Crowdsourcing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Approximations for binary gaussian process classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Nickisch and Rasmussen2008</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Automating spoken dialogue management design using machine learning: An industry perspective. Speech communication, 50</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieraccini2008] Tim</forename><surname>Paek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Pieraccini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Rasmussen and Williams2006. Chris Williams. 2006. Gaussian processes for machine learning</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning and evaluation of dialogue strategies for new applications: Empirical methods for optimization from small data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Rieser and Lemon2011</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rojas</forename><surname>Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cerisara2014] Lina Maria Rojas</forename><surname>Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Cerisara</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bayesian Inverse Reinforcement Learning for Modeling Conversational Agents in a Virtual Environment</title>
	</analytic>
	<monogr>
		<title level="m">Conference on Intelligent Text Processing and Computational Linguistics</title>
		<editor>Roy et al.2000] Nicholas Roy, Joelle Pineau, and Sebastian Thrun</editor>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Proc of SigDial</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning agents for uncertain environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of COLT</title>
		<meeting>of COLT</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies. The knowledge engineering review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schatzmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="97" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.04808</idno>
		<title level="m">Hierarchical neural network generative models for movie dialogues</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Active learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<idno>1648</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Sciences</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning from real users: Rating dialogue success with neural networks for reinforcement learning in spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of Interspeech</title>
		<editor>Su et al.2015b] Pei-Hao Su, David Vandyke, Milica Gaši´Gaši´c, Nikola Mrkši´Mrkši´c, Tsung-Hsien Wen, and Steve Young</editor>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Proc of SigDial</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hiroaki Sugiyama, Toyomi Meguro, and Yasuhiro Minami. 2012. Preferencelearning based inverse reinforcement learning for dialog control</title>
	</analytic>
	<monogr>
		<title level="m">Proc of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint/>
	</monogr>
	<note>Sugiyama et al.2012</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bayesian update of dialogue state: A pomdp framework for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young2010] Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="562" to="588" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Quality-adaptive spoken dialogue initiative selection and implications on reward modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ACL. [Ultes and Minker2015] Stefan Ultes and Wolfgang Minker</title>
		<meeting>of ACL. [Ultes and Minker2015] Stefan Ultes and Wolfgang Minker</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Proc of SigDial</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">] Laurens</forename><surname>Hinton2008</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-domain dialogue success classifiers for policy training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ASRU</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">Oriol Vinyals and Quoc Le. 2015. A neural conversational model</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Vinyals and Le2015</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PARADISE: A framework for evaluating spoken dialogue agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><forename type="middle">J</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Candace</forename><forename type="middle">A</forename><surname>Kamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alicia</forename><surname>Abella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Partially observable Markov decision processes for spoken dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="393" to="422" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Predicting user satisfaction in spoken dialog system evaluation with collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="971" to="981" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialogue systems: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gašic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of IEEE</title>
		<meeting>of IEEE</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Active learning from weak and strong labelers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaudhuri2015] Chicheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamalika</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chaudhuri</surname></persName>
		</author>
		<idno>abs/1510.02847</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Incremental relabeling for active learning with noisy crowdsourced annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of PASSAT and Proc of SocialCom</title>
		<meeting>of PASSAT and of SocialCom</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Liyue Zhao, Gita Sukthankar, and Rahul Sukthankar</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
