<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 2556</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Venice</orgName>
								<address>
									<postCode>90291</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Venice</orgName>
								<address>
									<postCode>90291</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Venice</orgName>
								<address>
									<postCode>90291</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Venice</orgName>
								<address>
									<postCode>90291</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2556" to="2565"/>
							<date type="published">July 15-20, 2018. 2018. 2556</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image cap-tioning models and show that a model architecture based on Inception-ResNet-v2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic image description is the task of pro- ducing a natural-language utterance (usually a sen- tence) which correctly reflects the visual content of an image. This task has seen an explosion in proposed solutions based on deep learning architec- tures <ref type="bibr" target="#b3">(Bengio, 2009)</ref>, starting with the winners of the 2015 COCO challenge ( <ref type="bibr" target="#b27">Vinyals et al., 2015a;</ref><ref type="bibr" target="#b11">Fang et al., 2015)</ref>, and continuing with a variety of improvements (see e.g. <ref type="bibr" target="#b4">Bernardi et al. (2016)</ref> for a review). Practical applications of automatic image description systems include leveraging descriptions for image indexing or retrieval, and helping those with visual impairments by transforming visual sig- nals into information that can be communicated via text-to-speech technology. The scientific challenge is seen as aligning, exploiting, and pushing further the latest improvements at the intersection of Com- puter Vision and Natural Language Processing. Conceptual Captions: pop artist performs at the festival in a city.</p><p>Figure 1: Examples of images and image descrip- tions from the Conceptual Captions dataset; we start from existing alt-text descriptions, and auto- matically process them into Conceptual Captions with a balance of cleanliness, informativeness, flu- ency, and learnability.</p><p>There are two main categories of advances re- sponsible for increased interest in this task. The first is the availability of large amounts of anno- tated data. Relevant datasets include the ImageNet dataset <ref type="bibr" target="#b7">(Deng et al., 2009)</ref>, with over 14 million images and 1 million bounding-box annotations, and the MS-COCO dataset ( <ref type="bibr" target="#b19">Lin et al., 2014)</ref>, with 120,000 images and 5-way image-caption anno- tations. The second is the availability of power- ful modeling mechanisms such as modern Con- volutional Neural Networks (e.g. <ref type="bibr" target="#b17">Krizhevsky et al. (2012)</ref>), which are capable of converting image pix- els into high-level features with no manual feature- engineering.</p><p>In this paper, we make contributions to both the data and modeling categories. First, we present a new dataset of caption annotations * , Conceptual Captions ( <ref type="figure">Fig. 1)</ref>, which has an or- der of magnitude more images than the COCO dataset. Conceptual Captions consists of about 3.3M image, description pairs. In contrast with the curated style of the COCO images, Concep- tual Captions images and their raw descriptions are harvested from the web, and therefore repre- sent a wider variety of styles. The raw descriptions are harvested from the Alt-text HTML attribute † associated with web images. We developed an au- tomatic pipeline <ref type="figure" target="#fig_1">(Fig. 2)</ref> that extracts, filters, and transforms candidate image/caption pairs, with the goal of achieving a balance of cleanliness, informa- tiveness, fluency, and learnability of the resulting captions.</p><p>As a contribution to the modeling category, we evaluate several image-captioning models. Based on the findings of <ref type="bibr" target="#b14">Huang et al. (2016)</ref>, we use Inception-ResNet-v2 ( <ref type="bibr" target="#b24">Szegedy et al., 2016</ref>) for image-feature extraction, which confers optimiza- tion benefits via residual connections and com- putationally efficient Inception units. For cap- tion generation, we use both RNN-based (Hochre- iter and Schmidhuber, 1997) and Transformer- based ( <ref type="bibr" target="#b25">Vaswani et al., 2017</ref>) models. Our results indicate that Transformer-based models achieve higher output accuracy; combined with the reports of <ref type="bibr" target="#b25">Vaswani et al. (2017)</ref> regarding the reduced num- ber of parameters and FLOPs required for training &amp; serving (compared with RNNs), models such as T2T8x8 (Section 4) push forward the performance on image-captioning and deserve further attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Automatic image captioning has a long history <ref type="bibr" target="#b13">(Hodosh et al., 2013;</ref><ref type="bibr" target="#b9">Donahue et al., 2014;</ref><ref type="bibr" target="#b15">Karpathy and Fei-Fei, 2015;</ref>. It has accelerated with the success of Deep Neu- ral Networks <ref type="bibr" target="#b3">(Bengio, 2009)</ref> and the availability of annotated data as offered by datasets such as Flickr30K ( <ref type="bibr" target="#b31">Young et al., 2014</ref>) and MS-COCO ( <ref type="bibr" target="#b19">Lin et al., 2014</ref>).</p><p>The COCO dataset is not large (order of 10 6 im- ages), given the training needs of DNNs. In spite of that, it has been very popular, in part because it offers annotations for images with non-iconic views, or non-canonical perspectives of objects, and therefore reflects the composition of everyday scenes (the same is true about <ref type="bibr">Flickr30K (Young et al., 2014)</ref>). COCO annotations-category label- ing, instance spotting, and instance segmentation- are done for all objects in an image, including those † https://en.wikipedia.org/wiki/Alt attribute in the background, in a cluttered environment, or partially occluded. Its images are also annotated with captions, i.e. sentences produced by human an- notators to reflect the visual content of the images in terms of objects and their actions or relations.</p><p>A large number of DNN models for image cap- tion generation have been trained and evaluated using COCO captions ( <ref type="bibr" target="#b27">Vinyals et al., 2015a;</ref><ref type="bibr" target="#b11">Fang et al., 2015;</ref><ref type="bibr" target="#b29">Xu et al., 2015;</ref><ref type="bibr" target="#b22">Ranzato et al., 2015;</ref><ref type="bibr" target="#b20">Liu et al., 2017;</ref><ref type="bibr" target="#b8">Ding and Soricut, 2017)</ref>. These models are inspired by sequence-to- sequence models <ref type="bibr" target="#b23">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) but use CNN-based encodings in- stead of RNNs <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b6">Chung et al., 2014)</ref>. Recently, the Transformer ar- chitecture ( <ref type="bibr" target="#b25">Vaswani et al., 2017</ref>) has been shown to be a viable alternative to RNNs (and CNNs) for sequence modeling. In this work, we evaluate the impact of the Conceptual Captions dataset on the image captioning task using models that combine CNN, RNN, and Transformer layers.</p><p>Also related to this work is the Pinterest image and sentence-description dataset ( <ref type="bibr" target="#b21">Mao et al., 2016)</ref>. It is a large dataset (order of 10 8 examples), but its text descriptions do not strictly reflect the visual content of the associated image, and therefore can- not be used directly for training image-captioning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conceptual Captions Dataset Creation</head><p>The Conceptual Captions dataset is programmat- ically created using a Flume ( <ref type="bibr" target="#b5">Chambers et al., 2010)</ref> pipeline. This pipeline processes billions of Internet webpages in parallel. From these web- pages, it extracts, filters, and processes candidate image, caption pairs. The filtering and process- ing steps are described in detail in the following sections.</p><p>Image-based Filtering The first filtering stage, image-based filtering, discards images based on encoding format, size, aspect ratio, and offensive content. It only keeps JPEG images where both dimensions are greater than 400 pixels, and the ratio of larger to smaller dimension is no more than 2. It excludes images that trigger pornography or profanity detectors. These filters discard more than 65% of the candidates.  We analyze candidate Alt-text using the Google Cloud Natural Language APIs, specifically part- of-speech (POS), sentiment/polarity, and pornogra- phy/profanity annotations. On top of these annota- tions, we have the following heuristics:</p><p>• a well-formed caption should have a high unique word ratio covering various POS tags; candidates with no determiner, no noun, or no preposition are discarded; candidates with a high noun ratio are also discarded;</p><p>• candidates with a high rate of token repetition are discarded;</p><p>• capitalization is a good indicator of well- composed sentences; candidates where the first word is not capitalized, or with too high capitalized-word ratio are discarded;</p><p>• highly unlikely tokens are a good indicator of not desirable text; we use a vocabulary V W of 1B token types, appearing at least 5 times in the English Wikipedia, and discard candidates that contain tokens that are not found in this vocabulary.</p><p>• candidates that score too high or too low on the polarity annotations, or trigger the pornog- raphy/profanity detectors, are discarded;</p><p>• predefined boiler-plate prefix/suffix sequences matching the text are cropped, e.g. "click to enlarge picture", "stock photo"; we also drop text which begins/ends in certain patterns, e.g. "embedded image permalink", "profile photo".</p><p>These filters only allow around 3% of the incoming candidates to pass to the later stages.</p><p>Image&amp;Text-based Filtering In addition to the separate filtering based on image and text content, we filter out candidates for which none of the text tokens can be mapped to the content of the image.</p><p>To this end, we use classifiers available via the Google Cloud Vision APIs to assign class labels to images, using an image classifier with a large num- ber of labels (order of magnitude of 10 5 ). Notably, these labels are also 100% covered by the V w token types.</p><p>Images are generally assigned between 5 to 20 labels, though the exact number depends on the Original Alt-text</p><p>Harrison Ford and Calista Flockhart attend the premiere of 'Hollywood Homicide' at the 29th American Film Festival September 5, 2003 in Deauville, France.</p><p>Conceptual Captions actors attend the premiere at festival.</p><p>what-happened "Harrison Ford and Calista Flockhart" mapped to "actors"; name, location, and date dropped.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Transformation with Hypernymization</head><p>In the current version of the dataset, we consid- ered over 5 billion images from about 1 billion English webpages. The filtering criteria above are designed to be high-precision (which comes with potentially low recall). From the original input can- didates, only 0.2% image, caption pairs pass the filtering criteria described above. While the remaining candidate captions tend to be appropriate Alt-text image descriptions (see Alt-text in <ref type="figure">Fig. 1</ref>), a majority of these candidate captions contain proper names (people, venues, locations, etc.), which would be extremely diffi- cult to learn as part of the image captioning task. To give an idea of what would happen in such cases, we train an RNN-based captioning model (see Section 4) on non-hypernymized Alt-text data and present an output example in <ref type="figure" target="#fig_3">Fig. 3</ref>. If auto- matic determination of person identity, location, etc. is needed, it should be attempted as a sepa- rate task and would need to leverage image meta- information about the image (e.g. location).</p><p>Using the Google Cloud Natural Language APIs, we obtain named-entity and syntactic-dependency annotations. We then use the Google Knowl- edge Graph (KG) Search API to match the named- entities to KG entries and exploit the associated hy- pernym terms. For instance, both "Harrison Ford" and "Calista Flockhart" identify as named-entities, so we match them to their corresponding KG en- tries. These KG entries have "actor" as their hyper- nym, so we replace the original surface tokens with that hypernym.</p><p>The following steps are applied to achieve text transformations:</p><p>• noun modifiers of certain types (proper nouns, numbers, units) are removed;</p><p>• dates, durations, and preposition-based loca- tions (e.g., "in Los Angeles") are removed;</p><p>• named-entities are identified, matched against the KG entries, and substitute with their hy- pernym;</p><p>• resulting coordination noun-phrases with the same head (e.g., "actor and actor") are re- solved into a single-head, pluralized form (e.g., "actors");</p><p>Around 20% of samples are discarded during this transformation because it can leave sentences too short or inconsistent. Finally, we perform another round of text analy- sis and entity resolution to identify concepts with low-count. We cluster all resolved entities (e.g., "actor", "dog", "neighborhood", etc.) and keep only the candidates for which all detected types have a count of over 100 (around 55% of the can- didates). These remaining image, caption pairs contain around 16,000 entity types, guaranteed to be well represented in terms of number of examples. <ref type="table" target="#tab_1">Table 1</ref>    We present in <ref type="table" target="#tab_4">Table 3</ref> statistics over the Train/Validation/Test splits for the Conceptual Cap- tions dataset. The training set consists of slightly over 3.3M examples, while there are slightly over 28K examples in the validation set and 22.5K ex- amples in the test set. The size of the training set vocabulary (unique tokens) is 51,201. Note that the test set has been cleaned using human judgements (2+ GOOD), while both the training and valida- tion splits contain all the data, as produced by our automatic pipeline. The mean/stddev/median statis- tics for tokens-per-caption over the data splits are consistent with each other, at around 10.3/4.5/9.0, respectively.</p><note type="other">contains several examples of before/after- transformation pairs. Conceptual Captions Quality To evaluate the precision of our pipeline, we consider a random sample of 4K examples extracted from the test split of the Conceptual Captions dataset. We perform a human evaluation on this sample, using the same methodology described in Section 5.4</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Image Captioning Models</head><p>In order to assess the impact of the Conceptual Cap- tions dataset, we consider several image captioning models previously proposed in the literature. These models can be understood using the illustration in <ref type="figure" target="#fig_4">Fig. 4</ref>, as they mainly differ in the way in which they instantiate some of these components. There are three main components to this archi- tecture:</p><p>• A deep CNN that takes a (preprocessed) im- age and outputs a vector of image embeddings</p><formula xml:id="formula_0">X = (x 1 , x 2 , ..., x L ).</formula><p>• An Encoder module that takes the image embeddings and encodes them into a tensor H = f enc (X).</p><p>• A Decoder model that generates outputs z t = f dec (Y 1:t , H) at each step t, conditioned on H as well as the decoder inputs Y 1:t .</p><p>We explore two main instantiations of this architec- ture. One uses RNNs with LSTM cells <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RNN-based Models</head><p>Our instantiation of the RNN-based model is close to the Show-And-Tell (Vinyals et al., 2015b) model.</p><formula xml:id="formula_1">h l RNN enc (x l , h l−1 ), and H = h L , z t RNN dec (y t , z t−1 ), where z 0 = H .</formula><p>In the original Show-And-Tell model, a single im- age embedding of the entire image is fed to the first cell of an RNN, which is also used for text gener- ation. In our model, a single image embedding is fed to an RNN enc with only one cell, and then a dif- ferent RNN dec is used for text generation. We tried both single image (1x1) embeddings and 8x8 parti- tions of the image, where each partition has its own embedding. In the 8x8 case, image embeddings are fed in a sequence to the RNN enc . In both cases, we apply plain RNNs without cross attention, same as the Show-And-Tell model. RNNs with cross atten- tion were used in the Show-Attend-Tell model ( <ref type="bibr" target="#b29">Xu et al., 2015</ref>), but we find its performance to be inferior to the Show-And-Tell model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Transformer Model</head><p>In the Transformer-based models, both the encoder and the decoder contain a stack of N layers. We denote the n-th layer in the encoder by X n = {x n,1 , . . . , x n,L }, and X 0 = X, H = X N . Each of these layers contains two sub-layers: a multi- head self-attention layer ATTN, and a position-wise feedforward network FFN:</p><formula xml:id="formula_2">x n,j =ATTN(x n,j ,Xn;W e q ,W e k ,W e v ) softmax(x n,j W e q ,Xn W e k ) Xn W e v x (n+1),j =FFN(x n,j ;W e f )</formula><p>where W e q , W e k , and W e v are the encoder weight matrices for query, key, and value transformation in the self-attention sub-layer; and W e f denotes the encoder weight matrix of the feedforward sub-layer. Similar to the RNN-based model, we consider us- ing a single image embedding (1x1) and a vector of 8x8 image embeddings.</p><p>In the decoder, we denote the n-th layer by Z n = {z n,1 , . . . , z n,T } and Z 0 = Y. There are two main differences between the decoder and en- coder layers. First, the self-attention sub-layer in the decoder is masked to the right, in order to pre- vent attending to "future" positions (i.e. z n,j does not attend to z n,(j+1) , . . . , z n,T ). Second, in be- tween the self-attention layer and the feedforward layer, the decoder adds a third cross-attention layer that connects z n,j to the top-layer encoder repre- sentation H = X N .</p><formula xml:id="formula_3">z n,j =ATTN(z n,j ,Z n,1:j ;W d q ,W d k ,W d v ) z n,j =ATTN(z n,j ,H;W c q ,W c k ,W c v ) z (n+1),j =FFN(z n,j ;W d f )</formula><p>where W d q , W d k , and W d v are the weight matrices for query, key, and value transformation in the de- coder self-attention sub-layer; W c q , W c k , W c v are the corresponding decoder weight matrices in the cross-attention sub-layer; and W d f is the decoder weight matrix of the feedforward sub-layer.</p><p>The Transformer-based models utilize position information at the embedding layer. In the 8x8 case, the 64 embedding vectors are serialized to a 1D sequence with positions from <ref type="bibr">[0, . . . , 63]</ref>. The po- sition information is modeled by applying sine and cosine functions at each position and with differ- ent frequencies for each embedding dimension, as in ( <ref type="bibr" target="#b25">Vaswani et al., 2017)</ref>, and subsequently added to the embedding representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section, we evaluate the impact of using the Conceptual Captions dataset (referred to as 'Conceptual' in what follows) for training image captioning models. To this end, we train the models described in Section 4 under two exper- imental conditions: using the training &amp; devel- opment sets provided by the COCO dataset ( <ref type="bibr" target="#b19">Lin et al., 2014</ref>), versus training &amp; development sets using the Conceptual dataset. We quantitatively evaluate the resulting models using three differ- ent test sets: the blind COCO-C40 test set (in- domain for COCO-trained models, out-of-domain for Conceptual-trained models); the Conceptual test set (out-of-domain for COCO-trained mod- els, in-domain for Conceptual-trained models); and the Flickr ( <ref type="bibr" target="#b31">Young et al., 2014</ref>) 1K test set (out- of-domain for both COCO-trained models and Conceptual-trained models).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset Details</head><p>COCO Image Captions The COCO image cap- tioning dataset is normally divided into 82K images for training, and 40K images for validation. Each of these images comes with at least 5 groundtruth captions. Following standard practice, we combine the training set with most of the validation dataset for training our model, and only hold out a subset of 4K images for validation.</p><p>Conceptual Captions The Conceptual Captions dataset contains around 3.3M images for training, 28K for validation and 22.5K for the test set. For more detailed statistics, see <ref type="table" target="#tab_4">Table 3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>Image Preprocessing Each input image is first preprocessed by random distortion and cropping (using a random ratio from 50%∼100%). This prevents models from overfitting individual pixels of the training images.</p><p>Encoder-Decoder For RNN-based models, we use a 1-layer, 512-dim LSTM as the RNN cell. For the Transformer-based models, we use the default setup from ( <ref type="bibr" target="#b25">Vaswani et al., 2017)</ref>, with N = 6 encoder and decoder layers, a hidden-layer size of 512, and 8 attention heads.</p><p>Text Handling Training captions are truncated to maximum 15 tokens. We use a token type min- count of 4, which results in around 9,000 token types for the COCO dataset, and around 25,000 token types for the Conceptual Captions dataset. All other tokens are replaced with special token UNK. The word embedding matrix has size 512 and is tied to the output projection matrix.</p><p>Optimization All models are trained using MLE loss and optimized using Adagrad (Duchi et al., 2011) with learning rate 0.01. Mini-batch size is 25. All model parameters are trained for a total number of 5M steps, with batch updates asynchronously distributed across 40 workers. The final model is selected based on the best CIDEr score on the development set for the given training condition.</p><p>Inference During inference, the decoder predic- tion of the previous position is fed to the input of the next position. We use a beam search of beam size 4 to compute the most likely output sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative Results</head><p>Before we present the numerical results for our experiments, we discuss briefly the patterns that we have observed.</p><p>One difference between COCO-trained models and Conceptual-trained models is their ability to use the appropriate natural language terms for the entities in an image. For the left-most image in <ref type="figure" target="#fig_7">Fig. 5</ref>, COCO-trained models use "group of men" to refer to the people in the image; Conceptual- based models use the more appropriate and infor- mative term "graduates". The second image, from the Flickr test set, makes this even more clear. The Conceptual-trained T2T8x8 model is perfectly ren- dering the image content as "the cloister of the cathedral". None of the other models come close to producing such an accurate description.</p><p>A second difference is that COCO-trained mod- els often seem to hallucinate objects. For instance, they hallucinate "front of building" for the first im- age, "clock and two doors" for the second, and "birthday cake" for the third image. In contrast, Conceptual-trained models do not seem to have this problem. We hypothesize that the hallucina- tion issue for COCO-based models comes from the high correlations present in the COCO data (e.g., if there is a kid at a table, there is also cake). This high degree of correlation in the data does not allow the captioning model to correctly disentan- gle and learn representations at the right level of granularity.  A third difference is the resilience to a large spectrum of image types. COCO only contains nat- ural images, and therefore a cartoon image like the fourth one results in massive hallucination effects for COCO-trained models ("stuffed animal", "fish", "side of car"). In contrast, Conceptual-trained mod- els handle such images with ease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Quantitative Results</head><p>In this section, we present quantitative results on the quality of the outputs produced by several im- age captioning models. We present both automatic evaluation results and human evaluation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Human Evaluation Results</head><p>For human evaluations, we use a pool of profes- sional raters (tens of raters), with a double-blind evaluation condition. Raters are asked to assign a GOOD or BAD label to a given image, caption input, using just common-sense judgment. This approximates the reaction of a typical user, who normally would not accept predefined notions of GOOD vs. BAD. We ask 3 separate raters to rate each input pair and report the percentage of pairs that receive k or more (k+) GOOD annotations.</p><p>In <ref type="table" target="#tab_6">Table 4</ref>, we report the results on the Flickr 1K test set. This evaluation is out-of-domain for both training conditions, so all models are on rel- atively equal footing. The results indicate that the Conceptual-based models are superior. In 50.6% (for the <ref type="bibr">T2T8x8</ref>     <ref type="table">Table 7</ref>: Auto metrics on the Flickr 1K Test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Automatic Evaluation Results</head><p>In this section, we report automatic evaluation re- sults, using established image captioning metrics. For the COCO C40 test set ( <ref type="figure" target="#fig_7">Fig. 5)</ref>, we report the numerical values returned by the COCO on- line evaluation server ‡ , using the CIDEr ( <ref type="bibr" target="#b26">Vedantam et al., 2015)</ref>, ROUGE-L ( <ref type="bibr" target="#b18">Lin and Och, 2004</ref>), and METEOR ( <ref type="bibr" target="#b2">Banerjee and Lavie, 2005</ref>) metrics. For Conceptual Captions <ref type="figure">(Fig. 6)</ref> and Flickr <ref type="figure">(Fig. 7)</ref> test sets, we report numerical values for the CIDEr, ROUGE-L, and SPICE ( <ref type="bibr" target="#b0">Anderson et al., 2016</ref>) § . For all metrics, higher number means closer dis- tance between the candidates and the groundtruth captions.</p><p>The automatic metrics are good at detecting in- vs out-of-domain situations. For COCO-models tested on COCO, the results in <ref type="figure" target="#fig_7">Fig. 5</ref> show CIDEr scores in the 1.02-1.04 range, for both RNN-and Transformer-based models; the scores drop in the 0.35-0.41 range (CIDEr) for the Conceptual-based models tested against COCO groundtruth. For Conceptual-models tested on the Conceptual Cap- tions test set, the results in <ref type="figure">Fig. 6</ref> show scores as high as 1.468 CIDEr for the T2T8x8 model, which corroborates the human-eval results for the Transformer-based models being superior to the RNN-based models; the scores for the COCO- based models tested against Conceptual Captions groundtruth are all below 0.2 CIDEr.</p><p>The automatic metrics fail to corroborate the ‡ http://mscoco.org/dataset/#captions-eval. § https://github.com/tylin/coco-caption.</p><p>human evaluation results. According to the auto- matic metrics, the COCO-trained models are su- perior to the Conceptual-trained models (CIDEr scores in the mid-0.3 for the COCO-trained con- dition, versus mid-0.2 for the Conceptual-trained condition), and the RNN-based models are supe- rior to Transformer-based models. Notably, these are the same metrics which score humans lower than the methods that won the COCO 2015 chal- lenge ( <ref type="bibr" target="#b27">Vinyals et al., 2015a;</ref><ref type="bibr" target="#b11">Fang et al., 2015)</ref>, despite the fact that humans are still much better at this task. The failure of these metrics to align with the human evaluation results casts again grave doubts on their ability to drive progress in this field. A significant weakness of these metrics is that hal- lucination effects are under-penalized (a small pre- cision penalty for tokens with no correspondent in the reference), compared to human judgments that tend to dive dramatically in the presence of hallucinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We present a new image captioning dataset, Con- ceptual Captions, which has several key character- istics: it has around 3.3M examples, an order of magnitude larger than the COCO image-captioning dataset; it consists of a wide variety of images, including natural images, product images, profes- sional photos, cartoons, drawings, etc.; and, its captions are based on descriptions taken from orig- inal Alt-text attributes, automatically transformed to achieve a balance between cleanliness, informa- tiveness, and learnability. We evaluate both the quality of the resulting image/caption pairs, as well as the performance of several image-captioning models when trained on the Conceptual Captions data. The results indicate that such models achieve better performance, and avoid some of the pitfalls seen with COCO-trained models, such as object hallucination. We hope that the availability of the Conceptual Captions dataset will foster considerable progress on the automatic image-captioning task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Alt-text: A Pakistani worker helps to clear the debris from the Taj Ma- hal Hotel November 7, 2005 in Bal- akot, Pakistan. Conceptual Captions: a worker helps to clear the debris. Alt-text: Musician Justin Timber- lake performs at the 2017 Pilgrim- age Music &amp; Cultural Festival on September 23, 2017 in Franklin, Tennessee.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Conceptual Captions pipeline steps with examples and final output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Original</head><label></label><figDesc>Alt-text Side view of a British Airways Airbus A319 aircraft on approach to land with landing gear down -Stock Image Conceptual Captions side view of an aircraft on approach to land with landing gear down what-happened phrase "British Airways Airbus A319 aircraft" mapped to "aircraft"; boilerplate removed. Original Alt-text Two sculptures by artist Duncan McKellar adorn trees outside the derelict Norwich Union offices in Bristol, UK -Stock Image Conceptual Captions sculptures by person adorn trees outside the derelict offices what-happened object count (e.g. "Two") dropped; proper noun-phrase hypernymized to "person"; proper- noun modifiers dropped; location dropped; boilerplate removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of model output trained on clean, non-hypernymized Alt-text data.</figDesc><graphic url="image-35.png" coords="4,313.25,280.86,98.21,65.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The main model components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>) to implement the f enc and f dec functions, corresponding to the Show-And- Tell (Vinyals et al., 2015b) model. The other uses Transformer self-attention networks (Vaswani et al., 2017) to implement f enc and f dec . All models in this paper use Inception-ResNet-v2 as the CNN component (Szegedy et al., 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>COCO-trained RNN8x8 a group of men standing in front of a building a couple of people walk- ing down a walkway a child sitting at a table with a cake on it a close up of a stuffed animal on a table T2T8x8 a group of men in uni- form and ties are talking a narrow hallway with a clock and two doors a woman cutting a birth- day cake at a party a picture of a fish on the side of a car Conceptual-trained RNN8x8 graduates line up for the commencement cer- emony a view of the nave a child 's drawing at a birthday party a cartoon business- man thinking about something T2T8x8 graduates line up to re- ceive their diplomas the cloister of the cathe- dral learning about the arts and crafts a cartoon businessman asking for help</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Side by side comparison of model outputs under two training conditions. Conceptual-based models (lower half) tend to hallucinate less, are more expressive, and handle well a larger variety of images. The two images in the middle are from Flickr; the other two are from Conceptual Captions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Examples of Conceptual Captions as derived from their original Alt-text versions. 

image. We match these labels against the candi-
date text, taking into account morphology-based 
stemming as provided by the text annotation. Can-
didate image, caption pairs with no overlap are 
discarded. This filter discards around 60% of the 
incoming candidates. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>.</head><label></label><figDesc></figDesc><table>GOOD (out of 3) 
1+ 
2+ 
3 
Conceptual Captions 96.9% 90.3% 78.5% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Human evaluation results on a sample 
from Conceptual Captions. 

The results are presented in Table 2 and show 
that, out of 3 annotations, over 90% of the captions 
receive a majority (2+) of GOOD judgments. This 
indicates that the Conceptual Captions pipeline, 
though involving extensive algorithmic processing, 
produces high-quality image captions. 

Examples Unique 
Tokens/Caption 
Tokens Mean StdDev Median 
Train 3,318,333 51,201 10.3 
4.5 
9.0 
Valid. 
28,355 13,063 10.3 
4.6 
9.0 
Test 
22,530 11,731 10.1 
4.5 
9.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Statistics over Train/Validation/Test splits 
for Conceptual Captions. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 : Human eval results on Flickr 1K Test.</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><head>Table 5 : Auto metrics on the COCO C40 Test.</head><label>5</label><figDesc></figDesc><table>Model 
Training 
CIDEr ROUGE-L SPICE 

RNN1x1 

COCO 
0.183 
0.149 
0.062 

RNN8x8 

COCO 
0.191 
0.152 
0.065 

T2T1x1 

COCO 
0.184 
0.148 
0.062 

T2T8x8 

COCO 
0.190 
0.151 
0.064 

RNN1x1 

Conceptual 
1.351 
0.326 
0.235 

RNN8x8 

Conceptual 
1.401 
0.330 
0.240 

T2T1x1 

Conceptual 
1.588 
0.331 
0.254 

T2T8x8 

Conceptual 
1.676 
0.336 
0.257 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Auto metrics on the 22.5K Conceptual 
Captions Test set. 

Model 
Training 
CIDEr ROUGE-L SPICE 

RNN1x1 

COCO 
0.340 
0.414 
0.101 

RNN8x8 

COCO 
0.356 
0.413 
0.103 

T2T1x1 

COCO 
0.341 
0.404 
0.101 

T2T8x8 

COCO 
0.359 
0.416 
0.103 

RNN1x1 

Conceptual 
0.269 
0.310 
0.076 

RNN8x8 

Conceptual 
0.275 
0.309 
0.076 

T2T1x1 

Conceptual 
0.226 
0.280 
0.068 

T2T8x8 

Conceptual 
0.227 
0.277 
0.066 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SPICE: semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the ACL Workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Automatic description generation from images: A survey of models, datasets, and evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruket</forename><surname>Cakici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aykut</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkut</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Adrian Muscat, and Barbara Plank</pubPlace>
		</imprint>
	</monogr>
	<note>JAIR 55</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Flumejava: Easy, efficient data-parallel pipelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Raniwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frances</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename></persName>
		</author>
		<idno>10121-0701</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=1806638" />
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI). 2 Penn Plaza, Suite 701</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="363" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cold-start reinforcement learning with softmax policy gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>JAIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno>CoRR abs/1611.10012</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>In NIPS</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>CoRR abs/1405.0312</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimization of image description metrics using policy gradient methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Training and evaluating multimodal word embeddings with large-scale web annotated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno>CoRR abs/1511.06732</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno>abs/1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 32nd International Conference on Machine Learning (ICML)</title>
		<meeting>of the 32nd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Review networks for caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
	<note>Micah Hodosh, and Julia Hockenmaier</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
