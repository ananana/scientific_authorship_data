<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hybrid Simplification using Deep Semantics and Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
							<email>shashi.narayan@loria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit√© de Lorraine</orgName>
								<orgName type="institution" key="instit2">LORIA Villers-l` es-Nancy</orgName>
								<address>
									<postCode>F-54600</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
							<email>claire.gardent@loria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">UMR</orgName>
								<address>
									<addrLine>7503 Vandoeuvre-l` es-Nancy</addrLine>
									<postCode>F-54500</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hybrid Simplification using Deep Semantics and Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="435" to="445"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones. The approach differs from previous work in two main ways. First, it is semantic based in that it takes as input a deep semantic representation rather than e.g., a sentence or a parse tree. Second , it combines a simplification model for splitting and deletion with a monolin-gual translation model for phrase substitution and reordering. When compared against current state of the art methods, our model yields significantly simpler output that is both grammatical and meaning preserving.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentence simplification maps a sentence to a sim- pler, more readable one approximating its con- tent. Typically, a simplified sentence differs from a complex one in that it involves simpler, more usual and often shorter, words (e.g., use instead of exploit); simpler syntactic constructions (e.g., no relative clauses or apposition); and fewer mod- ifiers (e.g., He slept vs. He also slept). In prac- tice, simplification is thus often modeled using four main operations: splitting a complex sen- tence into several simpler sentences; dropping and reordering phrases or constituents; substituting words/phrases with simpler ones.</p><p>As has been argued in previous work, sentence simplification has many potential applications. It is useful as a preprocessing step for a variety of NLP systems such as parsers and machine trans- lation systems <ref type="bibr" target="#b6">(Chandrasekar et al., 1996)</ref>, sum- marisation <ref type="bibr" target="#b13">(Knight and Marcu, 2000</ref>), sentence fusion ( <ref type="bibr" target="#b11">Filippova and Strube, 2008</ref>) and semantic role labelling ( <ref type="bibr">Vickrey and Koller, 2008)</ref>. It also has wide ranging potential societal application as a reading aid for people with aphasis ( <ref type="bibr" target="#b4">Carroll et al., 1999</ref>), for low literacy readers ( <ref type="bibr">Watanabe et al., 2009</ref>) and for non native speakers <ref type="bibr" target="#b16">(Siddharthan, 2002)</ref>.</p><p>There has been much work recently on de- veloping computational frameworks for sentence simplification. Synchronous grammars have been used in combination with linear integer program- ming to generate and rank all possible rewrites of an input sentence <ref type="bibr" target="#b10">(Dras, 1999;</ref><ref type="bibr">Woodsend and Lapata, 2011</ref>). Machine Translation systems have been adapted to translate complex sentences into simple ones ( <ref type="bibr">Zhu et al., 2010;</ref><ref type="bibr">Wubben et al., 2012;</ref><ref type="bibr" target="#b7">Coster and Kauchak, 2011)</ref>. And handcrafted rules have been proposed to model the syntactic transformations involved in simplifications <ref type="bibr" target="#b15">(Siddharthan et al., 2004;</ref><ref type="bibr" target="#b19">Siddharthan, 2011;</ref><ref type="bibr" target="#b6">Chandrasekar et al., 1996)</ref>.</p><p>In this paper, we present a hybrid approach to sentence simplification which departs from this previous work in two main ways.</p><p>First, it combines a model encoding probabil- ities for splitting and deletion with a monolin- gual machine translation module which handles reordering and substitution. In this way, we ex- ploit the ability of statistical machine translation (SMT) systems to capture phrasal/lexical substi- tution and reordering while relying on a dedi- cated probabilistic module to capture the splitting and deletion operations which are less well (dele- tion) or not at all (splitting) captured by SMT ap- proaches.</p><p>Second, our approach is semantic based. While previous simplification approaches starts from ei- ther the input sentence or its parse tree, our model takes as input a deep semantic representation namely, the Discourse Representation Structure <ref type="bibr">(DRS, (Kamp, 1981)</ref>) assigned by Boxer ( <ref type="bibr" target="#b8">Curran et al., 2007)</ref> to the input complex sentence. As we shall see in Section 4, this permits a linguistically principled account of the splitting operation in that semantically shared elements are taken to be the basis for splitting a complex sentence into sev- eral simpler ones; this facilitates completion (the re-creation of the shared element in the split sen- tences); and this provide a natural means to avoid deleting obligatory arguments.</p><p>When compared against current state of the art methods ( <ref type="bibr">Zhu et al., 2010;</ref><ref type="bibr">Woodsend and Lapata, 2011;</ref><ref type="bibr">Wubben et al., 2012</ref>), our model yields sig- nificantly simpler output that is both grammatical and meaning preserving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Earlier work on sentence simplification relied on handcrafted rules to capture syntactic sim- plification e.g., to split coordinated and subor- dinated sentences into several, simpler clauses or to model active/passive transformations <ref type="bibr" target="#b16">(Siddharthan, 2002;</ref><ref type="bibr" target="#b5">Chandrasekar and Srinivas, 1997;</ref><ref type="bibr" target="#b2">Bott et al., 2012;</ref><ref type="bibr" target="#b3">Canning, 2002;</ref><ref type="bibr" target="#b19">Siddharthan, 2011;</ref><ref type="bibr" target="#b18">Siddharthan, 2010)</ref>. While these hand- crafted approaches can encode precise and linguis- tically well-informed syntactic transformation (us- ing e.g., detailed morphological and syntactic in- formation), they are limited in scope to purely syn- tactic rules and do not account for lexical simpli- fications and their interaction with the sentential context.</p><p>Using the parallel dataset formed by Simple En- glish Wikipedia (SWKP) 1 and traditional English Wikipedia (EWKP) 2 , more recent work has fo- cused on developing machine learning approaches to sentence simplification. <ref type="bibr">Zhu et al. (2010)</ref> constructed a parallel cor- pus (PWKP) of 108,016/114,924 complex/simple sentences by aligning sentences from EWKP and SWKP and used the resulting bitext to train a sim- plification model inspired by syntax-based ma- chine translation <ref type="bibr">(Yamada and Knight, 2001</ref>). Their simplification model encodes the probabil- ities for four rewriting operations on the parse tree of an input sentences namely, substitution, re- ordering, splitting and deletion. It is combined with a language model to improve grammatical- ity and the decoder translates sentences into sim- 1 SWKP (http://simple.wikipedia.org) is a corpus of simple texts targeting "children and adults who are learning English Language" and whose authors are requested to "use easy words and short sentences".</p><p>2 http://en.wikipedia.org pler ones by greedily selecting the output sentence with highest probability. Using both the PWKP corpus developed by <ref type="bibr">Zhu et al. (2010)</ref> and the edit history of Simple Wikipedia, Woodsend and Lapata (2011) learn a quasi synchronous grammar <ref type="bibr" target="#b20">(Smith and Eisner, 2006</ref>) describing a loose alignment between parse trees of complex and of simple sentences. Fol- lowing Dras (1999), they then generate all possi- ble rewrites for a source tree and use integer lin- ear programming to select the most appropriate simplification. They evaluate their model on the same dataset used by <ref type="bibr">Zhu et al. (2010)</ref> namely, an aligned corpus of 100/131 EWKP/SWKP sen- tences and show that they achieve better BLEU score. They also conducted a human evaluation on 64 of the 100 test sentences and showed again a better performance in terms of simplicity, gram- maticality and meaning preservation.</p><p>In <ref type="bibr">(Wubben et al., 2012;</ref><ref type="bibr" target="#b7">Coster and Kauchak, 2011</ref>), simplification is viewed as a monolingual translation task where the complex sentence is the source and the simpler one is the target. To ac- count for deletions, reordering and substitution, Coster and Kauchak (2011) trained a phrase based machine translation system on the PWKP corpus while modifying the word alignment output by GIZA++ in Moses to allow for null phrasal align- ments. In this way, they allow for phrases to be deleted during translation. No human evaluation is provided but the approach is shown to result in statistically significant improvements over a tradi- tional phrase based approach. Similarly, <ref type="bibr">Wubben et al. (2012)</ref> use Moses and the PWKP data to train a phrase based machine translation system aug- mented with a post-hoc reranking procedure de- signed to rank the output based on their dissim- ilarity from the source. A human evaluation on 20 sentences randomly selected from the test data indicates that, in terms of fluency and adequacy, their system is judged to outperform both <ref type="bibr">Zhu et al. (2010)</ref> and Woodsend and Lapata (2011) sys- tems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Simplification Framework</head><p>We start by motivating our approach and explain- ing how it relates to previous proposals w.r.t., the four main operations involved in simplifica- tion namely, splitting, deletion, substitution and reordering. We then introduce our framework. Sentence Splitting. Sentence splitting is ar- guably semantic based in that in many cases, split- ting occurs when the same semantic entity partici- pates in two distinct eventualities. For instance, in example (1) below, the split is on the noun bricks which is involved in two eventualities namely, "being resistant to cold" and "enabling the con- struction of permanent buildings".</p><p>(1) C. Being more resistant to cold, bricks enabled the con- struction of permanent buildings. S. Bricks were more resistant to cold. Bricks enabled the construction of permanent buildings.</p><p>While splitting opportunities have a clear coun- terpart in syntax (i.e., splitting often occurs when- ever a relative, a subordinate or an appositive clause occurs in the complex sentence), comple- tion i.e., the reconstruction of the shared element in the second simpler clause, is arguably seman- tically governed in that the reconstructed element corefers with its matching phrase in the first sim- pler clause. While our semantic based approach naturally accounts for this by copying the phrase corresponding to the shared entity in both phrases, syntax based approach such as <ref type="bibr">Zhu et al. (2010)</ref> and <ref type="bibr">Woodsend and Lapata (2011)</ref> will often fail to appropriately reconstruct the shared phrase and in- troduce agreement mismatches because the align- ment or rules they learn are based on syntax alone. For instance, in example (2), <ref type="bibr">Zhu et al. (2010)</ref> fails to copy the shared argument "The judge" to the second clause whereas Woodsend and Lapata (2011) learns a synchronous rule matching (VP and VP) to (VP. NP(It) VP) thereby failing to pro- duce the correct subject pronoun ("he" or "she") for the antecedent "The judge". Deletion. By handling deletion using a proba- bilistic model trained on semantic representations, we can avoid deleting obligatory arguments. Thus in our approach, semantic subformulae which are related to a predicate by a core thematic roles (e.g., agent and patient) are never considered for dele- tion. By contrast, syntax based approaches ( <ref type="bibr">Zhu et al., 2010;</ref><ref type="bibr">Woodsend and Lapata, 2011</ref>) do not distinguish between optional and obligatory argu- ments. For instance <ref type="bibr">Zhu et al. (2010)</ref> simplifies (3C) to (3S) thereby incorrectly deleting the oblig- atory theme (gifts) of the complex sentence and modifying its meaning to giving knights and war- riors (instead of giving gifts to knights and war- riors).</p><p>(3) C. Women would also often give knights and warriors gifts that included thyme leaves as it was believed to bring courage to the bearer. S. Women also often give knights and warriors. Gifts included thyme leaves as it was thought to bring courage to the saint. ( <ref type="bibr">Zhu et al., 2010)</ref> We also depart from Coster and Kauchak (2011) who rely on null phrasal alignments for deletion during phrase based machine translation. In their approach, deletion is constrained by the training data and the possible alignments, independent of any linguistic knowledge.</p><p>Substitution and Reordering SMT based ap- proaches to paraphrasing ( <ref type="bibr" target="#b1">Barzilay and Elhadad, 2003;</ref><ref type="bibr" target="#b0">Bannard and Callison-Burch, 2005</ref>) and to sentence simplification ( <ref type="bibr">Wubben et al., 2012)</ref> have shown that by utilising knowledge about align- ment and translation probabilities, SMT systems can account for the substitutions and the reorder- ings occurring in sentence simplification. Fol- lowing on these approaches, we therefore rely on phrase based SMT to learn substitutions and re- ordering. In addition, the language model we in- tegrate in the SMT module helps ensuring better fluency and grammaticality. <ref type="figure" target="#fig_3">Figure 1</ref> shows how our approach simplifies (4C) into (4S). The DRS for (4C) produced using Boxer <ref type="bibr" target="#b8">(Curran et al., 2007</ref>) is shown at the top of the Figure and a graph representation 3 of the dependencies between its variables is shown immediately below. Each DRS variable labels a node in the graph and each edge is labelled with the relation holding be- tween the variables labelling its end vertices. The   two tables to the right of the picture show the pred- icates <ref type="table" target="#tab_4">(top table) associated with each variable and  the relation label (bottom table)</ref> associated with each edge. Boxer also outputs the associated po- sitions in the complex sentence for each predicate (not shown in the DRS but in the graph tables). Or- phan words (OW) i.e., words which have no cor- responding material in the DRS (e.g., which at po- sition 16), are added to the graph (node O 1 ) thus ensuring that the position set associated with the graph exactly matches the positions in the input sentence and thus deriving the input sentence.  <ref type="table">Table 1</ref>: Simplification: SPLIT Given the input DRS shown in <ref type="figure" target="#fig_3">Figure 1</ref>, simpli- fication proceeds as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">An Example</head><formula xml:id="formula_0">‚àß( X1 male(X1) ‚àß( X2 second(X2) paper(X2) of(X2, X1) ‚àß( X3 publish(X3) agent(X3, X0) patient(X3, X2) ; ( X4 named(X4, physical, org) named(X4, review, org) named(X4, letters, org) ‚àß X5 thing(X5) event(X3) in(X3, X4) in(X3, X5) timex(X5) = 1964 ))))) ; ( X6 ; ( X7, X8 mechanism(X8) nn(X7, X8) named(X7, higgs, org) ‚àß X9, X10, X11, X12 new(X9) massive(X9) spin-zero(X9) boson(X9) predict(X10) event(X10) describe(X11) event(X11) first(X12) time(X12) agent(X10, X8) patient(X10, X9) agent(X11, X6) patient(X11, X8) for(X10, X12) [Discourse Representation Structure produced by BOXER] ROOT O1 X10 X12 X9 R10 R11 X11 X8 X7 R8 X6 R6 R7 X3 X5 X4 X2 X1 R3 X0 R1 R2 R4 R5 R9 [DRS Graph Representation]<label>O1</label></formula><formula xml:id="formula_1">R11 23 f or, X10 ‚Üí X12 R10 17 patient, X10 ‚Üí X9 R9 17 agent, X10 ‚Üí X8 R8 ‚àí‚àí nn, X8 ‚Üí X7 R7 13 patient, X11 ‚Üí X8 R6 13 agent, X11 ‚Üí X6 R5 1 in, X3 ‚Üí X5 R4 9 in, X3 ‚Üí X4 R3 6 of, X2 ‚Üí X1 R2 5 patient, X3 ‚Üí X2 R1 5 agent, X3 ‚Üí X0 rel pos. in S predicate ROOT X11 X8 X7 R8 X6 R6 R7 X3 X5 X4 X2 X1 R3 X0 R1 R2 R4 R5 ROOT O1 X10 X12 X9 X8 X7 R8 R9 R10 R11 ( ) SPLIT ROOT X11 X8 X7 R8 X6 R6 R7 X3 X5 X ‚Ä≤ 2 X1 R3 X0 R1 R2</formula><p>Splitting. The splitting candidates of a DRS are event pairs contained in that DRS. More precisely, the splitting candidates are pairs 4 of event vari- ables associated with at least one of the core the- matic roles (e.g., agent and patient). The features conditioning a split are the set of thematic roles as- sociated with each event variable. The DRS shown in <ref type="figure" target="#fig_3">Figure 1</ref> contains three such event variables X 3 , X 11 and X 10 with associated thematic role sets {agent, in, in, patient}, {agent, patient} and {agent, for, patient} respectively. Hence, there are 3 splitting candidates (X 3 -X 11 , X 3 -X 10 and X 10 - X 11 ) and 4 split options: no split or split at one of the splitting candidates. Here the split with highest probability (cf. <ref type="table">Table 1</ref>) is chosen and the DRS is split into two sub-DRS, one containing X 3 , and the other containing X 10 . After splitting, dan- gling subgraphs are attached to the root of the new subgraph maximizing either proximity or position overlap. Here the graph rooted in X 11 is attached to the root dominating X 3 and the orphan word O 1 to the root dominating X 10 .</p><p>Deletion. The deletion model (cf. <ref type="table" target="#tab_3">Table 2</ref>) reg- ulates the deletion of relations and their associated subgraph; of adjectives and adverbs; and of orphan words. Here, the relations in between X 3 and X 4 and for between X 10 and X 12 are deleted resulting in the deletion of the phrases "in Physical Review Letters" and "for the first time" as well as the ad-jectives second, massive, spin-zero and the orphan word which.</p><p>Substitution and Reordering. Finally the trans- lation and language model ensures that published, describing and boson are simplified to wrote, ex- plaining and elementary particle respectively; and that the phrase "In 1964" is moved from the be- ginning of the sentence to its end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Simplification Model</head><p>Our simplification framework consists of a prob- abilistic model for splitting and dropping which we call DRS simplification model (DRS-SM); a phrase based translation model for substitution and reordering (PBMT); and a language model learned on Simple English Wikipedia (LM) for fluency and grammaticality. Given a complex sen- tence c, we split the simplification process into two steps. First, DRS-SM is applied to D c (the DRS representation of the complex sentence c) to produce one or more (in case of splitting) in- termediate simplified sentence(s) s ‚Ä≤ . Second, the simplified sentence(s) s ‚Ä≤ is further simplified to s using a phrase based machine translation system (PBMT+LM). Hence, our model can be formally defined as:</p><formula xml:id="formula_2">ÀÜ s = argmax s p(s|c) = argmax s p(s ‚Ä≤ |c)p(s|s ‚Ä≤ ) = argmax s p(s ‚Ä≤ |Dc)p(s ‚Ä≤ |s)p(s)</formula><p>where the probabilities p(s ‚Ä≤ |D c ), p(s ‚Ä≤ |s) and p(s) are given by the DRS simplification model, the phrase based machine translation model and the language model respectively.</p><p>To get the DRS simplification model, we com- bine the probability of splitting with the probabil- ity of deletion:</p><formula xml:id="formula_3">p(s ‚Ä≤ |Dc) = Œ∏:str(Œ∏(Dc))=s ‚Ä≤ p(D split |Dc)p(D del |D split )</formula><p>where Œ∏ is a sequence of simplification opera- tions and str(Œ∏(D c )) is the sequence of words as- sociated with a DRS resulting from simplifying D c using Œ∏.</p><p>The probability of a splitting operation for a given DRS D c is:  That is, if the DRS is split on the splitting candi- date sp cand , the probability of the split is then given by the SPLIT table <ref type="table">(Table 1)</ref> for the isSplit value "true" and the split candidate sp cand ; else it is the product of the probability given by the SPLIT table for the isSplit value "false" for all split candidate considered for D c . As mentioned above, the fea- tures used for determining the split operation are the role sets associated with pairs of event vari- ables (cf. <ref type="table">Table 1</ref>).</p><formula xml:id="formula_4">p(D split |Dc) = Ô£± Ô£≤ Ô£≥ SPLIT(</formula><p>The deletion probability is given by three mod- els: a model for relations determining the deletion of prepositional phrases; a model for modifiers (adjectives and adverbs) and a model for orphan words <ref type="table" target="#tab_3">(Table 2)</ref>. All three deletion models use the associated word itself as a feature. In addition, the model for relations uses the PP length-range as a feature while the model for orphan words relies on boundary information i.e., whether or not, the OW occurs at the associated sentence boundary. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Estimating the parameters</head><p>We use the EM algorithm <ref type="bibr" target="#b9">(Dempster et al., 1977)</ref> to estimate our split and deletion model parame- ters. For an efficient implementation of EM algo- rithm, we follow the work of <ref type="bibr">Yamada and Knight (2001)</ref>   <ref type="figure">(c, s)</ref>, the root M-node (D c , s) is followed by a single split O-node producing an M- node (D c , s) and counting all split candidates in D c for failed split. The M-nodes created after split op- erations are then tried for multiple deletion opera- tions of relations, modifiers and OW respectively. Each deletion candidate creates a deletion O-node marking successful or failed deletion of the can- didate and a result M-node. The deletion process continues on the result M-node until there is no deletion candidate left to process. The governing criteria for the construction of the training graph is that, at each step, it tries to minimize the Leven- shtein edit distance between the complex and the simple sentences. Moreover, for the splitting op- eration, we introduce a split only if the reference sentence consists of several sentences (i.e., there is a split in the training data); and only consider splits which maximises the overlap between split and simple reference sentences.</p><p>We initialize our probability tables <ref type="table" target="#tab_3">Table 1 and  Table 2</ref> with the uniform distribution, i.e., 0.5 be- cause all our features are binary. The EM algo- rithm iterates over training graphs counting model features from O-nodes and updating our probabil- ity tables. Because of the space constraints, we do not describe our algorithm in details. We refer the reader to <ref type="bibr">(Yamada and Knight, 2001</ref>) for more details.</p><p>Our phrase based translation model is trained using the Moses toolkit 5 with its default command line options on the PWKP corpus (except the sen- tences from the test set) considering the complex sentence as the source and the simpler one as the target. Our trigram language model is trained us- ing the SRILM toolkit 6 on the SWKP corpus <ref type="bibr">7</ref> .</p><p>Decoding. We explore the decoding graph sim- ilar to the training graph but in a greedy approach always picking the choice with maximal probabil- ity. Given a complex input sentence c, a split O- node will be selected corresponding to the deci- sion of whether to split and where to split. Next, deletion O-nodes are selected indicating whether or not to drop each of the deletion candidate. The DRS associated with the final M-node D f in is then mapped to a simplified sentence s ‚Ä≤ f in which is further simplified using the phrase-based machine translation system to produce the final simplified sentence s simple .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We trained our simplification and translation mod- els on the PWKP corpus. To evaluate perfor- mance, we compare our approach with three other state of the art systems using the test set provided by <ref type="bibr">Zhu et al. (2010)</ref> and relying both on automatic metrics and on human judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training and Test Data</head><p>The DRS-Based simplification model is trained on PWKP, a bi-text of complex and simple sen- tences provided by <ref type="bibr">Zhu et al. (2010)</ref>. To construct this bi-text, <ref type="bibr">Zhu et al. (2010)</ref> extracted complex and simple sentences from EWKP and SWKP re- spectively and automatically aligned them using TF*IDF as a similarity measure. PWKP contains 108016/114924 complex/simple sentence pairs.</p><p>We tokenize PWKP using Stanford CoreNLP toolkit <ref type="bibr">8</ref> . We then parse all complex sentences in PWKP using Boxer <ref type="bibr">9</ref> to produce their DRSs. Finally, our DRS-Based simplification model is trained on 97.75% of PWKP; we drop out 2.25% of the complex sentences in PWKP which are re- peated in the test set or for which Boxer fails to produce DRSs. We evaluate our model on the test set used by <ref type="bibr">Zhu et al. (2010)</ref> namely, an aligned corpus of 100/131 EWKP/SWKP sentences. Boxer pro- duces a DRS for 96 of the 100 input sentences. These input are simplified using our simplifica- tion system namely, the DRS-SM model and the phrase-based machine translation system (Section 3.2). For the remaining four complex sentences, Boxer fails to produce DRSs. These four sen- tences are directly sent to the phrase-based ma- chine translation system to produce simplified sen- tences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Automatic Evaluation Metrics</head><p>To assess and compare simplification systems, two main automatic metrics have been used in previ- ous work namely, BLEU and the Flesch-Kincaid Grade Level Index (FKG).</p><p>The FKG index is a readability metric taking into account the average sentence length in words and the average word length in syllables. In its original context (language learning), it was ap- plied to well formed text and thus measured the simplicity of a well formed sentence. In the con- text of the simplification task however, the auto- matically generated sentences are not necessarily well formed so that the FKG index reduces to a measure of the sentence length (in terms of words and syllables) approximating the simplicity level of an output sentence irrespective of the length of the corresponding input. To assess simplifica- tion, we instead use metrics that are directly re- lated to the simplification task namely, the number of splits in the overall (test and training) data and in average per sentences; the number of generated sentences with no edits i.e., which are identical to the original, complex one; and the average Leven- shtein distance between the system's output and both the complex and the simple reference sen- tences.</p><p>BLEU gives a measure of how close a system's output is to the gold standard simple sentence. Be- cause there are many possible ways of simplifying a sentence, BLEU alone fails to correctly assess the appropriateness of a simplification. Moreover BLEU does not capture the degree to which the system's output differs from the complex sentence input. We therefore use BLEU as a means to eval- uate how close the systems output are to the refer- ence corpus but complement it with further man- ual metrics capturing other important factors when evaluating simplifications such as the fluency and the adequacy of the output sentences and the de- gree to which the output sentence simplifies the input. <ref type="table" target="#tab_6">Table 3</ref> shows the proportion of input whose simplification involved a splitting operation. While our system splits in proportion similar to that observed in the training data, the other systems either split very often (80% of the time for Zhu and 63% of the time for Woodsend) or not at all (Wubben). In other words, when com- pared to the other systems, our system performs splits in proportion closest to the reference both in terms of total number of splits and of average number of splits per sentence.   <ref type="table" target="#tab_8">Table 4</ref> indicates the edit dis- tance of the output sentences w.r.t. both the com- plex and the simple reference sentences as well as the number of input for which no simplification occur. The right part of the table shows that our system generate simplifications which are closest to the reference sentence (in terms of edits) com- pared to those output by the other systems. It also produces the highest number of simplifica- tions which are identical to the reference. Con- versely our system only ranks third in terms of dis- similarity with the input complex sentences (6.32 edits away from the input sentence) behind the Woodsend (8.63 edits) and the Zhu (7.87 edits) system. This is in part due to the difference in splitting strategies noted above : the many splits applied by these latter two systems correlate with a high number of edits.   <ref type="table" target="#tab_8">Table 4</ref> show that our sys- tem produces simplifications that are closest to the reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Splits</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Edits</head><p>In sum, the automatic metrics indicate that our system produces simplification that are consis- tently closest to the reference in terms of edit dis- tance, number of splits and BLEU score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Human Evaluation</head><p>The human evaluation was done online using the LG-Eval toolkit <ref type="bibr" target="#b14">(Kow and Belz, 2012)</ref>  <ref type="bibr">11</ref> . The evaluators were allocated a trial set using a Latin Square Experimental Design (LSED) such that each evaluator sees the same number of output from each system and for each test set item. Dur- ing the experiment, the evaluators were presented with a pair of a complex and a simple sentence(s) and asked to rate this pair w.r.t. to adequacy (Does the simplified sentence(s) preserve the meaning of the input?) and simplification (Does the gen- erated sentence(s) simplify the complex input?). They were also asked to rate the second (sim- plified) sentence(s) of the pair w.r.t. to fluency (Is the simplified output fluent and grammatical?). Similar to the Wubben's human evaluation setup, we randomly selected 20 complex sentences from Zhu's test corpus and included in the evaluation corpus: the corresponding simple (Gold) sentence from Zhu's test corpus, the output of our system (Hybrid) and the output of the other three sys- tems (Zhu, Woodsend and Wubben) which were provided to us by the system authors. The eval- uation data thus consisted of 100 complex/simple pairs. We collected ratings from 27 participants.</p><p>All were either native speakers or proficient in En- glish, having taken part in a Master taught in En- glish or lived in an English speaking country for an extended period of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Systems</head><p>Simplification   <ref type="table" target="#tab_10">Table 5</ref> shows the average ratings of the human evaluation on a slider scale from 0 to 5. Pairwise comparisons between all models and their statisti- cal significance were carried out using a one-way ANOVA with post-hoc Tukey HSD tests and are shown in <ref type="table" target="#tab_11">Table 6</ref>. With regard to simplification, our system ranks first and is very close to the manually simpli- fied input (the difference is not statistically signif- icant). The low rating for Woodsend reflects the high number of unsimplified sentences (24/100 in the test data used for the automatic evaluation and 6/20 in the evaluation data used for human judg- ments). Our system data is not significantly differ- ent from the manually simplified data for simplic- ity whereas all other systems are.</p><formula xml:id="formula_5">Systems GOLD Zhu Woodsend Wubben Zhu ‚ô¶‚ñ≥ Woodsend ‚ô¶‚ñ≥ ‚ô¶‚ñ≥ Wubben ‚ô¶ ‚ô¶‚ñ≥ ‚ñ≥ Hybrid ‚ñ≥ ‚ô¶ ‚ô¶</formula><p>For fluency, our system rates second behind Wubben and before Woodsend and Zhu. The difference between our system and both Zhu and Woodsend system is significant. In partic- ular, Zhu's output is judged less fluent proba- bly because of the many incorrect splits it li- censes. Manual examination of the data shows that Woodsend's system also produces incorrect splits. For this system however, the high propor- tion of non simplified sentences probably counter- balances these incorrect splits, allowing for a good fluency score overall.</p><p>Regarding adequacy, our system is against clos- est to the reference (3.50 for our system vs. 3.66 for manual simplification). Our system, the Wubben system and the manual simplifications are in the same group (the differences between these systems are not significant). The Wood- send system comes second and the Zhu system third (the difference between the two is signifi- cant). Wubben's high fluency, high adequacy but low simplicity could be explained with their min- imal number of edit (3.33 edits) from the source sentence.</p><p>In sum, if we group together systems for which there is no significant difference, our system ranks first (together with GOLD) for simplicity; first for fluency (together with GOLD and Wubben); and first for adequacy (together with GOLD and Wubben).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>A key feature of our approach is that it is seman- tically based. Typically, discourse level simplifi- cation operations such as sentence splitting, sen- tence reordering, cue word selection, referring ex- pression generation and determiner choice are se- mantically constrained. As argued by <ref type="bibr" target="#b17">Siddharthan (2006)</ref>, correctly capturing the interactions be- tween these phenomena is essential to ensuring text cohesion. In the future, we would like to investigate how our framework deals with such discourse level simplifications i.e., simplifications which involves manipulation of the coreference and of the discourse structure. In the PWKP data, the proportion of split sentences is rather low (6.1 %) and many of the split sentences are simple sen- tence coordination splits. A more adequate but small corpus is that used in <ref type="bibr" target="#b17">(Siddharthan, 2006</ref>) which consists of 95 cases of discourse simplifica- tion. Using data from the language learning or the children reading community, it would be interest- ing to first construct a similar, larger scale corpus; and to then train and test our approach on more complex cases of sentence splitting. <ref type="bibr">David Vickrey and Daphne Koller. 2008</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 2 )</head><label>2</label><figDesc>C. The judge ordered that Chapman should receive psychiatric treatment in prison and sentenced him to twenty years to life. S1. The judge ordered that Chapman should get psychi- atric treatment. In prison and sentenced him to twenty years to life. (Zhu et al., 2010) S2. The judge ordered that Chapman should receive psychiatric treatment in prison. It sentenced him to twenty years to life. (Woodsend and Lapata, 2011)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 4 )</head><label>4</label><figDesc>C. In 1964 Peter Higgs published his second paper in Physical Review Letters describing Higgs mechanism which predicted a new massive spin-zero boson for the first time. S. Peter Higgs wrote his paper explaining Higgs mech- anism in 1964. Higgs mechanism predicted a new ele- mentary particle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Simplification of "In 1964 Peter Higgs published his second paper in Physical Review Letters describing Higgs mechanism which predicted a new massive spin-zero boson for the first time ."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 2: An example training graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Simplification: DELETION (Relations, modifiers and OW respectively) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>p (D del |D split ) =</head><label>p</label><figDesc></figDesc><table>rel cand 

DELrel(relcand) 

mod cand 

DELmod(modcand) 

ow cand 

DELow(owcand) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Proportion of Split Sentences (% split) 
in the training/test data and in average per sen-
tence (average split / sentence). GOLD is the 
test data with the gold standard SWKP sentences; 
Zhu, Woodsend, Wubben are the best output of the 
models of Zhu et al. (2010), Woodsend and Lap-
ata (2011) and Wubben et al. (2012) respectively; 
Hybrid is our model. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Automated Metrics for Simplification: 
average Levenshtein distance (LD) to complex and 
simple reference sentences per system ; number of 
input sentences for which no simplification occur 
(No edit). 

BLEU score We used Moses support tools: 
multi-bleu 10 to calculate BLEU scores. The 
BLEU scores shown in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Average Human Ratings for simplicity, 
fluency and adequacy 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>‚ô¶/ is/not significantly different (sig. 
diff.) wrt simplicity. / is/not sig. diff. wrt 
fluency. ‚ñ≥/ is/not sig. diff. wrt adequacy. (sig-
nificance level: p &lt; 0.05) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>semantic role labeling. In Pro- ceedings of the 46th Annual Meeting of the Associ- ation for Computational Linguistics (ACL) and the Human Language Technology Conference (HLT), pages 344-352.</head><label></label><figDesc>. Sentence simplification for</figDesc><table>Willian Massami Watanabe, Arnaldo Candido Junior, 
Vin√≠cius Rodriguez Uz√™da, Renata Pontin de Mat-
tos Fortes, Thiago Alexandre Salgueiro Pardo, and 
Sandra Maria Alu√≠sio. 2009. Facilita: reading as-
sistance for low-literacy readers. In Proceedings of 
the 27th ACM international conference on Design of 
communication, pages 29-36. ACM. 

Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous 
grammar and integer programming. In Proceedings 
of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 409-420. 
Association for Computational Linguistics. 

Sander Wubben, Antal van den Bosch, and Emiel 
Krahmer. 2012. Sentence simplification by mono-
lingual machine translation. In Proceedings of the 
50th Annual Meeting of the Association for Com-
putational Linguistics (ACL): Long Papers-Volume 
1, pages 1015-1024. Association for Computational 
Linguistics. 

Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings 
of the 39th Annual Meeting on Association for Com-
putational Linguistics (ACL), pages 523-530. Asso-
ciation for Computational Linguistics. 

Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych. 
2010. A monolingual tree-based translation model 
for sentence simplification. In Proceedings of the 
23rd International Conference on Computational 
Linguistics (COLING), pages 1353-1361, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics. </table></figure>

			<note place="foot" n="3"> The DRS to graph conversion goes through several preprocessing steps: the relation nn is inverted making modifier noun (higgs) dependent of modified noun (mechanism), named and timex are converted to unary predicates, e.g., named(x, peter) is mapped to peter(x) and timex(x) = 1964 is mapped to 1964(x); and nodes are introduced for orphan words (e.g., which).</note>

			<note place="foot" n="4"> The splitting candidates could be sets of event variables depending on the number of splits required. Here, we consider pairs for 2 splits.</note>

			<note place="foot" n="5"> http://www.statmt.org/moses/ 6 http://www.speech.sri.com/projects/srilm/ 7 We downloaded the snapshots of Simple Wikipedia dated 2013-10-30 available at http://dumps.wikimedia.org/. 8 http://nlp.stanford.edu/software/corenlp.shtml 9 http://svn.ask.it.usyd.edu.au/trac/candc, Version 1.00</note>

			<note place="foot" n="10"> http://www.statmt.org/moses/?n=Moses.SupportTools 11 http://www.nltg.brighton.ac.uk/research/lg-eval/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to Zhemin Zhu, Kristian Wood-send and Sander Wubben for sharing their data. We would like to thank our annotators for partic-ipating in our human evaluation experiments and to anonymous reviewers for their insightful com-ments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Paraphrasing with bilingual parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Bannard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL)</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="597" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sentence alignment for monolingual comparable corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2003 conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Text simplification tools for spanish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Mille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the 8th International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1665" to="1671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Syntactic simplification of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvonne</forename><forename type="middle">Margaret</forename><surname>Canning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>University of Sunderland</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simplifying text for language-impaired readers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvonne</forename><surname>Canning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siobhan</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 9th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>9th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="269" to="270" />
		</imprint>
	</monogr>
	<note>Citeseer</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Automatic induction of rules for text simplification. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Chandrasekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangalore</forename><surname>Srinivas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Motivations and methods for text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Chandrasekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangalore</forename><surname>Srinivas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International conference on Computational linguistics (COLING)</title>
		<meeting>the 16th International conference on Computational linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1041" to="1044" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to simplify sentences using wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Coster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Monolingual Text-To-Text Generation</title>
		<meeting>the Workshop on Monolingual Text-To-Text Generation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Linguistically motivated large-scale NLP with C&amp;C and Boxer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>James R Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL) on Interactive Poster and Demonstration Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics (ACL) on Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tree adjoining grammar and the reluctant paraphrasing of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Macquarie University NSW 2109 Australia</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dependency tree based sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Natural Language Generation Conference (INLG)</title>
		<meeting>the Fifth International Natural Language Generation Conference (INLG)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A theory of truth and semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Kamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Formal methods in the study of language</title>
		<editor>J.A.G. Groenendijk, T.M.V. Janssen, B.J. Stokhof, and M.J.B. Stokhof</editor>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
	<note>number pt. 1 in Mathematical Centre tracts. Mathematisch Centrum</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statisticsbased summarization-step one: Sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth National Conference on Artificial Intelligence (AAAI) and Twelfth Conference on Innovative Applications of Artificial Intelligence (IAAI)</title>
		<meeting>the Seventeenth National Conference on Artificial Intelligence (AAAI) and Twelfth Conference on Innovative Applications of Artificial Intelligence (IAAI)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="703" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">LG-Eval: A Toolkit for Creating Online Language Evaluation Experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Belz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the 8th International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="4033" to="4037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Syntactic simplification for improving content selection in multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Advaith</forename><surname>Siddharthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computational Linguistics</title>
		<meeting>the 20th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">896</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An architecture for a text simplification system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Advaith</forename><surname>Siddharthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Language Engineering Conference (LEC)</title>
		<meeting>the Language Engineering Conference (LEC)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="64" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Syntactic simplification and text cohesion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Advaith</forename><surname>Siddharthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research on Language and Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="109" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Complex lexico-syntactic reformulation of sentences using typed dependency representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Advaith</forename><surname>Siddharthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Natural Language Generation Conference (INLG)</title>
		<meeting>the 6th International Natural Language Generation Conference (INLG)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="125" to="133" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Text simplification using typed dependencies: a comparison of the robustness of different generation strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Advaith</forename><surname>Siddharthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Workshop on Natural Language Generation (ENLG)</title>
		<meeting>the 13th European Workshop on Natural Language Generation (ENLG)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Quasisynchronous grammars: Alignment by soft projection of syntactic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the HLT-NAACL Workshop on Statistical Machine Translation</title>
		<meeting>the HLT-NAACL Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
