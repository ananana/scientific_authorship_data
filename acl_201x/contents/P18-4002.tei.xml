<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NovelPerspective: Identifying Point of View Characters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyndon</forename><surname>White</surname></persName>
							<email>lyndon.white@research.uwa.edu.au, roberto.togneri@uwa.edu.au, wei.liu@uwa.edu.au,</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Western Australia</orgName>
								<address>
									<addrLine>35 Stirling Highway</addrLine>
									<settlement>Crawley</settlement>
									<region>Western</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Togneri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Western Australia</orgName>
								<address>
									<addrLine>35 Stirling Highway</addrLine>
									<settlement>Crawley</settlement>
									<region>Western</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Western Australia</orgName>
								<address>
									<addrLine>35 Stirling Highway</addrLine>
									<settlement>Crawley</settlement>
									<region>Western</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
							<email>mohammed.bennamoun@uwa.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Western Australia</orgName>
								<address>
									<addrLine>35 Stirling Highway</addrLine>
									<settlement>Crawley</settlement>
									<region>Western</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NovelPerspective: Identifying Point of View Characters</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics-System Demonstrations</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics-System Demonstrations <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="7" to="12"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>7</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present NovelPerspective: a tool to allow consumers to subset their digital literature , based on point of view (POV) character. Many novels have multiple main characters each with their own storyline running in parallel. A well-known example is George R. R. Martin&apos;s novel: &quot;A Game of Thrones&quot;, and others from that series. Our tool detects the main character that each section is from the POV of, and allows the user to generate a new ebook with only those sections. This gives consumers new options in how they consume their media; allowing them to pursue the storylines sequentially, or skip chapters about characters they find boring. We present two heuristic-based baselines, and two machine learning based methods for the detection of the main character.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Often each section of a novel is written from the perspective of a different main character. The characters each take turns in the spot-light, with their own parallel storylines being unfolded by the author. As readers, we have often desired to read just one storyline at a time, particularly when read- ing the book a second-time. In this paper, we present a tool, NovelPerspective, to give the con- sumer this choice.</p><p>Our tool allows the consumer to select which characters of the book they are interested in, and to generate a new ebook file containing just the sec- tions from that character's point of view (POV). The critical part of this system is the detection of the POV character. This is not an insurmountable task, building upon the well established field of named entity recognition. However to our knowl- edge there is no software to do this. Such a tool would have been useless, in decades past when booked were distributed only on paper. But today, the surge in popularity of ebooks has opened a new niche for consumer narrative processing. Meth- ods are being created to extract social relationships between characters ( <ref type="bibr" target="#b3">Elson et al., 2010;</ref><ref type="bibr" target="#b8">Wohlgenannt et al., 2016)</ref>; to align scenes in movies with those from books ( <ref type="bibr" target="#b9">Zhu et al., 2015)</ref>; and to oth- erwise augment the literature consumption experi- ence. Tools such as the one presented here, give the reader new freedoms in controlling how they consume their media.</p><p>Having a large cast of characters, in particu- lar POV characters, is a hallmark of the epic fan- tasy genre. Well known examples include: George R.R. Martin's "A Song of Ice and Fire", Robert Jordan's "Wheel of Time", Brandon Sander- son's "Cosmere" universe, and Steven Erikson's "Malazan Book of the Fallen", amongst thousands of others. Generally, these books are written in limited third-person POV; that is to say the reader has little or no more knowledge of the situation described than the main character does.</p><p>We focus here on novels written in the lim- ited third-person POV. In these stories, the main character is, for our purposes, the POV character. Limited third-person POV is written in the third- person, that is to say the character is referred to by name, but with the observations limited to be- ing from the perspective of that character. This is in-contrast to the omniscient third-person POV, where events are described by an external narra- tor. Limited third-person POV is extremely popu- lar in modern fiction. It preserves the advantages of first-person, in allowing the reader to observe inside the head of the character, while also al- lowing the flexibility to the perspective of another character <ref type="bibr" target="#b2">(Booth, 1961)</ref>. This allows for multiple concurrent storylines around different characters.</p><p>Our tool helps users un-entwine such storylines, giving the option to process them sequentially.</p><p>The utility of dividing a book in this way varies with the book in question. Some books will cease to make sense when the core storyline crosses over different characters. Other novels, particularly in epic fantasy genre, have parallel storylines which only rarely intersect. While we are unable to find a formal study on this, anecdotally many readers speak of:</p><p>• "Skipping the chapters about the boring char- acters."</p><p>• "Only reading the real main character's sec- tions."</p><p>• "Reading ahead, past the side-stories, to get on with the main plot."</p><p>Particularly if they have read the story before, and thus do not risk confusion. Such opinions are a matter of the consumer's personal taste. The Nov- elPerspective tool gives the reader the option to customise the book in this way, according to their personal preference. We note that sub-setting the novel once does not prevent the reader from going back and reading the intervening chapters if it ceases to make sense, or from sub-setting again to get the chapters for an- other character whose path intersects with the sto- ryline they are currently reading. We can person- ally attest for some books reading the chapters one character at a time is indeed possible, and pleas- ant: the first author of this paper read George R.R. Martin's "A Song of Ice and Fire" series in exactly this fashion.</p><p>The primary difficulty in segmenting ebooks this way is attributing each section to its POV char- acter. That is to say detecting who is the point of view character. Very few books indicate this clearly, and the reader is expected to infer it dur- ing reading. This is easy for most humans, but au- tomating it is a challenge. To solve this, the core of our tool is its character classification system. We investigated several options which the main text of this paper will discuss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Character Classification Systems</head><p>The full NovelPerspective pipeline is shown in <ref type="figure">Figure 1</ref>. The core character classification step (step 3), is detailed in <ref type="figure">Figure 2</ref>. In this step the raw text is first enriched with parts of speech, and named entity tags. We do not perform co- reference resolution, working only with direct en- tity mentions. From this, features are extracted for each named entity. These feature vectors are used to score the entities for the most-likely POV char- acter. The highest scoring character is returned by the system. The different systems presented mod- ify the Feature Extraction and Character Scor- ing steps. A broadly similar idea, for detecting the focus location of news articles, was presented by <ref type="bibr" target="#b4">(Imani et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Baseline systems</head><p>To the best of our knowledge no systems have been developed for this task before. As such, we have developed two deterministic baseline charac- ter classifiers. These are both potentially useful to the end-user in our deployed system (Section 5), and used to gauge the performance of the more complicated systems in the evaluations presented in Section 4.</p><p>It should be noted that the baseline systems, while not using machine learning for the charac- ter classification steps, do make extensive use of machine learning-based systems during the pre- processing stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">"First Mentioned" Entity</head><p>An obvious way to determine the main character of the section is to select the first named entity. We use this to define the "First Mentioned" base- line In this system, the Feature Extraction step is simply retrieving the position of the first use of each name; and the Character Scoring step as- signs each a score such that earlier is higher. This works for many examples: "One dark and stormy night, Bill heard a knock at the door."; however it fails for many others: " 'Is that Tom?' called out Bill, after hearing a knock.''. Sometimes a sec- tion may go several paragraphs describing events before it even mentions the character who is per- ceiving them. This is a varying element of style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">"Most Mentioned" Entity</head><p>A more robust method to determine the main char- acter, is to use the occurrence counts. We call this the "Most Mentioned" baseline. The Fea- ture Extraction step is to count how often the name is used. The Character Scoring step as- signs each a score based what proportional of all names used were for this entity. This works well for many books. The more important a character  <ref type="figure">Figure 2</ref>: The general structure of the character classification systems. This repeated for each section of the book during step 3 of the full pipeline shown in <ref type="figure">Figure 1</ref>.</p><p>is, the more often their name occurs. However, it is fooled, for example, by book chapters that are about the POV character's relationship with a secondary character. In such cases the secondary character may be mentioned more often.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Machine learning systems</head><p>One can see the determination of the main charac- ter as a multi-class classification problem. From the set of all named entities in the section, classify that section as to which one is the main character. Unlike typical multi-class classification problems the set of possible classes varies per section being classified. Further, even the total set of possible named characters, i.e. classes, varies from book to book. An information extraction approach is re- quired which can handle these varying classes. As such, a machine learning model for this task can not incorporate direct knowledge of the classes (i.e. character names). We reconsider the problem as a series of bi- nary predictions. The task is to predict if a given named entity is the point of view character. For each possible character (i.e. each named-entity that occurs), a feature vector is extracted (see Sec- tion 2.2.1). This feature vector is the input to a binary classifier, which determines the probability that it represents the main character. The Charac- ter Scoring step is thus the running of the binary classifier: the score is the output probability nor- malised over all the named entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Feature Extraction for ML</head><p>We investigated two feature sets as inputs for our machine learning-based solution. They cor- respond to different Feature Extraction steps in <ref type="figure">Figure 2</ref>. A hand-engineered feature set, that we call the "Classical" feature set; and a more modern "Word Embedding" feature set. Both feature sets give information about how the each named entity token was used in the text.</p><p>The "Classical" feature set uses features that are well established in NLP related tasks. The features can be described as positional features, like in the First Mentioned baseline; occurrence count fea- tures, like in the Most Mentioned baseline and ad- jacent POS counts, to give usage context. The po- sitional features are the index (in the token counts) of the first and last occurrence of the named en- tity. The occurrence count features are simply the number of occurrences of the named entity, sup- plemented with its rank on that count compared to the others. The adjacent POS counts are the occurrence counts of each of the 46 POS tags on the word prior to the named entity, and on the word after. We theorised that this POS informa- tion would be informative, as it seemed reason- able that the POV character would be described as doing more things, so co-occurring with more verbs. This gives 100 base features. To allow for text length invariance we also provide each of the base features expressed as a portion of its maxi- mum possible value (e.g. for a given POS tag oc- curring before a named entity, the potion of times this tag occurred). This gives a total of 200 fea- tures.</p><p>The "Word Embedding" feature set uses Fast- Text word vectors ( <ref type="bibr" target="#b1">Bojanowski et al., 2017</ref>). We use the pretrained 300 dimensional embeddings trained on English Wikipedia 1 . We concate- nate the 300 dimensional word embedding for the word immediately prior to, and immediately af- ter each occurrence of a named entity; and take the element-wise mean of this concatenated vector over all occurrences of the entity. Such averages of word embeddings have been shown to be a useful feature in many tasks ( <ref type="bibr" target="#b7">White et al., 2015;</ref><ref type="bibr" target="#b5">Mikolov et al., 2013)</ref>. This has a total of 600 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Classifier</head><p>The binary classifier, that predicts if a named en- tity is the main character, is the key part of the Character Scoring step for the machine learning systems. From each text in the training dataset we generated a training example for every named en- tity that occurred. All but one of these was a neg- ative example. We then trained it as per normal for a binary classifier. The score for a character is the classifier's predicted probability of its feature vector being for the main character.</p><p>Our approach of using a binary classifier to rate each possible class, may seem similar to the one-vs-rest approach for multi-class classification. However, there is an important difference. Our system only uses a single binary classifier; not one classifier per class, as the classes in our case vary with every item to be classified. The fundamental problem is information extraction, and the classi- fier is a tool for the scoring which is the correct information to report.</p><p>With the classical feature set we use logistic regression, with the features being preprocessed with 0-1 scaling. During preliminary testing we found that many classifiers had similar high de- gree of success, and so chose the simplest. With the word embedding feature set we used a radial bias support vector machine, with standardisation during preprocessing, as has been commonly used with word embeddings on other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We make use of three series of books selected from our own personal collections. The first four books of George R. R. Martin's "A Song of Ice and Fire" series (hereafter referred to as ASOIAF); The two books of Leigh Bardugo's "Six of Crows" duology (hereafter referred to as SOC); and the first 9 vol- umes of Robert Jordan's "Wheel of Time" series (hereafter referred to as WOT). In Section 4 we consider the use of each as a training and testing dataset. In the online demonstration (Section 5), we deploy models trained on the combined total of all the datasets.</p><p>To use a book for the training and evaluation of our system, we require a ground truth for each sec- tion's POV character. ASOIAF and SOC provide Dataset Chapters <ref type="table" target="#tab_2">POV Characters   ASOIAF  256  15  SOC  91  9  WOT  432  52</ref> combined 779 76 <ref type="table">Table 1</ref>: The number of chapters and point of view characters for each dataset.</p><p>ground truth for the main character in the chapter names. Every chapter only uses the POV of that named character. WOT's ground truth comes from an index created by readers. <ref type="bibr">2</ref> We do not have any datasets with labelled sub-chapter sections, though the tool does support such works. The total counts of chapters and characters in the datasets, after preprocessing, is shown in Ta- ble 1. Preprocessing consisted of discarding chap- ters for which the POV character was not identi- fied (e.g. prologues); and of removing the charac- ter names from the chapter titles as required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Details</head><p>In the evaluation, the systems are given the body text and asked to predict the character names. Dur- ing evaluation, we sum the scores of the char- acters alternative aliases/nick-names used in the books. For example merging Ned into Eddard in ASOIAF. This roughly corresponds to the case that a normal user can enter multiple aliases into our application when selecting sections to keep. We do not use these aliases during training, though that is an option that could be investigated in a fu- ture work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation</head><p>The full source code is available on GitHub. 3 Scikit-Learn ( <ref type="bibr" target="#b6">Pedregosa et al., 2011</ref>) is used for the machine learning and evaluations, and NLTK ( <ref type="bibr" target="#b0">Bird and Loper, 2004</ref>) is used for textual prepro- cessing. The text is tokenised, and tagged with POS and named entities using NLTK's default methods. Specifically, these are the Punkt sen- tence tokenizer, the regex-based improved Tree- Bank word tokenizer, greedy averaged perceptron POS tagger, and the max-entropy binary named entity chunker. The use of a binary, rather than  a multi-class, named entity chunker is significant. Fantasy novels often use "exotic" names for char- acters, we found that this often resulted in charac- ter named entities being misclassified as organisa- tions or places. Note that this is particularly dis- advantageous to the First Mentioned baseline, as any kind of named entity will steal the place. Nev- ertheless, it is required to ensure that all character names are a possibility to be selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>Our evaluation results are shown in <ref type="table" target="#tab_2">Table 2</ref> for all methods. This includes the two baseline methods, and the machine learning methods with the differ- ent feature sets. We evaluate the machine learning methods using each dataset as a test set, and using each of the other two and their combination as the training set.</p><p>The First Mentioned baseline is very weak. The Most Mentioned baseline is much stronger. In al- most all cases machine learning methods outper- form both baselines. The results of the machine learning method on the ASOIAF and SOC are very strong. The results for WOT are weaker, though they are still accurate enough to be useful when combined with manual checking.</p><p>It is surprising that using the combination of  two training sets does not always out-perform each on their own. For many methods training on just one dataset resulted in better results. We believe that the difference between the top result for a method and the result using the combined train- ing sets is too small to be meaningful. It can, per- haps, be attributed to a coincidental small similar- ity in writing style of one of the training books to the testing book. To maximise the generalisability of the NovelPerspective prototype (see Section 5), we deploy models trained on all three datasets combined. Almost all the machine learning models re- sulted in similarly high accuracy. The exception to this is word embedding features based model trained on SOC, which for both ASOIAF and WOT test sets performed much worse. We at- tribute the poor performance of these models to the small amount of training data. SOC has only 91 chapters to generate its training cases from, and the word embedding feature set has 600 dimen- sions. It is thus very easily to over-fit which causes these poor results. <ref type="table" target="#tab_4">Table 3</ref> shows the training set accuracy of each machine learning model. This is a rough upper bound for the possible performance of these mod- els on each test set, as imposed by the classifier and the feature set. The WOT bound is much lower than the other two texts. This likely re- lates to WOT being written in a style that closer to the line between third-person omniscient, than the more clear third-person limited POV of the other texts. We believe longer range features are required to improve the results for WOT. How- ever, as this achieves such high accuracy for the other texts, further features would not improve ac- curacy significantly, without additional more diffi- cult training data (and may cause over-fitting).</p><p>The results do not show a clear advantage to ei- ther machine learning feature set. Both the classi- cal features and the word embeddings work well.</p><p>Though, it seems that the classical feature are more robust; both with smaller training sets (like SOC), and with more difficult test sets (like WOT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Demonstration System</head><p>The demonstration system is deployed online at https://white.ucc.asn.au/tools/np. A video demonstrating its use can be found at https://youtu.be/iu41pUF4wTY. This web-app, made using the CherryPy framework, <ref type="bibr">4</ref> allows the user to apply any of the model discussed to their own novels.</p><p>The web-app functions as shown in <ref type="figure">Figure 1</ref>. The user uploads an ebook, and selects one of the character classification systems that we have discussed above. They are then presented with a page displaying a list of sections, with the pre- dicted main character(/s) paired with an excerpt from the beginning of the section. The user can adjust to show the top-k most-likely characters on this screen, to allow for additional recall.</p><p>The user can select sections to retain. They can use a regular expression to match the charac- ter names(/s) they are interested in. The sections with matching predicted character names will be selected. As none of the models is perfect, some mistakes are likely. The user can manually correct the selection before downloading the book.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a tool to allow consumers to re- structure their ebooks around the characters they find most interesting. The system must discover the named entities that are present in each sec- tion of the book, and then classify each section as to which character's point of view the section is narrated from. For named entity detection we make use of standard tools. However, the clas- sification is non-trivial. In this design we im- plemented several systems. Simply selecting the most commonly named character proved success- ful as a baseline approach. To improve upon this, we developed several machine learning based ap- proaches which perform very well. While none of the classifiers are perfect, they achieve high enough accuracy to be useful.</p><p>A future version of our application will allow the users to submit corrections, giving us more training data. However, storing this information poses copyright issues that are yet to be resolved.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>: The full NovelPerspective pipeline. Note that step 5 uses the original ebook to subset.</head><label></label><figDesc></figDesc><table>1. 
User 
uploads 
ebook 

2. 
File is 
converted and 
preprocessed 

3. 
Sections are 
classifed by character 
See Figure 2 

4. 
User selects 
sections 
to keep 

5. 
Subsetted 
ebook 
is created 

6. 
User 
downloads 
new ebook 

original ebook 
+ settings 
section 
content 

section-character 
list 

section 
selection 

new 
ebook 

Figure 1Tokenization 
POS Tagging 
Named Entity Tagging 

Feature 
Extraction 

Character 
Scoring 

POV Character 
Classification 

raw 
text 

enriched 
text 

character-name 
feature-vector 
pairs 

character-name 
score pairs 
character 
name 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>The results of the character classifier sys- tems. The best results are bolded.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The training set accuracy of the machine 
learning character classifier systems. 

</table></figure>

			<note place="foot" n="1"> https://fasttext.cc/docs/en/ pretrained-vectors.html</note>

			<note place="foot" n="2"> http://wot.wikia.com/wiki/List_of_ Point_of_View_Characters 3 https://github.com/oxinabox/ NovelPerspective/</note>

			<note place="foot" n="4"> http://cherrypy.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nltk: the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2004 on Interactive poster and demonstration sessions</title>
		<meeting>the ACL 2004 on Interactive poster and demonstration sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The rhetoric of fiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Booth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1961" />
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extracting social networks from literary fiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dames</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="138" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Focus location extraction from political news reports with bias correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Imani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thuraisingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1956" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How well sentence embeddings capture meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Australasian Document Computing Symposium, ADCS &apos;15</title>
		<meeting>the 20th Australasian Document Computing Symposium, ADCS &apos;15</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extracting social networks from literary text with word embedding tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wohlgenannt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chernyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ilvovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities (LT4DH)</title>
		<meeting>the Workshop on Language Technology Resources and Tools for Digital Humanities (LT4DH)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
