<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Semantic Parsers For If-Then Statements</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
							<email>beltagy@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Texas at Austin</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
							<email>chrisq@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Texas at Austin</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Semantic Parsers For If-Then Statements</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="726" to="736"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Digital personal assistants are becoming both more common and more useful. The major NLP challenge for personal assistants is machine understanding: translating natural language user commands into an executable representation. This paper focuses on understanding rules written as If-Then statements, though the techniques should be portable to other semantic parsing tasks. We view understanding as structure prediction and show improved models using both conventional techniques and neural network models. We also discuss various ways to improve generalization and reduce overfitting: synthetic training data from paraphrase, grammar combinations , feature selection and ensembles of multiple systems. An ensemble of these techniques achieves a new state of the art result with 8% accuracy improvement.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability to instruct computers using natural lan- guage clearly allows novice users to better use modern information technology. Work in se- mantic parsing has explored mapping natural lan- guage to some formal domain-specific program- ming languages such as database queries <ref type="bibr" target="#b22">(Woods, 1977;</ref><ref type="bibr" target="#b24">Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b4">Berant et al., 2013;</ref><ref type="bibr" target="#b1">Andreas et al., 2016;</ref><ref type="bibr" target="#b23">Yin et al., 2016)</ref>, commands to robots <ref type="bibr" target="#b14">(Kate et al., 2005</ref>), operat- ing systems <ref type="bibr">(Branavan et al., 2009)</ref>, and spread- sheets ( <ref type="bibr" target="#b11">Gulwani and Marron, 2014</ref>). This pa- per explores the use of neural network models (NN) and conventional models for semantic pars- ing. Recently approaches using neural networks have shown great improvements in a number of areas such as parsing ( <ref type="bibr" target="#b20">Vinyals et al., 2015)</ref>, ma- chine translation <ref type="bibr">(Devlin et al., 2014)</ref>, and image captioning <ref type="bibr" target="#b13">(Karpathy and Fei-Fei, 2015</ref>). We are among the first to apply neural network methods to semantic parsing tasks ( <ref type="bibr" target="#b10">Grefenstette et al., 2014;</ref><ref type="bibr" target="#b5">Dong and Lapata, 2016)</ref>.</p><p>There are several benchmark datasets for se- mantic parsing, the most well known of which is Geoquery ( <ref type="bibr" target="#b24">Zelle and Mooney, 1996)</ref>. We target an If-Then dataset <ref type="bibr" target="#b17">(Quirk et al., 2015</ref>) for sev- eral reasons. First, it is both directly applica- ble to the end-user task of training personal dig- ital assistants. Second, the training data, drawn from the site http://ifttt.com, is com- paratively quite large, containing nearly 100,000 recipe-description pairs. That said, it is several or- ders of magnitude smaller than the data for other tasks where neural networks have been successful. Machine translation datasets, for instance, may contain billions of tokens. NN methods appear "data-hungry". They require larger datasets to out- perform sparse linear approaches with careful fea- ture engineering, as evidenced in work on syntac- tic parsing <ref type="bibr" target="#b20">(Vinyals et al., 2015)</ref>. This makes it interesting to compare NN models with conven- tional models on this dataset.</p><p>As in most prior semantic parsing attempts, we model natural language understanding as a struc- ture prediction problem. Each modeling decision predicts some small component of the target struc- ture, conditioned on the whole input and all prior decisions. Because this is a real-world task, the vocabulary is large and varied, with many words appearing only rarely. Overfitting is a clear dan- ger. We explore several methods to improve gen- eralization. A classic method is to apply feature selection. Synthetic data generated by paraphras- ing helps augment the data available. Adjusting the conditional structure of our model also makes sense, as does creating ensembles of the best per- forming approaches.</p><p>An ensemble of the resulting systems achieves a new state-of-the-art result, with an absolute im- provement of 8% in accuracy. We compare the performance of a neural network model with lo- gistic regression, and explore in detail the contri- bution of each of them, and why the logistic re- gression is performing better than the neural net- work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Parsing</head><p>Semantic parsing is the task of translating natu- ral language to a meaning representation language that the machine can execute. Various seman- tic parsing tasks have been proposed before, in- cluding querying a database ( <ref type="bibr" target="#b24">Zelle and Mooney, 1996)</ref>, following navigation instructions <ref type="bibr">(Chen, 2012)</ref>, translating to Abstract Meaning Represen- tation (AMR) ( <ref type="bibr" target="#b2">Artzi et al., 2015)</ref>, as well as the If-Then task we explore. Meaning representa- tion languages vary with the task. In database queries, the meaning representation language is ei- ther the native query language (e.g. SQL or Pro- log), or some alternative that can be deterministi- cally transformed into the native query language. To follow navigation instructions, the meaning representation language is comprised of sequences of valid actions: turn left, turn right, move for- ward, etc. For parsing If-Then rules, the meaning representation is an abstract syntax tree (AST) in a very simple language. Each root node expands into a "trigger" and "action" pair. These nodes in turn expand into a set of supported triggers and ac- tions. We model these trees as an (almost) context free grammar 1 that generates valid If-Then tasks.</p><p>A number of semantic parsing approaches have been proposed, but most fit into the following broad divisions. First, approaches driven by Combinatory Categorical Grammar (CCG) have proven successful at several semantic parsing tasks. This approach is attractive in that it simul- taneously provides syntactic and semantic parses of a natural language utterance. Syntactic struc- ture helps constrain and guide semantic interpre- tation. CCG relies heavily on a lexicon that spec- ifies both the syntactic category and formal se- 1 Information at the leaves of the action may use parame- ters drawn from the trigger. For instance, consider a rule that says "text me the daily weather report." The trigger is a new weather report, and the action is to send an SMS. The con- tents of that SMS are generated by the trigger, which is no longer context free. mantics of each lexical item in the language. In many instantiations, the lexicon is learned from the training data <ref type="bibr" target="#b25">(Zettlemoyer and Collins, 2005)</ref> and grounds directly in the meaning representa- tion.</p><p>Another approach is to view the semantic pars- ing task as a machine translation task, where the source language is natural language commands and the target language is the meaning represen- tation. Several approaches have applied standard machine translation techniques to semantic pars- ing ( <ref type="bibr" target="#b21">Wong and Mooney, 2006;</ref><ref type="bibr" target="#b0">Andreas et al., 2013;</ref><ref type="bibr" target="#b18">Ratnaparkhi, 1999</ref>) with successful results.</p><p>More recently, neural network approaches have been developed for semantic parsing, and espe- cially for querying a database. A neural network is trained to translate the query and the database into some continuous representation then use it to answer the query ( <ref type="bibr" target="#b1">Andreas et al., 2016;</ref><ref type="bibr" target="#b23">Yin et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">If-Then dataset</head><p>We use a semantic parsing dataset collected from http://ifttt.com, first introduced in <ref type="bibr" target="#b17">Quirk et al. (2015)</ref>. This website publishes a large set of recipes in the form of If-Then rules. Each recipe was authored by a website user to automate sim- ple tasks. For instance, a recipe could send you a message every time you are tagged on a pic- ture on Facebook. From a natural language stand- point, the most interesting part of this data is that alongside each recipe, there is a short natural lan- guage description intended to name or advertise the task. This provides a naturalistic albeit often noisy source of parallel data for training seman- tic parsing systems. Some of these descriptions faithfully represent the program. Others are under- specified or suggestive, with many details of the recipe are not uniquely specified or omitted alto- gether. The task is to predict the correct If-Then code given a natural language description.</p><p>As for the code, If-Then statements follow the format I f T r i g g e r C h a n n e l .</p><p>T r i g g e r F u n c t i o n ( a r g s ) Then A c t i o n C h a n n e l .</p><p>A c t i o n F u n c t i o n ( a r g s )</p><p>Every If-Then statement has exactly one trigger and one action. Each trigger and action consist of both a channel and a function. The channel repre- sents a connection to a service, website, or device (e.g., Facebook, Android, or ESPN) and provides a set of functions relevant to that channel. Finally, each of these functions may take a number of argu- ments: to receive a trigger when it becomes sunny, we need to specify the location to watch. The re- sulting dataset after cleaning and separation con- tains 77,495 training recipes, 5,171 development recipes and 4,294 testing recipes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Semantic parsing for If-Then rules</head><p>Both CCG and MT-inspired approaches assume a fairly strong correspondence between the words in the natural language request and the concepts in the meaning representation. That is, most words in the description should correspond to some con- cept in the code, and most concepts in the code should correspond to some word in the descrip- tion. However, prior work on this dataset <ref type="bibr" target="#b17">(Quirk et al., 2015)</ref> found that this strong correspondence is often missing. The descriptions may mention only the most crucial or interesting concepts; the remainder of the meaning representation must be inferred from context. The best performing meth- ods focused primarily on generating well-formed meaning representations, conditioning their deci- sions on the source language. <ref type="bibr" target="#b17">Quirk et al. (2015)</ref> proposed two models that rely on a grammar to generate all valid ASTs. The first model learns a simple classifier for each production in the grammar, treating the sentence as a bag of features. No alignment between the language and meaning representation is assumed. The second method attempts to learn a correspon- dence between the language and the code, jointly learning to select the correct productions in the meaning representation grammar. Although the latter approach is more appealing from a modeling standpoint, empirically it doesn't perform substan- tially better than the alignment-free model. Fur- thermore the alignment-free model is much sim- pler to implement and optimize. Therefore, we build upon the alignment-free approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Neural Networks</head><p>Neural network approaches have recently made great strides in several natural language process- ing tasks, including machine translation and de- pendency parsing. Partially these gains are due to better generalization ability. Until recently, the NLP community leaned heavily on feature- rich approaches that allow models to learn com- plex relationships from data. However, impor- tant features, such as indicator features for words and phrases, were often very sparse. Furthermore, the best systems often relied on manually-induced feature combinations <ref type="bibr">(Bohnet, 2010)</ref>. Multi-layer neural networks have several advantages. Words (or, more generally, features) are first embedded into a continuous space where similar features land in nearby locations; this helps lead to lexical generalization. The additional hidden layers can model feature interactions in complex ways, obvi- ating the need for manual feature template induc- tion. Feed-forward neural networks with relatively simple structure have shown great gains in both dependency parsing <ref type="bibr">(Chen and Manning, 2014</ref>) and machine translation <ref type="bibr">(Devlin et al., 2014</ref>) with- out the need for complex feature templates and large models. Our NN models here are inspired by these effective approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We next describe the details of how If-Then recipes are constructed given natural language de- scriptions. As in prior work, we treat semantic parsing as a structure prediction task. First we de- scribe the structure and features of the model, then expand on the details of inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Grammar</head><p>Along the lines of <ref type="bibr" target="#b17">Quirk et al. (2015)</ref>, we build a context-free grammar baseline. This grammar generates only well-formed meaning representa- tions. In the case of this dataset, meaning repre- sentations always consist of a root production with two children: a trigger and an action. Both trigger and action first generate a channel, then a function matching that action. Optionally we may also gen- erate the arguments of these functions; we do not evaluate these selections as they are often idiosyn- cratic and specific to the user. For example, the recipe Autosave your Instagram photos to Drop- box has the following meaning representation: This examples shows also that most of the function arguments are not crucial for the representation of the If-Then statement. <ref type="bibr">2</ref> The grammar we use has productions corre- sponding to every channel and every function. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example derivation tree D. This grammar consists of 892 productions: 128 trigger channels, 487 trigger functions, 99 action channels and 178 action functions. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model</head><p>Our goal is to learn a model of derivation trees D given a natural sentences S. To predict the deriva- tion for a sentence, we seek the derivation D with maximum probability given the sentence P (D|S).</p><p>For the purposes of modeling, we prefer to work with sequences rather than trees. Given a deriva- tion tree D, we transform it into a sequence of pro- ductions R(D) = r 1 , . . . , r n by a top-down, left- to-right tree traversal: r 1 is the top-most produc- tion, and r n is the bottom right production. The sentence S is represented as a set of features f (S).</p><p>The derivation score P (D|S) is a function of the productions of D and those features f (S):</p><formula xml:id="formula_0">P (D|S) = r i ∈R(D) P (r i |r 1 , . . . , r i−1 , f (S)) (1)</formula><p>The score of a derivation tree given the sentence is the product of probabilities of its productions. The probability of selecting production r i given the sentence S is dependent on the features of the sentence as well as the previous productions r 1 , . . . , r i−1 ; namely, all those productions that are above and to the left of the current produc- tion. Conditioning on previous productions helps predicting the next one because it captures the conditional dependencies between the productions of the derivation tree, an improvement over prior work ( <ref type="bibr" target="#b17">Quirk et al., 2015</ref>). In particular, we can model which combinations of triggers and actions are more compatible, both function and channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>To learn the derivation score P (D|S), we need to learn probability of productions P (r i |r 1 , . . . , r i−1 , f (S)). We learn this prob- ability using a multiclass classifier where the output classes are the possible productions in the grammar. The classifier is trained to predict the next production given previous productions and the sentence features. Each sentence S is represented with a sparse feature vector f (S). We used a simple set of fea- tures: word unigrams and bigrams, character tri- grams, and Brown clusters ( <ref type="bibr" target="#b15">Liang, 2005)</ref>. Each sentence is represented as a large sparse k-hot vec- tor, where k is the number of features representing S, |f (S)|. We use a simple one-hot representation of prior rules.</p><p>For training, we explored two approaches: a standard logistic regression classifier, and a feed forward neural network classifier. <ref type="bibr">4</ref> As for net- work structure, we evaluated models with either one or two 200-dimensional hidden layers (with sigmoid activation function) followed by a soft- max output layer to produce a probability for each production. We tried more than two hidden lay- ers and larger hidden layer size, but the results were similar or worse likely because training be- comes more difficult. <ref type="figure" target="#fig_2">Figure 2</ref> shows the archi- tecture of the network we use. For training, we used a variant of stochastic gradient descent called RMSprop ( <ref type="bibr">Dauphin et al., 2015</ref> <ref type="bibr" target="#b12">Johnson, 2015)</ref>.</p><note type="other">) that adjusts the learning rate for each parameter adaptively, along with a global learning rate of 1 −3 . The mini- batch size was 100, with dropout regularization for hidden layers at 0.5 along with an L2 regular- izer with weight 0.005. Each of these parameters were tuned on the validation set, though we found learning to be robust to minor variations in these parameters. All of the neural networks were im- plemented with Theanets (</note><p>Note that history features r 1 , . . . , r i−1 in clas- sifier training are always correct. The model is akin to a MEMM, rather than a CRF. We make this simplifying assumption for tractability, like many neural network approaches <ref type="bibr">(Devlin et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference</head><p>When, at test time, we are given a new sentence, we would like to infer its most probable deriva- tion tree D. Classifiers trained as in the prior section give probability distributions over produc- tions given the sentence and all prior productions P (r i |r 1 , . . . , r i−1 , f (S)). Were the distribution to be context free, we could rely on algorithms similar to Earley parsing <ref type="bibr" target="#b6">(Earley, 1970)</ref> to find the max derivation. However, the dependency on prior productions breaks the context free assump- tion. Therefore, we resort to approximate infer- ence, namely beam search. Each partial hypothe- sis is grouped into a beam based on the number of productions it contains; we use a beam width of 8, and search for the highest scoring hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Improving generalization</head><p>The data set we use for training and testing is pri- marily English but contains a broad vocabulary as <ref type="bibr">4</ref> We tried the sequence-to-sequence model with LSTMs ( <ref type="bibr" target="#b19">Sutskever et al., 2014</ref>) to map word sequence to the derivation tree productions, but the results were always lower than the feed forward network. This is probably because of the lack of enough training data.</p><p>well as many sentences from other languages such as Chinese, Arabic, and Russian. Thus, a seem- ingly large dataset of nearly eighty thousand ex- amples is likely to suffer from overfitting. In this section, we discuss a few attempts to improve gen- eralization in the sparse data setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Synthetic data using paraphrases</head><p>Arguably the best, though most expensive, way to reduce overfitting is to collect more training data. In our case, the training data available is limited and difficult to create. We propose to augment the training data in an automatic though potentially noisy way by generating synthetic training pairs.</p><p>The main idea is that two semantically equiv- alent sentences should have the same meaning representation. Given an existing training pair, replacing the pair's linguistic description with a paraphrase leads to a new synthetic training pair. For example, a recipe like Autosave your Insta- gram photos to Dropbox can be paraphrased to Au- tosave your Instagram pictures to Dropbox while retaining the meaning representation: I F I n s t a g r a m . AnyNewPhotoByYou THEN Dropbox . AddFileFromURL .</p><p>We first explore paraphrases using WordNet synonyms. Every word in the sentence can be re- placed by one of its synonyms that is picked ran- domly (a word is a synonym of itself). For words with multiple senses, we group all synonyms of all senses, then retain only those synonyms already in the vocabulary of the training data. This has two advantages. First, we do not increase the vo- cabulary size and therefore avoid overfitting. Sec- ond, this acts as a simple form of word sense dis- ambiguation. This adds around 50,000 additional training examples.</p><p>Next, we consider augmenting the data us- ing the Paraphrase Database ( <ref type="bibr" target="#b8">Ganitkevitch et al., 2013)</ref>. Each original description is converted into a lattice. The original word at each position is left in place with a constant score. For each word or phrase in the description found PPDB, we add one arc for each paraphrase, parameterized by the PPDB score of that phrase. The resulting lattice represents many possible paraphrases of the input. We select at most 10 diverse paths through this lattice using the method of <ref type="bibr" target="#b9">Gimpel et al. (2013)</ref>. <ref type="bibr">5</ref> This adds around 470,000 training examples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Alternative grammar formulation</head><p>We rely on a grammar to generate all valid mean- ing representations and learn models over the pro- ductions of this grammar. Different factorizations of the grammar lead to different model distribu- tions. Our primary grammar is described in Sec- tion 3.1. A second, alternate grammar formulation has fewer levels but more productions: it com- bines the channel and function into a single pro- duction, in both the trigger and the action. <ref type="figure" target="#fig_3">Figure 3</ref> shows an example derivation tree using this gram- mar. The size of this grammar is 780 productions (552 triggers + 228 actions). An advantage of this grammar is that it cannot assign probability mass to invalid ASTs, where the function is not applicaable to the channel. On the other hand, this grammar likely does not general- ize as well as the first grammar. The first grammar effectively has much more data about each chan- nel, which likely improves accuracy. Function predictions can condition on hopefully accurate channel predictions. It can also benefit from the fact that some function names are shared among channels. From that perspective, the second gram- mar has fewer training instances for each outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Feature selection</head><p>The training set contains approximately 77K train- ing examples, yet the number of distinct features types (word unigrams and bigrams, character tri- grams, Brown clusters) is approximately 230K. Only 80K features occur in the training set more than once. This ratio suggests overfitting may be a major issue.Feature selection likely can improve these issues. We used only simple count cutoffs, including only features that occur in the training set more than once and more than twice. Including features that occur more than once led to improve- ments in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ensemble</head><p>Finally, we explore improving generalization by building ensembles of multiple systems. Even if systems overfit, they likely overfit in different ways. When systems agree, they are likely to agree on the correct answer. Combining their re- sults will suffer less from overfitting. We use sim- ple majority voting as an ensemble strategy, re- solving ties in an arbitrary but deterministic way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We evaluate the performance of the systems by providing the model with descriptions unseen dur- ing training. Free parameters of the models were tuned using the development set. The separation of data into training, development, and test fol- lows <ref type="bibr" target="#b17">Quirk et al. (2015)</ref>. Two evaluation metrics are used: accuracy on just channel selection and accuracy of both channel and function.</p><p>Two major families of approaches are consid- ered: a baseline logistic regression classifier from scikit-learn <ref type="bibr" target="#b16">(Pedregosa et al., 2011)</ref>, as well as a feed-forward neural network. We explore a num- ber of variations, including feature selection and grammar formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison systems</head><p>Our default system was described in section 3, not including improvements from section 4 unless oth- erwise noted. The grammar uses the primary for- mulation from section 3.1. Neural network mod- els use a single hidden layer by default; we also explore two hidden layers.</p><p>We evaluate two approaches for generating syn- thetic data. The first approach, leaning primar- ily on WordNet to generate up to one paraphrase for each instance, is labeled WN. The second ap- proach using Paraphrase Database to generate up to ten paraphrases is labeled PPDB.</p><p>The Alternate grammar line uses the section 4.2 grammar, and otherwise default configurations (no synthetic data, single hidden layer for NN).</p><p>Feature selection again uses the default config- uration, but uses only those features that occurred more than once in the training data.</p><p>Finally we explore ensembles of all approaches. First, we combine all variations within the same model family; next, we bring all systems together. To evaluate the impact of individual systems, we also present results with specific systems removed.   <ref type="table">Table 1</ref>: Accuracy of the Neural Network (NN) and Logistic Regression (LR) implementations of our system with various configurations. Channel-only and full tree (channel+function) accuracies are listed. <ref type="table">Table 1</ref> shows the accuracy of each evaluated sys- tem, and <ref type="table" target="#tab_2">Table 2</ref> explores system performance on important subsets of the data. The first columns present accuracy of just the channel, and the last columns present the channel and the function to- gether (the full derivation). We achieve new state- of-the-art results, showing a 7% absolute improve- ment on the channel-only accuracy and 8% abso- lute improvement on the full derivation tree in the most difficult condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>Partly these improved results are driven by better features. Adding more robust representations of the input (e.g. Brown clusters) and conditioning on prior structure of the tree leads to more consis- tent and coherent trees.</p><p>One key observation is that the logistic regres- sion classifier consistently outperforms the neu- ral network, though by a small margin. We sus- pect two main causes: optimization difficulties and training size. To compare the optimization al- gorithms, <ref type="table">Table 1</ref> shows the result of a neural net- work with no hidden layers, which is effectively identical to a logistic regression model. Stochastic gradient descent used to train the neural network did not perform as well as the LIBLINEAR <ref type="bibr" target="#b7">(Fan et al., 2008</ref>) solver used to train the logistic re- gression, because the loss function was not opti- mized as well. Optimization problems are even more likely with hidden layers, since the objective is no longer convex.</p><p>Second, the training data is small by neural net- work standards. Prior attempts to use neural net- works for parsing required larger amounts of train- ing data to exceed the state-of-the-art. Non-linear models are able to capture regularities that linear models cannot, but may require more training data to do so. <ref type="table">Table 1</ref> shows that a network with a sin- gle hidden layer outperforms a one with two hid- den layers. This additional hidden layer seems to make learning harder (even with layer-wise pre- training). We also ran an additional experiment, limiting both NN and LR to use word unigram features, and varying the vocabulary size by fre- quency thresholding; the results are in table 3. LR models were more effective when all features were present, likely due to their convex objective and simple regularization. NN models, on the other hand, actually outperform LR models when lim- ited to more common vocabulary items. Given more data, NN could likely find representations that outperformed manual feature engineering.</p><p>Although we only considered feed-forward nerual networks, results on recurrent architectures <ref type="bibr" target="#b5">Dong and Lapata (2016)</ref> are in accordance with our findings. Their LSTM-based approach does not achieve great gains on this data set because: "user curated descriptions are often of low quality, and thus align very loosely to their correspond- ing ASTs". Even though this training set is larger than other semantic parsing datasets, the vocabu- lary, sentence structures, and even languages here are much more diverse, which make it difficult for the NN to learn useful representations. <ref type="bibr" target="#b5">Dong and Lapata (2016)</ref> tried to reduce the impact of this problem by evaluating only on the English sub-   <ref type="formula">(2015)</ref>, we also evaluation on illustrative subsets. "posclass" rep- resents the best system from prior work. D&amp;L is the best-performing system from <ref type="bibr" target="#b5">Dong and Lapata (2016)</ref>. NN and LR are the single best neural net- work, logistic regression models, and Ensemble is the combination of all systems. "oracleturk" rep- resents cases where at least one turker agreed with the gold standard.</p><p>set of the data. Interestingly, our carefully built feed-forward networks outperform their approach in almost every subset.</p><p>Although the neural network with one hidden layer does not outperform logistic regression in a feature rich setting, it makes substantially different predictions. An ensemble of their outputs achieves better accuracy than either system individually.</p><p>Our techniques for improving generalization do not improve individual systems. Yet when all tech- niques are combined in an ensemble, the resulting predictions are better. Furthermore, an ensemble without the synthetic data or without the alternate grammar has lower accuracy: each technique con- tributes to the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Full tree accuracy  <ref type="table">Table 3</ref>: Accuracy of NN and LR limited to word unigram features, with three vocabulary sizes: all words, words occurring at least twice in the train- ing data (13,971 words), and those occurring at least three times in the training data (8,974 words).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison of logistic regression and neural network approaches</head><p>We performed a detailed exploration of the cases where either the LR model was correct and the NN model was wrong, or vice versa. <ref type="table" target="#tab_5">Table 4</ref> breaks these errors into a number of cases:</p><p>• Swapped trigger and action. Here the sys- tem misinterpreted a rule, swapping the trig- ger for the action. An example NN swap was "Backup Pinboard entries to diigo"; an exam- ple LR swap was "Like a photo on tumblr and upload it to your flickr photostream ."</p><p>• Duplicated. In this case, the system used the same channel for both trigger and action, de- spite clear evidence in the language. For in- stance, the LR model incorrectly used Face- book as both the trigger and channel in this recipe: "New photo on Facebook addec to my Pryv". The NN model correctly identified Pryv as the target channel, despite the typo in the recipe.</p><p>• Missed word cue. In many cases there was a clear "cue word" in the language that should have forced a correct channel, but the model picked the wrong one. For instance, in "tweet # stared youtube video", the trigger should be starred YouTube videos, but the NN model incorrectly selected feeds.</p><p>• Missed multi-word cue. Sometimes the cue was a multi-word phrase, such as "One Drive". The NN model tended to miss these cues.</p><p>• Missed inference. In certain cases the cue was more of a loose inference. Words such as "payment" and "refund" should tend to  refer to triggers from the Square payment provider; the NN seemed to struggle on these cases.</p><p>• Related channel. Often the true channel is very difficult to pick: should the system use iOS location or Android location? NN mod- els seemed to do better on these cases, per- haps picking up on some latent cues in the data that were not immediately evident to the authors.</p><p>In general, a slightly more powerful NN model with access to more relevant data might overcome some of the issues above. We also explored correlations with errors and a number of other criteria, such as text length and frequency of the channels and functions, but found no substantial differences. In general, the remain- ing errors are often plausible given the noisy input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Work</head><p>We have achieved a new state-of-the-art on this dataset, though derivation tree accuracy remains low, around 42%. While some errors are caused by training data noise and others are due to noisy test instances, there is still room for improvement.</p><p>We believe synthetic data is a promising direc- tion. Initial attempts show small improvements; better results may be within reach given more tun- ing. This may enable gains with recurrent archi- tectures (e.g., LSTMs).</p><p>The networks here rely primarily on word-based features. Character-based models have resulted in improved syntactic parsing results ( <ref type="bibr" target="#b3">Ballesteros et al., 2015)</ref>. We believe that noisy data such as the If-Then corpus would benefit from character mod- elings, since the models could be more robust to spelling errors and variations.</p><p>Another important future work direction is to model the arguments of the If-Then statements. However, that requires segmenting the arguments into those that are general across all users, and those that are specific to the recipe's author. Likely this would require further annotation of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we address a semantic parsing task, namely translating sentences to If-Then state- ments. We model the task as structure prediction, and show improved models using both neural net- works and logistic regression. We also discussed various ways to improve generalization and re- duce overfitting, including adding synthetic train- ing data by paraphrasing sentences, using multiple grammars, applying feature selection and ensem- bling multiple systems. We achieve a new state-of- the-art with 8% absolute accuracy improvement. <ref type="bibr">Bernd Bohnet. 2010</ref>. Top accuracy and fast depen- dency parsing is not a contradiction. <ref type="table" target="#tab_2">In Proceedings  of the 23rd International Conference on Computa- tional Linguistics (Coling 2010)</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Derivation tree of If-Then statement of the recipe Autosave your Instagram photos to Dropbox. Arguments of the functions AnyNewPhotoByYou and AddFileFromURL are ignored.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of the feed-forward neural networks used in this paper. When predicting rule r i , the prior rules and the whole sentence are used as input. Separate parameters are learned for each position i.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Derivation tree of IFTTT statement of the recipe Autosave your Instagram photos to Dropbox using the second grammar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>System</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>System comparisons on various subsets 
of the data. Following Quirk et al. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Count of error cases by type for NN and 
LR models, in their default configurations. This 
table only counts those instances in the most clean 
set (where three or more turkers agree with the 
gold program) where exactly one system made an 
error. 

</table></figure>

			<note place="foot" n="2"> Arguments are still important for a few If-Then recipes. For instance, in If there is snow tomorrow send a notification, &quot;snow&quot; is an argument to the function Tomorrow&apos;sForecastCallsFor. We are not handling such cases in this work. 3 For this task, it is possible to model the programs as a 4-tuple, but using the grammar approach allows us to port the same technique to other semantic parsing tasks.</note>

			<note place="foot" n="5"> We use a trigram language model, and a weight of 4.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic parsing as machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria, August</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="47" to="52" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Broad-coverage CCG semantic parsing with AMR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1699" to="1710" />
		</imprint>
	</monogr>
	<note>September. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved transition-based parsing by modeling characters instead of words with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="349" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.01280</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An efficient context-free parsing algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Earley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="94" to="102" />
			<date type="published" when="1970-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PPDB: The paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A systematic exploration of diversity in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1100" to="1111" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A deep architecture for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl Moritz</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2014 Workshop on Semantic Parsing</title>
		<meeting>the ACL 2014 Workshop on Semantic Parsing<address><addrLine>Baltimore, MD, June</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="22" to="27" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nlyze: Interactive programming by natural language for spreadsheet data analysis and manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Marron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leif</forename><forename type="middle">Johnson</forename></persName>
		</author>
		<ptr target="https://github.com/lmjohns3/theanets" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to transform natural to formal languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI-05)</title>
		<meeting>the Twentieth National Conference on Artificial Intelligence (AAAI-05)<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-07" />
			<biblScope unit="page" from="1062" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-supervised learning for natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language to code: Learning semantic parsers for if-this-then-that recipes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to parse natural language with maximum entropy models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N.D. Lawrence, D.D. Lee, M. Sugiyama, R. Garnett, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2755" to="2763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning for semantic parsing with statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Main Conference</title>
		<meeting>the Human Language Technology Conference of the NAACL, Main Conference<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="439" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lunar rocks in natural English: Explorations in natural language question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woods</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Structures Processing</title>
		<editor>Antonio Zampoli</editor>
		<meeting><address><addrLine>North-Holland, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Neural enquirer: Learning to query tables with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben Kao Hang</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI96)</title>
		<meeting>the Thirteenth National Conference on Artificial Intelligence (AAAI96)<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-08" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Uncertainty in AI</title>
		<meeting>the 21st Conference on Uncertainty in AI</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
