<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Apples to Apples: Learning Semantics of Common Entities Through a Novel Comprehension Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Bakhshandeh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">F</forename><surname>Allen</surname></persName>
						</author>
						<title level="a" type="main">Apples to Apples: Learning Semantics of Common Entities Through a Novel Comprehension Task</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="906" to="916"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1084</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Understanding common entities and their attributes is a primary requirement for any system that comprehends natural language. In order to enable learning about common entities, we introduce a novel machine comprehension task, GuessTwo: given a short paragraph comparing different aspects of two real-world semantically-similar entities, a system should guess what those entities are. Accomplishing this task requires deep language understanding which enables inference , connecting each comparison paragraph to different levels of knowledge about world entities and their attributes. So far we have crowdsourced a dataset of more than 14K comparison paragraphs comparing entities from a variety of categories such as fruits and animals. We have designed two schemes for evaluation: open-ended, and binary-choice prediction. For benchmarking further progress in the task, we have collected a set of paragraphs as the test set on which human can accomplish the task with an accuracy of 94.2% on open-ended prediction. We have implemented various models for tackling the task, ranging from semantic-driven to neu-ral models. The semantic-driven approach outperforms the neural models, however, the results indicate that the task is very challenging across the models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the past few years, there has been great progress on core NLP tasks (e.g., parsing and part of speech tagging) which has renewed interest in primary language learning tasks which require text under- standing and reasoning, such as machine compre- hension ( <ref type="bibr" target="#b27">Schoenick et al., 2016;</ref><ref type="bibr" target="#b17">Hermann et al., 2015;</ref><ref type="bibr" target="#b25">Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b24">Mostafazadeh et al., 2016)</ref>. Our question is how far have we got in learning basic concepts of the world through lan- guage comprehension. If we look at the large body of work on extracting knowledge from un- structured corpora, we will see that they often lack some very basic pieces of information. For ex- ample, let us focus on the basic concept of ap- ple, the fruit. What do the state-of-the-art sys- tems and resources know about an apple? None of the state-of-the-art knowledge bases <ref type="bibr" target="#b29">(Speer and Havasi, 2012;</ref><ref type="bibr" target="#b6">Carlson et al., 2010;</ref>) include much precise information about the fact that apples have an edible skin, vary from sweet to sour, are round, and relatively the same size of a fist. Moreover, there is no clear approach on how to extract such information, if any, from trained word embeddings. This paper focuses on how we can automatically learn about various at- tributes of such generic entities in the world.</p><p>A key observation motivating this work is that we can learn more detail about objects when they are compared to other similar objects. When we compare things we often contrast, that is, we count their similarities along with their dissimilarities. This results in covering the primary attributes and aspects of objects. As humans, we tend to recall and mention the difference between things (say green skin vs. red skin in apples) as opposed to absolute measures (say the existence of skin). In- terestingly, there is evidence that human knowl- edge is structured by semantic similarity and the relations among objects are defined by their rel- ative perceptual and conceptual properties, such as their form, function, behavior, and environ- ment ( <ref type="bibr" target="#b10">Collins and Loftus, 1975;</ref><ref type="bibr" target="#b31">Tversky and Gati, 1978;</ref><ref type="bibr" target="#b11">Cree and Mcrae, 2003)</ref>. Our idea is to lever- age comparison as a way of naturally learning about common world concepts and their specific attributes.</p><p>Comparison, where we name the similarities and differences between things, is a unique cogni- tive ability in humans 1 which requires memorizing facts, experiencing things and integration of con- cepts of the world <ref type="bibr" target="#b16">(Hazlitt, 1933)</ref>. It is clear that developing AI systems that are capable of compre- hending comparison is crucial. In this paper, in or- der to enable learning through comparison, we in- troduce a new language comprehension task which requires understanding different attributes of basic entities that are being compared.</p><p>The contributions of this paper are as fol- lows: (1) To equip learning about common enti- ties through comparison comprehension, we have crowdsourced a dataset of more than 14K com- parison paragraphs comparing entities from nine broad categories (Section 2). This resource will be expanded over time and will be released to the public. (2) We introduce a novel task called GuessTwo, in which given a short paragraph com- paring two entities, a system should guess what the two things are. (Section 3). To make system- atic benchmarking on the task possible, we vet a collection of comparison paragraphs to obtain a test set on which human performs with an accu- racy 94.2%. (3) We present a host of neural ap- proaches and a novel semantic-driven model for tackling the GuessTwo task <ref type="bibr">(Sections 4, 5)</ref>. Our experiments show that the semantic approach out- performs the neural models. The results strongly suggest that closing the gap between system and human performances requires richer semantic pro- cessing (Section 6). We hope that this work will establish a new base for a machine comprehension test that requires systems to go beyond informa- tion extraction and towards levels of performing basic reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Collection</head><p>To enable learning about common entities, we aimed to create a dataset which meets the follow- ing goals:</p><p>1. The dataset should be a collection of high- quality documents which are rich in compar-ing and contrasting entities using their vari- ous attributes and aspects.</p><p>2. The comparisons in the dataset should in- volve everyday non-technical concepts, mak- ing their comprehension easy and common- sense for a human.</p><p>After many experiments with scraping existing Web resources, we decided to crowdsource the comparison paragraphs using Amazon Mechani- cal Turk 2 (Mturk). We prompt the crowd workers as follows: "Your task is to compare two given items in one simple language paragraph so that a knowledgeable person who reads it can guess what the two things are". The workers were instructed to compare only the major and well-known aspects of the two entities. We also asked them to use X and Y for anonymously referring to the two enti- ties. <ref type="table">Table 1</ref> shows three examples of our crowd- sourced comparison paragraphs. As these exam- ples show, the paragraphs are very contentful and rich in comparison which meets our initial goals in the dataset creation. Entity Pair Selection. The choice of the two entities which should be compared against each other plays a key role in the quality of the col- lected dataset. It is evident that naturally, we com- pare two things which are semantically similar, yet have some dissimilarities 3 , such as jam and jelly. Given the goals of our task, we experimented with concrete nouns which share a common taxonomy class. We choose semantic classes which have at least five well-known entities. So far, we have covered nine broad categories as shown in <ref type="figure" target="#fig_0">Fig- ure 2</ref>, with 21 subcategories shown in <ref type="figure" target="#fig_1">Figure 3</ref>. We use Wikipedia item categories and the Word- Net <ref type="bibr" target="#b23">(Miller, 1995)</ref> ontology for identifying en- tities from each subcategory. Then, we choose the most common entities by looking up their fre- quency on Google Web 1T N-grams <ref type="bibr">4</ref> . We manu- ally inspected the frequency-filtered list to make sure that the entities are rather easy to describe without getting technical. Given the list of enti- ties, we paired each entity with at most five and at least three other entities from the same subcate- gory. We also include inter-subcategory compar-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Paragraph</head><p>Entity X Entity Y Both X and Y are fruits and a variety of apples. X and Y are generally similar in size. X are dark red in color when ripe, while Y are a bright green color. X is sweeter and softer than Y in taste and texture, sometimes starchy. Y are tart and somewhat stringy. Y is often used in cooking, whereas X is not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Red Delicious Apple Fruit</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Granny Smith Apple Fruit</head><p>The X and Y are two types of vehicles. X is a smaller vehicle than Y. The X has two wheels while Y has none. The X travels on roadways and smooth surfaces, whereas Y is capable of flying. Only one or two people are able to ride on X at once, while Y can carry more people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motorcycle</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motor Vehicle Vehicle</head><p>Helicopter Aircraft Vehicle X and Y are both types of world cuisines. X incorporates a lot of pasta dishes and sauces, with basil, tomato, and cheese being major ingredients. Y consists of many curries and stir fried dishes, with coconut and lemongrass being used often. Y is generally spicier and more aromatic than X. X is a European cuisine, while Y is an Asian cuisine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Italian Cuisine Cuisine Cuisine</head><p>Thai Cuisine Cuisine Cuisine <ref type="table">Table 1</ref>: Examples from the GuessTwo comprehension dataset. Also provided with the dataset is the subcategory and the broad category of the entities which are listed below the entity names in this <ref type="table">Table.</ref> Figure 1: An example illustrating the entity pair matching process.</p><p>ison for a handful of entities at the boundaries. <ref type="figure">Figure 1</ref> illustrates our entity pair matching pro- cess with an example on subcategories 'apple' and 'citrus'.</p><p>Data Quality Control. Our task of free-form writing is trickier than many other tasks such as tagging on Mturk. To instruct the non-expert workers, we designed a qualification test on Mturk in which the workers had to judge whether or not a given paragraph is acceptable according to our cri- teria. We used three carefully selected paragraphs to be a part of the qualification test. Moreover, to further ensure the quality of the submissions, one of our team members qualitatively browsed through the submissions and gave the workers de- tailed feedback before approving their paragraphs.</p><p>For each pair of entities, we collected eight comparison paragraphs from different workers. Given that different workers have different per- spectives on what the major aspects to be com- pared are, collecting multiple paragraphs helps further enriching our dataset. We constrained the paragraphs to be at least 250 characters and at most 850 characters. <ref type="table" target="#tab_1">Table 2</ref> shows the basic statistics of our dataset. In this Table, we also in- cluded the median number of adjectives (includ- ing comparatives) per paragraph as a measure of descriptiveness of the comparison paragraphs. As a point of reference, the median number of adjec- tives in a random Wikipedia paragraph of the same length is 5.  Given the quality control we have in place, our data collection is going slowly. So far we have col- lected 14,142 paragraphs; however, we are aiming   to expand the resource over time.</p><p>Test Set Creation. In order to enable bench- marking on the task, we assessed the quality of a random sample of GuessTwo paragraphs as fol- lows: we show the paragraph to three human workers on Mturk and ask them to guess what the two things are. Then, we choose 520 paragraphs for which all three workers have made exactly cor- rect guesses for both entities. The test set will also be expanded along with the further data collection.</p><p>We divided the rest of the GuessTwo dataset into training and validation sets, with a 90%/10% split. To ensure that the test set requires some level of basic reasoning, our training set does not share any exact entity pairs with the validation or test set. This further enforces systems to learn about entities indirectly by processing across para- graphs. For instance, as shown in <ref type="figure" target="#fig_2">Figure 4</ref>, at test time, a system should be able to guess a compari- son involving the entities blood orange vs. lemon by having seen comparisons of blood orange vs. tangerine and tangerine vs. lemon.</p><p>Our dataset will be released to the pub- lic through https://omidb.github.io/ guesstwo/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The GuessTwo Task Definition</head><p>We define the following two different schemes for the GuessTwo task:</p><p>• Open-ended GuessTwo. Given a short para- graph P which compares two entities X and Y, guess what the two entities are. The scope of this prediction is the set of all entities appearing in the training dataset.</p><p>• Binary Choice GuessTwo. Given a short para- graph P which compares two entities X and Y, and two nominals n 1 and n 2 , choose 0 if n 1 = X and n 2 = Y, choose 1 otherwise. We speculate that system which can success- fully tackle the GuessTwo task, has achieved two major objectives: (1) Has successfully learned the knowledge about entities stored in any form (e.g., continuous-space representation or symbolic) (2) Has a basic natural language understanding ca- pability, using which, it can comprehend a para- graph and access its knowledge. We predict that our training dataset has enough detailed informa- tion about entities for learning the required knowl- edge for tackling the task. Given the design of our dataset, at test time, a system should perform some level of reasoning to go beyond understand- ing only one paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Neural Models</head><p>In this Section we present various end-to-end neu- ral models for tackling the task of GuessTwo.</p><p>Continuous Bag-of-words Language Model. This model computes the probability of a sequence of consecutive words in context. The premise is that the probability of a paragraph with the correct realization of X and Y should be higher than the a paragraph with incorrect realizations. In order to compute the probability of a word given a con- text we use Continuous Bag-of-words (CBOW) ( <ref type="bibr" target="#b21">Mikolov et al., 2013a</ref>) which models the follow- ing conditional probability:</p><formula xml:id="formula_0">p(w|C(w), θ)<label>(1)</label></formula><p>here, C(w) is the context of the word w and θ is the model parameters. Then, the probability of a sequence of words (in a paragraph) is computed as follows:</p><formula xml:id="formula_1">n i=1 p(w i |C(w i ), θ)<label>(2)</label></formula><p>We define context to be a window of five words.   </p><formula xml:id="formula_2">argmax x,y n i=1 p(w i |C(w i ) x,y , θ)<label>(3)</label></formula><p>where C(w i ) x,y indicates the context in which any occurrences of X have been replaced with x and Y's have been replaced with y. For binary choice classification, we use the same modeling except that we only consider x = n 1 , y = n 2 and x = n 2 , y = n 1 . Encoder-Decoder Recurrent Neural Net</p><formula xml:id="formula_3">5 http://mattmahoney.net/dc/text8.zip (RNN)</formula><p>. This model is a sequence-to-sequence generation model ( <ref type="bibr" target="#b30">Sutskever et al., 2014</ref>) that maps an input sequence to an output sequence using an encoder-decoder RNN with attention ( ). The en- coder RNN processes the comparison paragraph and the decoder generates the first item followed by the second item ( <ref type="figure" target="#fig_5">Figure 6</ref>). The paragraph is encoded into a state vector of size 512. This vector is then set as the initial recurrent state of the decoder. We tune the model parameters on the validation set, where we set the number of layers to 2. The model is trained end-to-end, using Stochastic Gradient Descent with early stopping.</p><p>For open-ended prediction, we use beam search with beam-width = 25 and then output the two tokens with the highest probability. For binary choice classification, we use the same model where we set the encoder RNN inputs to the in- put paragraph tokens, then, we set the input of the decoder RNN once to [n 1 , n 2 ] and next to [n 2 , n 1 ]. After running the network forward, we take the probability of the decoder logits and choose the ordering which has the highest probability. Skip-gram model (Mikolov et al., 2013b) on 100 billion words of Google News <ref type="bibr">6</ref> . For open-ended prediction, the output of CNN is fed forward and transformed into a 300 dimension vector. Then, we use a softmax layer to get the probability of each of the possible nominals for X and Y. For bi- nary choice classification, we use the same archi- tecture and settings as above. Additionally, we en- code each nominal into a 300-dimensional vector, which then gets concatenated with the paragraph vector. <ref type="figure" target="#fig_4">Figure 5c</ref> shows this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Semantic-driven Model</head><p>In this Section we present a semantic-driven ap- proach which models the comparison paragraph using semantic features and is capable of perform- ing basic reasoning across paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Representing Paragraphs</head><p>The question is, given a comparison paragraph, what is the best representation which can enable further reasoning? The comparison paragraphs of- ten have complex syntactic and semantic struc- tures, which might be challenging for many off- the-shelf NLP tools to process. For instance, con- sider the sentence X is much sweeter in taste than Y. as X is sweeter which shares the same shortcom- ings.</p><p>Our approach for better representation of com- parison paragraphs starts with a broad-coverage semantic parser ( <ref type="bibr" target="#b3">Banarescu et al., 2013;</ref><ref type="bibr" target="#b4">Bos, 2008;</ref><ref type="bibr" target="#b0">Allen et al., 2008)</ref>. A semantic parser maps an input sentence to its formal meaning represen- tation, operating at the generic natural language level. Here we use the TRIPS 7 ( <ref type="bibr" target="#b0">Allen et al., 2008)</ref> broad-coverage semantic parser. TRIPS provides a very rich semantic structure; mainly it provides sense disambiguated deep structures augmented with semantic ontology types. <ref type="figure" target="#fig_6">Figure 7</ref> shows an example TRIPS semantic parse. In this graph representation, each node specifies a word in bold along with its corresponding ontology type on its left. The edges in the graph are semantic roles <ref type="bibr">8</ref> . As you can see, this semantic parse represents the sentence by decoupling the token 'both' and at- tributing the property of 'be apple' to both X and Y.</p><p>In our comparison paragraphs there are two ma- jor types of sentences:</p><p>• Sentences with Absolute Information. These sentences contain direct information about the en- tities, such as X is red or Both X and Y are very sweet. From each absolute sentence, we extract frames which describe the absolute attributes of the corresponding entity. We define a frame to be a subgraph of a semantic parse which involves exactly one entity and all of its semantic roles. Relying on the deep semantic features offered by the semantic parser, we perform negation propaga- tion <ref type="bibr">9</ref> and sequence decoupling, among others fea- tures. For example, given a sentence which has a sequence, as the one depicted in <ref type="figure" target="#fig_6">Figure 7</ref>, we perform sequence decoupling and extract the two frames [X Be Apple] and [Y Be Apple].</p><p>• Sentences with Relative Information. These sentences contain relative information about the two entities, for instance, X is somewhat sweeter than Y. As opposed to the sentences with absolute information, we cannot extract frames from sen- tences with comparisons directly. Various proper- ties of entities can be associated with an abstract scale, such as 'size' or 'sweetness', on which dif-  Given a comparison structure such as the one presented in <ref type="figure" target="#fig_7">Figure 8</ref>, we can extract the informa- tion that on the scale of 'sweetness' X is higher than Y. It is clear that one can build a large knowl- edge base of such relations by reading large col- lections of comparison paragraphs. We populate our knowledge base of relative information about entities as follows: First, we predict the compar- ison structure of each sentence and then extract a binary relation s which shows the relation on the scale of s. Second, for any scale s, we apply tran- sitivity on its entities. As shown in equation 4, the binary relation s is transitive over the set of all entities, A. This process, called closure, en- ables us do basic reasoning and derives implicit relations on scales from explicit relations.</p><formula xml:id="formula_4">∀s ∈ S ∀x, y, z ∈ A : (x s y ∧ y s z) =⇒ x s z<label>(4)</label></formula><p>The product of this step is a structured knowl- edge base on entity ordering which we call the or- dering lattice. <ref type="figure" target="#fig_8">Figure 9</ref> shows an example partial ordering lattice inferred by our model, where the sweetness of Golden Delicious can be compared to Granny Smith through their direct link with Red Delicious.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Modeling</head><p>Given a paragraph P , we first extract the set of all the absolute information frames for X and Y (as described above), called F X (P ) and F Y (P ). Sec- ond, for the sentences with relative information, we extract all the binary relations s ∈ R(P ) that should hold between X and Y. Then, our objec- tive is to find two realizations for X and Y that maximize the following:</p><formula xml:id="formula_5">argmax x,y p(x|F X (P )) + p(y|F Y (P )) s.t. ∀ s ∈ R(P ) : x s y<label>(5)</label></formula><p>In order to compute the p(x|F X (P )) and p(y|F Y (P )) scores we used Regularized Gradient Boosting (XGBoost) classifier <ref type="bibr" target="#b15">(Friedman, 2000)</ref>, which uses a regularized model formulation to limit overfitting. We directly use each frame in the F X (P ) and F Y (P ) sets as the classifier features.</p><p>We use Integer Linear Programming (ILP) for for- mulating the constraints as follows: for each rela- tion r ∈ R on the scale s, we lookup the scale s in the ordering lattice and make the blacklist B(P ) containing each pair of entities which do not satisfy the relation r. Our ordering lattice does not have perfect complete information, hence, we have Open World Assumption and only prune our search space not to include the already observed pairs which violate the relation. our ILP objective function will be the following:</p><formula xml:id="formula_6">argmax b,b x∈N b x p(x|F X (P )) + y∈N b y p(y|F Y (P )) s.t. ∀ (j, j ) ∈ B(P ) : b j + b j ≤ 1<label>(6)</label></formula><p>where N is the set of all possible realizations and b and b are the binary indicator variables, so b x = 1 indicates the realization of x for X.</p><p>In the case of open-ended prediction, the maxi- mization presented in Equation 6 is carried out on the set N . In the case of binary choice classifica- tion, however, only the two choices of n 1 and n 2 are considered in the maximization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We evaluate all the models presented in Sections 4 and 5 using the following accuracy measure:</p><p>#correct predictions of both entities #test cases <ref type="formula">(7)</ref> As for the open-ended prediction we compute the nominator of the accuracy measure using three various matching methods on both entities: (1) exact-match, (2) subcategory match, (3) broad cat- egory match.</p><p>As <ref type="table">Table 3</ref> shows, the semantic model outper- forms all the neural models. Moreover, the ILP constraints have been very effective in directing the system in the correct search space. Among the neural models, the Encoder-Decoder RNN model performs noticeably better than other mod- els when matching the subcategory and broad cat- egory. According to the exact-matching, neither of the CBOW models could guess any of the two test entities correctly. Overall, it is evident that the end-to-end neural models have not been able to generalize well and learn about the attributes of entities across various training paragraphs. This can be partly due to not being trained on large enough comparison training dataset. The seman- tic model, however, could outperform the neural models using the same amount of data. To a de- gree, this is because the semantic model leverages the basic language understanding capabilities of- fered by the semantic parser.</p><p>It is also important to note that our seman- tic approach is not only capable of binary and open-ended prediction, but it also offers two by- products that can be used as knowledge in a vari- ety of other tasks: (1) a set of the most important absolute information frames which can be chosen based on feature importance in the classification, (2) the partial ordering lattice of entities. Over- all, the results strongly suggest that the GuessTwo task is challenging, with the open-ended scheme being the most challenging. There is a wide gap between human and system performance on this task, which makes it a very promising task for the community to pursue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Binary Open-ended Exact. Subcat. Human 100.0 94.2 100.0 CBOW-Wikipedia 51.9 0.0 1.5 CBOW-GuessTwo 51.7 0.0 1.1 Encoder-Decoder RNN 58.8 2.9 6.8 CNN 57.6 1.9 2.5 Semantic (no constraints) 61.5 10.5 38.5 Semantic (with ILP constraints) 69.2 11.7 40.4 <ref type="table">Table 3</ref>: System accuracy results on the GuessTwo test set. A random baseline on binary choice task achieves 51%. The open-ended evaluation has two columns: exact-match (exact) and subcate- gory match (subcat), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>The task of Machine Comprehension (MC) has gained a significant attention over the past few years. The major driver for MC has been the publicly available benchmarking datasets. A va- riety of MC tasks have been introduced in the community ( <ref type="bibr" target="#b26">Richardson et al.;</ref><ref type="bibr" target="#b17">Hermann et al., 2015;</ref><ref type="bibr" target="#b25">Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b18">Hill et al., 2015)</ref>, in which the system reads a short text and an- swers a few multiple-choice questions. The read- ing comprehension involved in these tests ranges from reading a short fictional story ( <ref type="bibr">Richardson et al.)</ref> to reading a short news article ( <ref type="bibr" target="#b17">Hermann et al., 2015)</ref>. In comparison, in the GuessTwo task the reading comprehension involves reading a short comparison paragraph and one can say the multiple-choice question is the constant What are X and Y?</p><p>The CNN/DailyMail dataset consists of more than 100K short news articles with the questions automatically created from the bullet-point sum- maries of the original article. This dataset uses fill-in-the-blank-style questions such as 'Producer X will not press charges against Jeremy Clark- son' where the system should choose among all the anonymized entities in the corresponding para- graph to fill in X. The Stanford Question Answer- ing (SQuAD) dataset is another recent machine comprehension test with over 500 Wikipedia ar- ticles and +100,000 crowdsourced questions. The answer to every question in this dataset is a span of text from the corresponding reading passage.</p><p>Human accuracy on CNN/DailyMail is esti- mated to be around 75% ( <ref type="bibr" target="#b7">Chen et al., 2016</ref>) with the current state-of-the-art at 76.1 on CNN <ref type="bibr" target="#b28">(Sordoni et al., 2016)</ref>, and 75.8 on <ref type="bibr">DailyMail (Chen et al., 2016)</ref>. The human F1 score on SQuAD dataset is reported to be at 86.8%, with the cur- rent state-of-the-art achieving 82.9%. Given these statistics, neither of these datasets leave enough room for further research. Given that in both these tasks the answer to the question is directly found in the provided passage, we argue that the commu- nity requires a more challenging MC task which goes beyond matching and needs some level of in- ference across passages. The GuessTwo task re- quires basic reasoning and inference across para- graphs for comprehending various aspects of enti- ties relative to one another.</p><p>Another interesting task is MCTest ( <ref type="bibr">Richardson et al.)</ref>, which is a reading comprehension test with 660 fictional stories as the passage and four questions per story. The human-level per- formance on MCTest is estimated to be around 90%, with the state-of-the-art achieving an accu- racy of 70% ( <ref type="bibr" target="#b32">Wang et al., 2015)</ref>. MCTest is also proven to be challenging, however, given its very limited training data, further progress on the task has been hindered. Yet another relevant QA task is the Allen AI Science Challenge ( <ref type="bibr" target="#b9">Clarke et al., 2010;</ref><ref type="bibr" target="#b27">Schoenick et al., 2016)</ref>, which is a dataset of multiple-choice questions and answers from a standardized 8th grade science exam. The ques- tions can range from simple fact lookup to com- plex ones which require extensive world knowl- edge and commonsense reasoning. This task re- quires machine reading of a variety of resources such as textbooks and goes beyond reading a cou- ple of passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We introduced the novel task of GuessTwo, in which given a short paragraph comparing two common entities, a system should guess what the two entities are. The comparison paragraphs of- ten have complex semantic structures which make this comprehension task demanding. Furthermore, guessing the two entities requires a system to go beyond only understanding one given passage and requires reasoning across paragraphs, which is one of the most under-explored, yet crucial, capabili- ties of an intelligent agent.</p><p>So far, we have crowdsourced a dataset of more than 14K comparison paragraphs comparing enti- ties from nine major categories. For benchmark- ing the progress, we filter a collection of these paragraphs to create a test set, on which humans perform with an accuracy of 94.2%. For contin- uing our data collection, we would like to have a targeted entity pair selection where we partic- ularly collect the missing relations in our partial ordering lattice. We believe that this process can help developing more effective systems. For the most recent statistics of the dataset and the best performing systems please check this website.</p><p>We presented a host of neural models and a novel semantic-driven approach for tackling the task of GuessTwo. Our experiments show that the semantic approach outperforms the neural mod- els by a large margin. The poor performance of the neural models we experimented with can mo- tivate designing new architectures which are ca- pable of performing basic reasoning across para- graphs. The results strongly suggest that bridging the gap between system and human performance on this task requires models with richer language representation and reasoning capabilities. As a fu- ture work, we would like to explore the feasibility of marrying our semantic and neural models to ex- ploit the benefits that each of them has to offer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgments</head><p>This work was supported in part by Grant W911NF-15-1-0542 with the US Defense Ad- vanced Research Projects Agency (DARPA) and the Army Research Office (ARO). We would like to thank Linxiuzhi Yang for her help in the data collection and anonymous reviewers for their in- sightful comments on this work. We specially thank William de Beaumont for his invaluable feedback on this paper. We also thank the inputs from Steven Piantadosi, Brad Mahon, and Gregory Carlson on cognitive aspects of comparison.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of broad category of the entities.</figDesc><graphic url="image-3.png" coords="3,325.70,536.09,181.41,138.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of subcategory of the entities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example showing the entity pairs in the test and training sets.</figDesc><graphic url="image-4.png" coords="4,104.74,256.81,152.79,93.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5a summarizes this model.</head><label></label><figDesc>We train this (a) The CBOW model. (b) The CNN open-ended model. (c) The CNN binary-choice model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Various neural models for tackling the task of GuessTwo.</figDesc><graphic url="image-8.png" coords="5,72.00,336.28,218.27,83.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The Encoder-Decoder model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Semantic parsing for the sentence Both X and Y are apples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The comparison construction predicted for the sentence X is sweeter than Y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The inferred partial ordering lattice comparing the sweetness of different apples.</figDesc><graphic url="image-9.png" coords="7,314.36,62.81,204.09,170.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics of the GuessTwo dataset as of 
April 2017. 

</table></figure>

			<note place="foot" n="1"> It has been suggested (Hazlitt, 1933) that children under seven years old cannot name differences between simple things such as peach and apple. This further shows that the ability for comparison develops at a later age and is cognitively complex.</note>

			<note place="foot" n="2"> www.mturk.com 3 Tversky&apos;s (1978) analysis of similarity suggests that similarity statements compare objects that belong to the same class of things. 4 https://catalog.ldc.upenn.edu/ ldc2006t13</note>

			<note place="foot" n="6"> https://code.google.com/archive/p/ word2vec/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep semantic analysis of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">F</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Beaumont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Semantics in Text Processing. Association for Computational Linguistics</title>
		<meeting>the 2008 Conference on Semantics in Text Processing. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA, STEP &apos;08</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="343" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1409.0473</idno>
		<ptr target="http://arxiv.org/abs/1409.0473" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to jointly predict ellipsis and comparison structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Bakhshandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><forename type="middle">Cornelia</forename><surname>Wellwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/K16-1007" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning. Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="62" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W13-2322" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse. Association for Computational Linguistics</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse. Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wide-coverage semantic analysis with boxer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantics in Text Processing</title>
		<editor>Johan Bos and Rodolfo Delmonte</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">College Publications, Research in Computational Semantics</title>
	</analytic>
	<monogr>
		<title level="m">Conference Proceedings</title>
		<imprint>
			<biblScope unit="page" from="277" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Estevam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A thorough examination of the cnn/daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the world&apos;s response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">Roth</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA, CoNLL &apos;10</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A spreading-activation theory of semantic processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><forename type="middle">F</forename><surname>Loftus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="407" to="428" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analyzing the factors underlying the structure and computation of the meaning of chipmunk, cherry, chisel, cheese, and cello (and many other such concrete nouns)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">S</forename><surname>Cree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Mcrae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="201" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Open information extraction: The second generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janara</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename><surname>Mausam</surname></persName>
		</author>
		<idno type="doi">10.5591/978-1-57735-516-8/IJCAI11-012</idno>
		<ptr target="https://doi.org/10.5591/978-1-57735-516-8/IJCAI11-012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Joint Conference on Artificial Intelli</title>
		<meeting>the Twenty-Second International Joint Conference on Artificial Intelli</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identifying relations for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Open question answering over curated and extracted knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<idno type="doi">10.1145/2623330.2623677</idno>
		<ptr target="https://doi.org/10.1145/2623330.2623677" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1156" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Greedy function approximation: A gradient boosting machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jerome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The psychology of infancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hazlitt</surname></persName>
		</author>
		<ptr target="https://books.google.com/books?id=I8svAAAAYAAJ" />
	</analytic>
	<monogr>
		<title level="j">E.P. Dutton and company</title>
		<imprint>
			<date type="published" when="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D14/D14-1181.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL. ACL</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The handbook of brain theory and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">chapter Convolutional Networks for Images, Speech, and Time Series</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="255" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<ptr target="http://arxiv.org/abs/1301.3781" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<idno type="doi">10.1145/219717.219748</idno>
		<ptr target="https://doi.org/10.1145/219717.219748" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1098" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
		<title level="m">Mctest: A challenge dataset for the open-domain machine comprehension of text</title>
		<imprint>
			<biblScope unit="page" from="193" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Moving beyond the turing test with the allen AI science challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<idno>CoRR abs/1604.04315</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Iterative alternating neural attention for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1606.02245</idno>
		<ptr target="http://arxiv.org/abs/1606.02245" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Representing general relational knowledge in conceptnet 5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bente</forename><surname>Mehmet Uur Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Maegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mariani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC&apos;12). European Language Resources Association (ELRA)</title>
		<meeting>the Eight International Conference on Language Resources and Evaluation (LREC&apos;12). European Language Resources Association (ELRA)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Asuncion Moreno, Jan Odijk, and Stelios Piperidis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-813" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Studies of similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Gati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and categorization</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="79" to="98" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Machine comprehension with syntax, frames, and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P/P15/P15-2115.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="700" to="706" />
		</imprint>
	</monogr>
	<note>Short Papers. The Association for Computer Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
