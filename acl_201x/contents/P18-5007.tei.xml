<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Reinforcement Learning for NLP</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Yang</surname></persName>
							<email>william@cs.ucsb.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Uc</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santa</forename><surname>Barbara</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
							<email>li@shannonai.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shannon</forename><surname>Ai Jiwei</surname></persName>
						</author>
						<title level="a" type="main">Deep Reinforcement Learning for NLP</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics-Tutorial Abstracts</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics-Tutorial Abstracts <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="19" to="21"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>19 Xiaodong He JD AI Research</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many Natural Language Processing (NLP) tasks (including generation, language grounding , reasoning, information extraction, coref-erence resolution, and dialog) can be formulated as deep reinforcement learning (DRL) problems. However, since language is often discrete and the space for all sentences is infinite , there are many challenges for formulating reinforcement learning problems of NLP tasks. In this tutorial, we provide a gentle introduction to the foundation of deep reinforcement learning, as well as some practical DRL solutions in NLP. We describe recent advances in designing deep reinforcement learning for NLP, with a special focus on generation, dialogue , and information extraction. We discuss why they succeed, and when they may fail, aiming at providing some practical advice about deep reinforcement learning for solving real-world NLP problems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Tutorial Description</head><p>Deep Reinforcement Learning (DRL) ( <ref type="bibr">Mnih et al., 2015</ref>) is an emerging research area that involves in- telligent agents that learn to reason in Markov Deci- sion Processes (MDP). Recently, DRL has achieved many stunning breakthroughs in Atari games ( <ref type="bibr">Mnih et al., 2013</ref>) and the Go game ( <ref type="bibr" target="#b3">Silver et al., 2016)</ref>. In addition, DRL methods have gained significantly more attentions in NLP in recent years, because many NLP tasks can be formulated as DRL problems that involve incremental decision making. DRL methods could easily combine embedding based representation learning with reasoning, and optimize for a variety of non-differentiable rewards. However, a key challenge for applying deep reinforcement learning techniques to real-world sized NLP problems is the model design is- sue. This tutorial draws connections from theories of deep reinforcement learning to practical applications in NLP.</p><p>In particular, we start with the gentle introduction to the fundamentals of reinforcement learning <ref type="bibr" target="#b4">(Sutton and Barto, 1998;</ref><ref type="bibr" target="#b5">Sutton et al., 2000</ref>). We further discuss their modern deep learning extensions such as Deep Q- Networks ( <ref type="bibr">Mnih et al., 2015)</ref>, Policy Networks <ref type="bibr" target="#b3">(Silver et al., 2016)</ref>, and Deep Hierarchical Reinforcement Learning ( <ref type="bibr">Kulkarni et al., 2016)</ref>. We outline the ap- plications of deep reinforcement learning in NLP, in- cluding dialog ( <ref type="bibr">Li et al., 2016)</ref>, semi-supervised text classification ( <ref type="bibr" target="#b11">Wu et al., 2018)</ref>, coreference <ref type="bibr" target="#b0">(Clark and Manning, 2016;</ref><ref type="bibr" target="#b14">Yin et al., 2018)</ref>, knowledge graph rea- soning ( <ref type="bibr" target="#b13">Xiong et al., 2017</ref>), text games ( <ref type="bibr">Narasimhan et al., 2015;</ref><ref type="bibr" target="#b1">He et al., 2016a</ref>), social media ( <ref type="bibr">He et al., 2016b;</ref><ref type="bibr" target="#b15">Zhou and Wang, 2018)</ref>, information extrac- tion ( <ref type="bibr">Narasimhan et al., 2016;</ref><ref type="bibr" target="#b2">Qin et al., 2018)</ref>, lan- guage and vision ( <ref type="bibr">Pasunuru and Bansal, 2017;</ref><ref type="bibr">Misra et al., 2017;</ref><ref type="bibr">Wang et al., 2018a,b,c;</ref><ref type="bibr" target="#b12">Xiong et al., 2018)</ref>, etc.</p><p>We further discuss several critical issues in DRL so- lutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and re- ward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating lin- guistic structures in DRL. To address the model design issue, we discuss several recent solutions <ref type="bibr">(He et al., 2016b;</ref><ref type="bibr">Li et al., 2016;</ref><ref type="bibr" target="#b13">Xiong et al., 2017</ref>). We then focus on a new case study of hierarchical deep rein- forcement learning for video captioning ( <ref type="bibr" target="#b7">Wang et al., 2018b)</ref>, discussing the techniques of leveraging hierar- chies in DRL for NLP generation problems. This tu- torial aims at introducing deep reinforcement learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in reinforce- ment learning. The intended length of the tutorial is 3 hours, including a coffee break.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Outline</head><p>Representation Learning, Reasoning (Learning to Search), and Scalability are three closely related re- search subjects in Natural Language Processing. In this tutorial, we touch the intersection of all the three research subjects, covering various aspects of the the- ories of modern deep reinforcement learning methods, and show their successful applications in NLP. This tu- torial is organized in three parts:</p><p>• Foundations of Deep Reinforcement Learn- ing. First, we will provide a brief overview of reinforcement learning (RL), and discuss the classic settings in RL. We describe clas- sic methods such as Markov Decision Pro- cesses, REINFORCE <ref type="bibr" target="#b10">(Williams, 1992)</ref>, and Q- learning ( <ref type="bibr" target="#b9">Watkins, 1989</ref>). We introduce model- free and model-based reinforcement learning ap- proaches, and the widely used policy gradi- ent methods. In this part, we also introduce the modern renovation of deep reinforcement learning ( <ref type="bibr">Mnih et al., 2015)</ref>, with a focus on games ( <ref type="bibr">Mnih et al., 2013;</ref><ref type="bibr" target="#b3">Silver et al., 2016</ref>).</p><p>• Practical Deep Reinforcement Learning: Case Studies in NLP Second, we will focus on the designing practical DRL models for NLP tasks. In particular, we will take the first deep rein- forcement learning solution for dialogue ( <ref type="bibr">Li et al., 2016</ref>) as a case study. We describe the main con- tributions of this work: including its design of the reward functions, and why they are necessary for dialog. We then introduce the gigantic ac- tion space issue for deep Q-learning in NLP (He et al., 2016a,b), including several solutions. To conclude this part, we discuss interesting applica- tions of DRL in NLP, including information ex- traction and reasoning.</p><p>• Lessons Learned, Future Directions, and Prac- tical Advices for DRL in NLP Third, we switch from the theoretical presentations to an interactive demonstration and discussion session: we aim at providing an interactive session to transfer the the- ories of DRL into practical insights. More specifi- cally, we will discuss three important issues, in- cluding problem formulation/model design, ex- ploration vs. exploitation, and the integration of linguistic structures in DRL. We will show case a recent study ( <ref type="bibr" target="#b7">Wang et al., 2018b</ref>) that leverages hierarchical deep reinforcement learning for lan- guage and vision, and extend the discussion. Prac- tical advice including programming advice will be provided as a part of the demonstration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">History</head><p>The full content of this tutorial has not yet been pre- sented elsewhere, but some parts of this tutorial has also been presented at the following locations in recent years:</p><p>1. "Deep Reinforcement Learning for Knowledge Graph Reasoning", William Wang, presented at the IBM T. J. Watson Research Center, York- town Heights, NY, Bloomberg, New York, NY and Facebook, Menlo Park, CA, total attendance: 150.</p><p>2. "Deep Learning and Continuous Representations for NLP", Wen-tau Yih, Xiaodong He, and Jian- feng Gao. Tutorial at IJCAI 2016, New York City, total attendance: 100.</p><p>3. "Teaching a Machine to Converse", Jiwei Li, pre- sented at OSU, UC Berkeley, UCSB, Harbin Inst. of Technology, total attendance: 500. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>4 Information About the Presenters</head><label>4</label><figDesc></figDesc><table>William Wang is an Assistant Professor at the De-
partment of Computer Science, University of Cali-
fornia, Santa Barbara. He received his PhD from 
School of Computer Science, Carnegie Mellon Uni-
versity. He focuses on information extraction and he 
is the faculty author of DeepPath-the first deep re-
inforcement learning system for multi-hop reasoning. 
He has published more than 50 papers at leading con-
ferences and journals including ACL, EMNLP, NAACL, 
CVPR, COLING, IJCAI, CIKM, ICWSM, SIGDIAL, 
IJCNLP, INTERSPEECH, ICASSP, ASRU, SLT, Ma-
chine Learning, and Computer Speech &amp; Language, 
and he has received paper awards and honors from 
CIKM, ASRU, and EMNLP. Website: http://www. 
cs.ucsb.edu/ ˜ william/ 

Jiwei Li recently spent three years and received his 
PhD in Computer Science from Stanford University. 
His research interests are deep learning and dialogue. 
He is the most prolific NLP/ML first author during 
2012-2016, and the lead author of the first study in deep 
reinforcement learning for dialogue generation. He is 
the recipient of a Facebook Fellowship in 2015. Web-
site: https://web.stanford.edu/ ˜ jiweil/ 

Xiaodong He is the Deputy Managing Director of JD 
AI Research and Head of the Deep learning, NLP 
and Speech Lab, and a Technical Vice President of 
JD.com. He is also an Affiliate Professor at the Uni-
versity of Washington (Seattle), serves in doctoral su-
pervisory committees. Before joining JD.com, He was 
with Microsoft for about 15 years, served as Princi-
pal Researcher and Research Manager of the DLTC 
at Microsoft Research, Redmond. His research inter-
ests are mainly in artificial intelligence areas includ-
ing deep learning, natural language, computer vision, 
speech, information retrieval, and knowledge represen-
tation. He has published more than 100 papers in ACL, 
EMNLP, NAACL, CVPR, SIGIR, WWW, CIKM, NIPS, 
ICLR, ICASSP, Proc. IEEE, IEEE TASLP, IEEE SPM, 
and other venues. He received several awards including 
the Outstanding Paper Award at ACL 2015. Website: 
http://air.jd.com/people2.html </table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning with a natural language action space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust distant supervision relation extraction via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengda</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Richard S Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">No metrics are perfect: Adversarial reward learning for visual storytelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video captioning via hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07729</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning from delayed rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher John Cornish Hellaby</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>King&apos;s College, Cambridge</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reinforced co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scheduled policy optimization for natural language communication with intelligent agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI-ECAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeppath: A reinforcement learning method for knowledge graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for chinese zero pronoun resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Nan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mojitalk: Generating emotional responses at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianda</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
