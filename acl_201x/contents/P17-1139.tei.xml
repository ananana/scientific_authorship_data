<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prior Knowledge Integration for Neural Machine Translation using Posterior Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Prior Knowledge Integration for Neural Machine Translation using Posterior Regularization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1514" to="1523"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1139</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Although neural machine translation has made significant progress recently, how to integrate multiple overlapping, arbitrary prior knowledge sources remains a challenge. In this work, we propose to use posterior regularization to provide a general framework for integrating prior knowledge into neural machine translation. We represent prior knowledge sources as features in a log-linear model, which guides the learning process of the neural translation model. Experiments on Chinese-English translation show that our approach leads to significant improvements.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The past several years have witnessed the rapid de- velopment of neural machine translation (NMT) <ref type="bibr" target="#b18">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, which aims to model the translation process using neural networks in an end-to-end manner. With the capability of capturing long-distance depen- dencies due to the gating <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b5">Cho et al., 2014</ref>) and attention <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref> mechanisms, NMT has shown remarkable superiority over conventional statisti- cal machine translation (SMT) across a variety of natural languages <ref type="bibr" target="#b11">(Junczys-Dowmunt et al., 2016</ref>).</p><p>Despite the apparent success, NMT still suffers from one significant drawback: it is difficult to in- tegrate prior knowledge into neural networks. On one hand, neural networks use continuous real- valued vectors to represent all language structures involved in the translation process. While these vector representations prove to be capable of cap- turing translation regularities implicitly (Sutskever * Corresponding author: Yang Liu.</p><p>et al., <ref type="bibr">2014</ref>), it is hard to interpret each hidden state in neural networks from a linguistic perspective. On the other hand, prior knowledge in machine translation is usually represented in discrete sym- bolic forms such as dictionaries and rules <ref type="bibr" target="#b15">(Nirenburg, 1989</ref>) that explicitly encode translation regu- larities. It is difficult to transform prior knowledge represented in discrete forms to continuous repre- sentations required by neural networks.</p><p>Therefore, a number of authors have endeav- ored to integrate prior knowledge into NMT in re- cent years, either by modifying model architec- tures ( <ref type="bibr" target="#b6">Cohn et al., 2016;</ref><ref type="bibr" target="#b19">Tang et al., 2016;</ref><ref type="bibr" target="#b7">Feng et al., 2016)</ref> or by modifying training objectives ( <ref type="bibr" target="#b6">Cohn et al., 2016;</ref><ref type="bibr" target="#b7">Feng et al., 2016;</ref>. For example, to address the over-translation and under-translation prob- lems widely observed in NMT,  directly extend standard NMT to model the cover- age constraint that each source phrase should be translated into exactly one target phrase ( <ref type="bibr" target="#b13">Koehn et al., 2003</ref>). Alternatively, <ref type="bibr" target="#b6">Cohn et al. (2016)</ref> and <ref type="bibr" target="#b7">Feng et al. (2016)</ref> propose to control the fertilities of source words by appending additional additive terms to training objectives.</p><p>Although these approaches have demonstrated clear benefits of incorporating prior knowledge into NMT, how to combine multiple overlapping, arbitrary prior knowledge sources still remains a major challenge. It is difficult to achieve this end by directly modifying model architectures because neural networks usually impose strong indepen- dence assumptions between hidden states. As a result, extending a neural model requires that the interdependence of information sources be mod- eled explicitly ( <ref type="bibr" target="#b19">Tang et al., 2016)</ref>, making it hard to extend. While this drawback can be partly alleviated by appending additional addi- tive terms to training objectives ( <ref type="bibr" target="#b6">Cohn et al., 2016;</ref><ref type="bibr" target="#b7">Feng et al., 2016)</ref>, these terms are restricted to a limited number of simple constraints.</p><p>In this work, we propose a general frame- work for integrating multiple overlapping, arbi- trary prior knowledge sources into NMT using posterior regularization ( <ref type="bibr" target="#b8">Ganchev et al., 2010)</ref>. Our framework is capable of incorporating in- direct supervision via posterior distributions of neural translation models. To represent prior knowledge sources as arbitrary real-valued fea- tures, we define the posterior distribution as a log- linear model instead of a constrained posterior set ( <ref type="bibr" target="#b8">Ganchev et al., 2010</ref>). This treatment not only leads to a simpler and more efficient training al- gorithm but also achieves better translation per- formance. Experiments show that our approach is able to incorporate a variety of features and achieves significant improvements over posterior regularization using constrained posterior sets on NIST Chinese-English datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Machine Translation</head><p>Given a source sentence x = x 1 , . . . , x i , . . . , x I and a target sentence y = y 1 , . . . , y j , . . . , y J , a neural translation model <ref type="bibr" target="#b18">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) is usually factorized as a product of word-level translation probabilities:</p><formula xml:id="formula_0">P (y|x; θ) = J j=1 P (y j |x, y &lt;j ; θ),<label>(1)</label></formula><p>where θ is a set of model parameters and y &lt;j = y 1 , . . . , y j−1 denotes a partial translation. The word-level translation probability is de- fined using a softmax function:</p><formula xml:id="formula_1">P (y j |x, y &lt;j ; θ) ∝ exp f (v y j , v x , v y &lt;j , θ) ,<label>(2)</label></formula><p>where f (·) is a non-linear function, v y j is a vector representation of the j-th target word y j , v x is a vector representation of the source sentence x that encodes the context on the source side, and v y &lt;j is a vector representation of the partial translation y &lt;j that encodes the context on the target side. Given a training set {{x (n) , y (n) } N n=1 , the stan- dard training objective is to maximize the log- likelihood of the training set:</p><formula xml:id="formula_2">ˆ θ MLE = argmax θ L(θ) ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">L(θ) = N n=1 log P (y (n) |x (n) ; θ).<label>(4)</label></formula><p>Although the introduction of vector represen- tations into machine translation has resulted in substantial improvements in terms of translation quality <ref type="bibr" target="#b11">(Junczys-Dowmunt et al., 2016)</ref>, it is dif- ficult to incorporate prior knowledge represented in discrete symbolic forms into NMT. For ex- ample, given a Chinese-English dictionary con- taining ground-truth translational equivalents such as baigong, the White House, it is non-trivial to leverage the dictionary to guide the learning process of NMT. To address this problem, <ref type="bibr" target="#b19">Tang et al. (2016)</ref> propose a new architecture called phraseNet on top of RNNsearch ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) that equips standard NMT with an external memory storing phrase tables.</p><p>Another important prior knowledge source is the coverage constraint ( <ref type="bibr" target="#b13">Koehn et al., 2003)</ref>: each source phrase should be translated into exactly one target phrase. To encode this linguistic in- tuition into NMT,  extend standard NMT with a coverage vector to keep track of the attention history.</p><p>While these approaches are capable of incor- porating individual prior knowledge sources sep- arately, how to combine multiple overlapping, ar- bitrary knowledge sources still remains a major challenge. This can be hardly addressed by mod- ifying model architectures because of the lack of interpretability in NMT and the incapability of neural networks in modeling arbitrary knowledge sources. Although modifying training objectives to include additional knowledge sources as ad- ditive terms can partially alleviate this problem, these terms have been restricted to a limited num- ber of simple constraints ( <ref type="bibr" target="#b6">Cohn et al., 2016;</ref><ref type="bibr" target="#b7">Feng et al., 2016</ref>) and incapable of combining arbitrary knowledge sources.</p><p>Therefore, it is important to develop a new framework for integrating arbitrary prior knowl- edge sources into NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Posterior Regularization</head><p>Ganchev et al. <ref type="bibr">(2010)</ref> propose posterior regular- ization for incorporating indirect supervision via constraints on posterior distributions of structured latent-variable models. The basic idea is to penal- ize the log-likelihood of a neural translation model with the KL divergence between a desired distri- bution that incorporates prior knowledge and the model posteriors. The posterior regularized likeli- hood is defined as</p><formula xml:id="formula_4">F (θ, q) = λ 1 L(θ) − λ 2 N n=1 min q∈Q KL q(y) P (y|x (n) ; θ),<label>(5)</label></formula><p>where λ 1 and λ 2 are hyper-parameters to balance the preference between likelihood and posterior regularization, Q is a set of constrained posteriors:</p><formula xml:id="formula_5">Q = {q(y) : E q [φ(x, y)] ≤ b},<label>(6)</label></formula><p>where φ(x, y) is constraint feature and b is the bound of constraint feature expectations. <ref type="bibr" target="#b8">Ganchev et al. (2010)</ref> use constraint features to encode structural bias and define the set of valid distribu- tions with respect to the expectations of constraint features to facilitate inference.</p><p>As maximizing F (θ, q) involves minimizing the KL divergence, <ref type="bibr" target="#b8">Ganchev et al. (2010)</ref> present a minorization-maximization algorithm akin to EM at sentence level:</p><formula xml:id="formula_6">E : q (t+1) = argmin q KL q(y) P (y|x (n) ; θ (t) ) M : θ (t+1) = argmax θ E q (t+1) log P (y|x (n) ; θ)</formula><p>However, directly applying posterior regulariza- tion to neural machine translation faces a major difficulty: it is hard to specify the hyper-parameter b to effectively bound the expectation of features, which are usually real-valued in translation ( <ref type="bibr" target="#b16">Och and Ney, 2002;</ref><ref type="bibr" target="#b13">Koehn et al., 2003;</ref><ref type="bibr" target="#b4">Chiang, 2005</ref>). For example, the coverage penalty constraint (  proves to be an essential feature for controlling the length of a translation in NMT. As the value of coverage penalty varies significantly over different sentences, it is difficult to set an ap- propriate bound for all sentences on the training data. In addition, the minorization-maximization algorithm involves an additional step to find q (t+1) as compared with standard NMT, which increases training time significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Posterior Regularization for Neural</head><p>Machine Translation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling</head><p>In this work, we propose to adapt posterior reg- ularization ( <ref type="bibr" target="#b8">Ganchev et al., 2010</ref>) to neural ma- chine translation. The major difference is that we represent the desired distribution as a log-linear model <ref type="bibr" target="#b16">(Och and Ney, 2002</ref>) rather than a con- strained posterior set as described in ( <ref type="bibr" target="#b8">Ganchev et al., 2010)</ref>:</p><formula xml:id="formula_7">J (θ, γ) = λ 1 L(θ) − λ 2 N n=1 KL Q(y|x (n) ; γ) P (y|x (n) ; θ) ,<label>(7)</label></formula><p>where the desired distribution that encodes prior knowledge is defined as: 1</p><formula xml:id="formula_8">Q(y|x; γ) = exp γ · φ(x, y) y exp γ · φ(x, y ) .<label>(8)</label></formula><p>As compared to previous work on integrating prior knowledge into NMT ( <ref type="bibr" target="#b6">Cohn et al., 2016;</ref><ref type="bibr" target="#b19">Tang et al., 2016)</ref>, our approach pro- vides a general framework for combining arbi- trary knowledge sources. This is due to log-linear models that offer sufficient flexibility to repre- sent arbitrary prior knowledge sources as features. We tackle the representation discrepancy prob- lem by associating the Q distribution that encodes discrete representations of prior knowledge with neural models using continuous representations learned from data in the KL divergence. Another advantage of our approach is the transparency to model architectures. In principle, our approach can be applied to any neural models for natural language processing.</p><p>Our approach also differs from the original ver- sion of posterior regularization ( <ref type="bibr" target="#b8">Ganchev et al., 2010</ref>) in the definition of desired distribution. We resort to log-linear models <ref type="bibr" target="#b16">(Och and Ney, 2002</ref>) to incorporate features that have proven effective in SMT. Another benefit of using log-linear models is the differentiability of our training objective (see Eq. <ref type="formula" target="#formula_7">(7)</ref>). It is easy to leverage standard stochastic gradient descent algorithms to optimize model pa- rameters (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Design</head><p>In this section, we introduce how to design fea- tures to encode prior knowledge in the desired dis-tribution.</p><p>Note that not all features in SMT can be adopted to our framework. This is because features in SMT are defined on latent structures such as phrase pairs and synchronous CFG rules, which are not accessible to the decoding process of NMT. For- tunately, we can still leverage internal information in neural models that is linguistically meaningful such as the attention matrix a ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>.</p><p>We will introduce a number of features used in our experiments as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Bilingual Dictionary</head><p>It is natural to leverage a bilingual dictionary D to improve neural machine translation. <ref type="bibr" target="#b0">Arthur et al. (2016)</ref> propose to incorporate discrete translation lexicons into NMT by using the attention vector to select lexical probabilities on which to be focused.</p><p>In our work, for each entry x, y ∈ D in the dictionary, a bilingual dictionary (BD) feature is defined at the sentence level:</p><formula xml:id="formula_9">φ BD x,y (x, y) = 1 if x ∈ x ∧ y ∈ y 0 otherwise .<label>(9)</label></formula><p>Note that number of bilingual dictionary features depends on the vocabulary of the neural transla- tion model. Entries containing out-of-vocabulary words has to be discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Phrase Table</head><p>Phrases, which are sequences of consecutive words, are capable of memorizing local context to deal with word ordering within phrases and trans- lation of short idioms, word insertions or deletions ( <ref type="bibr" target="#b13">Koehn et al., 2003;</ref><ref type="bibr" target="#b4">Chiang, 2005)</ref>. As a result, phrase tables that specify phrase-level correspon- dences between the source and target languages also prove to be an effective knowledge source in NMT ( <ref type="bibr" target="#b19">Tang et al., 2016)</ref>. Similar to the bilingual dictionary features, we define a phrase table (PT) feature for each entry˜x entry˜entry˜x, ˜ y in a phrase table P:</p><formula xml:id="formula_10">φ PT˜xPT˜PT˜x,˜ y (x, y) = 1 if˜xif˜ if˜x ∈ x ∧ ˜ y ∈ y 0 otherwise .<label>(10)</label></formula><p>The number of phrase table features also depends on the vocabulary of the neural translation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Coverage Penalty</head><p>To overcome the over-translation and under- translation problems widely observed in NMT, a number of authors have proposed to model the fer- tility ( <ref type="bibr" target="#b2">Brown et al., 1993</ref>) and converge constraint ( <ref type="bibr" target="#b13">Koehn et al., 2003)</ref> to improve the adequacy of translation ( <ref type="bibr" target="#b6">Cohn et al., 2016;</ref><ref type="bibr" target="#b7">Feng et al., 2016;</ref><ref type="bibr" target="#b14">Mi et al., 2016</ref>). We follow  to define a coverage penalty (CP) feature to penalize source words with lower sum of attention weights: 2</p><formula xml:id="formula_11">φ CP (x, y) = |x| i=1 log min |y| j=1 a i,j , 1.0 ,<label>(11)</label></formula><p>where a i,j is the attention probability of the j-th target word on the i-th source word. Note that the value of coverage penalty feature varies sig- nificantly over sentences of different lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Length Ratio</head><p>Controlling the length of translations is very im- portant in NMT as neural models tend to gener- ate short translations for long sentences, which deteriorates the translation performance of NMT for long sentences as compared with SMT ( ). Therefore, we define the length ratio (LR) fea- ture to encourage the length of a translation to fall in a reasonable range:</p><formula xml:id="formula_12">φ LR (x, y) = (β|x|)/|y| if β|x| &lt; |y| |y|/(β|x|) otherwise ,<label>(12)</label></formula><p>where β is a hyper-parameter for penalizing too long or too short translations. For example, to convey the same meaning, an English sentence is usually about 1.2 times longer than a Chinese sentence. As a result, we can set β = 1.2. If the length of a Chinese sentence |x| is 10 and the length of an English sentence |y| is 12, then, φ LR (x, y) = 1. If the translation is too long (e.g., |y| = 100), then the feature value is 0.12. If the translation is too short (e.g., |y| = 6), the feature value is 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>In training, our goal is to find a set of model pa- rameters that maximizes the posterior regularized likelihood:</p><formula xml:id="formula_13">ˆ θ, ˆ γ = argmax θ,γ J (θ, γ) .<label>(13)</label></formula><p>Note that unlike the original version of poste- rior regularization ( <ref type="bibr" target="#b8">Ganchev et al., 2010</ref>) that re- lies on a minorization-maximization algorithm to optimize model parameters, our training objective is differentiable with respect to model parameters. Therefore, it is easy to use standard stochastic gra- dient descent algorithms to train our model. However, a major difficulty in calculating gra- dients is that the algorithm needs to sum over all candidate translations in an exponential search space for KL divergence. For example, the partial derivative of J (θ, γ) with respect to γ is given by</p><formula xml:id="formula_14">∂J (θ, γ) ∂γ = −λ 2 × N n=1 ∂ ∂γ KL Q(y|x (n) ; γ) P (y|x (n) ; θ) .<label>(14)</label></formula><p>The KL divergence is defined as</p><formula xml:id="formula_15">KL Q(y|x (n) ; γ) P (y|x (n) ; θ) = y∈Y(x (n) ) Q(y|x (n) ; γ) log Q(y|x (n) ; γ) P (y|x (n) ; θ) ,<label>(15)</label></formula><p>where Y(x (n) ) is a set of all possible candidate translations for the source sentence x (n) .</p><p>To alleviate this problem, we follow  to approximate the full search space Y(x (n) ) with a sampled sub-space S(x (n) ). Therefore, the KL divergence can be approxi- mated as</p><formula xml:id="formula_16">KL Q(y|x (n) ; γ) P (y|x (n) ; θ) ≈ y∈S(x (n) ) ˜ Q(y|x (n) ; γ) log˜Q log˜ log˜Q(y|x (n) ; γ) ˜ P (y|x (n) ; θ) .<label>(16)</label></formula><p>Note that the Q distribution is also approxi- mated on the sub-space:</p><formula xml:id="formula_17">˜ Q(y|x (n) ; γ) = exp(γ · φ(x (n) , y)) y ∈S(x (n) ) exp(γ · φ(x (n) , y )) .<label>(17)</label></formula><p>We follow  to control the sharpness of approximated neural translation dis- tribution normalized on the sampled sub-space:</p><formula xml:id="formula_18">˜ P (y|x (n) ; θ) = P (y|x (n) ; θ) α y ∈S(x (n) ) P (y |x (n) ; θ) α . (18)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Search</head><p>Given learned model parametersˆθparametersˆ parametersˆθ andˆγandˆ andˆγ, the deci- sion rule for translating an unseen source sentence x is given byˆy byˆ byˆy = argmax</p><formula xml:id="formula_19">Y(x) P (y|x; ˆ θ) .<label>(19)</label></formula><p>The search process can be factorized at the word level:</p><formula xml:id="formula_20">ˆ y j = argmax y∈Vy P (y|x, ˆ y &lt;j ; ˆ θ) ,<label>(20)</label></formula><p>where V y is the target language vocabulary. Although this decision rule shares the same ef- ficiency and simplicity with standard NMT (Bah- danau et al., 2015), it does not involve prior knowl- edge in decoding. Previous studies reveal that in- corporating prior knowledge in decoding also sig- nificantly boosts translation performance ( <ref type="bibr" target="#b0">Arthur et al., 2016;</ref>.</p><p>As directly incorporating prior knowledge into the decoding process of NMT depends on both model structure and the locality of features, we re- sort to a coarse-to-fine approach instead to keep the architecture transparency of our approach. Given a source sentence x in the test set, we first use the neural translation model P (y|x; ˆ θ) to gen- erate a k-best list of candidate translation C(x). Then, the algorithm decides on the most probable candidate translation using the following decision rule:</p><formula xml:id="formula_21">ˆ y = argmax y∈C(x) log P (y|x; ˆ θ) + ˆ γ · φ(x, y) .<label>(21)</label></formula><p>4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We evaluate our approach on Chinese-English translation.</p><p>The evaluation metric is case- insensitive BLEU calculated by the multi- bleu.perl script. Our training set 3 consists of 1.25M sentence pairs with 27.9M Chinese words and 34.5M English words. We use the NIST 2002 dataset as validation set and the <ref type="bibr">NIST 2003</ref><ref type="bibr">NIST , 2004</ref><ref type="bibr">NIST , 2005</ref><ref type="bibr">NIST , 2006</ref>, 2008 datasets as test sets.</p><p>In the experiments, we compare our approach with the following two baseline approaches:  <ref type="formula" target="#formula_0">(11)</ref>) in decoding. POSTREG extends RNNSEARCH with posterior regularization ( <ref type="bibr" target="#b8">Ganchev et al., 2010)</ref>, which uses con- straint features to represent prior knowledge and a constrained posterior set to denote the desired distri- bution. Note that POSTREG cannot use the CP feature (Section 3.2.3) because it is hard to bound the feature value appropriately. On top of RNNSEARCH, our approach also exploits posterior regularization to incorporate prior knowledge but uses a log-linear model to denote the desired distribution. All results of this work are significantly better than RNNSEARCH (p &lt; 0.01). For RNNSEARCH, we use an in-house attention-based NMT system that achieves com- parable translation performance with GROUND- HOG ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, which serves as a baseline approach in our experiments. We limit vocabulary size to 30K for both languages. The word embedding dimension is set to 620. The dimension of hidden layer is set to 1,000. In training, the batch size is set to 80. We use the AdaDelta algorithm <ref type="bibr" target="#b23">(Zeiler, 2012</ref>) for optimizing model parameters. In decoding, the beam size is set to 10.</p><p>For CPR, we simply follow  to incorporate the coverage penalty into the beam search algorithm of RNNSEARCH.</p><p>For POSTREG, we adapt the original version of posterior regularization ( <ref type="bibr" target="#b8">Ganchev et al., 2010)</ref> to NMT on top of RNNSEARCH. Following <ref type="bibr" target="#b8">Ganchev et al. (2010)</ref>, we use a ten-step pro- jected gradient descent algorithm to search for an approximate desired distribution in the E step and a one-step gradient descent for the M step.</p><p>Our approach extends RNNSEARCH by incor- porating prior knowledge. For each source sen- tence, we sample 80 candidate translations to ap- proximate the˜Pthe˜ the˜P and˜Qand˜ and˜Q distributions. The hyper- parameter α is set to 0.2. The batch size is 1. The hyper-parameters λ 1 and λ 2 are set to 8×10 −5 and 2.5 × 10 −4 . Note that they not only balance the preference between likelihood and posterior regu- larization, but also control the values of gradients to fall in a reasonable range for optimization.</p><p>We construct bilingual dictionary and phrase ta- ble in an automatic way. First, we run the statis- tical machine translation system MOSES ( <ref type="bibr" target="#b12">Koehn and Hoang, 2007)</ref> to obtain probabilistic bilin- gual dictionary and phrase table. For the bilin- gual dictionary, we retain entries with probabili- ties higher than 0.1 in both source-to-target and   target-to-source directions. For phrase table, we first remove phrase pairs that occur less than 10 times and then retain entries with probabilities higher than 0.5 in both directions. As a result, both bilingual dictionary and phrase table contain high- quality translation correspondences. We estimate the length ratio on Chinese-English data and set the hyper-parameter β to 1.236. By default, both POSTREG and our approach use reranking to search for the most probable translations (Section 3.4). <ref type="table" target="#tab_0">Table 1</ref> shows the BLEU scores obtained by RNNSEARCH, POSTREG, and our approach on the Chinese-English datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>We find POSTREG achieves significant im- provements over RNNSEARCH by adding features that encode prior knowledge. The most effective single feature for POSTREG seems to be the length ratio (LR) feature, suggesting that it is important for NMT to control the length of translation to im- prove translation quality. Note that POSTREG is unable to include the coverage penalty (CP) fea- ture because the feature value varies significantly over different sentences. It is hard to specify an appropriate bound b for constraining the expected feature value. We observe that a loose bound often makes the training process very unstable and fail to converge. Combining features obtains further modest improvements.</p><p>Our approach outperforms both RNNSEARCH and POSTREG significantly. The bilingual dictio- nary (BD) feature turns out to make the most con- tribution. Compared with CPR that imposes cov- erage penalty during decoding, our approach that using a single CP feature obtains a significant im- provement (i.e., 30.76 over 29.72), suggesting that incorporating prior knowledge sources in model- ing might be more beneficial than in decoding.</p><p>We find that combining features only results in modest improvements for our approach. One pos- sible reason is that the bilingual dictionary and phrase table features overlap on single word pairs. <ref type="table" target="#tab_2">Table 2</ref> shows the effect of reranking on trans- lation quality. We find that using prior knowl- edge features to rescore the k-best list produced by the neural translation model usually leads to improvements. This finding confirms that adding prior knowledge is beneficial for NMT, either in the training or decoding process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of Reranking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training Speed</head><p>Initialized with the best RNNSEARCH model trained for 300K iterations, our model converges after about 100K iterations. For each iteration, our approach is 1.5 times slower than RNNSEARCH. On a single GPU device Tesla M40, it takes four days to train the RNNSEARCH model and three extra days to train our model. <ref type="table">Table 3</ref> gives four examples to demonstrate the benefits of adding features. Source lijing liang tian yu bingxue de fenzhan , 31ri shenye 23 shi 50 fen , shanghai jichang jituan yuangong yinglai le 2004nian de zuihou yige hangban . Reference after fighting with ice and snow for two days , staff members of shanghai airport group welcomed the last flight of 2004 at 23 : 50pm on the 31st . RNNSEARCH after a two -day and two -day journey , the team of shanghai 's airport in shanghai has ushered in the last flight in 2004 . + BD after two days and nights fighting with ice and snow , the shanghai airport group 's staff welcomed the last flight in 2004 . Source suiran tonghuopengzhang weilai ji ge yue reng jiang weizhi zai baifenzhier yishang , buguo niandi zhiqian keneng jiangdi . Reference although inflation will remain above 2 % for the coming few months , it may decline by the end of the year . RNNSEARCH although inflation has been maintained for more than two months from the year before the end of the year , it may be lower . + PT although inflation will remain at more than 2 percent in the next few months , it may be lowered before the end of the year . Source qian ji tian ta ganggang chuyuan , jintian jianchi lai yu lao pengyou daobie . Reference just discharged from the hospital a few days ago , he insisted on coming to say farewell to his old friend today . RNNSEARCH during the previous few days , he had just been given treatment to the old friends . + CP during the previous few days , he had just been discharged from the hos- pital , and he insisted on goodbye to his old friend today . Source ( guoji ) yiselie fuzongli fouren jihua kuojian gelan gaodi dingjudian Reference ( international ) israeli deputy prime minister denied plans to expand golan heights settlements RNNSEARCH ( world ) israeli deputy prime minister denies the plan to expand the golan heights in the golan heights + LR ( international ) israeli deputy prime minister denies planning to expand golan heights <ref type="table">Table 3</ref>: Example translations that demonstrate the effect of adding features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Example Translations</head><p>In the first example, source words "fen- zhan" (fighting), "yuangong" (staff), and "yinglai" (welcomed) are untranslated in the output of RNNSEARCH. Adding the bilingual dictionary (BD) feature encourages the model to translate these words if they occur in the dictionary.</p><p>In the second example, while RNNSEARCH fails to capture phrase cohesion, adding the phrase table (PT) feature is beneficial for translating short idioms, word insertions or deletions that are sensi- tive to local context.</p><p>In the third example, RNNSEARCH tends to omit many source content words such as "chuyuan" (discharged from the hospital), "jianchi" (insisted on), and "daobie" (say farewell). The coverage penalty (CP) feature helps to alleviate the word omission problem.</p><p>In the fourth example, the translation produced by RNNSEARCH is too long and "the golan heights" occurs twice. The length ratio (LR) fea- ture is capable of controlling the sentence length in a reasonable range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Our work is directly inspired by posterior regu- larization ( <ref type="bibr" target="#b8">Ganchev et al., 2010</ref>). The major dif- ference is that we use a log-linear model to rep- resent the desired distribution rather than a con- strained posterior set. Using log-linear models not only enables our approach to incorporate ar- bitrary knowledge sources as real-valued features, but also is differentiable to be jointly trained with neural translation models efficiently.</p><p>Our work is closely related to recent work on in- jecting prior knowledge into NMT ( <ref type="bibr" target="#b0">Arthur et al., 2016;</ref><ref type="bibr" target="#b6">Cohn et al., 2016;</ref><ref type="bibr" target="#b19">Tang et al., 2016;</ref><ref type="bibr" target="#b7">Feng et al., 2016;</ref>). The major difference is that our approach aims to pro- vide a general framework for incorporating arbi- trary prior knowledge sources while keeping the neural translation model unchanged.  also propose to combine the strengths of neural networks on learning represen- tations and log-linear models on encoding prior knowledge. But they treat neural translation mod- els as a feature in the log-linear model. In contrast, we connect the two models via KL divergence to keep the transparency of our approach to model ar- chitectures. This enables our approach to be easily applied to other neural models in NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a general framework for incor- porating prior knowledge into end-to-end neural machine translation based on posterior regulariza- tion ( <ref type="bibr" target="#b8">Ganchev et al., 2010)</ref>. The basic idea is to guide NMT models towards desired behavior us- ing a log-linear model that encodes prior knowl- edge. Experiments show that incorporating prior knowledge leads to significant improvements over both standard NMT and posterior regularization using constrained posterior sets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 .</head><label>1</label><figDesc>RNNSEARCH (Bahdanau et al., 2015): a standard attention-based neural machine translation model, 2. CPR (Wu et al., 2016): extending RNNSEARCH by introducing coverage penalty refinement (Eq. (11)) in decoding, 3. POSTREG (Ganchev et al., 2010): extend- ing RNNSEARCH with posterior regulariza- tion using constrained posterior set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Feature</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Comparison of BLEU scores on the Chinese-English datasets. RNNSEARCH is an attention- based neural machine translation model (Bahdanau et al., 2015) that does not incorporate prior knowl- edge. CPR extends RNNSEARCH by introducing coverage penalty refinement (Eq.</figDesc><table>Method 

Feature 
MT02 MT03 MT04 MT05 MT06 MT08 
All 
RNNSEARCH N/A 
33.45 30.93 32.57 29.86 29.03 21.85 29.11 
CPR 
N/A 
33.84 31.18 33.26 30.67 29.63 22.38 29.72 

POSTREG 

BD 
34.65 31.53 33.82 30.66 29.81 22.55 29.97 
PT 
34.56 31.32 33.89 30.70 29.84 22.62 29.99 
LR 
34.39 31.41 34.19 30.80 29.82 22.85 30.14 
BD+PT 
34.66 32.05 34.54 31.22 30.70 22.84 30.60 
BD+PT+LR 
34.37 31.42 34.18 30.99 29.90 22.87 30.20 

this work 

BD 
36.61 33.47 36.04 32.96 32.46 24.78 32.27 
PT 
35.07 32.11 34.73 31.84 30.82 23.23 30.86 
CP 
34.68 31.99 34.67 31.37 30.80 23.34 30.76 
LR 
34.57 31.89 34.95 31.80 31.43 23.75 31.12 
BD+PT 
36.30 33.83 36.02 32.98 32.53 24.54 32.29 
BD+PT+CP 
36.11 33.64 36.36 33.11 32.53 24.57 32.39 
BD+PT+CP+LR 36.10 33.64 36.48 33.08 32.90 24.63 32.51 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Effect of reranking on translation quality.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Ideally, the desired distribution Q should be fixed to guide the learning process of P. However, it is hard to manually specify the feature weights γ. Therefore, we propose to train both θ and λ jointly (see Section 3.3). We find that joint training results in significant improvements in practice (see Table 1).</note>

			<note place="foot" n="2"> For simplicity, we omit the attention matrix a in the input of the coverage feature function.</note>

			<note place="foot" n="3"> The training set includes LDC2002E18, LDC2003E07, LDC2003E14, part of LDC2004T07, LDC2004T08 and LDC2005T06.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Shiqi Shen for useful discussions and anonymous reviewers for insightful comments. This work is supported by the National Natu-ral Science Foundation of China (No.61432013), the 973 Program (2014CB340501), and the National Natural Science Foundation of China (No.61522204). This research is also supported by Sogou Inc. and the Singapore National Research Foundation under its International Research Cen-tre@Singapore Funding Initiative and adminis-tered by the IDM Programme.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Incorporating discrete translation lexicons into neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02006v2</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Agreement-based learning of parallel lexicons and phrases from non-parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A hirarchical phrase-based model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incorporating structural alignment biases into an attentional neural translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vymolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving attention modeling with implicit distortion and fertility for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Shi Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><forename type="middle">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved nerual machine translation with SMT features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Is neural machine translation ready for deployment? a case study on 30 translation directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01108v2</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Factored translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Coverage embedding models for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Knowledge-based machine translation. Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergei</forename><surname>Nirenburg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discriminative training and maximum entropy models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Neural machine translation with external phrase memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">L H</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01792v1</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Neural machine translation advised by statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05150</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144v2</idno>
	</analytic>
	<monogr>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<meeting><address><addrLine>Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick</addrLine></address></meeting>
		<imprint>
			<publisher>Macduff Hughes, and Jeffrey Dean</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
