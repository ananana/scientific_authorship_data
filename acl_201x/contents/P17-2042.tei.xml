<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Implicit Discourse Relation Recognition with Discourse-specific Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boli</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab of the Brain-like Intelligent Systems</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Implicit Discourse Relation Recognition with Discourse-specific Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="269" to="274"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2042</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a simple and effective method to learn discourse-specific word embeddings (DSWE) for implicit discourse relation recognition. Specifically, DSWE is learned by performing connective classification on massive explicit discourse data, and capable of capturing discourse relationships between words. On the PDTB data set, using DSWE as features achieves significant improvements over baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recognizing discourse relations (e.g., Contrast, Conjunction) between two sentences is a crucial subtask of discourse structure analysis. These re- lations can benefit many downstream NLP tasks, including question answering, machine translation and so on. A discourse relation instance is usually defined as a discourse connective (e.g., but, and) taking two arguments (e.g., clause, sentence). For explicit discourse relation recognition, using only connectives as features achieves more than 93% in accuracy . Without obvious clues like connectives, implicit discourse relation recognition is still challenging.</p><p>The earlier researches usually develop linguisti- cally informed features and use supervised learn- ing method to perform the task ( <ref type="bibr" target="#b6">Lin et al., 2009;</ref><ref type="bibr" target="#b10">Louis et al., 2010;</ref><ref type="bibr" target="#b18">Rutherford and Xue, 2014;</ref><ref type="bibr" target="#b1">Braud and Denis, 2015)</ref>. Among these features, word pairs occurring in argument pairs are considered as important features, since they can partially catch discourse relationships between two arguments. For example, synonym word pairs like (good, great) may indicate a Conjunction re- lation, while antonym word pairs like (good, bad) * Corresponding author. may mean a Contrast relation. However, classi- fiers based on word pairs in previous work do not work well because of the data sparsity problem. To address this problem, recent researches use word embeddings (aka distributed representations) in- stead of words as input features, and design vari- ous neural networks to capture discourse relation- ships between arguments ( <ref type="bibr" target="#b22">Zhang et al., 2015;</ref><ref type="bibr" target="#b4">Ji and Eisenstein, 2015;</ref><ref type="bibr" target="#b17">Qin et al., 2016;</ref>. While these researches achieve promising results, they are all based on pre-trained word embeddings ignoring discourse information (e.g., good, great, and bad are of- ten mapped into close vectors). Intuitively, using word embeddings sensitive to discourse relations would further boost the performance.</p><p>In this paper, we propose to learn discourse- specific word embeddings (DSWE) from explicit data for implicit discourse relation recognition. Our method is inspired by the observation that synonym (antonym) word pairs tend to appear around the discourse connective and (but). Other connectives can also provide some discourse clues. We expect to encode these discourse clues into the distributed representations of words, to capture discourse relationships between them. To this end, we use a simple neural network to per- form connective classification on massive explicit data. Explicit data can be considered to be au- tomatically labeled by connectives. While they cannot be directly used as training data for im- plicit discourse relation recognition and contain some noise, they are effective enough to pro- vide weakly supervised signals for training the discourse-specific word embeddings.</p><p>We apply DSWE as features in a supervised neural network for implicit discourse relations recognition. On the PDTB ( <ref type="bibr" target="#b16">Prasad et al., 2008</ref>), using DSWE yields significantly better perfor- mance than using off-the-shelf word embeddings, or recent systems incorporating explicit data. We detail our method in Section 2 and evaluate it in Section 3. Conclusions are given in Section 4. Our learned DSWE is publicly available at here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Discourse-specific Word Embeddings</head><p>In this section, we first introduce the neural net- work model for learning discourse-specific word embeddings (DSWE), and then the way of collect- ing explicit discourse data for training. Finally, we highlight the differences between our work and the related researches. We induce DSWE based on explicit data by per- forming connective classification. The connective classification task predicts which discourse con- nective is suitable for combining two given argu- ments. It is essentially similar to implicit rela- tion recognition, just with different output labels. Therefore, any existing neural network model for implicit relation recognition can be easily used for connective classification. We adapt the model in ( <ref type="bibr" target="#b21">Wu et al., 2016</ref>) for connective classification be- cause it is simple enough to enable us to train on massive data. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, an ar- gument is first represented as the average of dis- tributed representations of words in it. On the con- catenation of two arguments, multiple non-linear hidden layers are then used to capture the inter- actions between them. Finally, a softmax layer is stacked for classification. We combine the cross- entropy error and regularization error multiplied by the coefficient Î» as the objective function. Dur- ing training, we initialize distributed representa- tions of all words randomly and tune them to min- imize the objective function. The finally obtained distributed representations of all words are our discourse-specific word embeddings.</p><p>Collecting explicit discourse data includes two steps: 1) distinguish whether a connective occur- ring reflects a discourse relation. For example, the connective and can either function as a discourse connective to join two Conjunction arguments, or be just used to link two nouns in a phrase. 2) iden- tify the positions of two arguments. According to <ref type="bibr" target="#b16">(Prasad et al., 2008)</ref>, arg 2 is defined as the argu- ment following a connective, however, arg 1 can be located within the same sentence as the connec- tive, in some previous or following sentence. <ref type="bibr" target="#b7">Lin et al. (2014)</ref> show that the accuracy of distinguish- ing connectives is more than 97%, while identify- ing arguments is below than 80%. Therefore, we use the existing toolkit 1 to find discourse connec- tives, and just collect explicit instances using pat- terns like [arg1 because arg2], where two argu- ments are in the same sentence, to decrease noise. We believe these simple patterns are enough when using a very large corpus. Note that there are 100 discourse connectives in the PDTB, we ignore four parallel connectives (e.g., if...then) for simplicity. The way of collecting explicit data can be easily generalized to other languages, one just need to train a classifier to find discourse connectives fol- lowing ( <ref type="bibr" target="#b7">Lin et al., 2014)</ref>.</p><p>Some aspects of this work are similar to <ref type="bibr" target="#b0">(Biran and McKeown, 2013;</ref><ref type="bibr" target="#b2">Braud and Denis, 2016)</ref>. Based on massive explicit instances, they first build a word-connective co-occurrence frequency matrix 2 , and then weight these raw frequencies. In this way, they represent words in the space of con- nectives to directly encode their discourse func- tion. The major limitation of their approach is that the dimension of the word representations must be less than or equal to the number of connec- tives. By comparison, we learn DSWE by predict- ing connectives conditioning on arguments, which yields better performance and has no such dimen- sion limitation. Some researchers use explicit data as additional training data via multi-task learning ( <ref type="bibr" target="#b5">Lan et al., 2013;</ref> or data selec- tion ( <ref type="bibr" target="#b19">Rutherford and Xue, 2015;</ref><ref type="bibr" target="#b21">Wu et al., 2016</ref>).</p><p>In both cases, explicit data are directly used to esti- mate the parameters of implicit relation classifiers. As a result, it is hard for them to incorporate mas- sive explicit data because of the noise problem. By contrast, we leverage massive explicit data by learning word embeddings from them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data and Settings</head><p>We collect explicit data from the Xin and Ltw parts of the English Gigaword Corpus (3rd edition), and get about 4.92M explicit instances. We randomly sample 20,000 instances as the development set and the others as the training set for DSWE. Af- ter discarding words occurring less than 5 times, the size of the vocabulary is 185,048. For the con- nective classification task, we obtain an accuracy of about 53% on the development set.</p><p>We adapt the neural network model described in <ref type="figure" target="#fig_0">Figure 1</ref> as the classifier for implicit discourse re- lation recognition (CDRR). Specifically, we con- catenate some surface features with the last hid- den layer as the input of the softmax layer to pre- dict discourse relations. We choose 500 Produc- tion rule ( <ref type="bibr" target="#b6">Lin et al., 2009</ref>) and 500 Brown Cluster Pair ( <ref type="bibr" target="#b18">Rutherford and Xue, 2014</ref>) features based on mutual information using the toolkit provided by <ref type="bibr" target="#b12">Peng et al. (2005)</ref>. Our learned DSWE is used as the pre-trained word embeddings for CDRR, and fixed during training.</p><p>Hyper-parameters for training DSWE and CDRR are selected based on their corresponding development set, and listed in <ref type="table">Table 1</ref>  <ref type="bibr">[200,</ref><ref type="bibr">50]</ref> means that CDRR uses two layers with the sizes of 200 and 50, respectively. And the learning rate for training DSWE is decayed by a factor of 0.8 per epoch.</p><p>Following , we perform a 4- way classification on the four top-level relations in the PDTB: T emporal (T emp), Comparison (Comp), Contingency (Cont) and Expansion (Expa). The PDTB is split into the training set ( <ref type="bibr">Sections 2-20)</ref>, development set (Sections 0-1) and test set ( <ref type="bibr">Sections 21-22)</ref>. <ref type="table" target="#tab_2">Table 2</ref> lists the statistics of these data sets. Due to the small and uneven test data set, we run our method 10 times with different random seeds (therefore different initial parameters), and report the results (of a run) which are closest to the average results. Finally, we use both Accuracy and M acro F 1 (macro- averaged F 1 ) to evaluate our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head><p>Train  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>We compare our learned discourse-specific word embeddings (DSWE) with two publicly available embeddings 3 : 1) GloVe 4 : trained on 6B words from Wikipedia 2014 and Gigaword 5 using the count based model in ( <ref type="bibr" target="#b13">Pennington et al., 2014</ref>), with a vocabulary of 400K and a dimensionality of 300.</p><p>2) word2vec 5 : trained on 100B words from Google News using the CBOW model in ( <ref type="bibr" target="#b11">Mikolov et al., 2013)</ref>, with a vocabulary of 3M and a dimen- sionality of 300.</p><p>Results in <ref type="table">Table 3</ref> show that using DSWE gains significant improvements (one-tailed t-test with p&lt;0.05) over using GloVe or word2vec, on both Accuracy and M acro F 1 . Furthermore, using DSWE achieves better performance across all re- lations on the F 1 score, especially for minority re- lations (T emp, Comp and Cont). Overall, our DSWE can effectively incorporate discourse infor-   <ref type="table">Table 3</ref>: Results of using different word embed- dings. We also list the Precision, Recall and F 1 score for each relation.</p><note type="other">32.69 28.57 35.10 Cont P 44.90 51.81 55.29 R 40.29 36.63 42.12 F 1 42.47 42.92 47.82 Expa P 60.47 60.72 63.91 R 76.21 81.60 79.00 F 1 67.43 69.63 70.66 Accuracy 55.68 57.17 58.85 M acro F 1 41.27 40.71 44.84</note><p>mation in explicit data, and thus benefits implicit discourse relation recognition. We also compare our method with three recent systems which also use explicit data to boost the performance: 1) R&amp;X2015: Rutherford and Xue (2015) con- struct weakly labeled data from explicit data based on the chosen connectives, to enlarge the training data directly.</p><p>2) B&amp;D2016: Braud and Denis (2016) learn connective-based word representations and build a logistic regression model based on them 6 .</p><p>3) Liu2016: Liu et al. (2016) use a multi-task neural network to incorporate several discourse- related data, including explicit data and the RST- DT corpus <ref type="bibr" target="#b20">(William and Thompson, 1988</ref>  Results in <ref type="table" target="#tab_5">Table 4</ref> show the superiority of our method. Although Liu2016 performs slightly bet- ter on M acro F 1 , it uses the additional labeled RST-DT corpus. For R&amp;X2015 and Liu2016, they both incorporate relatively small explicit data be- cause of the noise problem, for example, 20,000 and 40,000 instances respectively. By contrast, our method benefits from about 4.9M explicit in- stances. While B&amp;D2016 uses massive explicit data, it is limited by the fact that the maximum dimension of word representations is restricted to the number of connectives, for example 96 in their work. Overall, our method can effectively utilize massive explicit data, and thus is more powerful than baselines.  To give an intuition of what information is en- coded into the learned DSWE, we list in <ref type="table" target="#tab_7">Table 5</ref> the top 15 closest words of not and good, accord- ing to the cosine similarity. We can find that, in DSWE, words similar to not to some extent have negative meanings. And since declined is similar to not, a classifier may easily identify the implicit instance [A network spokesman would not com- ment. ABC Sports officials declined to be inter- viewed.] as the Conjunction relation. For good in DSWE, the similar words no longer include words like bad. Furthermore, the similar score between good and great is 0.54 while the score between good and bad is just 0.33, which may make a clas- sifier easier to distinguish word pairs (good, great) and (good, bad), and thus is helpful for predicting the Conjunction relation. This qualitative analysis demonstrates the ability of our DSWE to capture the discourse relationships between words. Finally, we conduct experiments to investigate the impact of connectives used in training DSWE on our results. Specifically, we use the explicit discourse instances with the top 10, 20, 30, 60 most frequent or all connectives to learn DSWE, accounting for 78.9%, 91.9%, 95.8%, 99.4% or 100% of total instances, respectively. The top 10 most frequent connectives are: and, but, also, while, as, when, after, if, however and because, which cover all four top-level relations defined in the PDTB. As illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>, with only the top 10 connectives, the learned DSWE achieves better performance than the common word em- beddings. We observe a significant improvement when using top 20 connectives, almost the best performance with top 30 connectives, and no fur- ther substantial improvement with more connec- tives. These results indicate that we can use only top n most frequent connectives to collect explicit discourse data for DSWE, which is very conve- nient for most languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we learn discourse-specific word em- beddings from massive explicit data for implicit discourse relation recognition. Experiments on the PDTB show that using the learned word embed- dings as features can significantly boost the per- formance. We also show that our method can use explicit data more effectively than previous work. Since most of neural network models for implicit discourse relation recognition use pre- trained word embeddings as input, we hope that our learned word embeddings would benefit them.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Neural network model for learning DSWE. An explicit instance is denoted as (arg 1 , arg 2 , conn). w 1 arg 1 , ..., w m arg 1 mean the words in arg 1. Two arguments are concatenated as input and the number of hidden layers is not limited to two.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>CDRR</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Impact of connectives used in training DSWE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Hyper-parameter 
DSWE 
CDRR 
wdim 
300 
300 
hsizes 
[200] 
[200, 50] 
lr 
1.0 
0.005 
Î» 
0.0001 
0.0001 
update 
SGD 
AdaGrad 
f 
ReLU 
ReLU 

Table 1: Hyper-parameters for training DSWE and 
CDRR. wdim means the dimension of word em-
beddings, hsizes the sizes of hidden layers, lr 
the learning rate, Î» the regularization coefficient, 
update the parameter update strategy and f the 
nonlinear function. Note that </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Statistics of data sets on the PDTB.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>System 
Accuracy 
M acro F 1 
R&amp;X2015 
57.10 
40.50 
B&amp;D2016 
52.81 
42.27 
Liu2016 
57.27 
44.98 
CDRR+DSWE 
58.85 
44.84 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 : Comparison with recent systems.</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Top 15 closest words of not and good in 
both word2vec and DSWE. 

</table></figure>

			<note place="foot" n="1"> https://github.com/linziheng/pdtb-parser. 2 Biran and McKeown (2013) calculate co-occurrences between word pairs and connectives.</note>

			<note place="foot" n="3"> The reasons for using those publicly available word embeddings are: 1) They are both trained on massive data. 2) It will be convenient for other people to reproduce our experiments. 3) Using GloVe or word2vec word embeddings trained on the same corpus as DSWE achieves worse performance than using these two public ones. 4 http://nlp.stanford.edu/projects/glove/glove.6B.zip 5 https://code.google.com/archive/p/word2vec/GoogleNewsvectors-negative300.bin.gz</note>

			<note place="foot" n="6"> We carefully reproduce their model since they adopt a different setting in preprocessing the PDTB.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank all the reviewers for their constructive and helpful suggestions on this pa-per. This work is partially supported by the Nat-ural Science Foundation of China (Grant Nos. 61573294, 61672440, 61075058), the Ph.D. Pro-grams Foundation of Ministry of Education of <ref type="bibr">China (Grant No. 20130121110040)</ref>, the Foun-dation of the State Language Commission of China (Grant No. WT135-10), the Natural Sci-ence Foundation of Fujian Province (Grant No. 2016J05161).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL. Sofia, Bulgaria</title>
		<meeting>ACL. Sofia, Bulgaria</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="69" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparing Word Representations for Implicit Discourse Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">ChloÃ©</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP. Lisbon</title>
		<meeting>EMNLP. Lisbon<address><addrLine>Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2201" to="2211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Connective-based Word Representations for Implicit Discourse Relation Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">ChloÃ©</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Implicit Discourse Relation Detection via a Deep Architecture with Gated Relevance Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL. Berlin, Germany</title>
		<meeting>ACL. Berlin, Germany</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1726" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">One Vector is Not Enough: Entity-Augmented Distributed Semantics for Discourse Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="329" to="344" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyu</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="476" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recognizing Implicit Discourse Relations in the Penn Discourse Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A PDTB-styled End-to-end Discourse Parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="151" to="184" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1224" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Implicit Discourse Relation Classification via Multi-Task Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI<address><addrLine>Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2750" to="2756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using Entity Features to Classify Implicit Discourse Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGDIAL</title>
		<meeting>SIGDIAL<address><addrLine>PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="59" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient Estimation of Word Representations in Vector Space</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature Selection Based on Mutual Information Criteria of Max-dependency, Max-relevance, and Min-redundancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fulmi</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on PAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1226" to="1238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic Sense Prediction for Implicit Discourse Relations in Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP<address><addrLine>PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="683" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using Syntax to Disambiguate Explicit Discourse Connectives in Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP<address><addrLine>PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Penn Discourse TreeBank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2961" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Stacking Gated Neural Architecture for Implicit Discourse Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2263" to="2270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discovering Implicit Discourse Relations Through Brown Cluster Pair Representation and Coreference Patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attapol</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL. Gothenburg, Sweden</title>
		<meeting>EACL. Gothenburg, Sweden</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="645" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving the Inference of Implicit Discourse Relations via Classifying Explicit Discourse Connectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attapol</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="799" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rhetorical Structure Theory: Towards a Functional Theory of Text Organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mann</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bilinguallyconstrained Synthetic Data for Implicit Discourse Relation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2306" to="2312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shallow Convolutional Neural Network for Implicit Discourse Relation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP. Lisbon</title>
		<meeting>EMNLP. Lisbon<address><addrLine>Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2230" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
