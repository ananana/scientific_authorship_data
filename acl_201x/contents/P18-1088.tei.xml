<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Target-Sensitive Memory Networks for Aspect Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
							<email>shuaiwanghk@gmail.com, sahisnumazumder@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahisnu</forename><surname>Mazumder</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
							<email>liub@uic.edu, mianwei.zhou@gmail.com, yichang@acm.org</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mianwei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Plus.AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Artificial Intelligence School</orgName>
								<orgName type="institution">Jilin University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Target-Sensitive Memory Networks for Aspect Sentiment Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="957" to="967"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>957</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Aspect sentiment classification (ASC) is a fundamental task in sentiment analysis. Given an aspect/target and a sentence, the task classifies the sentiment polarity expressed on the target in the sentence. Memory networks (MNs) have been used for this task recently and have achieved state-of-the-art results. In MNs, attention mechanism plays a crucial role in detecting the sentiment context for the given target. However, we found an important problem with the current MNs in performing the ASC task. Simply improving the attention mechanism will not solve it. The problem is referred to as target-sensitive sentiment, which means that the sentiment polarity of the (detected) context is dependent on the given target and it cannot be inferred from the context alone. To tackle this problem, we propose the target-sensitive memory networks (TMNs). Several alternative techniques are designed for the implementation of TMNs and their effectiveness is experimentally evaluated.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aspect sentiment classification (ASC) is a core problem of sentiment analysis <ref type="bibr" target="#b11">(Liu, 2012)</ref>. Given an aspect and a sentence containing the aspect, ASC classifies the sentiment polarity expressed in the sentence about the aspect, namely, positive, neutral, or negative. Aspects are also called opin- ion targets (or simply targets), which are usually product/service features in customer reviews. In this paper, we use aspect and target interchange- ably. In practice, aspects can be specified by the user or extracted automatically using an aspect ex- traction technique <ref type="bibr" target="#b11">(Liu, 2012)</ref>. In this work, we assume the aspect terms are given and only focus on the classification task.</p><p>Due to their impressive results in many NLP tasks <ref type="bibr" target="#b4">(Deng et al., 2014</ref>), neural networks have been applied to ASC (see the survey ( <ref type="bibr" target="#b28">Zhang et al., 2018)</ref>). Memory networks (MNs), a type of neu- ral networks which were first proposed for ques- tion answering <ref type="bibr" target="#b22">Sukhbaatar et al., 2015)</ref>, have achieved the state-of-the-art re- sults in ASC ( <ref type="bibr" target="#b23">Tang et al., 2016)</ref>. A key factor for their success is the attention mechanism. How- ever, we found that using existing MNs to deal with ASC has an important problem and simply relying on attention modeling cannot solve it. That is, their performance degrades when the sentiment of a context word is sensitive to the given target.</p><p>Let us consider the following sentences:</p><p>(1) The screen resolution is excellent but the price is ridiculous.</p><p>(2) The screen resolution is excellent but the price is high.</p><p>3) The price is high. (4) The screen resolution is high.</p><p>In sentence (1), the sentiment expressed on as- pect screen resolution (or resolution for short) is positive, whereas the sentiment on aspect price is negative. For the sake of predicting correct senti- ment, a crucial step is to first detect the sentiment context about the given aspect/target. We call this step targeted-context detection. Memory networks (MNs) can deal with this step quite well because the sentiment context of a given aspect can be captured by the internal attention mechanism in MNs. Concretely, in sentence (1) the word "ex- cellent" can be identified as the sentiment context when resolution is specified. Likewise, the con- text word "ridiculous" will be placed with a high attention when price is the target. With the correct targeted-context detected, a trained MN, which recognizes "excellent" as positive sentiment and "ridiculous" as negative sentiment, will infer cor- rect sentiment polarity for the given target. This is relatively easy as "excellent" and "ridiculous" are both target-independent sentiment words, i.e., the words themselves already indicate clear senti- ments.</p><p>As illustrated above, the attention mechanism addressing the targeted-context detection problem is very useful for ASC, and it helps classify many sentences like sentence (1) accurately. This also led to existing and potential research in improving attention modeling (discussed in Section 5). How- ever, we observed that simply focusing on tackling the target-context detection problem and learning better attention are not sufficient to solve the prob- lem found in sentences (2), (3) and (4).</p><p>Sentence <ref type="formula" target="#formula_0">(2)</ref> is similar to sentence (1) ex- cept that the (sentiment) context modifying as- pect/target price is "high". In this case, when "high" is assigned the correct attention for the as- pect price, the model also needs to capture the sen- timent interaction between "high" and price in or- der to identify the correct sentiment polarity. This is not as easy as sentence (1) because "high" itself indicates no clear sentiment. Instead, its sentiment polarity is dependent on the given target.</p><p>Looking at sentences (3) and (4), we further see the importance of this problem and also why relying on attention mechanism alone is insuffi- cient. In these two sentences, sentiment contexts are both "high" (i.e., same attention), but sentence (3) is negative and sentence (4) is positive simply because their target aspects are different. There- fore, focusing on improving attention will not help in these cases. We will give a theoretical insight about this problem with MNs in Section 3.</p><p>In this work, we aim to solve this problem. To distinguish it from the aforementioned targeted- context detection problem as shown by sentence (1), we refer to the problem in (2), (3) and (4) as the target-sensitive sentiment (or target-dependent sentiment) problem, which means that the senti- ment polarity of a detected/attended context word is conditioned on the target and cannot be directly inferred from the context word alone, unlike "ex- cellent" and "ridiculous". To address this prob- lem, we propose target-sensitive memory networks (TMNs), which can capture the sentiment interac- tion between targets and contexts. We present sev- eral approaches to implementing TMNs and ex- perimentally evaluate their effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Memory Network for ASC</head><p>This section describes our basic memory network for ASC, also as a background knowledge. It does not include the proposed target-sensitive sen- timent solutions, which are introduced in Sec- tion 4. The model design follows previous stud- ies ( <ref type="bibr" target="#b22">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b23">Tang et al., 2016</ref>) ex- cept that a different attention alignment function is used (shown in Eq. 1). Their original models will be compared in our experiments as well. The def- initions of related notations are given in <ref type="table" target="#tab_0">Table 1</ref>.</p><formula xml:id="formula_1">t a target word, t ∈ R V ×1 vt target embedding of t, vt ∈ R d×1 xi</formula><p>a context word in a sentence, xi ∈ R V ×1 mi, ci input, output context embedding of word xi, and mi, ci ∈ R d×1 V number of words in vocabulary Input Representation: Given a target aspect t, an embedding matrix A is used to convert t into a vector representation, v t (v t = At). Similarly, each context word (non-aspect word in a sentence) x i ∈ {x 1 , x 2 , ...x n } is also projected to the con- tinuous space stored in memory, denoted by m i (m i = Ax i ) ∈ {m 1 , m 2 , ...m n }. Here n is the number of words in a sentence and i is the word position/index. Both t and x i are one-hot vectors. For an aspect expression with multiple words, its aspect representation v t is the averaged vector of those words ( <ref type="bibr" target="#b23">Tang et al., 2016)</ref>.</p><formula xml:id="formula_2">d vector/embedding dimension A input embedding matrix A ∈ R d×V C output embedding matrix C ∈ R d×V α attention distribution in a sentence αi attention of context word i, αi ∈ (0, 1) o output representation, o ∈ R d×1 K number of sentiment classes s sentiment score, s ∈ R K×1 y sentiment probability</formula><p>Attention: Attention can be obtained based on the above input representation. Specifically, an at- tention weight α i for the context word x i is com- puted based on the alignment function:</p><formula xml:id="formula_3">α i = sof tmax(v T t M m i )<label>(1)</label></formula><p>where M ∈ R d×d is the general learning ma- trix suggested by <ref type="bibr" target="#b14">Luong et al. (2015)</ref>. In this manner, attention α = {α 1 , α 2 , ..α n } is rep- resented as a vector of probabilities, indicating the weight/importance of context words towards a given target. Note that α i ∈ (0, 1) and</p><formula xml:id="formula_4">i α i = 1.</formula><p>Output Representation: Another embedding matrix C is used for generating the individual (out- put) continuous vector c i (c i = Cx i ) for each con- text word x i . A final response/output vector o is produced by summing over these vectors weighted with the attention α, i.e., o = i α i c i .</p><p>Sentiment Score (or Logit): The aspect sen- timent scores (also called logits) for positive, neutral, and negative classes are then calculated, where a sentiment-specific weight matrix W ∈ R K×d is used. The sentiment scores are repre- sented in a vector s ∈ R K×1 , where K is the num- ber of (sentiment) classes, which is 3 in ASC.</p><formula xml:id="formula_5">s = W (o + v t )<label>(2)</label></formula><p>The final sentiment probability y is produced with a sof tmax operation, i.e., y = sof tmax(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem of the above Model for Target-Sensitive Sentiment</head><p>This section analyzes the problem of target- sensitive sentiment in the above model. The anal- ysis can be generalized to many existing MNs as long as their improvements are on attention α only. We first expand the sentiment score calculation from Eq. 2 to its individual terms:</p><formula xml:id="formula_6">s = W (o + v t ) = W ( i α i c i + v t ) = α 1 W c 1 + α 2 W c 2 + ...α n W c n + W v t<label>(3)</label></formula><p>where "+" denotes element-wise summation. In Eq. 3, α i W c i can be viewed as the individual sen- timent logit for a context word and W v t is the sentiment logit of an aspect. They are linearly combined to determine the final sentiment score s. This can be problematic in ASC. First, an aspect word often expresses no sentiment, for example, "screen". However, if the aspect term v t is sim- ply removed from Eq. 3, it also causes the prob- lem that the model cannot handle target-dependent sentiment. For instance, the sentences (3) and (4) in Section 1 will then be treated as identical if their aspect words are not considered. Second, if an aspect word is considered and it directly bears some positive or negative sentiment, then when an aspect word occurs with different context words for expressing opposite sentiments, a contradic- tion can be resulted from them, especially in the case that the context word is a target-sensitive sen- timent word. We explain it as follows.</p><p>Let us say we have two target words price and resolution (denoted as p and r). We also have two possible context words "high" and "low" (de- noted as h and l). As these two sentiment words can modify both aspects, we can construct four snippets "high price", "low price", "high resolu- tion" and "low resolution". Their sentiments are negative, positive, positive, and negative respec- tively. Let us set W to R 1×d so that s becomes a 1-dimensional sentiment score indicator. s &gt; 0 indicates a positive sentiment and s &lt; 0 indi- cates a negative sentiment. Based on the above example snippets or phrases we have four corre- sponding inequalities:</p><formula xml:id="formula_7">(a) W (α h c h + v p ) &lt; 0, (b) W (α l c l + v p ) &gt; 0, (c) W (α h c h + v r ) &gt; 0 and (d) W (α l c l + v r ) &lt; 0.</formula><p>We can drop all α terms here as they all equal to 1, i.e., they are the only context word in the snippets to attend to (the target words are not contexts). From (a) and (b) we can infer</p><formula xml:id="formula_8">(e) W c h &lt; −W v p &lt; W c l . From (c) and (d) we can infer (f) W c l &lt; −W v r &lt; W c h . From (e) and (f) we have (g) W c h &lt; W c l &lt; W c h , which is a contradiction.</formula><p>This contradiction means that MNs cannot learn a set of parameters W and C to correctly clas- sify the above four snippets/sentences at the same time. This contradiction also generalizes to real- world sentences. That is, although real-world review sentences are usually longer and contain more words, since the attention mechanism makes MNs focus on the most important sentiment con- text (the context with high α i scores), the problem is essentially the same. For example, in sentences (2) and (3) in Section 1, when price is targeted, the main attention will be placed on "high". For MNs, these situations are nearly the same as that for classifying the snippet "high price". We will also show real examples in the experiment section.</p><p>One may then ask whether improving attention can help address the problem, as α i can affect the final results by adjusting the sentiment effect of the context word via α i W c i . This is unlikely, if not impossible. First, notice that α i is a scalar ranging in (0,1), which means it essentially assigns higher or lower weight to increase or decrease the senti- ment effect of a context word. It cannot change the intrinsic sentiment orientation/polarity of the con- text, which is determined by W c i . For example, if W c i assigns the context word "high" a positive sentiment (W c i &gt; 0), α i will not make it negative (i.e., α i W c i &lt; 0 cannot be achieved by chang-ing α i ). Second, other irrelevant/unimportant con- text words often carry no or little sentiment infor- mation, so increasing or decreasing their weights does not help. For example, in the sentence "the price is high", adjusting the weights of context words "the" and "is" will neither help solve the problem nor be intuitive to do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Proposed Approaches</head><p>This section introduces six (6) alternative target- sensitive memory networks (TMNs), which all can deal with the target-sensitive sentiment problem. Each of them has its characteristics.</p><p>Non-linear Projection (NP): This is the first approach that utilizes a non-linear projection to capture the interplay between an aspect and its context. Instead of directly following the common linear combination as shown in Eq. 3, we use a non-linear projection (tanh) as the replacement to calculate the aspect-specific sentiment score.</p><formula xml:id="formula_9">s = W · tanh( i α i c i + v t )<label>(4)</label></formula><p>As shown in Eq. 4, by applying a non-linear pro- jection over attention-weighted c i and v t , the con- text and aspect information are coupled in a way that the final sentiment score cannot be obtained by simply summing their individual contributions (compared with Eq. 3). This technique is also in- tuitive in neural networks. However, notice that by using the non-linear projection (or adding more sophisticated hidden layers) over them in this way, we sacrifice some interpretability. For example, we may have difficulty in tracking how each indi- vidual context word (c i ) affects the final sentiment score s, as all context and target representations are coupled. To avoid this, we can use the follow- ing five alternative techniques. Contextual Non-linear Projection (CNP): Despite the fact that it also uses the non-linear pro- jection, this approach incorporates the interplay between a context word and the given target into its (output) context representation. We thus name it Contextual Non-linear Projection (CNP).</p><formula xml:id="formula_10">s = W i α i · tanh(c i + v t )<label>(5)</label></formula><p>From Eq. 5, we can see that this approach can keep the linearity of attention-weighted context aggre- gation while taking into account the aspect infor- mation with non-linear projection, which works in a different way compared to NP. If we define˜c define˜ define˜c i = tanh(c i + v t ), ˜ c i can be viewed as the target-aware context representation of context x i and the final sentiment score is calculated based on the aggregation of such˜csuch˜ such˜c i . This could be a more reasonable way to carry the aspect informa- tion rather than simply summing the aspect repre- sentation (Eq. 3).</p><p>However, one potential disadvantage is that this setting uses the same set of vector representa- tions (learned by embeddings C) for multiple pur- poses, i.e., to learn output (context) representa- tions and to capture the interplay between contexts and aspects. This may degenerate its model per- formance when the computational layers in mem- ory networks (called "hops") are deep, because too much information is required to be encoded in such cases and a sole set of vectors may fail to capture all of it.</p><p>To overcome this, we suggest the involvement of an additional new set of embeddings/vectors, which is exclusively designed for modeling the sentiment interaction between an aspect and its context. The key idea is to decouple different functioning components with different representa- tions, but still make them work jointly. The fol- lowing four techniques are based on this idea.</p><p>Interaction Term (IT): The third approach is to formulate explicit target-context sentiment inter- action terms. Different from the targeted-context detection problem which is captured by atten- tion (discussed in Section 1), here the target- context sentiment (TCS) interaction measures the sentiment-oriented interaction effect between tar- gets and contexts, which we refer to as TCS inter- action (or sentiment interaction) for short in the rest of this paper. Such sentiment interaction is captured by a new set of vectors, and we thus also call such vectors TCS vectors. </p><formula xml:id="formula_11">s = i α i (W s c i + w I d i , d t )<label>(6)</label></formula><formula xml:id="formula_12">= Dx i , d t = Dt (D ∈ R d×V and d i , d t ∈ R d×1 ).</formula><p>Unlike input and output embeddings A and C, D is designed to capture the sentiment interac-tion. The vectors from D affect the final sentiment score through w I d i , d t , where w I is a sentiment- specific vector and d i , d t ∈ R denotes the dot product of the two TCS vectors d i and d t . Com- pared to the basic MNs, this model can better cap- ture target-sensitive sentiment because the inter- actions between a context word h and different aspect words (say, p and r) can be different, i.e.,</p><formula xml:id="formula_13">d h , d p = d h , d r .</formula><note type="other">The key advantage is that now the sentiment ef- fect is explicitly dependent on its target and con- text. For example, d h , d p can help shift the final sentiment to negative and d h , d r can help shift it to positive. Note that α is still needed to con- trol the importance of different contexts. In this manner, targeted-context detection (attention) and TCS interaction are jointly modeled and work to- gether for sentiment inference. The proposed tech- niques introduced below also follow this core idea but with different implementations or properties. We thus will not repeat similar discussions.</note><p>Coupled Interaction (CI): This proposed tech- nique associates the TCS interaction with an ad- ditional set of context representation. This rep- resentation is for capturing the global correlation between context and different sentiment classes.</p><formula xml:id="formula_14">s = i α i (W s c i + W I d i , d t e i )<label>(7)</label></formula><p>Specifically, e i is another output representation for x i , which is coupled with the sentiment interaction factor d i , d t . For each context word x i , e i is gen- erated as e i = Ex i where E ∈ R d×V is an embed- ding matrix. d i , d t and e i function together as a target-sensitive context vector and are used to pro- duce sentiment scores with W I (W I ∈ R K×d ). Joint Coupled Interaction (JCI): A natural variant of the above model is to replace e i with c i , which means to learn a joint output representa- tion. This can also reduce the number of learning parameters and simplify the CI model.</p><formula xml:id="formula_15">s = i α i (W s c i + W I d i , d t c i )<label>(8)</label></formula><p>Joint Projected Interaction (JPI): This model also employs a unified output representation like JCI, but a context output vector c i will be projected to two different continuous spaces before senti- ment score calculation. To achieve the goal, two projection matrices W 1 , W 2 and the non-linear projection function tanh are used. The intuition is that, when we want to reduce the (embedding) pa- rameters and still learn a joint representation, two different sentiment effects need to be separated in different vector spaces. The two sentiment effects are modeled as two terms:</p><formula xml:id="formula_16">s = i α i W J tanh(W 1 c i ) + i α i W J d i , d t tanh(W 2 c i )<label>(9)</label></formula><p>where the first term can be viewed as learn- ing target-independent sentiment effect while the second term captures the TCS interaction. A joint sentiment-specific weight matrix W J (W J ∈ R K×d ) is used to control/balance the interplay be- tween these two effects.</p><p>Discussions: (a) In IT, CI, JCI, and JPI, their first-order terms are still needed, because not in all cases sentiment inference needs TCS interac- tion. For some simple examples like "the battery is good", the context word "good" simply indicates clear sentiment, which can be captured by their first-order term. However, notice that the mod- eling of second-order terms offers additional help in both general and target-sensitive scenarios. (b) TCS interaction can be calculated by other model- ing functions. We have tried several methods and found that using the dot product d i , d t or d T i W d t (with a projection matrix W ) generally produces good results. (c) One may ask whether we can use fewer embeddings or just use one universal em- bedding to replace A, C and D (the definition of D can be found in the introduction of IT). We have investigated them as well. We found that merging A and C is basically workable. But merging D and A/C produces poor results because they es- sentially function with different purposes. While A and C handle targeted-context detection (atten- tion), D captures the TCS interaction. (d) Except NP, we do not apply non-linear projection to the sentiment score layer. Although adding non-linear transformation to it may further improve model performance, the individual sentiment effect from each context will become untraceable, i.e., losing some interpretability. In order to show the effec- tiveness of learning TCS interaction and for anal- ysis purpose, we do not use it in this work. But it can be flexibly added for specific tasks/analyses that do not require strong interpretability.</p><p>Loss function: The proposed models are all trained in an end-to-end manner by minimizing the cross entropy loss. Let us denote a sentence and a target aspect as x and t respectively. They appear together in a pair format (x, t) as input and all such pairs construct the dataset H. g (x,t) is a one-hot vector and g k (x,t) ∈ {0, 1} denotes a gold senti- ment label, i.e., whether (x, t) shows sentiment k. y x,t is the model-predicted sentiment distribution for (x, t). y k x,t denotes its probability in class k. Based on them, the training loss is constructed as:  <ref type="bibr" target="#b24">Wang et al., 2016)</ref>.</p><formula xml:id="formula_17">loss = − (x,t)∈H k∈K g k (x,t) log y k (x,t)<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>As discussed in Section 1, the attention mech- anism is suitable for ASC because it effectively addresses the targeted-context detection problem. Along this direction, researchers have studied more sophisticated attentions to further help the ASC task <ref type="bibr" target="#b1">(Chen et al., 2017;</ref><ref type="bibr" target="#b15">Ma et al., 2017;</ref><ref type="bibr" target="#b12">Liu and Zhang, 2017)</ref>. <ref type="bibr" target="#b1">Chen et al. (2017)</ref> proposed to use a recurrent attention mechanism. <ref type="bibr" target="#b15">Ma et al. (2017)</ref> used multiple sets of attentions, one for modeling the attention of aspect words and one for modeling the attention of context words. <ref type="bibr" target="#b12">Liu and Zhang (2017)</ref> also used multiple sets of at- tentions, one obtained from the left context and one obtained from the right context of a given tar- get. Notice that our work does not lie in this direc- tion. Our goal is to solve the target-sensitive sen- timent and to capture the TCS interaction, which is a different problem. This direction is also finer- grained, and none of the above works addresses this problem. Certainly, both directions can im- prove the ASC task. We will also show in our ex- periments that our work can be integrated with an improved attention mechanism.</p><p>To the best of our knowledge, none of the ex- isting studies addresses the target-sensitive senti- ment problem in ASC under the purely data-driven and supervised learning setting. Other concepts like sentiment shifter ( <ref type="bibr" target="#b19">Polanyi and Zaenen, 2006</ref>) and sentiment composition <ref type="bibr" target="#b16">(Moilanen and Pulman, 2007;</ref><ref type="bibr" target="#b3">Choi and Cardie, 2008;</ref><ref type="bibr" target="#b21">Socher et al., 2013</ref>) are also related, but they are not learned automatically and require rule/patterns or external resources ( <ref type="bibr" target="#b11">Liu, 2012)</ref>. Note that our approaches do not rely on handcrafted patterns ( <ref type="bibr" target="#b5">Ding et al., 2008;</ref><ref type="bibr" target="#b26">Wu and Wen, 2010)</ref>, manually compiled sentiment constraints and review ratings ( <ref type="bibr" target="#b13">Lu et al., 2011</ref>), or parse trees ( <ref type="bibr" target="#b21">Socher et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We perform experiments on the datasets of Se- mEval Task 2014 ( <ref type="bibr" target="#b20">Pontiki et al., 2014</ref>), which contain online reviews from domain Laptop and Restaurant. In these datasets, aspect sentiment polarities are labeled. The training and test sets have also been provided. Full statistics of the datasets are given in  with a state-of-the-art attention-based LSTM for ASC, AE-LSTM ( <ref type="bibr" target="#b24">Wang et al., 2016)</ref>. ATAE-LSTM: Another attention-based LSTM for ASC reported in ( <ref type="bibr" target="#b24">Wang et al., 2016)</ref>. Target-sensitive Memory Networks (TMNs):</p><p>The six proposed techniques, NP, CNP, IT, CI, JCI, and JPI give six target-sensitive memory networks.</p><p>Note that other non-neural network based mod- els like SVM and neural models without atten- tion mechanism like traditional LSTMs have been compared and reported with inferior performance in the ASC task ( <ref type="bibr" target="#b23">Tang et al., 2016;</ref><ref type="bibr" target="#b24">Wang et al., 2016</ref>), so they are excluded from comparisons here. Also, note that non-neural models like SVMs require feature engineering to manually encode aspect information, while this work aims to improve the aspect representation learning based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation Measure</head><p>Since we have a three-class classification task (positive, negative and neutral) and the classes are imbalanced as shown in <ref type="table" target="#tab_2">Table 2</ref>, we use F1-score as our evaluation measure. We report both F1- Macro over all classes and all individual class- based F1 scores. As our problem requires fine- grained sentiment interaction, the class-based F1 provides more indicative information. In addition, we report the accuracy (same as F1-Micro), as it is used in previous studies. However, we suggest us- ing F1-score because accuracy biases towards the majority class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Training Details</head><p>We use the open-domain word embeddings 1 for the initialization of word vectors. We initialize other model parameters from a uniform distribu- tion U (-0.05, 0.05). The dimension of the word embedding and the size of the hidden layers are 300. The learning rate is set to 0.01 and the dropout rate is set to 0.1. Stochastic gradient de- scent is used as our optimizer. The position encod- ing is also used ( <ref type="bibr" target="#b23">Tang et al., 2016</ref>). We also com- pare the memory networks in their multiple com- putational layers version (i.e., multiple hops) and the number of hops is set to 3 as used in the men- tioned previous studies. We implemented all mod- els in the TensorFlow environment using same in- put, embedding size, dropout rate, optimizer, etc. so as to test our hypotheses, i.e., to make sure the achieved improvements do not come from else- where. Meanwhile, we can also report all evalua- tion measures discussed above 2 . 10% of the train- ing data is used as the development set. We report the best results for all models based on their F-1 Macro scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Result Analysis</head><p>The classification results are shown in <ref type="table" target="#tab_5">Table 3</ref>. Note that the candidate models are all based on classic/standard attention mechanism, i.e., without sophisticated or multiple attentions involved. We compare the 1-hop and 3-hop memory networks as two different settings. The top three F1-Macro scores are marked in bold. Based on them, we have the following observations:</p><p>1. Comparing the 1-hop memory networks (first nine rows), we see significant performance gains achieved by CNP, CI, JCI, and JPI on both datasets, where each of them has p &lt; 0.01 over the strongest baseline (BL-MN) from paired t-test using F1-Macro. IT also outperforms the other baselines while NP has similar performance to BL-MN. This indi- cates that TCS interaction is very useful, as BL-MN and NP do not model it. 2. In the 3-hop setting, TMNs achieve much better results on Restaurant. JCI, IT, and CI achieve the best scores, outperforming the strongest baseline AMN by 2.38%, 2.18%, and 2.03%. On Laptop, BL-MN and most TMNs (except CNP and JPI) perform sim- ilarly. However, BL-MN performs poorly on Restaurant (only better than two models) while TMNs show more stable performance. 3. Comparing all TMNs, we see that JCI works the best as it always obtains the top-three scores on two datasets and in two settings. CI and JPI also perform well in most cases. IT, NP, and CNP can achieve very good scores in some cases but are less stable. We also ana- lyzed their potential issues in Section 4. 4. It is important to note that these improve- ments are quite large because in many cases sentiment interactions may not be necessary (like sentence (1) in Section 1   Integration with Improved Attention: As dis- cussed, the goal of this work is not for learn- ing better attention but addressing the target- sensitive sentiment. In fact, solely improving at- tention does not solve our problem (see Sections 1 and 3). However, better attention can certainly help achieve an overall better performance for the ASC task, as it makes the targeted-context detec- tion more accurate. Here we integrate our pro- posed technique JCI with a state-of-the-art sophis- ticated attention mechanism, namely, the recurrent attention framework, which involves multiple at- tentions learned iteratively ( <ref type="bibr" target="#b10">Kumar et al., 2016;</ref><ref type="bibr" target="#b1">Chen et al., 2017)</ref>. We name our model with this integration as Target-sensitive Recurrent-attention Memory Network (TRMN) and the basic memory network with the recurrent attention as Recurrent- attention Memory Network (RMN). Their results are given in <ref type="table" target="#tab_6">Table 4</ref>. TRMN achieves significant performance gain with p &lt; 0.05 in paired t-test.  <ref type="table" target="#tab_9">Table 5</ref> shows two records in Laptop. In record 1, to identify the senti- ment of target price in the presented sentence, the sentiment interaction between the context word "higher" and the target word price is the key. The   <ref type="figure">1)</ref>), so it can be inferred that the first value in vector W c i is greater than the other two values. Here c i denotes the vec- tor representation of "higher" so we use c higher to highlight it and we have {W c higher } N egative &gt; {W c higher } N eutral/P ositive as an inference. In record 2, the target is resolution and its sen- timent is positive in the presented sentence. Al- though we have the same context word "higher", different from record 1, it requires a positive sen- timent interaction with the current target. Look- ing at the results, we see TMN assigns the high- est sentiment score of word "higher" to positive class correctly, whereas MN assigns it to neg- ative class. This error is expected if we con- sider the above inference {W c higher } N egative &gt; {W c higher } N eutral/P ositive in MN. The cause of this unavoidable error is that W c i is not conditioned on the target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Effect of TCS Interaction for Identifying</head><p>In contrast, W J d i , ·d t tanh(W 2 c i ) can change the sentiment polarity with the aspect vector d t encoded. Other TMNs also achieve it (like W I d i , d t c i in JCI).</p><p>One may notice that the aspect information (v t ) is actually also considered in the form of α i W c i + W v t in MNs and wonder whether W v t may help address the problem given different v t . Let us as- sume it helps, which means in the above exam- ple an MN makes W v resolution favor the positive class and W v price favor the negative class. But then we will have trouble when the context word is "lower", where it requires W v resolution to favor the negative class and W v price to favor the posi- tive class. This contradiction reflects the theoreti- cal problem discussed in Section 3.</p><p>Other Examples: We also found other interesting target-sensitive sentiment expressions like "large bill" and "large portion", "small tip" and "small portion" from Restaurant. Notice that TMNs can also improve the neutral sentiment (see <ref type="table" target="#tab_5">Ta- ble 3</ref>). For instance, TMN generates a sentiment score vector of the context "over" for target as- pect price: {0.1373, 0.0066, -0.1433} (negative) and for target aspect dinner: {0.0496, 0.0591, - 0.1128} (neutral) accurately. But MN produces both negative scores {0.0069, 0.0025, -0.0090} (negative) and {0.0078, 0.0028, -0.0102} (nega- tive) for the two different targets. The latter one in MN is incorrect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we first introduced the target- sensitive sentiment problem in ASC. After that, we discussed the basic memory network for ASC and analyzed the reason why it is incapable of cap- turing such sentiment from a theoretical perspec- tive. We then presented six techniques to construct target-sensitive memory networks. Finally, we re- ported the experimental results quantitatively and qualitatively to show their effectiveness.</p><p>Since ASC is a fine-grained and complex task, there are many other directions that can be further explored, like handling sentiment negation, better embedding for multi-word phrase, analyzing sen- timent composition, and learning better attention. We believe all these can help improve the ASC task. The work presented in this paper lies in the direction of addressing target-sensitive sentiment, and we have demonstrated the usefulness of cap- turing this signal. We believe that there will be more effective solutions coming in the near future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Aspect sentiment classification (ASC) (Hu and Liu, 2004), which is different from document or sentence level sentiment classification (Pang et al., 2002; Kim, 2014; Yang et al., 2016), has recently been tackled by neural networks with promising results (Dong et al., 2014; Nguyen and Shirai, 2015) (also see the survey (Zhang et al., 2018)). Later on, the seminal work of using attention mechanism for neural machine translation (Bah- danau et al., 2015) popularized the application of the attention mechanism in many NLP tasks (Her- mann et al., 2015; Cho et al., 2015; Luong et al., 2015), including ASC. Memory networks (MNs) (Weston et al., 2015; Sukhbaatar et al., 2015) are a type of neural mod- els that involve such attention mechanisms (Bah- danau et al., 2015), and they can be applied to ASC. Tang et al. (2016) proposed an MN vari- ant to ASC and achieved the state-of-the-art per- formance. Another common neural model using attention mechanism is the RNN/LSTM (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Definition of Notations</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Dataset 
Positive 
Neutral 
Negative 
Train Test Train Test Train Test 
Restaurant 2164 728 
637 
196 
807 
196 
Laptop 
994 
341 
464 
169 
870 
128 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics of Datasets 

6.1 Candidate Models for Comparison 

MN: The classic end-to-end memory net-
work (Sukhbaatar et al., 2015). 
AMN: A state-of-the-art memory network used 
for ASC (Tang et al., 2016). The main difference 
from MN is in its attention alignment function, 
which concatenates the distributed representations 
of the context and aspect, and uses an additional 
weight matrix for attention calculation, following 
the method introduced in (Bahdanau et al., 2015). 
BL-MN: Our basic memory network presented in 
Section 2, which does not use the proposed tech-
niques for capturing target-sensitive sentiments. 
AE-LSTM: RNN/LSTM is another popular 
attention based neural model. Here we compare </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results of all models on two datasets. Top three F1-Macro scores are marked in bold. The first 
nine models are 1-hop memory networks. The last nine models are 3-hop memory networks. 

techniques do not bring harm while capturing 
additional target-sensitive signals. 
5. Micro-F1/accuracy is greatly affected by the 
majority class, as we can see the scores from 
Pos. and Micro are very consistent. TMNs, in 
fact, effectively improve the minority classes, 
which are reflected in Neg. and Neu., for 
example, JCI improves BL-MN by 3.78% in 
Neg. on Restaurant. This indicates their use-
fulness of capturing fine-grained sentiment 
signals. We will give qualitative examples in 
next section to show their modeling superior-
ity for identifying target-sensitive sentiments. 

Restaurant 
Model Macro Neg. Neu. 
Pos. Micro 
TRMN 69.00 68.66 50.66 87.70 78.86 
RMN 
67.48 66.48 49.11 86.85 77.14 
Laptop 
Model Macro Neg. Neu. 
Pos. Micro 
TRMN 68.18 62.63 57.37 84.30 72.92 
RMN 
67.17 62.65 55.31 83.55 72.07 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 : Results with Recurrent Attention</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Sample Records and Model Comparison between MN and TMN 

specific sentiment scores of the word "higher" to-
wards negative, neutral and positive classes in both 
models are reported. We can see both models 
accurately assign the highest sentiment scores to 
the negative class. We also observe that in MN 
the negative score (0.3641) in the 3-dimension 
vector {0.3641, −0.3275, −0.0750} calculated by 
α i W c i is greater than neutral (−0.3275) and pos-
itive (−0.0750) scores. Notice that α i is always 
positive (ranging in (0, </table></figure>

			<note place="foot" n="1"> https://github.com/mmihaltz/word2vec-GoogleNewsvectors</note>

			<note place="foot" n="2"> Most related studies report accuracy only.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by National Science Foundation (NSF) under grant nos. IIS-1407927 and IIS-1650900, and by Huawei Tech-nologies Co. Ltd with a research gift.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Describing multimedia content using attention-based encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1875" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning with compositional semantics as structural inference for subsentential sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="793" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning: methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and Trends R in Signal Processing</title>
		<imprint>
			<publisher>Now Publishers, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="197" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A holistic lexicon-based approach to opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Web Search and Data Mining</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Synthesis lectures on human language technologies</title>
		<imprint>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention modeling for targeted sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="572" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic construction of a context-aware sentiment lexicon: an optimization approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malu</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umeshwar</forename><surname>Dayal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on World Wide Web</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interactive attention networks for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4068" to="4074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sentiment composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karo</forename><surname>Moilanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RANLP</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="378" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Phrasernn: Phrase recursive neural network for aspect-based sentiment analysis</title>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<editor>Thien Hai Nguyen and Kiyoaki Shirai</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2509" to="2514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Contextual valence shifters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livia</forename><surname>Polanyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Zaenen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing attitude and affect in text: Theory and applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semeval-2014 task4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ProWorkshop on Semantic Evaluation (SemEval-2014)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, and Suresh Manandhar</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention-based lstm for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Disambiguating dynamic sentiment ambiguous adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1191" to="1199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning for sentiment analysis: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">1253</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
