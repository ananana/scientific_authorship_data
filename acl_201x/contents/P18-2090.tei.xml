<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GNEG: Graph-Based Negative Sampling for word2vec</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<email>zheng.zhang@limsi.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Zweigenbaum</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LIMSI</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Université Paris-Saclay</orgName>
								<address>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">LRI</orgName>
								<orgName type="institution" key="instit1">Univ. Paris-Sud</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Université Paris-Saclay</orgName>
								<address>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">LIMSI</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Université Paris-Saclay</orgName>
								<address>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GNEG: Graph-Based Negative Sampling for word2vec</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="566" to="571"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>566</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Negative sampling is an important component in word2vec for distributed word representation learning. We hypothesize that taking into account global, corpus-level information and generating a different noise distribution for each target word better satisfies the requirements of negative examples for each training word than the original frequency-based distribution. In this purpose we pre-compute word co-occurrence statistics from the corpus and apply to it network algorithms such as random walk. We test this hypothesis through a set of experiments whose results show that our approach boosts the word analogy task by about 5% and improves the performance on word similarity tasks by about 1% compared to the skip-gram negative sampling baseline.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Negative sampling, as introduced by <ref type="bibr" target="#b13">Mikolov et al. (2013b)</ref>, is used as a standard compo- nent in both the CBOW and skip-gram models of word2vec. For practical reasons, instead of using a softmax function, earlier work explored different alternatives which approximate the soft- max in a computationally efficient way. These alternative methods can be roughly divided into two categories: softmax-based approaches (hier- archical softmax <ref type="bibr" target="#b15">(Morin and Bengio, 2005</ref>), dif- ferentiated softmax <ref type="bibr" target="#b2">(Chen et al., 2015)</ref> and CNN- softmax ( <ref type="bibr" target="#b8">Kim et al., 2016)</ref>) and sampling-based approaches (importance sampling ( <ref type="bibr" target="#b1">Bengio et al., 2003</ref>), target sampling ( <ref type="bibr" target="#b7">Jean et al., 2014</ref>), noise contrastive estimation <ref type="bibr" target="#b14">(Mnih and Teh, 2012</ref>) and negative sampling <ref type="bibr" target="#b13">Mikolov et al. (2013b)</ref>). Gener- ally speaking, among all these methods, negative sampling is the best choice for distributed word representation learning <ref type="bibr" target="#b17">(Ruder, 2016)</ref>.</p><p>Negative sampling replaces the softmax with bi- nary classifiers. For instance, in the skip-gram model, word representations are learned by pre- dicting a training word's surrounding words given this training word. When training, correct sur- rounding words provide positive examples in con- trast to a set of sampled negative examples (noise). To find these negative examples, a noise distribu- tion is empirically defined as the unigram distribu- tion of the words to the 3/4 th power:</p><formula xml:id="formula_0">P n (w) = U (w) 3 4 / |vocab| i=1 U (w i ) 3 4</formula><p>(1)</p><p>Although this noise distribution is widely used and significantly improves the distributed word repre- sentation quality, we believe there is still room for improvement in the two following aspects: First, the unigram distribution only takes into account word frequency, and provides the same noise dis- tribution when selecting negative examples for dif- ferent target words. <ref type="bibr" target="#b9">Labeau and Allauzen (2017)</ref> already showed that a context-dependent noise distribution could be a better solution to learn a language model. But they only use information on adjacent words. Second, unlike the positive target words, the meaning of negative examples remain unclear: For a training word, we do not know what a good noise distribution should be, while we do know what a good target word is (one of its sur- rounding words).</p><p>Our contributions: To address these two prob- lems, we propose a new graph-based method to calculate noise distribution for negative sampling. Based on a word co-occurrence network, our noise distribution is targeted to training words. Besides, through our empirical exploration of the noise dis- tribution, we get a better understanding of the meaning of 'negative' and of the characteristics of good noise distributions.</p><p>The rest of the paper is organized as follows: Section 2 defines the word co-occurrence network concepts and introduces our graph-based negative sampling approach. Section 3 shows the experi- mental settings and results, then discusses our un- derstanding of the good noise distributions. Fi- nally, Section 4 draws conclusions and mentions future work directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Graph-based Negative Sampling</head><p>We begin with the word co-occurrence network generation (Section 2.1). By comparing it with the word2vec models, we show the relation between the stochastic matrix of the word co-occurrence network and the distribution of the training word contexts in word2vec. We introduce three methods to generate noise distributions for negative sam- pling based on the word co-occurrence network:</p><p>• Directly using the training word context distribution extracted from the word co- occurrence network (Section 2.2)</p><p>• Calculating the difference between the orig- inal unigram distribution and the training word context distribution (Section 2.3)</p><p>• Performing t-step random walks on the word co-occurrence network (Section 2.4)</p><p>We finally insert our noise distribution into the word2vec negative sampling training (Sec. 2.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Word Co-occurrence Network and Stochastic Matrix</head><p>A word co-occurrence network is a graph of word interactions representing the co-occurrence of words in a corpus. An undirected edge can be created when two words co-occur within a sentence; these words are possibly non- adjacent, with a maximum distance defined by a parameter d max <ref type="bibr" target="#b3">(Ferrer-i-Cancho and Solé, 2001</ref>). Given two words w i u and w j v that co-occur within a sentence at positions i and j (i, j ∈ {1 . . . l}), we define the distance d(w i u , w j v ) = |j − i| and the co-occurrence of w u and w v at a distance δ as cooc (δ, w u , w v ) =</p><formula xml:id="formula_1">w i u , w j v d w i u , w j v = δ</formula><p>. We define the weight w(d max , w u , w v ) of an edge (w u , w v ) as the total number of co-occurrences of w u and w v with distances δ ≤ d max :</p><formula xml:id="formula_2">w (d max , w u , w v ) = dmax δ=1 cooc(δ, w u , w v ).</formula><p>An undirected weighted word co-occurrence network can also be represented as a symmetric adjacency matrix ( <ref type="bibr" target="#b11">Mihalcea and Radev, 2011</ref>), a square matrix A of dimension |W | × |W |. In our case, W is the set of words used to generate the word co-occurrence network, and the matrix ele- ments A uv and A vu have the same value as the weight of the edge w(d max , w u , w v ). Then each row of the adjacency matrix A can be normalized (i.e., so that each row sums to 1), turning it into a right stochastic matrix S.</p><p>Negative sampling, in the skip-gram model, uniformly draws at random for each training word one of its surrounding words as the (positive) tar- get word. This range is determined by the size of the training context c. In other words, a surround- ing word w s of the training word w t must satisfy the following condition: d(w i t , w j s ) ≤ c. For the same corpus, let us set d max equal to c in word2vec and generate a word co-occurrence network of the whole corpus. Then element S uv in the adjacency matrix extracted from the net- work represents the probability that word w v be selected as the target word for training word w u (P bigram (w u , w v ) in Eq. 2). Row S u thus shows the distribution of target words for training word w u after training the whole corpus. Note that no matter how many training iterations are done over the corpus, this distribution will not change.</p><formula xml:id="formula_3">P bigram (w u , w v ) = dmax δ=1 cooc(δ, w u , w v ) |vocab| i=1 dmax δ=1 cooc(δ, w u , w i ) = S uv (2)</formula><p>Networks and matrices are interchangeable. The adjacency matrix of the word co-occurrence network can also be seen as the matrix of word- word co-occurrence counts calculated in a statis- tical way as in GloVe ( <ref type="bibr" target="#b16">Pennington et al., 2014</ref>). But unlike Glove, where the matrix is used for fac- torization, we use word co-occurrence statistics to generate a network, then use network algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training-Word Context Distribution</head><p>As discussed in Section 2.1, the stochastic ma- trix S of the word co-occurrence network repre- sents the context distribution of the training words. Here, we use S directly as one of the three types of bases for noise distribution matrix calculation.</p><p>The idea behind this is to perform nonlinear logistic regression to differentiate the observed data from some artificially generated noise. This idea was introduced by <ref type="bibr" target="#b5">Gutmann and Hyvärinen (2010)</ref> with the name Noise Contrastive Estima- tion (NCE). Negative sampling (NEG) can be con- sidered as a simplified version of NCE that follows the same idea and uses the unigram distribution as the basis of the noise distribution. We attempt to improve this by replacing the unigram distribution with a bigram distribution (word co-occurrence, not necessarily contiguous) to make the noise dis- tribution targeted to the training word (see Eq. 2).</p><p>Compared to the word-frequency-based uni- gram distribution, the word co-occurrence based bigram distribution is sparser. With the unigram distribution, for any training word, all the other vocabulary words can be selected as noise words because of their non-zero frequency. In con- trast, with the bigram distribution, some vocabu- lary words may never co-occur with a given train- ing word, which makes them impossible to be se- lected for this training word. To check the influ- ence of this zero co-occurrence case, we also pro- vide a modified stochastic matrix S smoothed by replacing all zeros in matrix S with the minimum non-zero value of their corresponding rows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Difference Between the Unigram Distribution and the Training Words Contexts Distribution</head><p>Contrary to the hypothesis underlying the previous section, here we take into account the positive tar- get words distribution in the training word context distribution. Starting from the 'white noise' uni- gram distribution, for each training word w u , we subtract from it the corresponding context distri- bution of this training word. Elements in this new basis matrix S difference u,v of noise distribution are:</p><formula xml:id="formula_4">P difference (w u , w v ) = P n (w u ) − S uv (3)</formula><p>where w v is one of the negative examples of w u , P n is the unigram distribution defined in Eq. 1 and S is the stochastic matrix we used in Section 2.2. For zeros and negative values in this matrix, we reset them to the minimum non-zero value of the corresponding row P difference (w u ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Random Walks on the Word Co-occurrence Network</head><p>After generating the word co-occurrence network, we apply random walks ( <ref type="bibr" target="#b0">Aldous and Fill, 2002</ref>) to it to obtain yet another noise distribution matrix for negative sampling. Let us define random walks on the co- occurrence network: Starting from an initial ver- tex w u , at each step we can cross an edge attached to w u that leads to another vertex, say w v . For a weighted word co-occurrence network, we define the transition probability P (w u , w v ) from vertex w u to vertex w v as the ratio of the weight of the edge (w u , w v ) over the sum of weights on all ad- jacency edges of vertex w u . Using the adjacency matrix A and the right stochastic matrix S pre- sented in Section 2.1, P u,v can be calculated by:</p><formula xml:id="formula_5">P (w u , w v ) = A uv / |Au| i=1 A ui = S uv .</formula><p>As we want to learn transition probabilities for all training words, we apply random walks on all vertices by making each training word an initial vertex of one t-step random walk at the same time.</p><p>The whole set of transition probabilities can be represented as a transition matrix, which is ex- actly the right stochastic matrix S of the word co- occurrence network in our case. We found that the self-loops (edges that start and end at the same ver- tex: the main diagonal of an adjacency matrix or a stochastic matrix) in matrix S represent the oc- currence of a word in its own context, which may happen in repetitions. We hypothesize they con- stitute spurious events and therefore test the t-step random walk both on matrix S and its smoothed version R in which the self-loops are removed. To see the effect of the self-loops, we perform the t-step random walk on both matrices S and R . Based on that, the elements of the t-step random walk transition matrix can be calculated by:</p><formula xml:id="formula_6">P random-walk (w u , w v ) = S t uv or (R ) t uv<label>(4)</label></formula><p>Ferrer-i- <ref type="bibr" target="#b3">Cancho and Solé (2001)</ref> showed that a word co-occurrence network is highly connected. For such networks, random walks converge to a steady state in just a few steps. Steady state means that no matter which vertex one starts with, the distribution of the destination vertex probabilities remains the same. In other words, all S t columns will have the same value. So we set the maximum step number t max to 4. We will use these t-step random walk transition matrices as the basis for one of our noise distributions matrices for negative sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Noise Distribution Matrix</head><p>Starting from the basic noise distribution matrix, we use the power function to adjust the distribu- tion. Then we normalize all rows of this adjusted matrix to let each row sum to 1. Finally, we get:</p><formula xml:id="formula_7">P n (w u , w v ) = (B uv ) p / |Bu| i=1 (B ui ) p (5)</formula><p>where B is the basic noise distribution calculated according to Eq. 2, 3 or 4 and p is the power. When performing word2vec training with neg- ative sampling, for each training word, we use the corresponding row in our noise distribution matrix to replace the original unigram noise distribution for the selection of noise candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Set-up and Evaluation Methods</head><p>We use the skip-gram negative sampling model with window size 5, vocabulary size 10000, vec- tor dimension size 200, number of iterations 5 and negative examples 5 to compute baseline word embeddings. Our three types of graph-based skip- gram negative sampling models share the parame- ters of the baseline. In addition to these common parameters, they have their own parameters: the maximum distance d max for co-occurrence net- works generation, a Boolean replace zeros to con- trol whether or not to replace zeros with the mini- mum non-zero values, a Boolean no self loops to control whether or not to remove the self-loops, the number of random walk steps t (Eq. 4) and the power p (Eq. 5).</p><p>All four models are trained on an English Wikipedia dump from April 2017 of three sizes: about 19M tokens, about 94M tokens (both are for detailed analyses and non-common parameters grid search in each of the three graph-based mod- els) and around 2.19 billion tokens (for four mod- els comparison). During corpus preprocessing, we use CoreNLP ( ) for sentence segmentation and word tokenization, then convert tokens to lowercase, replace all expressions with numbers by 0 and replace rare tokens with UNK s.</p><p>We perform a grid search on the ∼19M to- kens corpus, with d max ∈ {2, . . . 10}, t ∈ {1, . . . 4}, p ∈ {−2, −1, 0.01, 0.25, 0.75, 1, 2} and T rue, F alse for the two Boolean parameters. We retain the best parameters obtained by this grid search and perform a tighter grid search around them on the ∼94M tokens corpus. Then based on the two grid search results, we select the final pa- rameters for the entire Wikipedia dump test. We evaluate the resulting word embeddings on word similarity tasks using <ref type="bibr">WordSim-353 (Finkelstein et al., 2001</ref>) and SimLex-999 ( <ref type="bibr" target="#b6">Hill et al., 2014</ref>) (correlation with humans), and on the word anal- ogy task of <ref type="bibr" target="#b12">Mikolov et al. (2013a)</ref>  <ref type="figure">(% correct)</ref>. Therefore, we use the correlation coefficients be- tween model similarity judgments and human sim- ilarity judgments for WordSim-353 and SimLex- 999 tasks and the accuracy of the model prediction with gold standard for the word analogy task (the metrics in <ref type="table" target="#tab_0">Table 1</ref>) as objective functions for these parameter tuning processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>The best grid search parameters are shown in Ta- ble 2, final evaluation results on the entire English Wikipedia in <ref type="table" target="#tab_0">Table 1</ref>. The results show that graph- based negative sampling boosts the word analogy task by about 5% and improves word similarity by about 1%.</p><p>As vocabulary size is set to 10000, not all data in evaluation datasets is used. We report here the sizes of the datasets and of the subsets that con- tained no unknown word, that we used for eval- uation: <ref type="bibr">WordSim-353: 353;</ref><ref type="bibr">261;</ref><ref type="bibr">SImLex-999: 999;</ref><ref type="bibr">679;</ref><ref type="bibr">Word Analogy: 19544;</ref><ref type="bibr">6032</ref>. We also computed the statistical significance of the differ- ences between our models and the baseline model. Both word similarity tasks use correlation coeffi- cients, so we computed Steiger's Z tests <ref type="bibr" target="#b18">(Steiger, 1980)</ref> between the correlation coefficients of each of our models (bigram distribution, difference dis- tribution and random walk distribution) versus the word2vec skip-gram negative sampling base- line. For WordSim-353, differences are signif- icant (2-tailed p &lt; 0.05) for difference distribu- tion and random walk distribution for both Pear- son and Spearman correlation coefficients; differ- ences are not significant for bigram distribution. For SimLex-999, no difference is significant (all 2-tailed p &gt; 0.05). The word analogy task uses accuracy, we tested statistical significance of the differences by approximate randomization <ref type="bibr" target="#b19">(Yeh, 2000</ref>). Based on 10000 shuffles, we confirmed that all differences between the accuracies of our <ref type="bibr">WordSim-353</ref> SimLex-999 Word Analogy Pearson Spearman Pearson Spearman Semantic Syntactic Total baseline <ref type="bibr">word2vec</ref> 66.12% 69.60% 37.36% 36.33% 73.00% 70.67% 71.37% bigram distr. <ref type="figure">(Eq. 2)</ref> 66.10% 69.77% 38.05% 37.18% 77.36% † 75.55% † 76.09% † difference distr. (Eq. 3) 67.71% † 71.51% † 37.65% 36.58% 77.14% † 75.98% † 76.33% † random walk <ref type="bibr">(Eq. 4)</ref> 66.94% † 70.70% † 37.73% 36.74% 77.75% † 74.86% † 75.73% †  The time complexity when using our modified negative sampling distribution is similar to that of the original skip-gram negative sampling except that the distribution from which negative exam- ples are sampled is different for each token. We pre-compute this distribution off-line for each to- ken so that the added complexity is proportional to the size of the vocabulary. Specifically, pre- computing the co-occurrences and graphs using corpus2graph ( <ref type="bibr" target="#b20">Zhang et al., 2018</ref>) takes about 2.5 hours on top of 8 hours for word2vec alone on the entire Wikipedia corpus using 50 logical cores on a server with 4 Intel Xeon E5-4620 processors : the extra cost is not excessive.</p><p>Let us take a closer look at each graph-based model. First, the word context distribution based model: we find that all else being equal, replac- ing zero values gives better performance. We be- lieve a reason may be that for a training word, all the other words should have a probability to be se- lected as negative examples-the job of noise dis- tributions is to assign these probabilities. We note that for SimLex-999, all combinations of param- eters in our grid search outperform the baseline. But unfortunately the differences are not signifi- cant. Initially it sounds contradictory that directly using the word context distribution S as the noise distribution: a higher probability denotes that the word w v is more likely to be the target word of w u (i.e., positive example). So we tried assigning different negative powers to adjust the distribution S (Section 2.5) so as to make lower co-occurrence frequencies lead to higher probability of being se- lected as negative examples. But all these perform poorly for all three tasks.</p><p>Second, the difference model: the word analogy task results show a strong dependency on power p: the lower the power p, the higher the performance.</p><p>Third, the random-walk model: we observe that all top 5 combinations of parameters in the grid search do random walks after removing self-loops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We presented in this paper three graph-based neg- ative sampling models for word2vec. Experiments show that word embeddings trained by using these models can bring an improvement to the word analogy task and to the word similarity task.</p><p>We found that pre-computing graph informa- tion extracted from word co-occurrence networks is useful to learn word representations. Possible extensions would be to test whether using this in- formation to select target words (positive exam- ples) could improve training quality, and whether using it to reorder training words could improve training efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Evaluation results on WordSim-353, SimLex-999 and the word analogy task for the plain 
word2vec model and our three graph-based noise distributions on the entire English Wikipedia dump. A 
dagger  † marks a statistically significant difference to the baseline word2vec. 

distribution d max p 
others 
bigram 
3 
0.25 replace zeros=T 
difference 
3 
0.01 
random walk 5 
0.25 t = 2, no self loops=T 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Best parameters 

models and the accuracy of word2vec skip-gram 
are statistically significant (p &lt; 0.0001). 
</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Reversible markov chains and random walks on graphs. Unfinished monograph, recompiled</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Aldous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Allen</forename><surname>Fill</surname></persName>
		</author>
		<ptr target="http://www.stat.berkeley.edu/˜aldous/RWG/book.html" />
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quick training of probabilistic neural nets by importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Welin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.04906</idno>
		<title level="m">Strategies for training large vocabulary neural language models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The small world of human language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Ferrer-I-Cancho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">V</forename><surname>Solé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of London B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="page" from="2261" to="2265" />
			<date type="published" when="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on World Wide Web</title>
		<meeting>the 10th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno>abs/1408.3456</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2007</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An experimental analysis of noise-contrastive estimation: the noise distribution matters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Labeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Graph-based Natural Language Processing and Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Radev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6426</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aistats</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">On word embeddings-part 2: Approximating the softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<ptr target="http://ruder.io/word-embeddings-softmax.Lastac-cessed11" />
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tests for comparing elements of a correlation matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">245</biblScope>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">More accurate tests for the statistical significance of result differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Computational Linguistics</title>
		<meeting>the 18th Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="947" to="953" />
		</imprint>
	</monogr>
	<note>COLING &apos;00. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient generation and processing of word co-occurrence networks using corpus2graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Zweigenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TextGraphs-12: the Workshop on Graph-based Methods for Natural Language Processing</title>
		<meeting>TextGraphs-12: the Workshop on Graph-based Methods for Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
