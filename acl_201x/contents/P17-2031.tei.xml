<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention Strategies for Multi-Source Sequence-to-Sequence Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovický</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
						</author>
						<title level="a" type="main">Attention Strategies for Multi-Source Sequence-to-Sequence Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="196" to="202"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2031</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Modeling attention in neural multi-source sequence-to-sequence learning remains a relatively unexplored area, despite its usefulness in tasks that incorporate multiple source languages or modalities. We propose two novel approaches to combine the outputs of attention mechanisms over each source sequence, flat and hierarchical. We compare the proposed methods with existing techniques and present results of systematic evaluation of those methods on the WMT16 Multimodal Translation and Automatic Post-editing tasks. We show that the proposed methods achieve competitive results on both tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequence-to-sequence (S2S) learning with atten- tion mechanism recently became the most suc- cessful paradigm with state-of-the-art results in machine translation (MT) ( <ref type="bibr" target="#b19">Sennrich et al., 2016a</ref>), image captioning ( <ref type="bibr" target="#b25">Xu et al., 2015;</ref><ref type="bibr" target="#b15">Lu et al., 2016</ref>), text summariza- tion ( <ref type="bibr" target="#b18">Rush et al., 2015</ref>) and other NLP tasks.</p><p>All of the above applications of S2S learning make use of a single encoder. Depending on the modality, it can be either a recurrent neural net- work (RNN) for textual input data, or a convolu- tional network for images.</p><p>In this work, we focus on a special case of S2S learning with multiple input sequences of possibly different modalities and a single output-generating recurrent decoder. We explore various strategies the decoder can employ to attend to the hidden states of the individual encoders.</p><p>The existing approaches to this problem do not explicitly model different importance of the inputs to the decoder ( <ref type="bibr" target="#b26">Zoph and Knight, 2016)</ref>. In multimodal MT (MMT), where an im- age and its caption are on the input, we might ex- pect the caption to be the primary source of in- formation, whereas the image itself would only play a role in output disambiguation. In automatic post-editing (APE), where a sentence in a source language and its automatically generated transla- tion are on the input, we might want to attend to the source text only in case the model decides that there is an error in the translation.</p><p>We propose two interpretable attention strate- gies that take into account the roles of the indi- vidual source sequences explicitly-flat and hier- archical attention combination. This paper is organized as follows: In Sec- tion 2, we review the attention mechanism in single-source S2S learning. Section 3 introduces new attention combination strategies. In Section 4, we evaluate the proposed models on the MMT and APE tasks. We summarize the related work in Sec- tion 5, and conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Attentive S2S Learning</head><p>The attention mechanism in S2S learning allows an RNN decoder to directly access information about the input each time before it emits a sym- bol. Inspired by content-based addressing in Neu- ral Turing Machines ( <ref type="bibr" target="#b9">Graves et al., 2014</ref>), the at- tention mechanism estimates a probability distri- bution over the encoder hidden states in each de- coding step. This distribution is used for comput- ing the context vector-the weighted average of the encoder hidden states-as an additional input to the decoder. The standard attention model as described by  defines the attention en- ergies e ij , attention distribution α ij , and the con-text vector c i in i-th decoder step as:</p><formula xml:id="formula_0">e ij = v a tanh(W a s i + U a h j ),<label>(1)</label></formula><formula xml:id="formula_1">α ij = exp(e ij ) Tx k=1 exp(e ik ) ,<label>(2)</label></formula><formula xml:id="formula_2">c i = Tx j=1 α ij h j .<label>(3)</label></formula><p>The trainable parameters W a and U a are projec- tion matrices that transform the decoder and en- coder states s i and h j into a common vector space and v a is a weight vector over the dimensions of this space. T x denotes the length of the input se- quence. For the sake of clarity, bias terms (applied every time a vector is linearly projected using a weight matrix) are omitted.</p><p>Recently, <ref type="bibr" target="#b15">Lu et al. (2016)</ref> introduced sentinel gate, an extension of the attentive RNN decoder with LSTM units <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997</ref>). We adapt the extension for gated recurrent units (GRU) ( ), which we use in our experiments:</p><formula xml:id="formula_3">ψ i = σ(W y y i + W s s i−1 )<label>(4)</label></formula><p>where W y and W s are trainable parameters, y i is the embedded decoder input, and s i−1 is the pre- vious decoder state. Analogically to Equation 1, we compute a scalar energy term for the sentinel:</p><formula xml:id="formula_4">e ψ i = v a tanh W a s i + U (ψ) a (ψ i s i )<label>(5)</label></formula><p>where</p><formula xml:id="formula_5">W a , U (ψ) a</formula><p>are the projection matrices, v a is the weight vector, and ψ i s i is the sentinel vector. Note that the sentinel energy term does not depend on any hidden state of any encoder. The sentinel vector is projected to the same vector space as the encoder state h j in Equation 1. The term e ψ i is added as an extra attention energy term to Equa- tion 2 and the sentinel vector ψ i s i is used as the corresponding vector in the summation in Equa- tion 3.</p><p>This technique should allow the decoder to choose whether to attend to the encoder or to fo- cus on its own state and act more like a language model. This can be beneficial if the encoder does not contain much relevant information for the cur- rent decoding step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Attention Combination</head><p>In S2S models with multiple encoders, the decoder needs to be able to combine the attention informa- tion collected from the encoders.</p><p>A widely adopted technique for combining mul- tiple attention models in a decoder is concatena- tion of the context vectors c <ref type="bibr" target="#b26">and Knight, 2016;</ref>. As men- tioned in Section 1, this setting forces the model to attend to each encoder independently and lets the attention combination to be resolved implicitly in the subsequent network layers.</p><formula xml:id="formula_6">(1) i , . . . , c (N ) i (Zoph</formula><p>In this section, we propose two alternative strategies of combining attentions from multiple encoders. We either let the decoder learn the α i distribution jointly over all encoder hidden states (flat attention combination) or factorize the dis- tribution over individual encoders (hierarchical combination).</p><p>Both of the alternatives allow us to explicitly compute distribution over the encoders and thus interpret how much attention is paid to each en- coder at every decoding step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Flat Attention Combination</head><p>Flat attention combination projects the hidden states of all encoders into a shared space and then computes an arbitrary distribution over the projec- tions. The difference between the concatenation of the context vectors and the flat attention combina- tion is that the α i coefficients are computed jointly for all encoders:</p><formula xml:id="formula_7">α (k) ij = exp(e (k) ij ) N n=1 T (n) x m=1 exp e (n) im<label>(6)</label></formula><p>where</p><formula xml:id="formula_8">T (n) x</formula><p>is the length of the input sequence of the n-th encoder and e (k) ij is the attention energy of the j-th state of the k-th encoder in the i-th decoding step. These attention energies are com- puted as in Equation 1. The parameters v a and W a are shared among the encoders, and U a is different for each encoder and serves as an encoder-specific projection of hidden states into a common vector space.</p><p>The states of the individual encoders occupy different vector spaces and can have a different di- mensionality, therefore the context vector cannot be computed as their weighted sum. We project them into a single space using linear projections:</p><formula xml:id="formula_9">c i = N k=1 T (k) x j=1 α (k) ij U (k) c h (k) j<label>(7)</label></formula><p>where</p><formula xml:id="formula_10">U (k) c</formula><p>are additional trainable parameters. The matrices U (k) c project the hidden states into a common vector space. This raises a question whether this space can be the same as the one that is projected into in the energy computation using matrices U (k) a in Equation 1, i.e., whether</p><formula xml:id="formula_11">U (k) c = U (k)</formula><p>a . In our experiments, we explore both options. We also try both adding and not adding the sentinel α</p><formula xml:id="formula_12">(ψ) i U (ψ) c (ψ i s i ) to the context vec- tor.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Attention Combination</head><p>The hierarchical attention combination model computes every context vector independently, similarly to the concatenation approach. Instead of concatenation, a second attention mechanism is constructed over the context vectors.</p><p>We divide the computation of the attention dis- tribution into two steps: First, we compute the context vector for each encoder independently us- ing Equation 3. Second, we project the context vectors (and optionally the sentinel) into a com- mon space (Equation 8), we compute another dis- tribution over the projected context vectors (Equa- tion 9) and their corresponding weighted average (Equation 10):</p><formula xml:id="formula_13">e (k) i = v b tanh(W b s i + U (k) b c (k) i ),<label>(8)</label></formula><formula xml:id="formula_14">β (k) i = exp(e (k) i ) N n=1 exp(e (n) i ) ,<label>(9)</label></formula><formula xml:id="formula_15">c i = N k=1 β (k) i U (k) c c (k) i<label>(10)</label></formula><p>where c and U (k) c are encoder-specific projection matrices, that can be set equal and shared, similarly to the case of flat attention combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the attention combination strategies presented in Section 3 on the tasks of multi- modal translation (Section 4.1) and automatic post-editing (Section 4.2). The models were implemented using the Neural Monkey sequence-to-sequence learning toolkit <ref type="bibr" target="#b10">(Helcl and Libovick´yLibovick´y, 2017)</ref>. <ref type="bibr">12</ref> In both setups, we process the textual input with bidirec- tional GRU network ( ) with 300 units in the hidden state in each direction and 300 units in embeddings. For the attention projection space, we use 500 hidden units. We optimize the network to minimize the output cross-entropy us- ing the Adam algorithm ( <ref type="bibr" target="#b13">Kingma and Ba, 2014</ref>) with learning rate 10 −4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multimodal Translation</head><p>The goal of multimodal translation ( ) is to generate target-language image cap- tions given both the image and its caption in the source language.</p><p>We train and evaluate the model on the Multi30k dataset ( ). It con- sists of 29,000 training instances (images together with English captions and their German trans- lations), 1,014 validation instances, and 1,000 test instances. The results are evaluated us- ing the BLEU ( <ref type="bibr" target="#b17">Papineni et al., 2002</ref>) and ME- TEOR ( <ref type="bibr" target="#b5">Denkowski and Lavie, 2011</ref>).</p><p>In our model, the visual input is processed with a pre-trained VGG 16 network <ref type="bibr" target="#b21">(Simonyan and Zisserman, 2014</ref>) without further fine-tuning. Atten-tion distribution over the visual input is computed from the last convolutional layer of the network. The decoder is an RNN with 500 conditional GRU units ) in the recurrent layer. We use byte-pair encoding <ref type="bibr" target="#b20">(Sennrich et al., 2016b</ref>) with a vocabulary of 20,000 subword units shared between the textual encoder and the decoder.</p><p>The results of our experiments in multimodal MT are shown in <ref type="table">Table 1</ref>. We achieved the best results using the hierarchical attention combina- tion without the sentinel mechanism, which also showed the fastest convergence. The flat com- bination strategy achieves similar results eventu- ally. Sharing the projections for energy and con- text vector computation does not improve over the concatenation baseline and slows the training al- most prohibitively. Multimodal models were not able to surpass the textual baseline <ref type="bibr">(BLEU 33.0)</ref>.</p><p>Using the conditional GRU units brought an improvement of about 1.5 BLEU points on aver- age, with the exception of the concatenation sce- nario where the performance dropped by almost 5 BLEU points. We hypothesize this is caused by the fact the model has to learn the implicit atten- tion combination on multiple places -once in the output projection and three times inside the con- ditional GRU unit (Firat and Cho, 2016, Equa- tions 10-12). We thus report the scores of the in- troduced attention combination techniques trained with conditional GRU units and compare them with the concatenation baseline trained with plain GRU units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Automatic MT Post-editing</head><p>Automatic post-editing is a task of improving an automatically generated translation given the source sentence where the translation system is treated as a black box.</p><p>We used the data from the WMT16 APE Task ( <ref type="bibr" target="#b1">Bojar et al., 2016;</ref>, which consists of 12,000 training, 2,000 validation, and 1,000 test sentence triplets from the IT domain. Each triplet contains an English source sentence, an automatically generated German translation of the source sentence, and a manually post-edited German sentence as a reference. In case of this dataset, the MT outputs are almost perfect in and only little effort was required to post-edit the sen- tences. The results are evaluated using the human- targeted error rate (HTER) <ref type="bibr" target="#b22">(Snover et al., 2006</ref>) and BLEU score ( <ref type="bibr" target="#b17">Papineni et al., 2002</ref>).  <ref type="table">MMT  APE  BLEU  METEOR  BLEU  HTER  concat.</ref> 31.4 ± .8 48.0 ± .7 62.3 ± .5 24.4 ± .4 flat × × 30.2 ± .8 46.5 ± .7 62.6 ± .5 24.2 ± .4 × 29.3 ± .8 45.4 ± .7 62.3 ± .5 24.3 ± .4 × 30.9 ± .8 47.1 ± .7 62.4 ± .6 24.4 ± . <ref type="bibr">4</ref> 29.4 ± .8 46.9 ± .7 62.5 ± .6 24.2 ± .4</p><p>hierarchical × × 32.1 ± .8 49.1 ± .7 62.3 ± .5 24.1 ± .4 × 28.1 ± .8 45.5 ± .7 62.6 ± .6 24.1 ± .4 × 26.1 ± .7 42.4 ± .7 62.4 ± .5 24.3 ± .4</p><p>22.0 ± .7 38.5 ± .6 62.5 ± .5 24.1 ± .4 <ref type="table">Table 1</ref>: Results of our experiments on the test sets of Multi30k dataset and the APE dataset. The col- umn 'share' denotes whether the projection matrix is shared for energies and context vector computa- tion, 'sent.' indicates whether the sentinel vector has been used or not.</p><p>Following Libovick´yLibovick´y et al. <ref type="formula" target="#formula_0">(2016)</ref>, we encode the target sentence as a sequence of edit operations transforming the MT output into the reference. By this technique, we prevent the model from para- phrasing the input sentences. The decoder is a GRU network with 300 hidden units. Unlike in the MMT setup (Section 4.1), we do not use the conditional GRU because it is prone to overfitting on the small dataset we work with.</p><p>The models were able to slightly, but signifi- cantly improve over the baseline -leaving the MT output as is (HTER 24.8). The differences be- tween the attention combination strategies are not significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Attempts to use S2S models for APE are relatively rare <ref type="bibr" target="#b1">(Bojar et al., 2016)</ref>. <ref type="bibr" target="#b16">Niehues et al. (2016)</ref> con- catenate both inputs into one long sequence, which forces the encoder to be able to work with both source and target language. Their attention is then similar to our flat combination strategy; however, it can only be used for sequential data.</p><p>The best system from the WMT'16 competi- tion (Junczys-Dowmunt and Grundkiewicz, 2016) trains two separate S2S models, one translating from MT output to post-edited targets and the second one from source sentences to post-edited targets. The decoders average their output dis- tributions similarly to decoder ensembling. The biggest source of improvement in this state-of-the- art posteditor came from additional training data generation, rather than from changes in the net- work architecture.</p><p>Source: a man sleeping in a green room on a couch . Reference: ein Mann schläft in einem grünen Raum auf einem Sofa . Output with attention:</p><p>e i n M a n n s c h l ä f t a u f e i n e m g r ü n e n S o f a i n e i n e m g r ü n e n R a u m .</p><formula xml:id="formula_16">(1) (2) (3) (1) source, (2) image, (3) sentinel</formula><p>Figure 2: Visualization of hierarchical attention in MMT. Each column in the diagram corresponds to the weights of the encoders and sentinel. Note that the despite the overall low importance of the image encoder, it gets activated for the content words. <ref type="bibr" target="#b2">Caglayan et al. (2016)</ref> used an architecture very similar to ours for multimodal translation. They made a strong assumption that the network can be trained in such a way that the hidden states of the encoder and the convolutional network occupy the same vector space and thus sum the context vec- tors from both modalities. In this way, their mul- timodal MT system (BLEU 27.82) remained far bellow the text-only setup (BLEU 32.50).</p><p>New state-of-the-art results on the Multi30k dataset were achieved very recently by <ref type="bibr" target="#b3">Calixto et al. (2017)</ref>. The best-performing architecture uses the last fully-connected layer of VGG-19 network <ref type="bibr" target="#b21">(Simonyan and Zisserman, 2014</ref>) as de- coder initialization and only attends to the text en- coder hidden states. With a stronger monomodal baseline (BLEU 33.7), their multimodal model achieved a BLEU score of 37.1. Similarly to <ref type="bibr" target="#b16">Niehues et al. (2016)</ref> in the APE task, even fur- ther improvement was achieved by synthetically extending the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We introduced two new strategies of combining attention in a multi-source sequence-to-sequence setup. Both methods are based on computing a joint distribution over hidden states of all en- coders.</p><p>We conducted experiments with the proposed strategies on multimodal translation and automatic post-editing tasks, and we showed that the flat and hierarchical attention combination can be applied to these tasks with maintaining competitive score to previously used techniques.</p><p>Unlike the simple context vector concatenation, the introduced combination strategies can be used with the conditional GRU units in the decoder. On top of that, the hierarchical combination strategy exhibits faster learning than than the other strate- gies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>is the context vector of the k-th en- coder, additional trainable parameters v b and W b are shared for all encoders, and U (k) b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Learning curves on validation data for context vector concatenation (blue), flat (green) and hierarchical (red) attention combination without sentinel and without sharing the projection matrices.</figDesc></figure>

			<note place="foot" n="1"> http://github.com/ufal/neuralmonkey 2 The trained models can be downloaded from http://ufallab.ms.mff.cuni.cz/ ˜ libovicky/acl2017_att_models/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Ondřej Dušek, Rudolf Rosa, Pavel Pecina, and Ondřej Bojar for a fruit-ful discussions and comments on the draft of the paper.</p><p>This research has been funded by the Czech Science Foundation grant no. P103/12/G084, the EU grant no. H2020-ICT-2014-1-645452 (QT21), and Charles University grant no. 52315/2014 and SVV project no. 260 453. This work has been using language resources developed and/or stored and/or distributed by the LINDAT-Clarin project of the Ministry of Education of the Czech Repub-lic (project LM2010013).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1409.0473</idno>
		<ptr target="http://arxiv.org/abs/1409.0473" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Findings of the 2016 conference on machine translation (WMT16)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Yepes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelie</forename><surname>Névéol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Zampieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shared Task Papers. Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="131" to="198" />
		</imprint>
	</monogr>
	<note>Proceedings of the First Conference on Machine Translation (WMT)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Does multimodality help human and machine for translation and image captioning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Aransa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W16-2358" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="627" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Incorporating global visual features into attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Campbell</surname></persName>
		</author>
		<idno>CoRR abs/1701.06521</idno>
		<ptr target="http://arxiv.org/abs/1701.06521" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W14-4012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation. Association for Computational Linguistics</title>
		<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation. Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W11-2107" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation<address><addrLine>Edinburgh, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="85" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi30k: Multilingual englishgerman image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno>CoRR abs/1605.00459</idno>
		<ptr target="http://arxiv.org/abs/1605.00459" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Conditional gated recurrent unit with attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<ptr target="https://github.com/nyu-dl/dl4mt-tutorial/blob/master/docs/cgru.pdf" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Published online, version adbaeea</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="866" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Neural turing machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno>CoRR abs/1410.5401</idno>
		<ptr target="http://arxiv.org/abs/1410.5401" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural monkey: An open-source tool for sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovick´ylibovick´y</surname></persName>
		</author>
		<idno type="doi">10.1515/pralin-2017-0001</idno>
		<ptr target="https://doi.org/10.1515/pralin-2017-0001" />
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="issue">107</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="doi">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Dowmunt</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="751" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ondřej Bojar, and Pavel Pecina. 2016. CUNI system for WMT16 automatic post-editing and multimodal translation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovick´ylibovick´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Tlust´ytlust´y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="646" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>CoRR abs/1612.01887</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pre-translation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Le</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<idno>CoRR abs/1610.05243</idno>
		<ptr target="http://arxiv.org/abs/1610.05243" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="doi">10.3115/1073083.1073135</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D/D15/D15-1044" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Edinburgh neural machine translation systems for WMT 16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="371" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>CoRR abs/1409.1556.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of association for machine translation in the Americas</title>
		<meeting>association for machine translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">200</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A shared task on multimodal machine translation and crosslingual image description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W16-2346" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation. Association for Computational Linguistics</title>
		<meeting>the First Conference on Machine Translation. Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="543" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">WMT16 APE shared task data. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<ptr target="http://hdl.handle.net/11372/LRT-1632" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Charles University in Prague</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15). JMLR Workshop and Conference Proceedings</title>
		<editor>David Blei and Francis Bach</editor>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15). JMLR Workshop and Conference Proceedings<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-source neural translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1004" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="30" to="34" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
