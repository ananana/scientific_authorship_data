<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Distributed Representation of Word Sense via WordNet Gloss Composition and Context Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Engineering Laboratory of Performance Robots at Digital Stage</orgName>
								<orgName type="institution">Harbin Institute of Technology Shenzhen Graduate School</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Engineering Laboratory of Performance Robots at Digital Stage</orgName>
								<orgName type="institution">Harbin Institute of Technology Shenzhen Graduate School</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering and Applied Science</orgName>
								<orgName type="institution">Aston University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Engineering Laboratory of Performance Robots at Digital Stage</orgName>
								<orgName type="institution">Harbin Institute of Technology Shenzhen Graduate School</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Distributed Representation of Word Sense via WordNet Gloss Composition and Context Clustering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="15" to="20"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In recent years, there has been an increasing interest in learning a distributed representation of word sense. Traditional context clustering based models usually require careful tuning of model parameters , and typically perform worse on infrequent word senses. This paper presents a novel approach which addresses these limitations by first initializing the word sense embeddings through learning sentence-level embeddings from WordNet glosses using a convolutional neural networks. The initialized word sense embeddings are used by a context clustering based model to generate the distributed representations of word senses. Our learned representations outperform the publicly available embeddings on 2 out of 4 metrics in the word similarity task, and 6 out of 13 sub tasks in the analogical reasoning task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapid development of deep neural net- works and parallel computing, distributed repre- sentation of knowledge attracts much research in- terest. Models for learning distributed representa- tions of knowledge have been proposed at differ- ent granularity level, including word sense level ( <ref type="bibr" target="#b7">Huang et al., 2012;</ref><ref type="bibr" target="#b17">Neelakantan et al., 2014;</ref><ref type="bibr" target="#b23">Tian et al., 2014;</ref><ref type="bibr" target="#b6">Guo et al., 2014</ref>), word level <ref type="bibr" target="#b18">(Rummelhart, 1986;</ref><ref type="bibr" target="#b0">Bengio et al., 2003;</ref><ref type="bibr" target="#b3">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b16">Mnih and Hinton, 2009;</ref><ref type="bibr" target="#b13">Mikolov et al., 2010;</ref><ref type="bibr" target="#b14">Mikolov et al., 2013)</ref>, phrase level ( <ref type="bibr" target="#b20">Socher et al., 2010;</ref><ref type="bibr" target="#b2">Cho et al., 2014</ref>), sentence level ( <ref type="bibr" target="#b13">Mikolov et al., 2010;</ref><ref type="bibr" target="#b22">Socher et al., 2013;</ref><ref type="bibr" target="#b9">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b11">Kim, 2014;</ref><ref type="bibr" target="#b12">Le and Mikolov, 2014)</ref>, discourse level ( <ref type="bibr" target="#b8">Ji and Eisenstein, 2014)</ref> and document level ( <ref type="bibr" target="#b12">Le and Mikolov, 2014</ref>).</p><p>In distributed representations of word senses, each word sense is usually represented by a dense and real-valued vector in a low-dimensional space which captures the contextual semantic informa- tion. Most existing approaches adopted a cluster- based paradigm, which produces different sense vectors for each polysemy or homonymy through clustering the context of a target word. However, this paradigm usually has two limitations: (1) The performance of these approaches is sensitive to the clustering algorithm which requires the setting of the sense number for each word. For exam- ple, <ref type="bibr" target="#b17">Neelakantan et al. (2014)</ref> proposed two clus- tering based model: the Multi-Sense Skip-Gram (MSSG) model and Non-Parametric Multi-Sense Skip-Gram (NP-MSSG) model. MSSG assumes each word has the same k-sense (e.g. k = 3), i.e., the same number of possible senses. How- ever, the number of senses in WordNet <ref type="bibr" target="#b15">(Miller, 1995)</ref> varies from 1 such as "ben" to 75 such as "break". As such, fixing the number of senses for all words would result in poor representations. NP-MSSG can learn the number of senses for each word directly from data. But it requires a tuning of a hyperparameter λ which controls the creation of cluster centroids during training. Different λ needs to be tuned for different datasets. (2) The initial value of sense representation is critical for most statistical clustering based approaches. How- ever, previous approaches usually adopted ran- dom initialization <ref type="bibr" target="#b17">(Neelakantan et al., 2014</ref>) or the mean average of candidate words in a gloss ( . As a result, they may not produce optimal clustering results for word senses.</p><p>Focusing on the aforementioned two problems, this paper proposes to learn distributed representa- tions of word senses through WordNet gloss com- position and context clustering. The basic idea is that a word sense is represented as a synonym set (synset) in WordNet. In this way, instead of as- signing a fixed sense number to each word as in the previous methods, different word will be assigned with different number of senses based on their corresponding entries in WordNet. Moreover, we notice that each synset has a textual definition (named as gloss). Naturally, we use a convolu- tional neural network (CNN) to learn distributed representations of these glosses (a.k.a. sense vec- tors) through sentence composition. Then, we modify MSSG for context clustering by initial- izing the sense vectors with the representations learned by our CNN-based sentence composition model. We expect that word sense vectors ini- tialized in this way would potentially lead to bet- ter representations of word senses generated from context clustering.</p><p>The obtained word sense representations are evaluated on two tasks. One is word similarity task, the other is analogical reasoning task pro- vided by WordRep ( ). The results show that our approach attains comparable perfor- mance on learning distributed representations of word senses. In specific, our learned represen- tation outperforms publicly available embeddings on the globalSim and localSim metrics in word similarity task, and 6 in 13 subtasks in the ana- logical reasoning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>Our proposed approach first train a Continuous Bag-Of-Words (CBOW) model <ref type="bibr" target="#b14">(Mikolov et al., 2013</ref>) from a large collection of raw text to gen- erate word embeddings. These word embeddings are then used by a Sentence Composition Model, which takes glosses in WordNet as positive train- ing data and randomly replaces part of the sen- tences as negative training data to construct the corresponding word sense vectors based on a one- dimensional CNN. For example, a WordNet gloss of word star is "an actor who plays a principal role". This is taken as a positive training example when learning the word sense vector for "star". We concatenate the word embedding generated by the CBOW model for each of the words in the gloss, take the concatenated word embeddings as an input to CNN, and get the output vector as one sense vector of word star.</p><p>The learned sense vectors are fed into a vari- ant of the previously proposed Multi-Sense Skip- Gram Model (MSSG) to generates distributed rep- resentations of word senses from a text corpus. We name our approach as CNN-VMSSG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training Sense Vectors From WordNet Glosses Using CNN</head><p>In this step, we learn the distributed representation of each gloss sentence as the representation of the corresponding synset. The training objective is to minimize the ranking loss below:</p><formula xml:id="formula_0">G s = ∑ s∈P max{0, 1 − f (s) + f (s ′ )} (1)</formula><p>Given a gloss sentence s as a positive training sam- ple, we randomly replace some words (controlled by a parameter λ) in s to construct a negative train- ing sample s ′ . We compute the scores f (s) and f (s ′ ) where f (·) is the scoring function represent- ing the whole CNN architecture without the soft- max layer. We expect f (s) and f (s ′ ) to be close to 1 and 0 respectively, and f (s) to be larger than f (s ′ ) by a margin of 1 for all the sentence in posi- tive training set P . The CNN architecture used in this component follows the architecture proposed by <ref type="bibr" target="#b11">(Kim, 2014)</ref> 1 which is a slight variant of the architecture pro- posed by <ref type="bibr" target="#b3">(Collobert and Weston, 2008)</ref>  <ref type="bibr">2</ref> . It takes a gloss matrix s as input where each column corre- sponds to the distributed representation v w i ∈ R d of a word w i in the sentence.</p><p>The idea behind the one-dimensional convolu- tion is to take the dot product of the vector w with each n-gram in the sentence to obtain an- other sequence c, where n is the width of filter in the convolutional layer. In order to make c to cover different words in the negative sample cor- responding a positive sample, in this work, we ran- domly replace half of the words in a positive train- ing sample to construct a negative training sample (λ = 0.5). For example, take the WordNet gloss "an actor who plays a principal role" as a positive sample, a negative training sample constructed by this method may be "x 1 actor who x 2 x 3 principal x 4 ", where x 1 to x 4 are randomly selected words in a vocabulary collected from a large corpus.</p><p>In the pooling layer, a max-overtime pooling operation <ref type="bibr" target="#b4">(Collobert et al., 2011</ref>), which forces the network to capture the most useful local features produced by the convolutional layers, is applied. The model uses multiple filters (with varying win- dow sizes) to obtain multiple features. These fea- tures form the penultimate layer and are passed to a fully connected softmax layer whose output is the probability distribution over labels. The train- ing error propagates back to fine-tune the parame- ters of the CNN and the input word vectors. The vector generated in the penultimate layer of the CNN architecture is regarded as the sense vector which captures the semantic content of the input gloss to a certain degree. <ref type="figure" target="#fig_0">1, 2</ref>, . . . , K) and k-context cluster with center µ k ∈ R d (k = 1, 2, . . . , K) are initialized randomly. The sense number K of each word is a fixed parameter in the training algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Context Clustering and VMSSG Model</head><note type="other">Neelakantan et al. (2014) proposed the MSSG model which extends the skip-gram model to learn multi-prototype word embeddings by clustering the word embeddings of context words around each word. In this model, for each word w, the corresponding word embedding v</note><formula xml:id="formula_1">w ∈ R d , k-sense vector v s k ∈ R d (k =</formula><p>We improve the MSSG model by using the learned CBOW word embedding to initialize v w and the sense vector trained by the sentence com- position model to initialize v s k . We also use the sense number of each word in WordNet K w to re- place K. We named this model as a variant of the MSSG (VMSSG) model. </p><formula xml:id="formula_2">ˆ k = arg max k {sim(µ w k , v c )} 8:</formula><p>Assign C to context clusterˆkclusterˆ clusterˆk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Update µ ˆ k .</p><p>10: of word w, µ w k is the centroid of cluster k for word w. The function NoisySamples(C) randomly re- places context words with noisy words from V .</p><formula xml:id="formula_3">C ′ = NoisySamples(C</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>In all experiments, we train word vectors and sense vectors on a snapshot of Wikipedia in April 2010 3 <ref type="bibr" target="#b19">(Shaoul, 2010)</ref>, previously used in ( <ref type="bibr" target="#b7">Huang et al., 2012;</ref><ref type="bibr" target="#b17">Neelakantan et al., 2014</ref>). WordNet 3.1 is used for training the sentence composition model. A publicly available word vectors trained by CBOW from Google News 4 are used as pre- trained word vectors for CNN.</p><p>For training CNN, we use: rectified linear units, filter windows of 3, 4, 5 with 100 feature maps each, AdaDelta decay parameter of 0.95, the dropout rate of 0.5. For training VMSSG, we use MSSG-KMeans as the clustering algorithm, and CBOW for learning sense vectors. We set the size of word vectors to 300, using boot vectors and sense vectors. For other parameter, we use default parameter settings for MSSG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word Similarity Task</head><p>We evaluate our embeddings on the Contextual Word Similarities (SCWS) dataset <ref type="bibr" target="#b7">(Huang et al., 2012)</ref>. It contains 2,003 pairs of words and their sentential contexts. Each pair is associated with 10 to 16 human judgments of similarity on a scale from 0 to 10. We use the same metrics in <ref type="bibr" target="#b17">(Neelakantan et al., 2014</ref>) to measure the simi- larity between two words given their respective context. The avgSim metric computes the aver- age similarity of all pairs of prototype vectors for each word, ignoring context. The avgSimC met- ric weights each similarity term in avgSim by the likelihood of the word context appearing in its re- spective cluster. The globalSim metric computes each word vector ignoring senses. The localSim metric chooses the most similar sense in context to estimate the similarity of a words pair.</p><p>We report the Spearman's correlation ρ × 100 between a model's similarity scores and the human judgments in <ref type="table">Table 1</ref>  <ref type="table">Table 2</ref>: Experimental results in the analogical reasoning task.</p><p>It is observed that our model achieves the best performance on the globalSim and localSim met- rics. It indicates that the use of pre-trained word vectors and initializing sense vectors with the em- beddings learned from WordNet glosses are in- deed helpful in improving the quality of both global word vectors and sense-level word vec- tors. Our approach performs worse on avgSim and avgSimC. One possible reason is that we set the number of context clusters for each word to be the same as the number of its corresponding senses in WordNet. However, not all senses appear in the our experimented corpus which could lead to frag- mented context clustering results. One possible way to alleviate this problem is to perform post- processing to merge clusters which have smaller inter-cluster differences or to remove sense clus- ters which are under-represented in our data. We will leave it as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analogical Reasoning Task</head><p>The analogical reasoning task introduced by <ref type="bibr" target="#b14">(Mikolov et al., 2013)</ref> consists of questions of the form "a is to b is as c is to ", where (a, b) and (c, ) are two word pairs. The goal is to find a word d * in vocabulary V whose representation vector is</p><formula xml:id="formula_4">the closest to v b − v a + v c .</formula><p>WordRep is a benchmark collection for the re- search on learning distributed word representa- tions, which expands the Mikolov et al.'s analog- ical reasoning questions. In our experiments, we use one evaluation set in WordRep, the WordNet collection which consists of 13 sub tasks.</p><p>We use the precision p × 100 as metric for each sub task. <ref type="table">Table 2</ref> shows the results on the 13 sub tasks. The Word Pair column is the num- ber of word pairs of each sub task. The results of C&amp;W were obtained using the 50-dimensional word embeddings that were made publicly avail- able by <ref type="bibr" target="#b24">Turian et al. (2010)</ref>. <ref type="bibr">6</ref> The CBOW results were previously reported in ( .</p><p>It can be observed that among 13 subtasks, our model outperforms the others by a good mar- gin in 6 subtasks, Attribute, Causes, Entails, IsA, MadeOf and RelatedTo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>Although our evaluation results on the word simi- larity task and the analogical reasoning task show that our proposed approach outperforms a number of existing word representation methods in some of the subtasks, it is worth noting that both tasks do not consider the full spectrum of senses. In spe- cific, the analogical reasoning task was originally designed for evaluating single-prototype word rep- resentations which ignore that a word could have multiple meanings. Compared to single-prototype word vectors, evaluating sense vectors requires a significantly larger search space since each word could be represented by multiple sense vectors de- pending on the context. One may also argue that the analogical reasoning task may not be the most appropriate one in evaluating multiple-prototype word vectors since the context information is not available. In the future, we plan to evaluate our learned multiple-prototype word vectors in more relevant NLP tasks such as word sense disam- biguation and question answering.</p><p>Our proposed approach initializes sense vec- tors using the learned sentence embeddings from WordNet glosses. In other low resourced lan- guages, it is still possible to intialize sense vectors based on, for example, the word meanings found in language-specific dictionaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>This paper presents a method of incorporating WordNet glosses composition and context cluster- ing based model for learning distributed represen- tations of word senses. By initializing sense vec- tors using the embeddings learned by a sentence composition from WordNet glosses, the context clustering method is able to generate better dis- tributed representations of word senses. The ob- tained word sense representations achieve state-of- the-art results on the globalSim and localSim met- rics in the word similarity task and in 6 sub tasks of the analogical reasoning task. It shows the ef- fectiveness of our proposed learning algorithm for generating word sense distributed representations.</p><p>Considering the coverage of word senses in our training data, in future work we plan to filter out those sense vectors which are under-represented in the training corpus. We will also further investi- gate the feasibility of applying the multi-prototype word embeddings in a wide range of NLP tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Algorithm of VMSSG model 1: Input: D, d,K 1 , ..., K w , ..., K |V | , M . 2: Initialize: ∀w ∈ V, k ∈ {1, . . . , K w }, initial- ize v w to a pre-trained word vector, v s w k to a pre-trained sense vector for word w with sense k, and µ w k to a vector of random real value ∈ (−1, 1) d . 3: for each w in D do 4: r ← random number ∈ [1, M ] 5: C ← {w i−r , ..., w i−1 , w i+1 , ..., w i+r } 6: v c ← 1 2×r ∑ w∈C v w 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>. 5</head><label>5</label><figDesc></figDesc><table>Model 

avgSim avgSimC globalSim localSim 
Huang et al. 50d 
62.8 
65.7 
58.6 
26.1 
Unified-WSR 200d 
66.2 
68.9 
64.2 
-
MSSG 300d 
67.2 
69.3 
65.3 
57.3 
NP-MSSG 300d 
67.3 
69.1 
65.5 
59.8 
CNN-VMSSG 300d 
65.7 
66.4 
66.3 
61.1 

Table 1: Experimental results in the SCWS task. 

Subtask 
Word Pairs 
C&amp;W 
CBOW 
MSSG 
NP-MSSG CNN-VMSSG 
Antonym 
973 
0.28 
4.57 
0.25 
0.10 
1.01 
Attribute 
184 
0.22 
1.18 
0.03 
0.15 
1.63 
Causes 
26 
0.00 
1.08 
0.31 
0.31 
1.23 
DerivedFrom 
6,119 
0.05 
0.63 
0.09 
0.05 
0.17 
Entails 
114 
0.05 
0.38 
0.49 
0.34 
1.29 
HasContext 
1,149 
0.12 
0.35 
1.73 
1.56 
1.41 
InstanceOf 
1,314 
0.08 
0.58 
2.52 
2.34 
2.46 
IsA 
10,615 
0.07 
0.67 
0.15 
0.08 
0.86 
MadeOf 
63 
0.03 
0.72 
0.80 
0.48 
1.28 
MemberOf 
406 
0.08 
1.06 
0.14 
0.86 
0.90 
PartOf 
1,029 
0.31 
1.27 
1.50 
0.73 
0.48 
RelatedTo 
102 
0.00 
0.05 
0.12 
0.11 
1.28 
SimilarTo 
3,489 
0.02 
0.29 
0.03 
0.01 
0.12 

</table></figure>

			<note place="foot" n="1"> https://github.com/yoonkim/CNN sentence 2 http://ronan.collobert.com/senna/</note>

			<note place="foot" n="3"> http://www.psych.ualberta.ca/ ˜ westburylab/downloads/ westburylab.wikicorp.download.html 4 https://drive.google.com/file/d/0B7XkCwpI5KDYNl NUTTlSS21pQmM/edit?usp=sharing 5 The localSim metric of Unified-WSR is not reported in (Chen et al., 2014).</note>

			<note place="foot" n="6"> http://metaoptimize.com/projects/wordreprs/</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rejean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning (ICML)</title>
		<meeting>the 25th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wordrep: A benchmark for research on learning word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2014 Workshop on Knowledge-Powered Deep Learning for Text Mining</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning sense-specific word embeddings by exploiting bilingual resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 25th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="497" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Representation learning for text-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting><address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>October</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning (ICML)</title>
		<meeting>the 31th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at the International Conference on Learning Representations</title>
		<meeting>Workshop at the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1059" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Rummelhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The westbury lab wikipedia corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Shaoul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Edmonton, AB</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Alberta</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations and syntactic parsing with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS-2010</title>
		<meeting>the NIPS-2010</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<title level="m">Deep Learning and Unsupervised Feature Learning Workshop</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A probabilistic model for learning multi-prototype word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 25th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics (ACL)</title>
		<meeting>the 48th annual meeting of the association for computational linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bilingually-constrained phrase embeddings for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="111" to="121" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
