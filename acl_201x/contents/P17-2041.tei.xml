<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discourse Annotation of Non-native Spontaneous Spoken Responses Using the Rhetorical Structure Theory Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">V</forename><surname>Bruno</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hillary</forename><forename type="middle">R</forename><surname>Molloy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keelan</forename><surname>Evanini</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Zechner</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Educational Testing Service</orgName>
								<address>
									<addrLine>1 90 New Montgomery St #1500</addrLine>
									<postCode>94105</postCode>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discourse Annotation of Non-native Spontaneous Spoken Responses Using the Rhetorical Structure Theory Framework</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="263" to="268"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2041</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The availability of the Rhetorical Structure Theory (RST) Discourse Treebank has spurred substantial research into discourse analysis of written texts; however , limited research has been conducted to date on RST annotation and parsing of spoken language, in particular, non-native spontaneous speech. Considering that the measurement of discourse coherence is typically a key metric in human scoring rubrics for assessments of spoken language, we initiated a research effort to obtain RST annotations of a large number of non-native spoken responses from a standardized assessment of academic English proficiency. The resulting inter-annotator κ agreements on the three different levels of Span, Nuclear-ity, and Relation are 0.848, 0.766, and 0.653, respectively. Furthermore, a set of features was explored to evaluate the discourse structure of non-native spontaneous speech based on these annotations; the highest performing feature showed a correlation of 0.612 with scores of discourse coherence provided by expert human raters.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The spread of English as the global language of education and commerce is continuing, and there is a strong interest in developing assessment sys- tems that can automatically score spontaneous speech from non-native speakers with the goals of reducing the burden on human raters, improv- ing reliability, and generating feedback that can be used by language learners. Discourse coher- ence, which refers to the conceptual relations be- tween different units within a response, is an im- portant aspect of communicative competence, as is reflected in human scoring rubrics for assess- ments of non-native English <ref type="bibr" target="#b4">(ETS, 2012)</ref>. How- ever, discourse-level features have rarely been in- vestigated in the context of automated speech scor- ing. This study aims to construct a discourse- level annotation of non-native spontaneous speech in the framework of Rhetorical Structure Theory (RST) ( <ref type="bibr" target="#b8">Mann and Thompson, 1988)</ref>, which can then be used in automated discourse analysis and coherence measurement for non-native spoken re- sponses, thereby improving the validity of the au- tomated scoring systems.</p><p>RST is a descriptive framework that has been widely used in the analysis of discourse organiza- tion of written texts <ref type="bibr" target="#b17">(Taboada and Mann, 2006b</ref>), and has been applied to various natural lan- guage processing tasks, including language gen- eration, text summarization, and machine trans- lation ( <ref type="bibr" target="#b16">Taboada and Mann, 2006a</ref>). In particu- lar, the availability of RST annotations on a se- lection of 385 Wall Street Journal articles from the Penn Treebank 1 ) has facilitated RST-based discourse analysis of writ- ten texts, since it provides a standard benchmark for comparing the performance of different tech- niques for document-level discourse parsing ( <ref type="bibr" target="#b7">Joty et al., 2013;</ref>.</p><p>Another important application of RST closely related to our research is the automated evaluation of discourse in student essays. For example, one study used features for each sentence in an essay to reflect the status of its parent node as well as its rhetorical relation based on automatically parsed RST trees with the goal of providing feedback to students about the discourse structure in the essay ( <ref type="bibr" target="#b0">Burstein et al., 2003)</ref>. Another study compared features derived from deep hierarchical discourse relations based on RST parsing and features de- rived from shallow discourse relations based on Penn Discourse Treebank (PDTB) ( <ref type="bibr" target="#b11">Prasad et al., 2008)</ref> parsing in the task of essay scoring and demonstrated the effectiveness of deep discourse structure in better differentiation of text coherence .</p><p>Related work has also been conducted to anno- tate discourse relations in spoken language, which is produced and processed differently from writ- ten texts <ref type="bibr" target="#b12">(Rehbein et al., 2016)</ref>, and often lacks explicit discourse connectives that are more fre- quent in written language. Instead of the rooted- tree structure that is employed in RST, the annota- tion scheme with shallow discourse structure and relations from the PDTB ( <ref type="bibr" target="#b11">Prasad et al., 2008</ref>) has been generally used for spoken language <ref type="bibr" target="#b3">(Demirahin and Zeyrek, 2014;</ref><ref type="bibr" target="#b15">Stoyanchev and Bangalore, 2015)</ref>. For example, Tonelli et al. adapted the PDTB annotation scheme to annotate discourse relations in spontaneous conversations in Italian ( <ref type="bibr" target="#b18">Tonelli et al., 2010</ref>) and Rehbein et al. compared two frameworks, PDTB and CCR (Cognitive ap- proach to Coherence Relations) ( <ref type="bibr" target="#b14">Sanders et al., 1992)</ref>, for the annotation of discourse relations in spoken language ( <ref type="bibr" target="#b12">Rehbein et al., 2016)</ref>.</p><p>In contrast to these previous studies, this study focuses on monologic spoken responses produced by non-native speakers within the context of a lan- guage proficiency assessment. A discourse an- notation scheme based on the RST framework was selected due to the fact that it can effectively demonstrate the deep hierarchical discourse struc- ture across an entire response, rather than focusing on the local coherence of adjacent units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>This study obtained manual RST annotations on a corpus of 600 spoken responses drawn from a large-scale, high-stakes standardized assessment of English for non-native speakers, the TOEFL R Internet-based Test (TOEFL R iBT), which as- sesses English communication skills for academic purposes <ref type="bibr" target="#b4">(ETS, 2012)</ref>. The speaking section of the TOEFL iBT assessment contains six tasks, each of which requires the test taker to provide an un- scripted spoken response 45 or 60 seconds in du- ration. The corpus used in this study includes 100 responses from each of six different test ques- tions that comprise two different speaking tasks: 1) Independent questions: providing an opinion based on personal experience (N = 200 responses) and 2) Integrated questions: summarizing or dis- cussing material provided in a reading and/or lis- tening passage (N = 400 responses). The spo- ken responses were all manually transcribed using standard punctuation and capitalization. The av- erage number of words per response is 104.4 (st. dev. = 34.4) and the average number of sentences is 5.5 (st. dev. = 2.1).</p><p>The spoken responses were all provided with holistic English proficiency scores on a scale of 1 to 4 by expert human raters in the context of opera- tional, high-stakes scoring for the spoken language assessment. The scoring rubrics address the fol- lowing three main aspects of speaking proficiency: delivery (pronunciation, fluency, prosody), lan- guage use (grammar and lexical choice), and topic development (content and coherence). In order to ensure a sufficient quantity of responses from each proficiency level, 25 responses were selected ran- domly from each of the 4 score points for each of the 6 test questions.</p><p>The current study builds on a previous study that investigated approaches for modeling dis- course coherence in non-native spontaneous speech (but which did not consider the hierarchical rhetorical structure of speech) ( <ref type="bibr" target="#b19">Wang et al., 2013)</ref>. In that study, each spoken response in the same corpus that was used for the current study was provided with global discourse coherence scores. Two expert annotators (not drawn from the pool of expert human raters who provided the holistic scores) provided each response with a score on a scale of 1 to 3 based on the orthographic tran- scriptions of the spoken response. The three score points were defined as follows: 3 = highly co- herent (contains no instances of confusing argu- ments or examples), 2 = somewhat coherent (con- tains some awkward points in which the speaker's line of argument is unclear), 1 = barely coherent (the entire response was confusing and hard to follow). In addition, the annotators were specif- ically required to ignore disfluencies and gram- matical errors as much as possible. The inter- annotator agreement for these coherence scores was κ = 0.68. These discourse coherence scores are reused in the current study (along with the holistic profiency scores presented above) to eval- uate the performance of features measuring dis- course coherence based on the RST annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Annotation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Guidelines</head><p>We used a modified version of the tagging ref- erence manual for the <ref type="bibr">RST Discourse Treebank (Carlson and Marcu, 2001</ref>) for this study. Ac- cording to these guidelines, annotators segment a transcribed spoken response into Elementary Discourse Unit (EDU) spans of text (correspond- ing to clauses or clause-like units), and indi- cate rhetorical relations between non-overlapping spans which typically consist of a nucleus (the most essential information in the rhetorical re- lation) and a satellite (supporting or background information). In contrast to well-formed writ- ten text, non-native spontaneous speech frequently contains ungrammatical sentences, disfluencies, fillers, hesitations, false starts, and unfinished ut- terances. In some cases, these spoken responses do not constitute coherent, well-formed discourse. On the other hand, spoken responses are relatively shorter and comprise simpler discourse structures with fewer relations, which simplifies the RST an- notation task in comparison to written text. In order to account for these differences, we cre- ated an addendum to the RST Discourse Treebank manual introducing the following additional rela- tions: Disfluency relations (in which the disflu- ent span is the satellite and the corresponding flu- ent span is the nucleus), Awkward relations (cor- responding to portions of the response where the speaker's discourse structure is infelicitous; awk- ward relations are based on pre-existing relations, such as awkward-Reason, if the intended relation is clear but is expressed incoherently or are labeled as awkward-Other if there is no clear relation between the awkward EDU and the surrounding discourse), Unfinished Utterance relations (repre- senting EDUs at the end of a response that are in- complete because the test taker ran out of time in which the incomplete span is the satellite and the root node of the discourse tree is the nucleus), and Discourse Particle relations (such as you know and right, which are satellites of adjacent spans).</p><p>The discourse annotation tool used in the RST Discourse Treebank 2 was also adopted for this study. Using this tool, annotators incrementally build hierarchical discourse trees in which the leaves are the EDUs and the internal nodes cor- respond to contiguous spans of text. When the an- 2 Downloaded from http://www.isi.edu/ licensed-sw/RSTTool/index.html notators assign the rhetorical relation for a node of the tree, they provide the relation's label (drawn from the pre-defined set of relations in the anno- tation guidelines) and also indicate whether the spans that comprise the relation are nuclei or satel- lites. <ref type="figure">Figure 1</ref> shows an example of an annotated RST tree for a response with a proficiency score of 1. This response includes three disfluencies (EDUs 3, 6, and 9), which are satellites of the corresponding repair nuclei. In addition, the re- sponse also includes an awkward Comment-Topic relation between EDU 2 and the node combin- ing EDUs 3-11, indicated by awkward-Comment- Topic-2; in this multinuclear relation, the annota- tor judged that the second branch of the relation was awkward, which is indicated by the 2 that was appended to the relation label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pilot Annotation</head><p>The manual annotations were provided by two ex- perts with prior experience in various types of data annotation on both text and speech. First, a pi- lot annotation was conducted to train and cali- brate the annotators based on 48 training samples drawn from the TOEFL R Practice Online (TPO) product 3 , which offers practice tests simulating the TOEFL iBT testing experience with authentic test questions. The training samples were selected from a TPO test form with 6 test questions and were balanced according to test question and pro- ficiency score, i.e., 2 responses from each score level for each question.</p><p>Human annotators were trained in a two-step process: 1) after a comprehensive study of the an- notation guidelines described in Section 3.1, the two annotators were initially trained with 16 TPO responses (8 responses from an Independent ques- tion and 8 responses from an Integrated question) by first performing independent annotation and then resolving all disagreements through a discus- sion of the guidelines; 2) another round of training was conducted on an additional set of 32 TPO re- sponses (8 responses from an Independent ques- tion and 24 responses from an Integrated ques- tion). Each annotator first annotated this set of 32 responses independently; the two annotators sub- sequently conducted a thorough joint review and discussion of each other's annotations in order to resolve all disagreements on this set.</p><p>In order to measure the human agreement on <ref type="figure">Figure 1</ref>: Example of an annotated RST tree on a response with a proficiency score of 1.</p><p>the EDU segmentation task, we first converted the segmentation sequences into 0/1 sequences: for each word in a response, 1 is assigned if a seg- ment boundary exists after the word; otherwise, 0 is assigned. The inter-annotator agreement rate on the EDU segmentations of the 32 pilot samples (from stage 2) was κ = 0.876. On the hierarchi- cal tree building task, inter-annotator agreement was evaluated on the levels of Span (assignment of discourse segment), Nuclearity (assignment of nucleus vs. satellite), and Relation (assignment of rhetorical relation) using κ, as described in ( <ref type="bibr" target="#b10">Marcu et al., 1999</ref>); on the 32 samples, the κ values are 0.861, 0.769, and 0.631 for the three levels, re- spectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Formal Annotation</head><p>For the formal annotation on the full set of 600 TOEFL spoken responses, 120 responses from 6 test questions (5 responses from each score level from each question) were selected for double an- notation, and the remaining 480 responses re- ceived a single annotation. The 120 double-annotated responses were split into two batches of equal size and the two annota- tors each performed EDU segmentation indepen- dently on one of the batches. Subsequently, the annotators reviewed each other's EDU segmen- tations and adjudicated all disagreements to ob- tain gold-standard EDU segmentation for the 120 responses in the double-annotation set. The av- erage number of EDUs per response in the two batches in this set of 120 responses were 15.1 and 14.1. The annotators subsequently performed the remaining steps of RST annotation (assigning the relations, nuclearity, and hierarchical structure) in- dependently on all 120 responses using the adjudi- cated EDU segmentations. <ref type="table" target="#tab_0">Table 1</ref> shows that the κ agreements on the three levels of Span, Nucle-  arity, and Relation are 0.848, 0.766, and 0.653, re- spectively. Besides the κ evaluation, the standard ways of F1-Measure on three levels of Span, Nu- clearity, and Relation <ref type="bibr" target="#b9">(Marcu, 2000)</ref>, commonly used to evaluate the performance of RST parsers, are also reported in <ref type="table" target="#tab_0">Table 1</ref>. The F1-measures were calculated according to each pair of trees from two annotators on the same sample and then averaged across all samples, i.e., a macroaveraged F1-measure.</p><p>The human agreement results also indicate that two annotators tend to agree better on responses from speakers with higher speaking proficiency levels. This is demonstrated by positive correla- tions between the F1 agreement scores and the hu- man proficiency ratings: 0.197 for Span annota- tions, 0.210 for Nuclearity, and 0.188 for Relation.</p><p>In addition, we also examined the distribution of the manually identified awkward relations. As shown in <ref type="table" target="#tab_1">Table 2</ref>, awkward points occur with higher frequency in responses with lower profi- ciency scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discourse Features</head><p>The ultimate aim of this line of research is to use an RST-annotated corpus to investigate fea- tures for automatically assessing discourse struc- ture in spontaneous non-native speech. Using the annotated discourse trees, we extracted sev- eral different features based on the distribution of relations and the structure of the trees, includ- ing the number of EDUs (n edu), the number of relations (n rel), the number of awkward re- lations (n awk rel), the number of rhetorical re- lations, i.e., relations that were neither classified as awkward nor as a disfluency (n rhe rel), the number of different types of rhetorical relations (n rhe relTypes), the percentage of rhetorical rela- tions (perc rhe rel) out of all relations, the depth of the RST trees (tree depth), and the ratio between n edu and tree depth (ratio nedu depth). <ref type="table">Table 3</ref> lists the Pearson correlation coefficients of these features with both the holistic proficiency scores and the discourse coherence scores and demon- strates the effectiveness of these features. The n rhe rel feature achieves the highest correlation of 0.691 with the holistic proficiency scores, and the normalized feature perc rhe rel achieves the highest correlation of 0.612 with the discourse co- herence scores. It is interesting to note that RST- based discourse features generally have higher correlations with the holistic speaking proficiency scores than with the more specific discourse co- herence scores. This result is somewhat unex- pected, since the holistic proficiency scores are based only partially on discourse coherence and also cover other aspects of speaking proficiency, such as pronunciation, fluency, grammar, and vo- cabulary. One potential explanation for the higher correlations could be the difference in score range (1-4 for the holistic proficiency scores and 1-3 for the discourse scores). In addition, as described in Section 2, the data set used in this study was cre- ated using a stratified random sample with an even distribution of holistic scores (which may increase the features' correlations with holistic scores), but this constraint does not apply to the discourse co- herence scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this study, we obtained discourse coherence an- notations based on Rhetorical Structure Theory for a corpus of 600 non-native spontaneous spo- ken responses drawn from a standardized assess- <ref type="table">Table 3</ref>: Pearson correlation coefficients (r) of dis- course features with both the holistic proficiency scores as well as the discourse coherence scores. For the 120 double-annotated responses, the aver- aged feature values were used.</p><p>Features Proficiency Coherence n edu 0.58 0.397 n rel 0.584 0.396 n awk rel -0.396 -0.509 n rhe rel 0.691 0.541 n rhe relTypes 0.64 0.557 perc rhe rel 0.589 0.612 tree depth 0.365 0.25 ratio nedu depth 0.529 0.367 ment of non-native English. The RST annotation results show that the annotators achieved similar inter-annotator agreement rates as have been re- ported in previous studies that investigated well- formed written text ( <ref type="bibr" target="#b10">Marcu et al., 1999</ref>). In ad- dition, we demonstrate the potential of using fea- tures derived from these RST annotations for as- sessing non-native spoken English through moder- ately high correlations with both holistic speaking proficiency scores and discourse coherence scores; the highest performing feature when evaluated on the discourse coherence scores provided by expert raters was the percentage of rhetorical relations in the entire spoken response (perc rhe rel), with a correlation of 0.612.</p><p>In the future, we will continue this work by ad- dressing the following main research questions: a) how can we develop additional effective features from the discourse trees; b) how well can an auto- matic discourse parser trained on the obtained an- notations perform; c) how well will the proposed features perform when extracted using an auto- matic RST parser; d) how well will the features perform when using an automated speech recog- nizer (rather than human transcribers) to obtain the textual transcriptions of a spoken response.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Human agreement on RST annotations in 
terms of κ and F1-Measure. 
Span Nuclearity Relation 
κ 
0.848 
0.766 
0.653 
F1-Measure 0.872 
0.724 
0.522 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The average number of awkward relations 
appearing in responses from each of the four pro-
ficiency score levels. 
1 
2 
3 
4 
Annotator 1 3.2 1.1 1.1 0.3 
Annotator 2 2.1 1.2 0.7 0.3 

</table></figure>

			<note place="foot" n="1"> https://catalog.ldc.upenn.edu/ LDC2002T07</note>

			<note place="foot" n="3"> https://toeflpractice.ets.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Finding the write stuff: Automatic identification of discourse structure in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="39" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Discourse tagging reference manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<idno>ISI-TR- 545</idno>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Building a discourse-tagged corpus in the framework of rhetorical structure theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd SIGDIAL Workshop on Discourse and Dialogue</title>
		<meeting><address><addrLine>Aalborg, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Annotating discourse connectives in spoken Turkish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><surname>Demirahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Zeyrek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 8th Liguistic Annotation Workshop</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="105" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The official guide to the TOEFL R test. Fourth Edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ets</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A lineartime bottom-up discourse parser with constraints and post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Vanessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="511" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The impact of deep hierarchical discourse structures in the evaluation of text coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Wei Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="940" to="949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Combining intra-and multisentential rhetorical parsing for document-level discourse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="486" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rhetorical structure theory: Toward a functional theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text-Interdisciplinary Journal for the Study of Discourse (Text)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The Theory and Practice of Discourse Parsing and Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Experiments in constructing a corpus of discourse trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estibaliz</forename><surname>Amorrortu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magdalena</forename><surname>Romera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop on Standards and Tools for Discourse Tagging</title>
		<meeting><address><addrLine>College Park, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="48" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Penn Discourse TreeBank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 6th International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting><address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2961" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Annotating discourse relations in spoken language: A comparison of the PDTB and CCR frameworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Rehbein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Merel</forename><surname>Scholman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Language Resources and Evaluation (LREC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slovenia</forename><surname>Portorož</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1039" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toward a taxonomy of coherence relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ted</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilbert</forename><forename type="middle">P M</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">G M</forename><surname>Spooren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noordman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discourse Processes</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Discourse in customer care dialogues. Poster presented at the Workshop of Identification and Annotation of Discourse Relations in Spoken Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Stoyanchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Saarbrücken, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Applications of Rhetorical Structure Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maite</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">C</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discourse Studies</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="567" to="588" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rhetorical Structure Theory: Looking back and moving ahead</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maite</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">C</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discourse Studies</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="423" to="459" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Annotation of discourse relations for conversational spoken dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Tonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Riccardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)</title>
		<meeting><address><addrLine>Valetta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2084" to="2090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Coherence modeling for the automated assessment of spontaneous spoken responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keelan</forename><surname>Evanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Zechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Atlanta</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Atlanta<address><addrLine>Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="814" to="819" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
