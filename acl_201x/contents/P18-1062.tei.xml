<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokan</forename><surname>Shang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensi</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tixier</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polykarpos</forename><surname>Meladianos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pierre</forename><surname>Lorré</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ecole</forename><surname>Polytechnique</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Linagora</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aueb</forename></persName>
						</author>
						<title level="a" type="main">Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="664" to="674"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>664</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a novel graph-based framework for abstractive meeting speech sum-marization that is fully unsupervised and does not rely on any annotations. Our work combines the strengths of multiple recent approaches while addressing their weaknesses. Moreover, we leverage recent advances in word embeddings and graph degeneracy applied to NLP to take exterior semantic knowledge into account, and to design custom diversity and informative-ness measures. Experiments on the AMI and ICSI corpus show that our system improves on the state-of-the-art. Code and data are publicly available 1 , and our system can be interactively tested 2 .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>People spend a lot of their time in meetings. The ubiquity of web-based meeting tools and the rapid improvement and adoption of Automatic Speech Recognition (ASR) is creating pressing needs for effective meeting speech summarization mecha- nisms.</p><p>Spontaneous multi-party meeting speech tran- scriptions widely differ from traditional docu- ments. Instead of grammatical, well-segmented sentences, the input is made of often ill-formed and ungrammatical text fragments called utter- ances. On top of that, ASR transcription and seg- mentation errors inject additional noise into the in- put.</p><p>In this paper, we combine the strengths of 6 approaches that had previously been applied * Work done as part of 3 rd year project, with equal con- tribution. to 3 different tasks (keyword extraction, multi- sentence compression, and summarization) into a unified, fully unsupervised end-to-end meeting speech summarization framework that can gener- ate readable summaries despite the noise inherent to ASR transcriptions. We also introduce some novel components. Our method reaches state-of- the-art performance and can be applied to lan- guages other than English in an almost out-of-the- box fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Framework Overview</head><p>As illustrated in <ref type="figure">Figure 1</ref>, our system is made of 4 modules, briefly described in what follows.</p><p>Note that our approach is fully unsupervised and does not rely on any annotations. Our in- put simply consists in a list of utterances without any metadata. All we need in addition to that is a part-of-speech tagger, a language model, a set of pre-trained word vectors, a list of stopwords and fillerwords, and optionally, access to a lexical database such as WordNet. Our system can work out-of-the-box in most languages for which such resources are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work and Contributions</head><p>As detailed below, our framework combines the strengths of 6 recent works. It also includes novel components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-Sentence Compression Graph</head><p>(MSCG) <ref type="bibr" target="#b3">(Filippova, 2010)</ref> Description: a fully unsupervised, simple ap- proach for generating a short, self-sufficient sen- tence from a cluster of related, overlapping sen- tences. As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, a word graph is con- structed with special edge weights, the K-shortest weighted paths are then found and re-ranked with a scoring function, and the best path is used as the compression. The assumption is that redun- dancy alone is enough to ensure informativeness and grammaticality. Limitations: despite making great strides and showing promising results, Filippova (2010) re- ported that 48% and 36% of the generated sen- tences were missing important information and were not perfectly grammatical. Contributions: to respectively improve informa- tiveness and grammaticality, we combine ideas found in <ref type="bibr" target="#b2">Boudin and Morin (2013)</ref> and <ref type="bibr" target="#b15">Mehdad et al. (2013)</ref>, as described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">More informative MSCG (Boudin and Morin, 2013)</head><p>Description: same task and approach as in Filip- pova <ref type="bibr">(2010)</ref>, except that a word co-occurrence net- work is built from the cluster of sentences, and that the PageRank scores of the nodes are computed in the manner of <ref type="bibr" target="#b17">Mihalcea and Tarau (2004)</ref>. The scores are then injected into the path re-ranking function to favor informative paths. Limitations: PageRank is not state-of-the-art in capturing the importance of words in a document. Grammaticality is not considered. Contributions: we take grammaticality into ac- count as explained in subsection 3.4. We also follow recent evidence <ref type="bibr" target="#b24">(Tixier et al., 2016a</ref>) that spreading influence, as captured by graph degeneracy-based measures, is better correlated with "keywordedness" than PageRank scores, as explained in the next subsection. Figure 2: Word co-occurrence graph example, for the input text shown in <ref type="figure" target="#fig_3">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph</head><p>Important words are influential nodes. In social networks, it was shown that influential spreaders, that is, those individuals that can reach the largest part of the network in a given number of steps, are better identified via their core numbers rather than via their PageRank scores or degrees <ref type="bibr" target="#b7">(Kitsak et al., 2010)</ref>. See <ref type="figure" target="#fig_1">Figure 3</ref> for the intuition. Sim- ilarly, in NLP, <ref type="bibr" target="#b24">Tixier et al. (2016a)</ref> have shown that keywords are better identified via their core numbers rather than via their TextRank scores, that is, keywords are influencers within their word co- occurrence network. Graph degeneracy <ref type="bibr" target="#b23">(Seidman, 1983)</ref>. Let G(V, E) be an undirected, weighted graph with n = |V | nodes and m = |E| edges. A k-core of G is a maximal subgraph of G in which ev- ery vertex v has at least weighted degree k. As shown in <ref type="figure" target="#fig_1">Figures 3 and 4</ref>, the k-core decomposi- tion of G forms a hierarchy of nested subgraphs whose cohesiveness and size respectively increase and decrease with k. The higher-level cores can be viewed as a filtered version of the graph that excludes noise. This property is highly valuable when dealing with graphs constructed from noisy text, like utterances. The core number of a node is the highest order of a core that contains this node. However, the blue node is a much more influential spreader as it is strategically placed in the core of the network, as cap- tured by its higher core number.</p><p>The CoreRank number of a node <ref type="bibr" target="#b24">(Tixier et al., 2016a;</ref><ref type="bibr" target="#b0">Bae and Kim, 2014</ref>) is defined as the sum of the core numbers of its neighbors. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, CoreRank more finely captures the structural position of each node in the graph than raw core numbers. Also, stabilizing scores across node neighborhoods enhances the inherent noise robustness property of graph degeneracy, which is desirable when working with noisy speech-to-text output. Time complexity. Building a graph-of-words is O(nW ), and computing the weighted k-core decomposition of a graph requires O(m log(n)) ( <ref type="bibr" target="#b1">Batagelj and Zaveršnik, 2002</ref>). For small pieces of text, this two step process is so affordable that it can be used in real-time ( . Finally, computing CoreRank scores can be done with only a small overhead of O(n), provided that the graph is stored as a hash of adjacency lists. Getting the CoreRank numbers from scratch for a community of utterances is therefore very fast, especially since typically in this context, n ∼ 10 and m ∼ 100.</p><p>3.4 Fluency-aware, more abstractive MSCG ( <ref type="bibr" target="#b15">Mehdad et al., 2013)</ref> Description: a supervised end-to-end framework for abstractive meeting summarization. Commu- nity Detection is performed by (1) building an ut- terance graph with a logistic regression classifier, and (2) applying the CONGA algorithm. Then, before performing sentence compression with the MSCG, the authors also (3) build an entailment graph with a SVM classifier in order to eliminate redundant and less informative utterances. In ad- dition, the authors propose the use of WordNet <ref type="bibr" target="#b18">(Miller, 1995)</ref> during the MSCG building phase to capture lexical knowledge between words and thus generate more abstractive compressions, and of a language model when re-ranking the shortest paths, to favor fluent compressions. Limitations: this effort was a significant advance, as it was the first application of the MSCG to the meeting summarization task, to the best of our knowledge. However, steps (1) and (3) above are complex, based on handcrafted features, and respectively require annotated training data in the form of links between human-written abstractive sentences and original utterances and multiple external datasets (e.g., from the Recognizing Textual Entailment Challenge). Such annotations are costly to obtain and very seldom available in practice.</p><p>Contributions: while we retain the use of WordNet and of a language model, we show that, without deteriorating the quality of the results, steps <ref type="formula">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref> above (Community Detection) can be performed in a much more simple, completely un- supervised way, and that step (3) can be removed. That is, the MSCG is powerful enough to remove redundancy and ensure informativeness, should proper edge weights and path re-ranking function be used.</p><p>In addition to the aforementioned contributions, we also introduce the following novel components into our abstractive summarization pipeline:</p><p>• we inject global exterior knowledge into the edge weights of the MSCG, by using the Word At- traction Force of <ref type="bibr" target="#b27">Wang et al. (2014)</ref>, based on distance in the word embedding space,</p><p>• we add a diversity term to the path re-ranking function, that measures how many unique clusters in the embedding space are visited by each path,</p><p>• rather than using all the abstractive sentences as the final summary like in <ref type="bibr" target="#b15">Mehdad et al. (2013)</ref>, we maximize a custom submodular function to se- lect a subset of abstractive sentences that is near- optimal given a budget constraint (summary size). A brief background of submodularity in the con- text of summarization is provided next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Submodularity for summarization (Lin and Bilmes, 2010; Lin, 2012)</head><p>Selecting an optimal subset of abstractive sen- tences from a larger set can be framed as a bud- geted submodular maximization task:</p><formula xml:id="formula_0">argmax S⊆S f (S)| s∈S c s ≤ B (1)</formula><p>where S is a summary, c s is the cost (word count) of sentence s, B is the desired summary size in words (budget), and f is a summary quality scor- ing set function, which assigns a single numeric score to a summary S. This combinatorial optimization task is NP- hard. However, near-optimal performance can be guaranteed with a modified greedy algorithm ( <ref type="bibr" target="#b12">Lin and Bilmes, 2010</ref>) that iteratively selects the sen- tence s that maximizes the ratio of quality function gain to scaled cost f (S∪s)−f (S) /c r s (where S is the current summary and r ≥ 0 is a scaling factor).</p><p>In order for the performance guarantees to hold however, f has to be submodular and monotone non-decreasing. Our proposed f is described in subsection 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Framework</head><p>We detail next each of the four modules in our ar- chitecture (shown in <ref type="figure">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Text preprocessing</head><p>We adopt preprocessing steps tailored to the char- acteristics of ASR transcriptions. Consecutive re- peated unigrams and bigrams are reduced to single terms. Specific ASR tags, such as {vocalsound}, {pause}, and {gap} are filtered out. In addition, filler words, such as uh-huh, okay, well, and by the way are also discarded. Consecutive stopwords at the beginning and end of utterances are stripped.</p><p>In the end, utterances that contain less than 3 non- stopwords are pruned out. The surviving utter- ances are used for the next steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Utterance community detection</head><p>The goal here is to cluster utterances into commu- nities that should be summarized by a common ab- stractive sentence.</p><p>We initially experimented with techniques cap- italizing on word vectors, such as k-means and hi- erarchical clustering based on the Euclidean dis- tance or the Word Mover's Distance ( <ref type="bibr" target="#b8">Kusner et al., 2015</ref>). We also tried graph-based approaches, such as community detection in a complete graph where nodes are utterances and edges are weighted based on the aforementioned distances.</p><p>Best results were obtained, however, with a sim- ple approach in which utterances are projected into the vector space and assigned standard TF- IDF weights. Then, the dimensionality of the utterance-term matrix is reduced with Latent Se- mantic Analysis (LSA), and finally, the k-means algorithm is applied. Note that LSA is only used here, during the utterance community detection phase, to remove noise and stabilize clustering. We do not use a topic graph in our approach.</p><p>We think using word embeddings was not ef- fective, because in meeting speech, as opposed to traditional documents, participants tend to use the same term to refer to the same thing throughout the entire conversation, as noted by <ref type="bibr" target="#b20">Riedhammer et al. (2010)</ref>, and as verified in practice. This is probably why, for clustering utterances, capturing synonymy is counterproductive, as it artificially reduces the distance between every pair of utter- ances and blurs the picture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-Sentence Compression</head><p>The following steps are performed separately for each community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word importance scoring</head><p>From a processed version of the community (stem- ming and stopword removal), we construct an undirected, weighted word co-occurrence network as described in subsection 3.3. We use a sliding window of size W = 6 not overspanning utter- ances. Note that stemming is performed only here, and for the sole purpose of building the word co- occurrence network.</p><p>We then compute the CoreRank numbers of the nodes as described in subsection 3.3. : Compressed sentence (in bold red) generated by our multi-sentence com- pression graph (MSCG) for a 3-utterance community from meeting IS1009b of the AMI corpus. Using Filippova (2010)'s weighting and re-ranking scheme here would have selected another path: design different remotes for different people bit of it's from their tend to for ti. Note that the compressed sentence does not appear in the initial set of utterances, and is compact and grammatical, despite the redundancy, tran- scription and segmentation errors of the in- put. The abstractive and robust nature of the MSCG makes it particularly well-suited to the meeting domain. generally we can design a remote which is mean need for people bit of it's from their tend to for ti design different remotes for different people like for each to be the that will be big buttons doubt like with it because flies that if we design of remote having all the different features for different people are designing three different remotes for three different categories of people</p><p>We finally reweigh the CoreRank scores, in- dicative of word importance within a given com- munity, with a quantity akin to an Inverse Docu- ment Frequency, where communities serve as doc- uments and the full meeting as the collection. We thus obtain something equivalent to the TW-IDF weighting scheme of <ref type="bibr" target="#b22">Rousseau and Vazirgiannis (2013)</ref>, where the CoreRank scores are the term weights TW:</p><formula xml:id="formula_1">T W -IDF (t, d, D) = T W (t, d) × IDF (t, D)<label>(2)</label></formula><p>where t is a term belonging to community d, and D is the set of all utterance communities. We compute the IDF as IDF (t, D) = 1 + log |D| /Dt, where |D| is the number of communities and D t the number of communities containing t.</p><p>The intuition behind this reweighing scheme is that a term should be considered important within a given meeting if it has a high CoreRank score within its community and if the number of com- munities in which the term appears is relatively small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word graph building</head><p>The backbone of the graph is laid out as a directed sequence of nodes corresponding to the words in the first utterance, with special START and END nodes at the beginning and at the end (see <ref type="figure" target="#fig_3">Figure 5</ref>). Edge direction follows the natural flow of text. Words from the remaining utterances are then iteratively added to the graph (between the START and END nodes) based on the following rules: 1) if the word is a non-stopword, the word is mapped onto an existing node if it has the same lowercased form and the same part-of-speech tag <ref type="bibr">3</ref> . In case of multiple matches, we check the imme- diate context (the preceding and following words in the utterance and the neighboring nodes in the graph), and we pick the node with the largest con- text overlap or which has the greatest number of words already mapped to it (when no overlap). When there is no match, we use WordNet as de- scribed in Appendix A.</p><p>2) if the word is a stopword and there is a match, it is mapped only if there is an overlap of at least one non-stopword in the immediate context. Otherwise, a new node is created.</p><p>Finally, note that any two words appearing within the same utterance cannot be mapped to the same node. This ensures that every utterance is a loop- less path in the graph. Of course, there are many more paths in the graphs than original utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge Weight Assignment</head><p>Once the word graph is constructed, we assign weights to its edges as:</p><formula xml:id="formula_2">w (p i , p j ) = w (p i , p j ) w (p i , p j )<label>(3)</label></formula><p>where p i and p j are two neighbors in the MSCG.</p><p>As detailed next, those weights combine local co- occurrence statistics (numerator) with global exte- rior knowledge (denominator). Note that the lower <ref type="figure">Figure 6</ref>: t-SNE visualization <ref type="bibr" target="#b13">(Maaten and Hinton, 2008</ref>) of the Google News vectors of the words in the utterance community shown in <ref type="figure" target="#fig_3">Fig- ure 5</ref>. Arrows join the words in the best com- pression path shown in <ref type="figure" target="#fig_3">Figure 5</ref>. Movements in the embedding space, as measured by the num- ber of unique clusters covered by the path (here, 6/11), provide a sense of the diversity of the compressed sentence, as formalized in Equation 10. Local co-occurrence statistics. We use Filippova (2010)'s formula:</p><formula xml:id="formula_3">w (p i , p j ) = f (p i ) + f (p j ) P ∈G ,p i ,p j ∈P diff(P, p i , p j ) −1<label>(4)</label></formula><p>where f (p i ) is the number of words mapped to node p i in the MSCG G , and diff(P, p i , p j ) −1 is the inverse of the distance between p i and p j in a path P (in number of hops). This weighting func- tion favors edges between infrequent words that frequently appear close to each other in the text (the lower, the better).</p><p>Global exterior knowledge. We introduce a second term based on the Word At- traction Force score of <ref type="bibr" target="#b27">Wang et al. (2014)</ref>:</p><formula xml:id="formula_4">w (p i , p j ) = f (p i ) × f (p j ) d 2 p i ,p j<label>(5)</label></formula><p>where d p i ,p j is the Euclidean distance between the words mapped to p i and p j in a word embedding space <ref type="bibr">4</ref> . This component favor paths going through salient words that have high semantic similarity (the higher, the better). The goal is to ensure read- ability of the compression, by avoiding to generate a sentence jumping from one word to a completely unrelated one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path re-ranking</head><p>As in <ref type="bibr" target="#b2">Boudin and Morin (2013)</ref>, we use a short- est weighted path algorithm to find the K paths between the START and END symbols having the lowest cumulative edge weight:</p><formula xml:id="formula_5">W (P ) = |P |−1 i=1 w (p i , p i+1 )<label>(6)</label></formula><p>4 GoogleNews vectors https://code.google.com/archive/p/word2vec</p><p>Where |P | is the number of nodes in the path. Paths having less than z words or that do not con- tain a verb are filtered out (z is a tuning parame- ter). However, unlike in Boudin and Morin <ref type="formula" target="#formula_1">(2013)</ref>, we rerank the K best paths with the following novel weighting scheme (the lower, the better), and the path with the lowest score is used as the compression:</p><formula xml:id="formula_6">score(P ) = W (P ) |P | × F (P ) × C(P ) × D(P )<label>(7)</label></formula><p>The denominator takes into account the length of the path, and its fluency (F ), coverage (C), and diversity (D). F , C, and D are detailed in what follows.</p><p>Fluency. We estimate the grammaticality of a path with an n-gram language model. In our ex- periments, we used a trigram model <ref type="bibr">5</ref> :</p><formula xml:id="formula_7">F (P ) = |P | i=1 logP r(p i |p i−1 i−n+1 ) #n-gram (8)</formula><p>where |P | denote path length, and p i and #n-gram are respectively the words and number of n-grams in the path.</p><p>Coverage. We reward the paths that visit impor- tant nouns, verbs and adjectives:</p><formula xml:id="formula_8">C(P ) = p i ∈P TW-IDF(p i ) #p i (9)</formula><p>where #p i is the number of nouns, verbs and ad- jectives in the path. The TW-IDF scores are com- puted as explained in subsection 4.3.</p><p>Diversity. We cluster all words from the MSCG in the word embedding space by applying the k- means algorithm. We then measure the diversity of the vocabulary contained in a path as the number of unique clusters visited by the path, normalized by the length of the path:</p><formula xml:id="formula_9">D(P ) = k j=1 1 ∃p i ∈P |p i ∈cluster j |P |<label>(10)</label></formula><p>The graphical intuition for this measure is pro- vided in <ref type="figure">Figure 6</ref>. Note that we do not normalize D by the total number of clusters (only by path length) because k is fixed for all candidate paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Budgeted submodular maximization</head><p>We apply the previous steps separately for all ut- terance communities, which results in a set S of abstractive sentences (one for each community). This set of sentences can already be considered to be a summary of the meeting. However, it might exceed the maximum size allowed, and still con- tain some redundancy or off-topic sections unre- lated to the general theme of the meeting (e.g., chit-chat). Therefore, we design the following submodular and monotone non-decreasing objective function:</p><formula xml:id="formula_10">f (S) = s i ∈S n s i w s i + λ k j=1 1 ∃s i ∈S|s i ∈group j (11)</formula><p>where λ ≥ 0 is the trade-off parameter, n s i is the number of occurrences of word s i in S, and w s i is the CoreRank score of s i .</p><p>Then, as explained in subsection 3.5, we ob- tain a near-optimal subset of abstractive sentences by maximizing f with a greedy algorithm. Cor- eRank scores and clusters are found as previ- ously described, except that this time they are ob- tained from the full processed meeting transcrip- tion rather than from a single utterance commu- nity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We conducted experiments on the widely-used AMI ( <ref type="bibr" target="#b14">McCowan et al., 2005</ref>) and ICSI (Janin et al., 2003) benchmark datasets. We used the tra- ditional test sets of 20 and 6 meetings respectively for the AMI and ICSI corpora ( <ref type="bibr" target="#b21">Riedhammer et al., 2008)</ref>. Each meeting in the AMI test set is asso- ciated with a human abstractive summary of 290 words on average, whereas each meeting in the ICSI test set is associated with 3 human abstrac- tive summaries of respective average sizes 220, 220 and 670 words. For parameter tuning, we con- structed development sets of 47 and 25 meetings, respectively for AMI and ICSI, by randomly sam- pling from the training sets. The word error rate of the ASR transcriptions is respectively of 36% and 37% for AMI and ICSI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We compared our system against 7 baselines, which are listed below and more thoroughly de- tailed in Appendix B. Note that preprocessing was exactly the same for our system and all baselines.</p><p>• Random and Longest Greedy are basic base- lines recommended by ( <ref type="bibr" target="#b21">Riedhammer et al., 2008</ref>), • TextRank ( <ref type="bibr" target="#b17">Mihalcea and Tarau, 2004</ref>),</p><formula xml:id="formula_11">• ClusterRank (Garg et al., 2009), • CoreRank &amp; PageRank submodular (Tixier et al., 2017),</formula><p>• Oracle is the same as the random baseline, but uses the human extractive summaries as input.</p><p>In addition to the baselines above, we included in our comparison 3 variants of our system using different MSCGs: Our System (Baseline) uses the original MSCG of Filippova (2010), Our Sys- tem (KeyRank) uses that of <ref type="bibr" target="#b2">Boudin and Morin (2013)</ref>, and Our System (FluCovRank) that of <ref type="bibr" target="#b15">Mehdad et al. (2013)</ref>. Details about each approach were given in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Parameter tuning</head><p>For Our System and each of its variants, we con- ducted a grid search on the development sets of each corpus, for fixed summary sizes of 350 and 450 words (AMI and ICSI). We searched the fol- lowing parameters:</p><p>• n: number of utterance communities (see Sec- tion 4.2). We tested values of n ranging from 20 to 60, with steps of 5. This parameter controls how much abstractive should the summary be. If all ut- terances are assigned to their own singleton com- munity, the MSCG is of no utility, and our frame- work is extractive. It becomes more and more ab- stractive as the number of communities decreases.</p><p>• z: minimum path length (see Section 4.3). We searched values in the range <ref type="bibr">[6,</ref><ref type="bibr">16]</ref> with steps of 2. If a path is shorter than a certain minimum number of words, it often corresponds to an invalid sen- tence, and should thereby be filtered out.</p><p>• λ and r, the trade-off parameter and the scaling factor (see Section 4.4). We searched <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> and <ref type="bibr">[0,</ref><ref type="bibr">2]</ref> (respectively) with steps of 0.1. The parame- ter λ plays a regularization role favoring diversity.</p><p>The scaling factor makes sure the quality function gain and utterance cost are comparable.</p><p>The best parameter values for each corpus are summarized in <ref type="table">Table 1</ref>. λ is mostly non-zero, in- dicating that it is necessary to include a regular- ization term in the submodular function. In some cases though, r is equal to zero, which means that utterance costs are not involved in the greedy de- cision heuristic. These observations contradict the conclusion of Lin <ref type="formula" target="#formula_1">(2012)</ref>   <ref type="table">Table 1</ref>: Optimal parameter values n, z, (λ, r).</p><p>Apart from the tuning parameters, we set the number of LSA dimensions to 30 and 60 (resp. on AMI and ISCI). The small number of LSA di- mensions retained can be explained by the fact that the AMI and ICSI transcriptions feature 532 and 1126 unique words on average, which is much smaller than traditional documents. This is due to relatively small meeting duration, and to the fact that participants tend to stick to the same terms throughout the entire conversation. For the k- means algorithm, k was set equal to the minimum path length z when doing MSCG path re-ranking (see Equation 10), and to 60 when generating the final summary (see Equation 11).</p><p>Following <ref type="bibr" target="#b2">Boudin and Morin (2013)</ref>, the num- ber of shortest weighted paths K was set to 200, which is greater than the K = 100 used by <ref type="bibr" target="#b3">Filippova (2010)</ref>. Increasing K from 100 improves performance with diminishing returns, but sig- nificantly increases complexity. We empirically found 200 to be a good trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Interpretation</head><p>Metrics. We evaluated performance with the widely-used ROUGE-1, ROUGE-2 and ROUGE- SU4 metrics <ref type="bibr" target="#b9">(Lin, 2004</ref>). These metrics are re- spectively based on unigram, bigram, and unigram plus skip-bigram overlap with maximum skip dis- tance of 4, and have been shown to be highly correlated with human evaluations <ref type="bibr" target="#b9">(Lin, 2004</ref>). ROUGE-2 scores can be seen as a measure of sum- mary readability ( <ref type="bibr" target="#b10">Lin and Hovy, 2003;</ref><ref type="bibr" target="#b4">Ganesan et al., 2010)</ref>. ROUGE-SU4 does not require con- secutive matches but is still sensitive to word or- der.</p><p>Macro-averaged results for summaries gener- ated from automatic transcriptions can be seen in <ref type="figure" target="#fig_7">Figure 7</ref> and <ref type="table" target="#tab_3">Table 2</ref>. <ref type="table" target="#tab_3">Table 2</ref> provides detailed comparisons over the fixed budgets that we used for parameter tuning, while <ref type="figure" target="#fig_7">Figure 7</ref> shows the performance of the models for budgets ranging from 150 to 500 words. The same information for summaries generated from manual transcriptions is available in Appendix C. Finally, summary ex- amples are available in Appendix D. ROUGE-1. Our systems outperform all baselines on AMI (including Oracle) and all baselines on ICSI (except Oracle). Specifically, Our System is best on ICSI, while Our System (KeyRank) is su- perior on AMI. We can also observe on <ref type="figure" target="#fig_7">Figure 7</ref> that our systems are consistently better throughout the different summary sizes, even though their pa- rameters were tuned for specific sizes only. This shows that the best parameter values are quite ro- bust across the entire budget range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROUGE-2. Again, our systems (except Our Sys- tem (Baseline)) outperform all baselines, except</head><p>Oracle. In addition, Our System and Our System (FluCovRank) consistently improve on Our Sys- tem (Baseline), which proves that the novel com- ponents we introduce improve summary fluency. ROUGE-SU4. ROUGE-SU4 was used to mea- sure the amount of in-order word pairs overlap- ping. Our systems are competitive with all base- lines, including Oracle. Like with ROUGE-1, Our System is better than Our System (KeyRank) on ICSI, whereas the opposite is true on AMI. General remarks.</p><p>• The summaries of all systems except Oracle were generated from noisy ASR transcriptions, but were compared against human abstractive sum- maries. ROUGE being based on word overlap, it makes it very difficult to reach very high scores, because many words in the ground truth sum- maries do not appear in the transcriptions at all.</p><p>• The scores of all systems are lower on ICSI than on AMI. This can be explained by the fact that on ICSI, the system summaries have to jointly match 3 human abstractive summaries of different con- tent and size, which is much more difficult than matching a single summary.</p><p>• Our framework is very competitive to Oracle, which is notable since the latter has direct access to the human extractive summaries. Note that Or-    acle does not reach very high ROUGE scores be- cause the overlap between the human extractive and abstractive summaries is low (19% and 29%, respectively on AMI and ICSI test sets).</p><formula xml:id="formula_12">AMI ROUGE-1 AMI ROUGE-2 AMI ROUGE-SU4 ICSI ROUGE-1 ICSI ROUGE-2 ICSI ROUGE-SU4 R P F-1 R P F-1 R P F-1 R P F-1 R P F-1 R P F-1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Next Steps</head><p>Our framework combines the strengths of 6 ap- proaches that had previously been applied to 3 dif- ferent tasks (keyword extraction, multi-sentence compression, and summarization) into a uni- fied, fully unsupervised end-to-end summarization framework, and introduces some novel compo- nents. Rigorous evaluation on the AMI and ICSI corpora shows that we reach state-of-the-art per- formance, and generate reasonably grammatical abstractive summaries despite taking noisy utter- ances as input and not relying on any annotations or training data. Finally, thanks to its fully unsu- pervised nature, our method is applicable to other languages than English in an almost out-of-the- box manner. Our framework was developed for the meeting domain. Indeed, our generative component, the multi-sentence compression graph (MSCG), needs redundancy to perform well. Such redundancy is typically present in meeting speech but not in traditional documents. In addition, the MSCG is by design robust to noise, and our custom path re-ranking strategy, based on graph degeneracy, makes it even more robust to noise. As a result, our framework is advantaged on ASR input. Fi- nally, we use a language model to favor fluent paths, which is crucial when working with (meet- ing) speech but not that important when dealing with well-formed input.</p><p>Future efforts should be dedicated to improv- ing the community detection phase and generating more abstractive sentences, probably by harness- ing Deep Learning. However, the lack of large training sets for the meeting domain is an obsta- cle to the use of neural approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>https://bitbucket.org/dascim/acl2018_abssumm 2 http://datascience.open-paas.org/abs_summ_app</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: k-core decomposition. The blue and the yellow nodes have same degree and similar PageRank numbers. However, the blue node is a much more influential spreader as it is strategically placed in the core of the network, as captured by its higher core number.</figDesc><graphic url="image-1.png" coords="3,124.44,129.33,113.38,104.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Value added by CoreRank: while nodes and have the same core number (=2), node has a greater CoreRank score (3+2+2=7 vs 2+2+1=5), which better reflects its more central position in the graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5</head><label>5</label><figDesc>Figure 5: Compressed sentence (in bold red) generated by our multi-sentence compression graph (MSCG) for a 3-utterance community from meeting IS1009b of the AMI corpus. Using Filippova (2010)'s weighting and re-ranking scheme here would have selected another path: design different remotes for different people bit of it's from their tend to for ti. Note that the compressed sentence does not appear in the initial set of utterances, and is compact and grammatical, despite the redundancy, transcription and segmentation errors of the input. The abstractive and robust nature of the MSCG makes it particularly well-suited to the meeting domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: ROUGE-1 F-1 scores for various budgets (ASR transcriptions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Macro-averaged results for 350 and 450 word summaries (ASR transcriptions). 

</table></figure>

			<note place="foot" n="3"> We used NLTK&apos;s averaged perceptron tagger, available at: http://www.nltk. org/api/nltk.tag.html#module-nltk.tag.perceptron</note>

			<note place="foot" n="5"> CMUSphinx English LM: https://cmusphinx.github.io</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to the four anonymous review-ers for their detailed and constructive feedback. This research was supported in part by the Open-PaaS::NG project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Identifying and ranking influential spreaders in complex networks by neighborhood coreness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonhyun</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwook</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">395</biblScope>
			<biblScope unit="page" from="549" to="559" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Batagelj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matjaž</forename><surname>Zaveršnik</surname></persName>
		</author>
		<idno>cs/0202039</idno>
		<title level="m">Generalized cores</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Keyphrase extraction for n-best reranking in multi-sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Boudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Morin</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N13-1030" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="298" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-sentence compression: Finding shortest paths in word graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C10-1037" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
	<note>Coling 2010 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Opinosis: A graph based approach to abstractive summarization of highly redundant opinions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C10-1039" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="340" to="348" />
		</imprint>
	</monogr>
	<note>Coling 2010 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Clusterrank: a graph based method for meeting summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Korbinian</forename><surname>Reidhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The icsi meeting corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Janin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gelbart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wooters</surname></persName>
		</author>
		<idno type="doi">10.1109/ICASSP.2003.1198793</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2003.1198793" />
	</analytic>
	<monogr>
		<title level="m">Proceedings. (ICASSP &apos;03). 2003 IEEE International Conference on</title>
		<meeting>(ICASSP &apos;03). 2003 IEEE International Conference on</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="364" to="367" />
		</imprint>
	</monogr>
	<note>Acoustics, Speech, and Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identification of influential spreaders in complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lazaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Gallos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Havlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Liljeros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Muchnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Makse</surname></persName>
		</author>
		<idno type="doi">10.1038/nphys1746</idno>
		<ptr target="https://doi.org/10.1038/nphys1746" />
	</analytic>
	<monogr>
		<title level="j">Nature Physics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="888" to="893" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">I</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
	<note>JMLR.org, ICML&apos;15</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W04-1013" />
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using n-gram co-occurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N03-1020" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Submodularity in natural language processing: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>University of Washington</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-document summarization via budgeted maximization of submodular functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N10-1134" />
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="912" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The ami meeting corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Mccowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Carletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bourban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karaiskos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Methods and Techniques in Behavioral Research</title>
		<meeting>the 5th International Conference on Methods and Techniques in Behavioral Research</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">88</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Abstractive meeting summarization with entailment and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Tompa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W13-2117" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Workshop on Natural Language Generation. Association for Computational Linguistics</title>
		<meeting>the 14th European Workshop on Natural Language Generation. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="136" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ioannis Nikolentzos, and Michalis Vazirgiannis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polykarpos</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Tixier</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/E17-2074" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="462" to="467" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Textrank: Bringing order into text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W04-3252" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<idno type="doi">10.1145/219717.219748</idno>
		<ptr target="https://doi.org/10.1145/219717.219748" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using the omega index for evaluating abstractive community detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W12-2602" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization. Association for Computational Linguistics</title>
		<meeting>Workshop on Evaluation Metrics and System Comparison for Automatic Summarization. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long story short-global unsupervised models for keyphrase based meeting summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Korbinian</forename><surname>Riedhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Benoit Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hakkani-Tür</surname></persName>
		</author>
		<idno type="doi">10.1016/j.specom.2010.06.002</idno>
		<ptr target="https://doi.org/10.1016/j.specom.2010.06.002" />
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="801" to="815" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Packing the meeting summarization knapsack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Korbinian</forename><surname>Riedhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph-of-word and tw-idf: New approach to ad hoc ir</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<idno type="doi">10.1145/2505515.2505671</idno>
		<ptr target="https://doi.org/10.1145/2505515.2505671" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 22Nd ACM International Conference on Information &amp; Knowledge Management<address><addrLine>New York, NY, USA, CIKM</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Network structure and minimum degree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stephen B Seidman</surname></persName>
		</author>
		<idno type="doi">10.1016/0378-8733(83)90028-X</idno>
		<ptr target="https://doi.org/10.1016/0378-8733(83)90028-X" />
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="287" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A graph degeneracy-based approach to keyword extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fragkiskos</forename><surname>Malliaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/D16-1191</idno>
		<ptr target="https://doi.org/10.18653/v1/D16-1191" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1860" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Combining graph degeneracy and submodularity for unsupervised extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polykarpos</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W17-4507" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on New Frontiers in Summarization. Association for Computational Linguistics</title>
		<meeting>the Workshop on New Frontiers in Summarization. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="48" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gowvis: A web application for graph-of-words-based text visualization and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Skianis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-4026</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-4026" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-2016 System Demonstrations</title>
		<meeting>ACL-2016 System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="151" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Corpus-independent generic keyphrase extraction using word embedding vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Engineering Research Conference</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
