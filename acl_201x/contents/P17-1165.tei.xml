<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Topical Coherence in LDA-based Models through Induced Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hesam</forename><surname>Amoualian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Balikas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massih</forename><forename type="middle">R</forename><surname>Amini</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianne</forename><surname>Clausel</surname></persName>
						</author>
						<title level="a" type="main">Topical Coherence in LDA-based Models through Induced Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1799" to="1809"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1165</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words. The coherence between topics is ensured through a copula, binding the topics associated to the words of a segment. In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments. We show that the proposed model naturally encompasses other state-of-the-art LDA-based models designed for similar tasks. Furthermore, our experiments, conducted on six different publicly available datasets, show the effectiveness of our model in terms of perplexity, Normalized Pointwise Mutual Information, which captures the coherence between the generated topics, and the Micro F1 measure for text classification.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since the seminal works of <ref type="bibr" target="#b14">Hofmann (1999)</ref> and <ref type="bibr" target="#b5">Blei et al. (2003)</ref>, there have been several develop- ments in probabilistic topic models. Many exten- sions have indeed been proposed for different ap- plications, including ad-hoc information retrieval ( <ref type="bibr" target="#b30">Wei and Croft, 2006</ref>), clustering search results ( <ref type="bibr" target="#b32">Zeng et al., 2004</ref>) and driving faceted browsing <ref type="bibr" target="#b18">(Mimno and McCallum, 2007)</ref>. In most of these studies, the initial exchangeability assumptions of PLSA and LDA, stipulating that words within a document are interdependent, has led to incoherent topic assignments within semantically meaningful text units, even though the importance of having topically coherent phrases is generally admitted ( <ref type="bibr" target="#b12">Griffiths et al., 2005</ref>). More recently, ( <ref type="bibr" target="#b3">Balikas et al., 2016b</ref>) has shown that binding topics, so as to obtain more coherent topic assignments, within such text segments as noun phrases improves the performance (e.g. in terms of perplexity) of LDA- based models. The question nevertheless remains as to which segmentation one should rely on.</p><p>Furthermore, text segments can refer to topics that are barely present in other parts of the doc- ument. For example, the segment "the Kurdish regional capital" in the sentence 1 "A thousand protesters took to the main street in Erbil, the Kur- dish regional capital, to condemn a new law requir- ing all public demonstrations to have government permits." refers to geography in a document that is mainly devoted to politics. Relying on a single topic distribution, as done in most previous studies including ( <ref type="bibr" target="#b3">Balikas et al., 2016b</ref>), may prevent one from capturing those segment specific topics.</p><p>In this paper, we propose a novel LDA-based model that automatically segments documents into topically coherent sequences of words. The coher- ence between topics is ensured through copulas <ref type="bibr" target="#b10">(Elidan, 2013</ref>) that bind the topics associated to the words of a segment. In addition, this model relies on both document and segment specific topic distri-butions so as to capture fine grained differences in topic assignments. A simple switching mechanism is used to select the appropriate distribution (doc- ument or segment specific) for assigning a topic to a word. We show that this model naturally en- compasses other state-of-the-art LDA-based models proposed to accomplish the same task, and that it outperforms these models over six publicly avail- able collections in terms of perplexity, Normalized Pointwise Mutual Information (NPMI), a measure used to assess the coherence of topics with docu- ments, and the Micro F1-measure in a text classifi- cation context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Probabilistic Latent Semantic Analysis (PLSA) proposed by <ref type="bibr" target="#b14">(Hofmann, 1999</ref>) is the first proba- bilistic model that explains the generation of co- occurrence data using latent random topics and, the EM algorithm for parameter estimation. The model was found more flexible and scalable than the La- tent Semantic Analysis <ref type="bibr" target="#b7">(Deerwester et al., 1990)</ref>, which is based on the singular value decomposi- tion of the document-term matrix, however PLSA is not a generative model as parameter estimation should be performed at each addition of new doc- uments. To overcome this drawback, <ref type="bibr" target="#b5">Blei et al. (2003)</ref> proposed the Latent Dirichlet Allocation (LDA) by assuming that the latent topics are ran- dom variables sampled from a Dirichlet distribu- tion and that the generated words, occurring in a document, are exchangeable. The interdependence assumption allows the parameter estimation and the inference of the LDA model to be carried out efficiently, but it is not realistic in the sense that topics assigned to similar words of a text span are generally incoherent.</p><p>Different studies, presented in the following sec- tions, attempted to remedy this problem and they can be grouped in two broad families depending on whether they make use of external knowledge- based tools or not in order to exhibit text structure for word-topic assignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Knowledge-based topic assignments</head><p>The main assumption behind these models are that text-spans such as sentences, phrases or segments are related in their content. Therefore, the inte- gration of these dependent structures can help to discover coherent latent topics for words. Different attempts to combine LDA-based models with sta- tistical tools to discover document structures have been successfully proposed, such as the study of <ref type="bibr" target="#b12">Griffiths et al. (2005)</ref> who investigated the effect of combining a Hidden Markov Model with LDA to capture long and short distance dependencies. Similarly, <ref type="bibr" target="#b6">(Boyd-Graber and Blei, 2008;</ref><ref type="bibr" target="#b2">Balikas et al., 2016a</ref>,b) integrated text structure exhibited by a parser or a chunker in their topic models. In this line, <ref type="bibr" target="#b9">Du et al. (2013)</ref> following <ref type="bibr" target="#b8">(Du et al., 2010)</ref> presented a hierarchical Bayesian model for unsupervised topic segmentation. This model in- tegrates a boundary sampling method used in a Bayesian segmentation model introduced by <ref type="bibr" target="#b24">Purver et al.(2006)</ref> to the topic model. For inference, a non-parametric Markov Chain inference is used that splits and merges the segments while a Pitman- Yor process <ref type="bibr" target="#b26">(Teh, 2006</ref>) binds the topics. Recently, <ref type="bibr" target="#b25">Tamura and Sumita (2016)</ref> extended this idea to the bilingual setting. They assume that documents con- sist of segments and the topic distribution of each segment is generated using a Pitman-Yor process <ref type="bibr" target="#b26">(Teh, 2006</ref>).</p><p>Though, the topic assignments follow the struc- ture of the text; these models suffer from the bias of statistical or linguistic tools they rely on. To overpass this limitation, other systems integrated automatically the extraction of text structure, in the form of phrases, in their process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge-free topic assignments</head><p>This type of models extract text-spans using n- gram counts and word collections and use bigrams to integrate the order of words as well as to capture the topical content of a phrase ( <ref type="bibr" target="#b17">Lau et al., 2013</ref>). In ( <ref type="bibr" target="#b29">Wang et al., 2007)</ref>, depending on the topic a particular bigram can be either considered as a sin- gle token or as two unigrams. Further, <ref type="bibr" target="#b28">Wang et al. (2009)</ref> merged topic models with a unigram model over sentences that assigns topics to the sentences instead of the words.</p><p>Our proposed approach also does not make use of external statistical tools to find text segments. The main difference with the previous knowledge- free topic model approaches is that the proposed approach assigns topics to words based on two, segment-specific and document-specific distribu- tions selected from a Bernoulli law. Topics within segments are then constrained using copulas that bind their distributions. In this way, segmentation is embedded in the model and it naturally comes along with the topic assignment.  </p><formula xml:id="formula_0">α θ d z1 zn w1 wn φ β λ |S| D K . . . . . . (a) copLDA α θ d z1 zn w1 wn S |S d | φ β λ |S| D K . . . . . . (b) segLDAcopp=0 α θ d fn θ d,s,n p θ s zn wn S |S d | φ β |S| D K (c) segLDAcop λ=0 α θ d f1 fn θ d,s,1 θ d,s,n p θ s z1 zn w1 wn S |S d | φ β λ |S| D K . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Joint latent model for topics and segments</head><p>We define here a segment as a topically coherent sequence of contiguous words. By topically coher- ent, we mean that, even though words in a segment can be associated to different topics, these topics are usually related. This view is in line with the one expressed in ( <ref type="bibr" target="#b3">Balikas et al., 2016b</ref>), in which a latent topic model, referred to as copLDA in the remainder, includes a binding mechanism between topics within coherent text spans, defined in their study as noun phrases (NPs). The relation between topics is captured through a copula that provides a joint probability for all the topics used in a seg- ment. That is, to generate words in a segment, one first jointly generates all the word specific topics z via a copula, and then generates each word in the segment from its word specific topic and the word-topic distribution φ. <ref type="figure" target="#fig_1">Figure 1</ref>(a) illustrates this. Copulas are particularly useful when mod- eling dependencies between random variables, as the joint cumulative distribution function (CDF) F X 1 ,··· ,Xn of any random vector X = (X 1 , · · · , X n ) can be written as a function of its marginals, according to <ref type="bibr">Sklar's Theorem (Nelsen, 2006</ref>):</p><formula xml:id="formula_1">FX 1 ,··· ,Xn (x1, · · · , xp) = C(FX 1 (x1), · · · , FX n (xn))</formula><p>where C is a copula. For latent topic models, as discussed in <ref type="bibr" target="#b0">(Amoualian et al., 2016</ref>), Frank's cop- ula is particularly interesting as (a) it is invariant by permutations and associative, as are the words and topics z in each segment due to the exchangeability assumption, and (b) it relies on a single parameter (denoted λ here) that controls the strength of de- pendence between the variables and is thus easy to implement. In Frank's copula, when the parameter λ approaches 0, the variables are independent of each other, whereas when λ approaches +∞, the variables take the same value. For further details on copulas, we refer the reader to <ref type="bibr" target="#b20">(Nelsen, 2006)</ref>.</p><p>One important problem, however, with copLDA is its reliance on a predefined segmentation. Al- though the information brought by the segmenta- tion based on NPs helps to improve topic assign- ment, it may not be flexible enough to capture all the possible segments of a text. It is easy to correct this problem by considering all possible segmen- tations of a document and by choosing the most appropriate one at the same time that topics are assigned to words. This is illustrated in <ref type="figure" target="#fig_1">Figure 1(b)</ref>, where a segmentation S is chosen from the set S d of possible segmentations for a document d, and where each segment in S are generated in turn. We refer to the associated model as segLDAcop p=0 for reasons that will become clear later.</p><p>Another point to be noted about copLDA (and segLDAcop p=0 ) is that the topics used in each segment come from the same document specific topic distribution θ d . This entails that, in these models, one cannot differentiate the main topics of a document from potential segment specific topics that can explain some parts of it. Indeed, some text segments can refer to topics that are barely present in other parts of the document; relying on a single topic distribution may prevent one from capturing those segment specific topics. It is possible to overcome this difficulty by gen- erating a segment specific topic distribution as il- lustrated in <ref type="figure" target="#fig_1">Figure 1</ref>(c) (this model is referred to as segLDAcop λ=0 , again for reasons that will be- come clear later). However, as some words in a segment can be associated to the general topics of a document, we introduce a mechanism to choose, for each word in a segment, a topic either from the segment specific topic distribution θ s or from the document specific topic distribution θ d (this mechanism is similar to the one used for routes and levels in <ref type="bibr" target="#b23">(Paul and Girju, 2010)</ref>). The choice between them is based on the Bernoulli variable f , as explained in the generative story given below.</p><p>The above developments can be combined in a single, complete model, illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>(d) and detailed below. We will simply refer to this model as segLDAcop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Complete generative model</head><p>As in standard LDA based models, with V de- noting the size of the vocabulary of the collec- tion and K the number of latent topics, β and</p><formula xml:id="formula_2">φ k , 1 ≤ k ≤ K, are V dimensional vectors, α and θ (i.e., θ d , θ s , θ d,s,n ) are K dimensional vec-</formula><p>tors, whereas z n takes value in {1, · · · , K}. Lower indices are used to denote coordinates of the above vectors. Lastly, Dir denotes the Dirichlet distri- bution, Cat the categorical distribution (which is a multinomial distribution with one draw) and we omit, as is usual, the generation of the length of the document. The complete model segLDAcop is then based on the following generative process:</p><p>1. Generate, for each topic k, 1 ≤ k ≤ K, a distribution over the words: φ k ∼ Dir(β);</p><formula xml:id="formula_3">2. For each document d, 1 ≤ d ≤ D:</formula><p>(a) Choose a document specific topic distribu- tion: θ d ∼ Dir(α); (b) Choose a segmentation S of the document uniformly from the set of all possible</p><formula xml:id="formula_4">segmentations S d : P (S) = 1 |S d | ;</formula><p>(c) For each segment s in S:</p><p>(i) Choose a segment specific topic distri- bution: θ s ∼ Dir(α); (ii) For each position n in s, choose f n ∼ Ber(p) and set:</p><formula xml:id="formula_5">θ d,s,n = θ s if f n = 1 θ d otherwise (iii) Choose topics Z s = {z 1 , .</formula><p>. . , z n } from Frank's copula with parameter λ and marginals Cat(θ d,s,n ); (iv) For each position n in s, choose word w n : w n ∼ Cat(φ zn ).</p><p>As on can note, the generative process relies on a segmentation uniformly chosen from the set of pos- sible segmentations (step 2.b) to generate related topics within each segment (Frank's copula in step 2.c.(iii)), the distribution underlying each word spe- cific topic z n being either specific to the segment or general to the document (steps 2.c.(i) and 2.c.(ii)). The other steps are similar to the standard LDA steps. As in almost all previous studies on LDA, α and β are considered fixed and symmetric, each coor- dinate of the vector being equal:</p><formula xml:id="formula_6">α 1 = · · · = α K .</formula><p>The hyperparameters p (∈ [0, 1]) of the Bernoulli distribution and λ (∈ [0, +∞]) of Frank's copula re- spectively regulate the choice between the segment specific and the document specific topic distribu- tions and the strength of the dependence between topics in a segment. As for the other hyperparame- ters, we consider them fixed here (the values for all hyperparameters are given in Section 4).</p><p>As mentioned before, all the models presented in <ref type="figure" target="#fig_1">Figure 1</ref> are special cases of the complete model segLDAcop: hence segLDAcop λ=0 is obtained by dropping the topic dependencies, which amounts to setting λ to (a value close to) 0, segLDAcop p=0 is obtained by relying only on the topic distribution obtained for the document, which amounts to setting p to 0, and the previously introduced copLDA model is obtained by setting p to 0, and fixing the segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference with Gibbs sampling</head><p>The parameters of the complete model can be di- rectly estimated through Gibbs sampling. The Gibbs updates for the parameters φ and θ are the same as the ones for standard LDA ( <ref type="bibr" target="#b5">Blei et al., 2003</ref>). The parameters f n are directly estimated through: f n ∼ Ber(p). Lastly, for the variables z, we follow the same strategy as the one described in ( <ref type="bibr" target="#b3">Balikas et al., 2016b</ref>) and based on ( <ref type="bibr" target="#b0">Amoualian et al., 2016</ref>), leading to:</p><formula xml:id="formula_7">P (Z s |Z −s , W, Θ, Φ, λ) = p(Z s |Θ, λ) n φ zn wn</formula><note type="other">where W denotes the document collection, and Θ and Φ the sets of all θ and φ k , 1 ≤ k ≤ K, vectors. p(Z s |Θ, λ) is obtained by Frank's copula with parameter λ and marginals Cat(θ d,s,n ). As is standard in topic models, the notation −s means excluding the information from s.</note><p>From the above equation, one can formulate an acceptance/rejection algorithm based on the follow- ing steps: (a) sample Z s from p(Z s |Θ, λ) using Frank's copula, and (b) accept the sample with probability n φ zn wn , where n runs over all the po- sitions in segment s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Efficient segmentation</head><p>As topics may change from one sentence to another, we assume here that segments cannot overlap sen- tence boundaries. The different segmentations of a document are thus based on its sentence segmen- tations. In the remainder, we use L to denote the maximum length of a segment and g(M ; L) to de- note the number of segmentations in a sentence of length M , each segment comprising at most L words.</p><p>Generating all possible segmentations of a sen- tence and then selecting one at random is not an efficient process as the number of segments rapidly grows with the length of the sentence. In practice, however, one can define an efficient segmentation on the basis of the following proposition, the proof of which is given in Appendix A: Proposition 3.1. Let l s i be the random variable associated to the length of the segment starting at position i in a sentence of length M (positions go from 1 to M and l s i takes value in {1, · · · , L}).</p><formula xml:id="formula_8">Then P (l s i = l) := g(M +1−i−l);L) g(M +1−i;L)</formula><p>defines a proba- bility distribution over l s i . Furthermore, the following process is equivalent to choosing sentence segmentations uniformly from the set of possible segmentations. In practice, we thus replace steps 2.b and 2.c of the generative story by a loop over all sen- tences, and in each sentence use the process de- scribed in Prop, 3.1. Furthermore, as described in Appendix A, the values of g needed to compute P (l s i = l) can be efficiently computed by recur- rence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conducted a number of experiments aimed at studying the impact of simultaneously segment- ing and assigning topics to words within segments using the proposed segLDAcop model. The first two collections were considered in (Ba- likas et al., 2016a), we followed their setup by con- sidering 3 subsets of Wikipedia with different num- ber of classes (namely, Wiki0, Wiki1 and Wiki2). The Reuters dataset comes from Reuters-21578, Distribution 1.0 as investigated in ( <ref type="bibr" target="#b4">Bird et al., 2009)</ref> and the NYT dataset is collected from full text of New York Times global news, from January 1st to December 31st, 2011.</p><p>These collections were processed following ( <ref type="bibr" target="#b5">Blei et al., 2003</ref>) by removing a standard list of 50 stop words, lemmatizing, lowercasing and keeping only words made of letters. To deal with relatively homogeneous collections, we also removed doc- uments that are too long. The statistics of these datasets, as well as the admissible maximal length for documents, in terms of the number of words they contain, can be found in <ref type="table">Table 1</ref>.</p><p>Settings: We compared our mod- els (segLDAcop p=0 , segLDAcop λ=0 , segLDAcop) with three models, namely the standard LDA model, and two previously introduced models aiming at binding topics within segments:</p><p>1. LDA: Standard Latent Dirichlet Allocation im- plemented using collapsed Gibbs sampling in- ference ( <ref type="bibr" target="#b11">Griffiths and Steyvers, 2004</ref>) <ref type="bibr">5</ref> . Note <ref type="table" target="#tab_2">Wiki0   Wiki1  Wiki2  # words  32,354  70,954 103,308  -vocabulary size  7,853  12,689  14,715  # docs  1,014  2,138  3,152  -maximal length  100  100  100  # labels  17  42  53  Pubmed Reuters  NYT  # words  104,683</ref>  Both senLDA and copLDA implementations, can be found in https://github.com/ balikasg/topicModelling. In all models α and β play a symmetric role and are respectively fixed to 1/K, following <ref type="bibr" target="#b1">(Asuncion et al., 2009</ref>). For copula based models, λ is set to 5, following <ref type="bibr" target="#b3">(Balikas et al., 2016b</ref>). As already discussed, p is set to 0 for segLDAcop p=0 ; it is set to 0.5 for segLDAcop so as not to privilege a priori one topic distribution (document or segment specific) over the other. For sampling from Frank's copula, we relied on the R copula package <ref type="bibr" target="#b13">(Hofert and Maechler, 2011)</ref>  <ref type="bibr">6</ref> . We chose L (the maximum length of a segment) using line search for L ∈ <ref type="bibr">[2,</ref><ref type="bibr">5]</ref> and used L = 3 in all our experiments. Finally, to illustrate the behaviors of the different models with different number of topics, we present here the results obtained with K = 20 and K = 100.</p><p>We now compare the different models along three main dimensions: perplexity, use of topic representations for classification and topic coher- ence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Perplexity</head><p>We first randomly split here all the collections, us- ing 75% of them for training, and 25% for testing.</p><p>In order to see how well the models fit the data and following <ref type="bibr" target="#b5">(Blei et al., 2003)</ref>, we first evaluated the methods in terms of perplexity defined as:</p><formula xml:id="formula_9">P erplexity = exp − d∈D w∈d log K k=1 θ d k φ k w d∈D |d| ,</formula><p>where d is a test document from the test set D, and |d| is the total number of words in d, and K is the total number of topics. The lower the perplexity is, the better the model fits the test data. <ref type="table" target="#tab_2">Table 2</ref> shows perplexities of different methods for K = 20 and K = 100 topics. From <ref type="table" target="#tab_2">Table 2</ref>, it comes out that the best perform- ing model in terms of perplexity over all datasets and for different number of topics is segLDAcop. Further, segLDAcop λ=0 , that uses both document and segment specific topic distributions, performs better than segLDAcop p=0 , which in turn outper- forms copLDA, bringing evidence that using all possible segmentations rather than only NPs unit extracted using a chunker yields a more flexible and natural topic assignment.</p><p>segLDAcop also converges faster than the other methods to its minimum as it is shown in <ref type="figure" target="#fig_4">Figure 2</ref>, depicting the evolution of perplexity of different models over the number of iterations on the NYT collection (a similar behavior is observed on the other collections).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wiki0</head><p>Wiki1 <ref type="table" target="#tab_2">Wiki2  Pubmed  Reuters  NYT  20  100  20  100  20  100  20  100  20  100  20</ref>    <ref type="table">Table 3</ref>: MiF score (percent) with respect to different number of topics (20 and 100).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Topical induced representation for classification</head><p>Some studies compare topic models using extrin- sic tasks such as document classification. In this case, it is possible to reduce the dimensionality of the representation space by using the induced topics ( <ref type="bibr" target="#b5">Blei et al., 2003)</ref>. In this study, we first ran- domly splitted the datasets, except NYT that does not contain class information, into training (75%) and test (25%) sets. We then applied SVMs with a linear kernel; the value of the hyperparameter C was found by cross-validation over the training set {0.01, 0.1, 1, 10, 100}. For datasets where cer- tain documents have more than one label (Pubmed, Reuters), we used the one-versus-all approach for performing multi-label classification.</p><p>In <ref type="table">Table 3</ref>, we report the Micro F1 (MiF) score of different models on the test sets. Again, the best re- sults are obtained with segLDAcop, followed by segLDAcop λ=0 . This shows the importance of re- lying on both document and segment specific topic distributions. As conjectured before, our model is able to captures fine grained topic assignments within documents. In addition, all models rely- ing on an inferred segmentation (segLDAcop p=0 , segLDAcop λ=0 , segLDAcop) outperform the models relying on fixed segmentations (sentences or NPs). This shows the importance of being able to discover flexible segmentations for assigning topics within documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Topic coherence</head><p>Another common way to evaluate topic models is by examining how coherent the produced topics are. Doing this manually is a time consuming pro- cess and cannot scale. To overcome this limitation the task of automatically evaluating the coherence of topics produced by topic models received a lot of attention <ref type="bibr" target="#b19">(Mimno et al., 2011)</ref>. It has been found that scoring the topics using co-occurrence mea- sures, such as the pointwise mutual information (PMI) between the top-words of a topic, correlates well with human judgments <ref type="bibr" target="#b21">(Newman et al., 2010)</ref>. For this purpose an external, large corpus is used as a meta-document where the PMI scores of pairs of words are estimated using a sliding window. As dis- cussed above, calculating the co-occurrence mea- sures requires selecting the top-N words of a topic and performing the manual or automatic evaluation. Hence, N is a hyper-parameter to be chosen and its value can impact the results. Very recently, <ref type="bibr" target="#b16">Lau and Baldwin (2016)</ref> showed that N actually impacts the quality of the obtained results and, in particu- lar, the correlation with human judgments. In their work, they found that aggregating the topic coher- ence scores over several topic cardinalities leads to a substantially more stable and robust evaluation.  <ref type="bibr">1]</ref>, where in the limit of -1 two words w 1 and w 2 never occur together, while in the limit of +1 they always occur together (complete co-occurrence). For the reported scores, we aggregate the topic co- herence scores over three different topic cardinali- ties: N ∈ {5, 10, 15}. segLDAcop model which </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NPMI (%)</head><p>LDA senLDA copLDA segLDAcop p=0 segLDAcop λ=0 segLDAcop <ref type="figure">Figure 3</ref>: Topic coherence (NPMI) score with respect to 100 of topics.</p><p>uses copulas and segmentation together, shows the best score for the given reference meta-data (Wikipedia) in all of the datasets. It should be noted that segLDAcop λ=0 which has not cop- ula binder inside the model has less improvement against the segLDAcop p=0 which has the cop- ula. This means using copula has more effect on the topic coherence than only the segment-specific topic distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization</head><p>In order to illustrate the results obtained by segLDAcop, we display in <ref type="figure" target="#fig_6">Figure 4</ref> the top 10 most probable words over 5 topics (K = 20) for the Reuters dataset, for both segLDAcop and LDA. In segLDAcop, topic 1, the top-ranked words are mostly relevant to the topic "date" (e.g.,  march, january, year, fall, february, week). How- ever, a similar topic learned by LDA appears to involve less such words (year, january, february), indicating a less coherent topic. <ref type="figure">Figure 5</ref> illustrates another aspect of our model, namely the possibility to detect topically coherent segments. In particular, as one can note, the sen- tence is segmented in six parts by our model, the first one is a NP, Ralph Borsodi where one single topic is assigned to both words. We observe a sim- ilar coherence in topic assignments on other NPs and segments, in which a single topic is used for the words involved. The data-driven approach we have adopted here can discover such fine grained differences, something the approaches based on fixed segmentations (either based on sentences or NPs), are less likely to achieve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this paper, we have introduced an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words. The coherence between topics is ensured through Frank's copula, that binds the topics associated to the words of a segment. In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments. We have shown that this model natu- rally encompasses other state-of-the-art LDA-based models proposed to accomplish the same task, and that it outperforms these models over six publicly available collections in terms of perplexity, Nor- malized Pointwise Mutual Information (NPMI), a measure used to assess the coherence of topics with documents, and the Micro F1-measure in a text classification context. Our results confirm the importance of a flexible segmentation as well as a binding mechanism to produce topically coherent segments.</p><p>As regards complexity, it is true that more com- plex models, as the one we are considering, are more prone to underfitting (when data is scarce) and overfitting than simpler models. This said, the experimental results on perplexity (in which the word-topic distributions are fixed) and on classi- fication (based on the topical induced representa- tions) suggest that our model neither underfits nor overfits compared to simpler models. We believe that this is due to the fact that the main additional parameters in our model (the segment specific topic distribution) do not really add complexity as they are drawn from the same distribution as the stan- dard document specific topics. Furthermore, the parameters p and f are simple parameters to choose between these two distributions.</p><p>The comparison with other segmentation meth- ods is also an important point. While state-of-the- art supervised segmentation models can be used before applying the LDA model, we note such a pipeline approach comes with several limitations. The approach requires external annotated data to train the segmentation models, where certain do- main and language specific information need to be captured. By contrast, our unsupervised approach learns both segmentations and topics jointly in a domain and language independent manner. Fur- thermore, existing supervised segmentation models are largely designed for a very different purpose with strong linguistic motivations, which may not align well with our main goal in this paper which is improving topic coherence in topic modeling. Sim- ilarly, unsupervised approaches, used for example in the TDT (Topic Detection and Tracking) cam- paigns or more recently in <ref type="bibr" target="#b9">Du et al. (2013)</ref>, usually consider coarse-grained topics, that can encom- pass several sentences. In contrast, our approach aims at identifying fine-grained topics associated with coherent segments that do not overlap sen- tence boundaries. These considerations, explain the choice of the baselines retained: they are based on segments of different granularities (words, NPs, sentences) that do not overlap sentence boundaries.</p><p>In the future, we plan on relying on other infer- ence approaches, based for example on variational Bayes known to yield better estimates for perplex- ity ( <ref type="bibr" target="#b1">Asuncion et al., 2009)</ref>; it is however not certain that the gain in perplexity one can expect from the use of variational Bayes approaches will nec- essarily result in a gain in, say, topic coherence. Indeed, the impact of the inference approach on the different usages of latent topic models for text collections remains to be better understood.</p><p>Proof Any segmentation of the sentence of length M starts with either a segment of length 1, a seg- ment of length 2, · · · , or a segment of length L. Thus, g(M ; L) can be defined through the follow- ing recurrence relation:</p><formula xml:id="formula_10">g(M ; L) = L l=1 g(M − l; L)<label>(1)</label></formula><p>together with the initial values g(1; L), g(2; L), · · · , g(L; L), which can be computed offline (for example, for L = 3, one has: g(1; 3) = 1, g(2; 3) = 2, g(3; 3) = 4). Note that g(1; L) = 1 for all L.</p><p>Thus:</p><formula xml:id="formula_11">L l=1 P (l s i = l) = L l=1 g(M + 1 − i − l); L) g(M + 1 − i; L) = 1</formula><p>due to the recurrence relation on g. This proves the first part of the proposition. Using the process described above where seg- ments are generated one after another according to P , for a segmentation S, comprising |S| seg- ments, let us denote by l 1 , l 2 , · · · , l |S| the lengths of each segment and by i 1 , i 2 , · · · , i |S| the starting positions of each segment (with i 1 = 1). One has, as segments are independent of each other:</p><formula xml:id="formula_12">P (S) = |S| j=1 P (l s i j = lj) = |S| j=1 g(M + 1 − (ij + lj); L) g(M + 1 − ij; L) = g(M − l1; L) g(M ; L) g(M − l1 − l2; L) g(M − l1; L) · · · = 1 g(M ; L)</formula><p>as g(1; L) = 1. This concludes the proof of the proposition.</p><p>2 Furthermore, as one can note from Eq. 1, the various elements needed to compute P (l s i = l) can be efficiently computed, the time complexity being equal to O(M ). In addition, as the number of dif- ferent sentence lengths is limited, one can store the values of g to reuse them during the segmentation phase.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical model for Copula LDA (copLDA), extension of Copula LDA with segmentation (segLDAcop p=0 ), LDA with segmentation and topic shift (segLDAcop λ=0 ) and complete model (segLDAcop).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>From pos. 1 ,</head><label>1</label><figDesc>repeat till end of sentence: (a) Generate segment length acc. to P; (b) Add segment to current segmentation; (c) Move to position after the segment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Datasets: We considered six publicly available datasets derived from Pubmed 2 (Tsatsaronis et al., 2015), Wikipedia (Partalas et al., 2015), Reuters 3 and New York Times (NYT) 4 (Yao et al., 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Perplexity with respect to training iteration on NYT collection (20 topics).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Following</head><label></label><figDesc>the findings of Lau and Baldwin (2016) and using (Newman et al., 2010)'s equa- tion, we present in Figure 3 the topic coherence scores as measured by the Normalized Pointwise Mutual Information (NPMI) . Their values are in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Top-10 words of segLDAcop (left) vs LDA (right) for the Reuters (5 out of 20 topics).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Perplexity with respect to different number of topics (20 and 100). 

Models 
Wiki0 
Wiki1 
Wiki2 
Pubmed 
Reuters 
20 100 
20 100 
20 100 
20 100 
20 100 

LDA 
55.3 63.5 
42.4 51.4 
41.2 48.7 
54.1 63.5 
75.5 82.7 
senLDA 
41.4 53.2 
33.5 44.5 
36.4 40.9 
50.2 62.5 
69.4 74.2 
copLDA 
51.2 62.7 
43.4 52.1 
40.8 46.5 
53.5 63.1 
75.2 81.5 
segLDAcopp=0 59.1 64.2 
44.8 51.2 
42.3 50.1 
55.4 63.1 
76.8 82.5 
segLDAcop λ=0 61.1 67.4 
46.5 53.8 
44.1 52.2 
57.1 65.2 
79.6 84.4 
segLDAcop 
62.3 68.4 
48.4 55.2 
44.8 53.5 
59.3 66.5 
80.2 85.1 

</table></figure>

			<note place="foot" n="1"> This sentence is taken from New York Times news (NYT) collection described in Section 4.</note>

			<note place="foot" n="2"> https://github.com/balikasg/ topicModelling/tree/master/data 3 https://archive.ics.uci.edu/ ml/datasets/Reuters-21578+Text+ Categorization+Collection 4 https://github.com/yao8839836/COT/ tree/master/data 5 http://gibbslda.sourceforge.net</note>

			<note place="foot" n="6"> Our complete code will be available for research purposes.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the reviewers for their help-ful comments. Most of this work was done when Hesam Amoualian was visiting Singapore Univer-sity of Technology and Design. This work is sup-ported by MOE Tier 1 grant SUTDT12015008, also partly supported by the LabEx PERSYVAL-Lab ANR-11-LABX-0025.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Efficient segmentation</head><p>Let us recall the property presented before:</p><p>Proposition A.1. Let l s i be the random variable associated to the length of the segment starting at position i in a sentence of length M (positions go from 1 to M and l s i takes value in {1, · · · , L}).</p><p>defines a proba- bility distribution over l s i . Furthermore, the following process is equivalent to choosing sentence segmentations uniformly from the set of possible segmentations.</p><p>From pos. 1, repeat till end of sentence: (a) Generate segment length acc. to P; (b) Add segment to current segmentation; (c) Move to position after the segment.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Streaming-lda: A copula-based approach to modeling topic dependencies in document streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hesam</forename><surname>Amoualian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianne</forename><surname>Clausel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massih-Reza</forename><surname>Amini</surname></persName>
		</author>
		<idno type="doi">10.1145/2939672.2939781</idno>
		<ptr target="https://doi.org/10.1145/2939672.2939781" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, NY, USA, SIGKDD</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="695" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On smoothing and inference for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 25th Conference on Uncertainty in Artificial Intelligence<address><addrLine>Arlington, Virginia, United States, UAI</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On a topic model for sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massih-Reza</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianne</forename><surname>Clausel</surname></persName>
		</author>
		<idno type="doi">10.1145/2911451.2914714</idno>
		<ptr target="https://doi.org/10.1145/2911451.2914714" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA, SIGIR</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="921" to="924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling topic dependencies in semantically coherent text spans with copulas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hesam</forename><surname>Amoualian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianne</forename><surname>Clausel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massih R</forename><surname>Amini</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C16-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</title>
		<meeting>the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee<address><addrLine>Osaka, Japan, COLING</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1767" to="1776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<ptr target="http://www.nltk.org/book/" />
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>O&apos;Reilly, Beijing</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Syntactic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Graber</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Neural Information Processing Systems</title>
		<meeting>the 21st International Conference on Neural Information Processing Systems<address><addrLine>USA, NIPS</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="185" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harshman</surname></persName>
		</author>
		<idno type="doi">10.1002/(SICI)1097-4571</idno>
		<idno>6&lt;391::AID-ASI1&gt;3.0.CO</idno>
		<ptr target="http://dx.doi.org/10.1002/(SICI)1097-4571" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2" to="9" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Segmented Topic Model Based on the Two-parameter Poisson-Dirichlet Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huidong</forename><surname>Jin</surname></persName>
		</author>
		<idno type="doi">10.1007/s10994-010-5197-4</idno>
		<ptr target="https://doi.org/10.1007/s10994-010-5197-4" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="19" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Topic Segmentation with a Structured Topic Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/naacl/naacl2013.html/DuBJ13" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Annual Conference of the North American Chapter of the Association for Computational Linguistics, Human Language Technologies. HLT-NAACL</title>
		<meeting>The Annual Conference of the North American Chapter of the Association for Computational Linguistics, Human Language Technologies. HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gal Elidan</surname></persName>
		</author>
		<idno type="doi">10.1007/978-3-642-35407-6_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-35407-6_3" />
		<title level="m">Copulas in Machine Learning</title>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="39" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
		<idno type="doi">10.1073/pnas.0307752101</idno>
		<ptr target="https://doi.org/10.1073/pnas.0307752101" />
	</analytic>
	<monogr>
		<title level="j">Journal of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Integrating topics and syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Thomas L Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in the International Conference on Neural Information Processing Systems</title>
		<editor>L. K. Saul, Y. Weiss, and L. Bottou</editor>
		<meeting><address><addrLine>NIPS</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">537</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nested Archimedean Copulas Meet R: The nacopula Package</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Hofert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Maechler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">i09</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd</title>
		<meeting>the 22Nd</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<idno type="doi">10.1145/312624.312649</idno>
		<ptr target="https://doi.org/10.1145/312624.312649" />
		<title level="m">Annual International Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA, SIGIR</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The sensitivity of topic coherence evaluation to topic cardinality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Annual Conference of the North American Chapter of the Association for Computational Linguistics, Human Language Technologies</title>
		<meeting>The Annual Conference of the North American Chapter of the Association for Computational Linguistics, Human Language Technologies<address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="483" to="487" />
		</imprint>
		<respStmt>
			<orgName>NAACL</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On collocations and topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Jey Han Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newman</surname></persName>
		</author>
		<idno type="doi">10.1145/2483969.2483972</idno>
		<idno>10:1- 10:14</idno>
		<ptr target="https://doi.org/10.1145/2483969.2483972" />
	</analytic>
	<monogr>
		<title level="j">Journal of ACM Trans. Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Organizing the oca: Learning faceted subjects from a library of digital books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="doi">10.1145/1255175.1255249</idno>
		<ptr target="https://doi.org/10.1145/1255175.1255249" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Joint Conference on Digital Libraries</title>
		<meeting>the 7th Joint Conference on Digital Libraries<address><addrLine>New York, NY, USA, JCDL &apos;07</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="376" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA, EMNLP</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An Introduction to Copulas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Nelsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer Series in Statistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic evaluation of topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Annual Conference of the North American Chapter of the Association for Computational Linguistics, Human Language Technologies</title>
		<meeting>The Annual Conference of the North American Chapter of the Association for Computational Linguistics, Human Language Technologies<address><addrLine>Stroudsburg, PA, USA, NAACL</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">LSHTC: A Benchmark for Large-Scale Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aris</forename><surname>Kosmopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Baskiotis</surname></persName>
		</author>
		<idno>CoRR abs/1503.08581</idno>
		<ptr target="http://arxiv.org/abs/1503.08581" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A twodimensional topic-aspect model for discovering multi-faceted topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Conference on Artificial Intelligence</title>
		<meeting>the 24th Conference on Artificial Intelligence<address><addrLine>AAAI</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised topic modelling for multi-party spoken discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><forename type="middle">P</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Körding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="doi">10.3115/1220175.1220178</idno>
		<ptr target="https://doi.org/10.3115/1220175.1220178" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA, ACL</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bilingual segmented topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P/P16/P16-1120.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. ACL</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics. ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A hierarchical bayesian language model based on pitman-yor processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno type="doi">10.3115/1220175.1220299</idno>
		<ptr target="https://doi.org/10.3115/1220175.1220299" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA, ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="985" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prodromos</forename><surname>Malakasiotis</surname></persName>
		</author>
		<idno type="doi">10.1186/s12859-015-0564-6</idno>
		<ptr target="https://doi.org/10.1186/s12859-015-0564-6" />
	</analytic>
	<monogr>
		<title level="j">Journal of BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-document summarization using sentence-based topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingding</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the Conference on Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA, ACL-IJCNLP</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="297" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Topical n-grams: Phrase and topic discovery, with an application to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuerui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<idno type="doi">10.1109/ICDM.2007.86</idno>
		<ptr target="https://doi.org/10.1109/ICDM.2007.86" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Data Mining</title>
		<meeting>the 7th International Conference on Data Mining<address><addrLine>Washington, DC, USA, ICDM</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lda-based document models for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="doi">10.1145/1148170.1148204</idno>
		<ptr target="https://doi.org/10.1145/1148170.1148204" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th Annual International Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA, SIGIR</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Concept over time: the combination of probabilistic topic model with wikipedia knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baogang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Bian</surname></persName>
		</author>
		<idno type="doi">10.1016/j.eswa.2016.04.014</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2016.04.014" />
	</analytic>
	<monogr>
		<title level="j">Journal of Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="27" to="38" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to cluster web search results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua-Jun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Cai</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwen</forename><surname>Ma</surname></persName>
		</author>
		<idno type="doi">10.1145/1008992.1009030</idno>
		<ptr target="https://doi.org/10.1145/1008992.1009030" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International Conference on Research and Development in Information Retrieval</title>
		<meeting>the 27th Annual International Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA, SIGIR</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="210" to="217" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
