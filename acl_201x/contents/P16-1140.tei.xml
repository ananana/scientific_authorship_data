<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Investigating Language Universal and Specific Properties in Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Investigating Language Universal and Specific Properties in Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1478" to="1488"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently, many NLP tasks have benefited from distributed word representation. However, it remains unknown whether embedding models are really immune to the typological diversity of languages, despite the language-independent architecture. Here we investigate three representative models on a large set of language samples by mapping dense embedding to sparse linguistic property space. Experiment results reveal the language universal and specific properties encoded in various word representation. Additionally, strong evidence supports the utility of word form, especially for inflectional languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word representation is a core issue in natural language processing. Context-based word rep- resentation, which is inspired by <ref type="bibr" target="#b12">Harris (1954)</ref>, has achieved huge successes in many NLP ap- plications. Despite its popularity, character-based approach also comes out as an equal competitor <ref type="bibr" target="#b22">(Santos and Zadrozny, 2014;</ref><ref type="bibr" target="#b14">Kim et al., 2016;</ref><ref type="bibr" target="#b19">Ling et al., 2015b;</ref><ref type="bibr" target="#b18">Ling et al., 2015a;</ref><ref type="bibr" target="#b11">Faruqui et al., 2016;</ref><ref type="bibr" target="#b3">Ballesteros et al., 2015)</ref> . Moreover, questions arise when we consider what these models could capture from linguistic cues under the perspective of cross-language typological di- versity, as is argued by <ref type="bibr" target="#b4">Bender (2009)</ref>.</p><p>Despite previous efforts in empirically inter- preting word embedding and exploring the in- trinsic/extrinsic factors in learning process <ref type="bibr" target="#b2">(Andreas and Klein, 2014;</ref><ref type="bibr" target="#b16">Lai et al., 2015;</ref><ref type="bibr" target="#b15">Köhn, 2015;</ref><ref type="bibr" target="#b20">Melamud et al., 2016)</ref>, it remains unknown whether embedding models are really immune to the structural variance of languages. * Corresponding author.</p><p>Current research has gaps for understanding model behaviours towards language typological diversity as well as the utility of context and form for different languages. Thus, we select three representative types of models and design a series of experiments to reveal the universals and specifics of various word representations on decoding linguistic properties. Our work contributes to shedding new insights into the following topics: a) How do typological differences of language structure influence a word embedding mod- el? Does a model behave similarly towards phylogenetically-related languages? b) Is word form a more efficient predictor of a cer- tain grammatical function than word context for specific languages? c) How do the neurons of a model respond to linguistic features? Can we explain the utility of context and form by analyzing neuron activation pattern?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiment Design</head><p>To study the proposed questions above, we design four series of experiments to comprehensively compare context-based and character-based word representations on different languages, covering syntactic, morphological and semantic properties. The basic paradigm is to decode interpretable linguistic features from a target collection of word representations. We hypothesize that there exists a linear/nonlinear map between a word representation x and a high-level sparse feature vector y if the word vector implicitly encode sufficient information <ref type="bibr">1</ref> . Figure1 visualizes how a word embedding is mapped to different linguistic attribute vectors. For example, the Czech word dětem means children in English. Its grammatical gender is female. It is in the plural form and should be used in dative case. These are all impor- tant properties of a word. The word embedding of dětem is mapped to different sparse representation of these lexical properties respectively. Listed in Table1 is the outline of the experi- ments.  For linear map, we train a matrix Θ that maps word embedding x to a sparse feature vector y with the least L 2 error. For nonlinear map, we train a neural network (MLP) with 4 hidden layers via back propagation. Their dimensions are 50, 80, 80, and 50 in order. For each linguistic feature of each language, a mapping model is trained on the randomly-selected 90% of the words with the target feature and tested over the remaining 10%. Details about the construction of the linguistic feature vectors will be mentioned in the specific section of a certain experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID</head><p>For syntactic and morphological features, we construct the corresponding feature vectors of a word from the Universal Dependencies Treebank (Joakim Nivre and Zhu, 2015) and the Chinese Treebank (CTB 7.0) ( <ref type="bibr" target="#b24">Xue et al., 2010)</ref>. For a certain word w with a certain linguistic attribute a (e.g. POS), w may be annotated with one or different labels (e.g. NOUN, VERB, etc) from the possible label set of a in the whole treebank. We calculate the normalized label frequency distribu- tion y a w from the manual annotation of the corpus as the representation of the linguistic attribute a for the word w in each language.</p><p>For word sentiment feature, we use the manual- ly annotated data collected by <ref type="bibr" target="#b9">Dodds et al. (2015)</ref>. The data contains emotion scores for a list of words in several languages. In our experiment, the original score scale in <ref type="bibr" target="#b9">Dodds et al. (2015)</ref> is transformed into the interval <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Embedding Model Description</head><p>Faced with three questions proposed before, we select the following models from various candi- dates, as they are popular, representative and based on either word context or purely word form.</p><p>Type I C&amp;W Model (referred as CW in short), which aims to estimate the joint probability of a word sequence <ref type="bibr" target="#b6">(Collobert et al., 2011)</ref>. In this paper, C&amp;W word vectors are all from the released version of the polyglot multilingual embeddings (Al-Rfou et al., 2013) trained on Wikipedia.</p><p>Type II Skip-gram 2 (referred as SG in short), which aims to predict the context words based on the target word. We use word2vec ( <ref type="bibr" target="#b21">Mikolov et al., 2013</ref>) to train SG on multilingual Wikipediea provided by <ref type="bibr" target="#b0">(Al-Rfou et al., 2013</ref>).</p><p>Type III Character-based LSTM autoencoder (referred as AE in short), which takes the character sequence of a word as the input and reconstruct the input character sequence. It takes the advantage of pure word form instead of the context. The hidden layer vector of the model is used as a representation of the word. In this way, we are able to quantify the utility of pure word form by evaluating the representation generated from the character-based LSTM autoencoder on different decoding tasks. We trained one-hidden layer AE with the words covered in CW for each language independently.</p><p>To ensure a fair comparison, all the word vectors have the same dimension 64. CW and SG are trained with a common 5-word window size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Part-of-Speech</head><p>In experiment I, we decode Part-of-Speech, the most basic syntactic feature, from word embed-  . The accuracy is computed as:</p><formula xml:id="formula_0">acc = 1 |W | |W | i ∆(ˆ y a W i , y a W i ) (1) ∆(ˆ y a W i , y a W i ) = 1 ˆ y a W i = y a W i 0 otherwise (2)</formula><p>It is obvious that context-based representation (CW and SG) performs better than character-based representation (AE). We, however, notice that AE peforms nearly as well as the context-based embedding on Russian, Czech and Indonesian.  It turns out that these languages employ affix markers to indicate the POS category of a word. For example, in Indonesian, co-occurrence of the prefix 'me-' and the suffix '-kan' in the word form means that this word is a verb.</p><p>Besides, we explore the relationship between CW performances on decoding POS tags and the word order typology of different languages, since CW is sensitive to word order. We classify the languages into 8 types, based on the basic word order features (Order of Subject and Verb; Order of Object and Verb; Order of Noun and Adposition; Order of Noun and Relative clause) from the World Atlas of Language Structures <ref type="bibr">(Dryer and Haspelmath, 2013)</ref>. <ref type="figure" target="#fig_2">Figure 2</ref> shows that CW performs similar in this experiment for languages of the same word order type, indicating an implicit interaction between typological diver- sity and model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dependency Relation</head><p>In this section, we will get into the details of Experiment II: decoding dependency relation from word representation. Dependency relation refers to how a word is syntactically related to other words in a sentence. It is the label annotated on the arc of the dependency tree.</p><p>We compute the normalized frequency distri- bution of dependency relations for each word in the Universal Dependency Treebank and Chinese Treebank (CTB 7.0) ( <ref type="bibr" target="#b24">Xue et al., 2010</ref>). The distri- bution of dependency relations is the probabilistic distribution of different arc types, such as subject, object, nmod, etc. Evaluation is similar to that in Section 4.1.</p><p>We can see from <ref type="figure">Figure 3</ref> that the overall performance is worse than that in Experiment I, as dependency analysis is more difficult than POS induction. CW achieves the best performance. It is also interesting to see that all the embeddings work slightly better on Slavic languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Morphological Features</head><p>Experiment III aims to decode morphological information from various word representation. E- valuation is similar to that in Section 4.1. Morpho- logical information refers to the explicit marker of the grammatical functions. We consider 12 mor- phological features, as is shown in <ref type="table" target="#tab_1">Table 1</ref>. They can be split into 5 nominal features (GENDER, NUMBER, CASE, ANIMACY, DEFINITENESS) and 7 verbal features (PERSON, TENSE, ASPECT, MOOD, VOICE, PRONOUNTYPE, VERBFORM). Gender is a very special feature for western languages. It is partially based on semantics, such as biological sex. In most of the languages with gender features, there are agreements between the noun and the determiners. This could be a good indicator for context-based model. On the other hand, gender is also expressed as an inflectional feature via declension or umlaut, especially for adjectives and verbs. Therefore, we can see from <ref type="figure">Figure 4</ref> that the AE also achieves some good results without using context information.</p><p>From a typological perspective, we found that all the embeddings work well on decoding word gender of Romance languages (Italian, Spanish and Portuguese) but worst on Slavic languages (e.g. Czech, Slovenian). This is probably hi fa cs pl bg hr sl da sv no en el he it es pt ro fi et hu    <ref type="bibr">(adjective, verb)</ref> that have agreement with noun. The basic value can be singular, dual or plural. We can see from <ref type="figure" target="#fig_4">Figure 5</ref> that SG, CW and AE all perform well. AE performs almost as well as CW and SG on English, Spanish and Portuguese.</p><p>Case is one of the most significant features. Gender and number are indexical morphemes, which means that there is a phrase in the sentence that necessarily agrees with the target item. Case, on the contrary, is a relational morpheme, accord- ing to <ref type="bibr" target="#b7">(Croft, 2002</ref>). Case reflects the semantic role of a noun, relative to the pivot verb. All the languages studied in this paper, more or less, employ word inflection to explicitly express the specific case role. The model performances are listed in <ref type="table" target="#tab_7">Table 3</ref>.</p><p>We notice some important inter-language dif- ferences. Swedish has only two cases, nominal and genitive. The form of genitive case is very simple. Adding an s to the coda of a noun will change it to genitive case. Thus, we can see that character-based encoding performs well on Swedish. Since genitive case usually means possession, we also notice that context-based distributed representation also performs well in decoding case information from Swedish words. By classifying these languages into different morphological types in <ref type="table" target="#tab_7">Table 3</ref>, we find that word vectors of highly inflected fusional languages (e.g. Czech) performs worse than agglutinative lan- guages (e.g. Finnish). This is typically reflected in AE, as agglutinative languages simply concatenate the case marker with the nominative form of a noun. The morphological transformation of agglu- tinative languages is linear and simple. Besides, the case system of the analytic languages has been largely simplified due to historical change. Therefore, all the embeddings perform well on analytic languages. This evidence supports that morphological complexity is positively correlated with the quality of word embedding.</p><p>Besides, for fusional languages, using dis- tributed representation and context information would largely increase the performance. This, in turn, indicates that cases are a special semantic relations distributed in the words around the target noun. Although a case is not explicitly agreed with other components in an utterance, the word category might serve as a good indicator, such as preposition and verb.</p><p>Animacy is a special nominal feature in a few languages, which is used to discriminate alive and  animate objects from inanimate nouns. Generally, it is based on the lexical semantic feature. As is shown in <ref type="figure" target="#fig_6">Figure 6</ref>, it is easier to decode animacy from the context-based representations than character-based representation.</p><p>hi he cs pl bg hr sl da sv no en el fi et hu it es pt ro fa From <ref type="figure" target="#fig_6">Figure 6</ref>, 7, we can see that all the three models give quite perfect performance on decoding person, definiteness, tense, voice, mood, aspect, pronoun type and verb form.</p><p>Overall, character-based representation is most effective for Slavic languages on decoding verbal morphological features but not nominal features. The result is vice versa for Romance languages, which is not as morphologically complex as Slavic. It is worth noticing that models behave differently on Bulgarian, an analytic language, although Bulgarian belongs to Slavic language from the phylogenetic perspective. We think that this is because many morphological features in Bulgarian have been simplified or weakened.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Emotion Score</head><p>In Experiment IV, we use the manually annotated data collected by <ref type="bibr" target="#b9">Dodds et al. (2015)</ref>. The data contains emotion scores for a list of words in several languages. In our experiment, the original score scale is transformed into the interval <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. A nonlinear map is trained to regress the representation of a word (CW, SG, AE) to its emotion score.</p><p>To evaluate the predicted results, we measure the Spearman correlation between the gold scores and predicted scores. The result in <ref type="figure">Figure 8</ref> reveals a significantly strong correlation between the predicted emotion scores of SG and the real emotion scores. CW comes the second. For AE, it is hard to decode emotion just from the word form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Contrastive Analysis</head><p>As we have mentioned before, Type I C&amp;W model utilizes ordered context information to train the distributed word representation. Type II skip-gram model utilizes unordered context information. Type III character-based LSTM autoencoder model utilizes the grapheme infor- mation to represent a word. Towards the key questions that we raised at the very beginning of the paper, we propose our contrastive analysis based on the experiment results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Typology vs. Phylogeny</head><p>Experiment results have shown that word embed- ding models are influenced by the syntactic and morphological diversity more or less. Here we display how typological similarity and phyloge- netic relation is revealed from the observed model performance variation. We hierarchically cluster languages according to the model performance on decoding syntactic and morphological features. The dendrogram of the languages in <ref type="figure" target="#fig_8">Figure 9</ref> vividly shows that most of the phylogenetic- related languages are clustered together.</p><p>However, there is some interesting exceptions. Bulgarian does not form a primary cluster with other Slavic languages (e.g. Slovenian). We think that this is because Bulgarian is typologically dissimilar to Slavic language family. Therefore, <ref type="figure" target="#fig_8">Figure 9</ref> reflects that language typology explains the model variation better than language phyloge- ny.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Form vs. Context</head><p>Here we discuss the effectiveness of word form and different types of word context.</p><p>Regarding the correlation between context type and language function, previous results show that SG performs worse than CW on decoding POS and dependency relation while SG performs better than CW on decoding emotion score. Since CW keeps word order of the context, this comparison suggests that word order information is vital to syntactic information, but it might also be a kind of noise for the word vectors to encode semantic information.  Regarding the correlation between form and language function, previous results on POS, de- pendency relation and emotion scores show the effectiveness of the word context. However, for morphological features, results in <ref type="table" target="#tab_1">Table 10</ref> indi- cate that context-based word representation works slightly better than character-based representa- tion. Specifically, character-based embedding (AE) does outperform context-based embedding (CW, SG) on decoding verbal morphological fea- tures, even though AE does not access any context information. In other words, word form could be an explicit and discriminative cue for the model to decode the morphological feature of a word.</p><p>To prove that word form could provides infor- mative and explicit cues for grammatical function- s, we train another shuffled character-based word representation, which means that the autoencoder inputs shuffled letters and outputs the shuffled letters again. We use the hidden layer of the shuffled autoencoder as the representation for each word. The result in <ref type="table" target="#tab_10">Table 4</ref> shows that now the character-based model cannot perform as well as the original character-based autoencoder representation does, which again proves that the order of the word form is necessary for learning the grammatical function of a word.</p><p>Since many languages share similar phono- graphic writing systems, we naturally want to know whether the grapheme-phoneme knowledge from one language can be transferred to another language. We train an autoencoder purely on   <ref type="table">Table 5</ref>: Comparison of morpho-phonological knowledge transfer on different language pairs. The reconstruction accuracy is correlated with the overlapping proportion of grapheme patterns between source language and target language.</p><note type="other">Lan. Raw Shuf. Lan. Raw Shuf. Russian 0.</note><p>Finnish and directly test the trained model on memorizing raw English words, letter-shuffled English words and random letter sequences. Re- sults in <ref type="table">Table 5</ref> indicate that the character autoen- coder can successfully reconstruct raw English words instead of the letter-shuffled English words or random letter sequences. However, if we train an autoencoder purely on Arabic and then directly test the trained model on memorizing Urdu (ud) words or Persian (fa) words, the reconstruction accuracy is quite low, although Arabic, Persian and Urdu use the same Arabic writing system. To explain the behaviour of AE, we calculate the correlation between the bigram character fre- quency in the words of the training language (e.g. Finnish) and the bigram character frequency in the words of the testing language (e.g. English). <ref type="table">Table 5</ref> reveals that phonological knowledge can be transferred if two languages share similar bigram and trigram character frequency distribu- tion. For example, Finnish and English are both Indo-European language. Their writing system stores similar phonological structure. Arabic is a Semitic language. Persian is an Indo-European language. Their writing system stores different phonological structures respectively. This again proves that character-based LSTM autoencoder does 'memorize' the grapheme or phoneme clus- ters of a words. Morpho-phonological knowledge can be transferred among typologically-related languages.</p><p>Additionally, we are surprised to find that using the English word representations encoded    6587. This is probably due to the shared knowledge about the morphemes in the word form.</p><note type="other">School of Computer Science Fudan University Shanghai, PRC 200433 0</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Neuronal Activation Pattern</head><p>Le <ref type="formula">(2011)</ref> found out that it is empirically possible to learn a 'Grandmother neuron'-like face detector from unlabelled data. In their experiment on unlabeled data, one neuron learns to activate specifically towards the pictures with cat faces instead of other pictures. Based on this finding, we hypothesize that there should exist selective neuron activation towards a linguistic feature trigger. The feature trigger can be a special consonant cluster, a specific suffix or the syntactic category of a word.</p><p>To quantitatively show the collective neuron behaviours and the individual neuron response to- wards different linguistic trigger, we compute the maximum probability that a neuron discriminates the words with trigger f from the words without trigger f . We defined this probability as the Degree of Selectivity p. For a given neuron n in a given model M towards linguistic trigger f , we try to find a threshold t that maximizes p f,t ,</p><formula xml:id="formula_1">c f,t = N + f,t N f , c ¬f,t = N + ¬f,t N ¬f , Selectivity = p f,t = 2 × c f,t × c ¬f,t c f,t + c ¬f,t</formula><p>where N + f,t is the number of correctly discrim- inated words with linguistic feature f based on the threshold t. N f is the real number of words with linguistic feature f . N + ¬f,t is the number of correctly discriminated words without linguistic feature f based on the threshold t. N ¬f means the real number of words without linguistic feature f . c f,t / c ¬f,t is the accuracy for the neuron n of model M to detect the existence / nonexistence of the linguistic feature f . p f,t is the F-score of c f,t and c ¬f,t , indicating the degree to which a certain neuron discriminates the words with/without a certain trigger f at a certain threshold t.</p><p>After calculating the selectivity of 64 neurons in an embedding model towards a linguistic trigger f , we sort the neurons according to the value of selectivity and draw the curve in <ref type="figure" target="#fig_1">Figure 11</ref> for each model. The x-axis is the rank of the model neurons based on their selectivity towards a certain linguistic trigger. The y-axis is the selectivity of the corresponding neuron. The curve can tell us how many neurons selectively respond to trigger f to a certain degree. For example, we can see from <ref type="figure" target="#fig_1">Figure 11</ref> that the max selectivity of the AE neurons reaches nearly 0.9. This means that one neuron of the AE model is especially sensitive to the prefix 'Me-' and affix '-an'. It can detect the words with the prefix 'Me-' and the affix '-an' just from its activation pattern.</p><p>It is also interesting to see from <ref type="figure" target="#fig_1">Figure 11</ref> that neurons of AE respond more selectively to morphological triggers than those of the word- based model. For example, almost 30% of the AE neurons fall in the selectivity level [0.7, 1] towards the verb marker, namely prefix 'Me-' and affix '-an', in Indonesian. Context-based model also shows some selectivity towards this morphological triggers. For SG model, the max selectivity of the model neurons is only just above 0.7.</p><p>On the contrary, the context-based distributed models showed strong selective activation towards country names in Indonesian. However, the selectivity of all the AE neurons is below 0.7 towards these semantically-related words.</p><p>Similar patterns are found also in other lan- guages. We conclude that the character-based model captures much morphological information / syntactic marker than semantic information. The popular word-based model captures both semantic information and syntactic information, although the latter is not displayed as explicitly as the former.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related works</head><p>There have been a lot of research on interpreting or relating word embedding with linguistic features. <ref type="bibr" target="#b25">Yogatama et al. (2014)</ref> projects word embedding into a sparse vector. They found some linguisti- cally interpretable dimensions.  use linguistic features to build word vector. Their results show that these representation of word meaning can also achieve good performance in the analogy and similarity tasks. These work can be regarded as the foreshadowing of our experiment paradigm that mapping dense vector to a sparse linguistic property space.</p><p>Besides, a lot of study focus on empirical comparison of different word embedding model. <ref type="bibr" target="#b20">Melamud et al. (2016)</ref> investigates the influence of context type and vector dimension on word embedding. Their main finding is that concate- nating two different types of embeddings can still improve performance even if the utility of dimensionality has run out. <ref type="bibr" target="#b2">Andreas and Klein (2014)</ref> assess the potential syntactic information encoded in word embeddings by directly apply word embeddings to parser and they concluded that embeddings add redundant information to what the conventional parser has already extract- ed. <ref type="bibr" target="#b23">Tsvetkov et al. (2015)</ref> propose a method to evaluate word embeddings through the alignment of distributional vectors and linguistic word vec- tors. However, the method still lacks a direct and comprehensive investigation of the utility of form, context and language typological diversity. This is exactly our novelty and contribution.</p><p>It is worth noticing that Köhn (2015) evalu- ates multilingual word embedding and compares skip-gram, language model and other competitive embedding models. They show that dependency- based skip-gram embedding is effective, even at low dimension. Although <ref type="bibr">Köhn (2015)</ref> work involves different languages, they focus on the similarity among multilingual embeddings with only 7 languages. Our work, however, not only provides a comprehensive investigation with massive language samples (30 for Experiment I) and nonlinear mapping models, but also reveal the utility of pure word form and novelly point out the cross-language differences in word representation, which have been overlooked by huge amount of monolingual/bilingual research on well-studied languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we quantify the utility of word form and the effect of language typological di- versity in learning word representations. Cross- language perspective and novel analysis of neuron behaviours provide us with new evidence about the typological universal and specific revealed in word embedding. We summarize from our experiments on a massive set of languages that:</p><p>• Language typological diversity, especially the specific word order type and morphologi- cal complexity, does influence how linguistic information is encoded in word embedding.</p><p>• It is plausible (and sometimes even better) to decode grammatical function just from the word form, for certain inflectional languages.</p><p>• Quantification of neuron activation pattern reveals different characteristics of the context-based model and the character-based counterpart.</p><p>Therefore, we think that it is necessary to maximize both the utility of word form and the advantage of the context for a better word representation. It would also be a promising direction to incorporate the factor of language typological diversity when designing advanced word representation model for languages other than English.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>··· dětem (chidren.Female.Plural.Dative)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Visualizing experiment paradigm. The dense representation of a Czech word dětem is mapped to different sparse representation of the lexical properties respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Interaction between CW performances on decoding POS tag and WALS word order features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Comparison of models on decoding DEPENDENCY RELATION.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Model comparison on decoding NUMBER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Model comparison on decoding PERSON, DEFINITENESS and ANIMACY.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Model comparison on TENSE, VOICE, MOOD, ASPECT, PRONTYPE and VERBFORM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Comparison of the tree based on model performances and the WALS dendrogram manually constructed by linguists.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Overall performances of different models (averaged over languages) on decoding morphological features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Visualising the Neuron activation pattern for different word embedding models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Outline of Experiment Design.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Model comparison on decoding POS, 
along with WALS word-order features. Type 
I: VS+VO+Pre+NR. II: SV+VO+Pre+RN. III: 
SV+OV+Pre+NR. IV: SV+OV+Post+RN/Co. V: 
SV+OV+Post+NR. VI: SV+ND+Pre+NR. VII: 
SV+VO+Pre+NR. VIII: ND+VO+Pre+NR. 

ding. To construct the POS vector for each word, 
we calculate the normalized POS-tag frequency 
distribution from the manual annotation of the U-
niversal Dependencies (Version 1.2) (De Marneffe 
et al., 2014) and Chinese Treebank (CTB 7.0) 
(Xue et al., 2010) for each language. 
We evaluate the predicted results by judging 
whether the most probable POS tag of a word 
predicted by the model equals to the most probable 
correct POS tag of the word. Formally, for a set of 
words W in a language, the correct tag of the i th 
word W i is y a 

W i 

and the predicted tag isˆyisˆ isˆy a 

W i 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Model comparison on decoding CASE. 

because that Romance languages employ regular 
rules to judge the gender of a word. However, 
Slavic languages have other nonlinear fusional 
morphological features that are not easy to tackle. 
Number refers to the linguistic abstraction of 
objects' quantities. It is an inflectional feature of 
noun and other parts of speech </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="true"><head>Table 4 : Comparison of original and shuffled character-based word representation on decoding POS tag.</head><label>4</label><figDesc></figDesc><table>Source Language 
Arabic 
Finnish 
Target Language 
fa 
ud 
en 
shuf en 
rand 
Bigram type overlap. 
0.176 
0.761 
0.891 
0.864 
0.648 
Bigram token overlap. 
0.689 
0.881 
0.999 
0.993 
0.650 
Trigram type overlap. 
0.523 
0.522 
0.665 
0.449 
0.078 
Trigram token overlap. 
0.526 
0.585 
0.978 
0.796 
0.078 

Reconstruction Acc. 
0.586 
0.689 
0.95 
0.83 
0.22 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>64 Neurons of the Model</head><label>64</label><figDesc></figDesc><table>Select 

AE 

(a) Me-kan 

0 
20 
40 
60 

0.2 

0.4 

0.6 

64 Neurons of the Model 

Select 

CW 

(b) -nya 

0 
20 

0.4 

0.6 

64 Neuro 

Select 

CW 

(c) -ind 

0 
20 
40 
60 
0.4 

0.6 

0.8 

1 

64 Neurons of the Model 

Selectivity 

CW 
SG 
AE 

(d) country name 

0 
20 
40 
60 
0.4 

0.6 

0.8 

64 Neurons of the Model 

Selectivity 

AE 
SG 
CW 

(e) verb 

−1 
−0.5 

0 

0.5 

1 

1.5 

2 

Threshold t 

Ac 

(f) Neuron act 

 *  Student ID: 15210240086 

1 

(b) Country names 

0 
20 
40 
60 

0.6 

64 Neurons of the Model 

Select 

AE 

(a) Me-kan 

0 
20 
40 
60 

0.2 

0.4 

0.6 

64 Neurons of the Model 

Selecti 

CW 

(b) -nya 

0 
20 
40 
60 

0.4 

0.6 

64 Neurons of the Model 

Selecti 

CW 

(c) -indonesia 

0 
20 
40 
60 
0.4 

0.6 

0.8 

1 

64 Neurons of the Model 

Selectivity 

CW 
SG 
AE 

(d) country name 

0 
20 
40 
60 
0.4 

0.6 

0.8 

64 Neurons of the Model 

</table></figure>

			<note place="foot" n="1"> Our experiment results show that nonlinear mapping model significantly works better than linear map for all languages. Only nonlinear mapping accuracies are mentioned in the following sections due to the space limit.</note>

			<note place="foot" n="2"> SG results for some languages are missed due to the lack of the corpus data or special preprocessing.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for their valuable comments. This work was partially funded by National Natural Science Foundation of China (No. 61532011, 61473092, and 61472088), the National High Technology Research and Development Program of China (No. 2015AA015408).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting><address><addrLine>Sofia, Bulgaria, August</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How much do word embeddings encode about syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="822" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved transition-based parsing by modeling characters instead of words with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Linguistically na¨ıvena¨ıve!= language independent: why nlp needs linguistic typology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Emily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL</title>
		<meeting>the EACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<title level="m">Workshop on the Interaction between Linguistics and Computational Linguistics: Virtuous, Vicious or Vacuous?, pages 26-32. Association for Computational Linguistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Typology and universals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Croft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Universal stanford dependencies: A cross-linguistic typology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katri</forename><surname>Haverinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="4585" to="4592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human language reveals a universal positivity bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Peter Sheridan Dodds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suma</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Desu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><forename type="middle">Ryland</forename><surname>Andrew J Reagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kameron Decker</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">P</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bagrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2389" to="2394" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nondistributional word vector representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Morphological inflection generation using character sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributional structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese Language Library</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Universal dependencies 1.2</title>
	</analytic>
	<monogr>
		<title level="m">LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics</title>
		<editor>Maria Jesus Aranzabe Masayuki Asahara Aitziber Atutxa Miguel Ballesteros John Bauer Kepa Bengoetxea Riyaz Ahmad Bhat Cristina Bosco Sam Bowman Giuseppe G. A. Celano Miriam Connor Marie-Catherine de Marneffe Arantza Diaz de Ilarraza Kaja Dobrovoljc Timothy Dozat Tomaž Erjavec Richárd Farkas Jennifer Foster Daniel Galbraith Filip Ginter Iakes Goenaga Koldo Gojenola Yoav Goldberg Berta Gonzales Bruno Guillaume Jan Hajič Dag Haug Radu Ion Elena Irimia Anders Johannsen Hiroshi Kanayama Jenna Kanerva Simon Krek Veronika Laippala Alessandro Lenci Nikola Ljubeši´Ljubeši´c Teresa Lynn Christopher Manning Ctlina Mrnduc David Mareček Héctor Martínez Alonso Jan Mašek Yuji Matsumoto Ryan McDonald Anna Missilä Verginica Mititelu Yusuke Miyao Simonetta Montemagni Shunsuke Mori Hanna Nurmi Petya Osenova Lilja Øvrelid Elena Pascual Marco Passarotti Cenel-Augusto Perez Slav Petrov Jussi Piitulainen Barbara Plank Martin Popel Prokopis Prokopidis Sampo Pyysalo Loganathan Ramasamy Rudolf Rosa Shadi Saleh Sebastian Schuster Wolfgang Seeker Mojgan Seraji Natalia Silveira Maria Simi Radu Simionescu Katalin Simkó Kiril Simov Aaron Smith Jaň Stěpánek Alane Suhr Zsolt Szántó Takaaki Tanaka Reut Tsarfaty Sumire Uematsu Larraitz Uria Viktor Varga Veronika Vincze ZdeněkZdeněkˇZdeněkŽabokrtsk´ZdeněkŽabokrtsk´y Daniel Zeman Joakim Nivre, ˇ Zeljko Agi´cAgi´c and Hanzhi Zhu</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Charles University in Prague</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Whats in an embedding? analyzing word embeddings through multilingual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Köhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Poceedings of EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05523</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="8595" to="8598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04586</idno>
		<title level="m">Character-based neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The role of context types and dimensionality in learning word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluation of word vector representations by subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuhong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiyu</forename><surname>Chang</surname></persName>
		</author>
		<title level="m">Chinese treebank 7.0. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning word representations with hierarchical sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
