<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic Graph-based Dependency Parsing with Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic Graph-based Dependency Parsing with Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1382" to="1392"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents neural probabilistic parsing models which explore up to third-order graph-based parsing with maximum likelihood training criteria. Two neural network extensions are exploited for performance improvement. Firstly, a convo-lutional layer that absorbs the influences of all words in a sentence is used so that sentence-level information can be effectively captured. Secondly, a linear layer is added to integrate different order neu-ral models and trained with perceptron method. The proposed parsers are evaluated on English and Chinese Penn Tree-banks and obtain competitive accuracies.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural network methods have shown great promise in the field of parsing and other related natural language processing tasks, exploiting more complex features with distributed representation and non-linear neural network ( <ref type="bibr" target="#b36">Wang et al., 2013;</ref><ref type="bibr" target="#b37">Wang et al., 2014;</ref><ref type="bibr" target="#b0">Cai and Zhao, 2016;</ref><ref type="bibr" target="#b38">Wang et al., 2016)</ref>. In transition-based dependency pars- ing, neural models that can represent the partial or whole parsing histories have been explored <ref type="bibr" target="#b39">(Weiss et al., 2015;</ref><ref type="bibr" target="#b8">Dyer et al., 2015)</ref>. While for graph- based parsing, on which we focus in this work, <ref type="bibr" target="#b30">Pei et al. (2015)</ref> also show the effectiveness of neural methods.</p><p>The graph-based parser generally consists of two components: one is the parsing algorithm for inference or searching the most likely parse tree, the other is the parameter estimation approach for the machine learning models. For the former, clas- sical dynamic programming algorithms are usu- ally adopted, while for the latter, there are vari- ous solutions. Like some previous neural methods ( <ref type="bibr" target="#b31">Socher et al., 2010;</ref><ref type="bibr" target="#b33">Socher et al., 2013)</ref>, to tackle the structure prediction problems, <ref type="bibr" target="#b30">Pei et al. (2015)</ref> utilize a max-margin training criterion, which does not include probabilistic explanations. Re-visiting the traditional probabilistic criteria in log-linear models, this work utilizes maximum likelihood for neural network training. <ref type="bibr" target="#b7">Durrett and Klein (2015)</ref> adopt this method for constituency pars- ing, which scores the anchored rules with neu- ral models and formalizes the probabilities with tree-structured random fields. Motivated by this work, we utilize the probabilistic treatment for de- pendency parsing: scoring the edges or high-order sub-trees with a neural model and calculating the gradients according to probabilistic criteria. Al- though scores are computed by a neural network, the existing dynamic programming algorithms for gradient calculation remain the same as those in log-linear models.</p><p>Graph-based methods search globally through the whole space for trees and get the highest- scored one, however, the scores for the sub-trees are usually locally decided, considering only sur- rounding words within a limited-sized window. Convolutional neural network (CNN) provides a natural way to model a whole sentence. By in- troducing a distance-aware convolutional layer, sentence-level representation can be exploited for parsing. We will especially verify the effec- tiveness of such representation incorporated with window-based representation.</p><p>Graph-based parsing has a natural extension through raising its order and higher-order parsers usually perform better. In previous work on high- order graph-parsing, the scores of high-order sub- trees usually include the lower-order parts in their high-order factorizations. In traditional linear models, combining scores can be implemented by including low-order features. However, for neural models, this is not that straightforward because of nonlinearity. A straightforward strategy is simply adding up all the scores, which in fact works well; another way is stacking a linear layer on the top of the representation from various already-trained neural parsing models of different orders. This paper presents neural probabilistic mod- els for graph-based projective dependency pars- ing, and explores up to third-order models. Here are the three highlights of the proposed methods:</p><p>• Probabilistic criteria for neural network train- ing. (Section 2.2)</p><p>• Sentence-level representation learned from a convolutional layer. (Section 3.2)</p><p>• Ensemble models with a stacked linear out- put layer. (Section 3.3)</p><p>Our main contribution is exploring sub-tree scor- ing models which combine local features with a window-based neural network and global features from a distance-aware convolutional neural net- work. A free distribution of our implementation is publicly available 1 . The remainder of the paper is organized as fol- lows: Section 2 explains the probabilistic model for graph-based parsing, Section 3 describes our neural network models, Section 4 presents our ex- periments and Section 5 discusses related work, we summarize this paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Probabilistic Graph-based Dependency Parsing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph-based Dependency Parsing</head><p>Dependency parsing aims to predict a dependency tree, in which all the edges connect head-modifier pairs. In graph-based methods, a dependency tree is factored into sub-trees, from single edge to mul- tiple edges with different patterns; we will call these specified sub-trees factors in this paper. Ac- cording to the sub-tree size of the factors, we can  define the order of the graph model. Three differ- ent ordered factorizations considered in this work and their sub-tree patterns are shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>The score for a dependency tree (T ) is defined as the sum of the scores of all its factors (p):</p><formula xml:id="formula_0">Score(T ) = p∈T Score(p)</formula><p>In this way, the dependency parsing task is to find a max-scoring tree. For projective depen- dency parsing considered in this work, this search- ing problem is conquered by dynamic program- ming algorithms with the key assumption that the factors are scored independently. Previous work <ref type="bibr" target="#b9">(Eisner, 1996;</ref><ref type="bibr" target="#b26">McDonald et al., 2005;</ref><ref type="bibr" target="#b25">McDonald and Pereira, 2006;</ref><ref type="bibr" target="#b16">Koo and Collins, 2010;</ref><ref type="bibr" target="#b22">Ma and Zhao, 2012</ref>) explores ingenious algorithms for de- coding ranging from first-order to higher-orders. Our proposed parsers also take these algorithms as backbones and use them for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Probabilistic Model</head><p>With the graph factorization and inference, the re- maining problems are how to obtain the scores and how to train the scoring model. For the scor- ing models, traditional linear methods utilize man- ually specified features and linear scoring mod- els, while we adopt neural network models, which may exploit better feature representations.</p><p>For the training methods, in recent neural graph-based parsers, non-probabilistic margin- based methods are usually used. However, follow- ing the maximum likelihood criteria in traditional log-linear models, we can treat it in a probabilistic way. In fact, the probabilistic treatment still uti- lizes the scores of sub-tree factors in graph mod- els. As in log-linear models like Conditional Ran- dom Field (CRF) ( <ref type="bibr" target="#b17">Lafferty et al., 2001</ref>), the expo- nentials of scores are taken before re-normalizing, and the probability distribution over trees condi-tioned on a sentence X is defined as follows:</p><formula xml:id="formula_1">Pr(T |X, θ) = 1 Z(X) exp(Score(T |θ)) Z(X) = T exp(Score(T |θ))</formula><p>where θ represents the parameters and Z(X) is the re-normalization partition function. The intuition is that the higher the score is, the more potential or mass it will get, leading to higher probability. The training criteria will be log-likelihood in the classical setting of maximum likelihood esti- mation, and we define the loss for a parse tree as negative log-likelihood:</p><formula xml:id="formula_2">L(θ) = − log Pr(T g |X, θ) = −Score(T g |θ) + log(Z(X))</formula><p>where T g stands for the golden parse tree. Now we need to calculate the gradients of θ according to gradient-based optimization. Focusing on the second term, we have (some conditions are left out for simplicity):</p><formula xml:id="formula_3">∂ log(Z(X)) ∂θ = T Pr(T ) p∈T ∂Score(p) ∂θ = p ∂Score(p) ∂θ T ∈T (p) Pr(T )</formula><p>Here, T (p) is the set of trees that contain the fac- tor p, and the inner summation is defined as the marginal probability m(p):</p><formula xml:id="formula_4">m(p) = T ∈T (p) Pr(T )</formula><p>which can be viewed as the mass of all the trees containing the specified factor p. The calculation of m(p) <ref type="bibr" target="#b29">(Paskin, 2001;</ref><ref type="bibr" target="#b23">Ma and Zhao, 2015</ref>) is solved by a variant of inside-outside algorithm, which is of the same complexity compared with the corresponding inference algorithms. Finally, the gradients can be represented as:</p><formula xml:id="formula_5">∂L(θ) ∂θ = p ∂Score(p) ∂θ − p ∈ T g + m(p)</formula><p>where [p ∈ T g ] is a binary value which indicates whether p is in tree T g . Traditional models usually utilize linear func- tions for the Score function, which might need carefully feature engineering such as ( <ref type="bibr" target="#b45">Zhao et al., 2009a;</ref><ref type="bibr" target="#b46">Zhao et al., 2009b;</ref><ref type="bibr" target="#b47">Zhao et al., 2009c;</ref><ref type="bibr" target="#b49">Zhao, 2009;</ref>), while we adopt neural models with the probabilistic training crite- ria unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Criteria</head><p>We take a further look between the maximum- likelihood criteria and the max-margin criteria. For the max-margin method, the loss is the differ- ence between the scores of the golden tree and a predicted tree, and its sub-gradient can be written in a similar form:</p><formula xml:id="formula_6">∂L m (θ) ∂θ = p ∂Score(p) ∂θ − p ∈ T g + p ∈ T b</formula><p>Here, the predicted tree T b is the best-scored tree with a structured margin loss in the score.</p><p>Comparing the derivatives, we can see that the one of probabilistic criteria can be viewed as a soft version of the max-margin criteria, and all the possible factors are considered when calcu- lating gradients for the probabilistic way, while only wrongly predicted factors have non-zero sub- gradients for max-margin training. This observa- tion is not new and <ref type="bibr" target="#b13">Gimpel and Smith (2010)</ref> pro- vide a good review of several training criteria. It might be interesting to explore the impacts of dif- ferent training criteria on the parsing performance, and we will leave it for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Labeled Parsing</head><p>In a dependency tree, each edge can be given a la- bel indicating the type of the dependency relation, this labeling procedure can be integrated directly into the parsing task, instead of a second pass af- ter obtaining the structure.</p><p>For the probabilistic model, integrating labeled parsing only needs some extensions for the in- ference procedure and marginal probability cal- culations. For the simplicity, we only consider a single label for each factor (even for high-order ones) which corresponds to Model 1 in <ref type="bibr" target="#b21">(Ma and Hovy, 2015)</ref>: the label of the edge between head and modifier word, which will only multiply O(l) to the complexity. We find this direct approach not only achieves labeled parsing in one pass, but also improves unlabeled attachment accuracies (see Section 4.3), which may benefit from the joint learning with the labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Model</head><p>The task for the neural models is computing the labeled scores of the factors. The inputs are the words in a factor with contexts, and the outputs are the scores for this factor to be valid in the de- pendency tree. We propose neural models to in- </p><formula xml:id="formula_7">h 1 = tanh(W 1 h 0 + b 1 ) h 0 Figure 2:</formula><p>The architecture for the basic model (second order parsing).</p><p>tegrate features from both local word-neighboring windows and the entire sentence, and furthermore explore ensemble models with different orders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Local Model</head><p>Architecture The basic model uses a window- based approach, which includes only surround- ing words for the contexts. <ref type="figure">Figure 2</ref> illustrates a second-order sibling model and models of other orders adopt similar structures. It is simply a stan- dard feed-forward neural network with two hid- den layers (h 1 and h 2 ) above the embedding layer (h 0 ), the hidden layers all adopt tanh activation function, and the output layer (noted as s) directly represents the scores for different labels.</p><p>Feature Sets All the features representing the input factor are atomic and projected to embed- dings, then the embedding layer is formed by con- catenating them. There are three categories of fea- tures: word forms, POS (part-of-speech) tags and distances. For each node in the factor, word forms and POS tags of the surrounding words in a spec- ified window are also considered. Special tokens for start or end of sentences, root node and un- known words are added for both word forms and POS tags. Distances can be negative or positive to represent the relative positions between the factor nodes in surface string. Take the situation for the second-order model as an example, there are three nodes in a factor: h for head, m for modifier and s for sibling. When considering three-word win- dows, there will be three word forms and three tags for each node and its surrounding context. m and s both have one distance feature while h does not have one as its parent does not exist in the factor.</p><p>Training As stated in Section 2.2, we use the maximum likelihood criteria. Moreover, we add two L2-regularizations: one is for all the weights θ (biases and embeddings not included) to avoid over-fitting and another is for preventing the final output scores from growing too large. The for- mer is common practice for neural network, while the latter is to set soft limits for the norms of the scores. Although the second term is not usually adopted, it directly puts soft constraints on the scores and improves the accuracies (about 0.1% for UAS/LAS overall) according to our primary experiments. So the final loss function will be:</p><formula xml:id="formula_8">L (θ) = p Score(p) · − p ∈ T g + m(p) + λ s · Score(p) 2 + λ m · θ 2</formula><p>where λ m and λ s respectively represent regular- ization parameters for model and scores. The training process utilizes a mini-batched stochastic gradient descent method with momentum.</p><p>Comparisons Our basic model resembles the one of <ref type="bibr" target="#b30">Pei et al. (2015)</ref>, but with some ma- jor differences: probabilistic training criteria are adopted, the structures of the proposed networks are different and direction information is encoded in distance features. Moreover, they simply av- erage embeddings in specified regions for phrase- embedding, while we will include sentence- embedding in convolutional model as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolutional Model</head><p>To encode sentence-level information and obtain sentence embeddings, a convolutional layer of the whole sentence followed by a max-pooling layer is adopted. However, we intend to score a factor in a sentence and the position of the nodes should also be encoded. The scheme is to use the distance embedding for the whole convolution window as the position feature. We will take the second-order model as an ex- ample to introduce the related operations. <ref type="figure">Figure  3</ref> shows the convolution operation for a convo- lution window, the input atomic features are the word forms and POS tags for each word inside the window, and the distances of only the center word (assuming an odd-sized window) to spec- ified nodes in the factor are adopted as position features. In the example, "game-good-a" is to be scored as a second-order sibling factor, and for a </p><formula xml:id="formula_9">Output Lexical distance v ' l = W l v l + b l v l v ' d = W d v d + b d v d d h =-3 d m =-1 d s =-2</formula><p>Figure 3: The operations for one convolution win- dow (second order parsing). convolution window of "This is a", word forms and corresponding POS tags are projected to em- beddings and concatenated as the lexical vector v l , the distances of the center word "is" to all the three nodes in the factor are also projected to embed- dings and concatenated as the distance vector v d , then these two vectors go through difference linear transformations into the same dimension and are combined together through element-wise addition or multiplication.</p><p>In general, assuming after the projection layer, embeddings of the word forms and POS tags of the sentence are represented as [w 0 , w 1 , ..., w n−1 ] and [p 0 , p 1 , ..., p n−1 ]. Those embeddings in the basic model may be reused here by sharing the em- bedding look-up table. The second-order sibling factor to be scored has nodes with indexes of m (modifier), h (head) and s (sibling). The distance embeddings are denoted by d, which can be either negative or positive. These distance embeddings are different from the ones in the basic model, be- cause here we measure the distances between the convolution window (its center word) and factor nodes, while the distances between nodes inside the factors are measured in the basic model. For a specified window [i : j], always assuming an odd number sized window, and the center token is indexed to c = i+j 2 , the v l and v d are obtained through simple concatenation:</p><formula xml:id="formula_10">v l = [w i , p i , w i+1 , p i+1 , ..., w j , p j ] v d = [d c−h , d c−m , d c−s ]</formula><p>then v l and v d go through difference linear trans- formations into same dimension space: v l , v d ∈ R n , where n is also the dimension of the output vector v o for the window. The linear operations can be expressed as:</p><formula xml:id="formula_11">v l = W l · v l + b l v d = W d · v d + b d</formula><p>The final vector v o is obtained by element-wise operations of v l and v d . We consider two strate- gies: (1) add: simple element-wise addition, (2) mul: element-wise multiplication with v d acti- vated by tanh. They can be formalized as:</p><formula xml:id="formula_12">v o-add = v l ⊕ v d v o-mul = v l tanh(v d )</formula><p>All the windows whose center-located word is valid (exists) in the sentence are considered and we will get a sequence of convolution outputs whose number is the same as the sentence length. The convolution outputs (all v o ) are collapsed into one global vector v g using a standard max-pooling operation. Finally, for utilizing the sentence-level representation in the basic model, we can either replace the original first hidden layer h 1 with v g or concatenate v g to h 1 for combining local and global features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ensemble Models</head><p>For higher-order dependency parsing, it is a stan- dard practice to include the impact of lower-order parts in the scoring of higher-order factors, which actually is an ensemble method of different order models for scoring.</p><p>A simple adding scheme is often used. For non- linear neural models, we use an explicit adding method. For example, in third-order parsing, the final score for the factor (g, h, m, s) will be:</p><formula xml:id="formula_13">s add (g, h, m, s) = s o3 (g, h, m, s) + s o2 (h, m, s) + s o1 (h, m)</formula><p>Here, g, h, m and s represent the grandparent, head, modifier and sibling nodes in the grand- sibling third-order factor; s o1 , s o2 and s o3 stand for the corresponding lower-order scores from first, second and third order models, respectively.</p><p>We notice that ensemble or stacking methods for dependency parsing have explored in previous work <ref type="bibr" target="#b28">(Nivre and McDonald, 2008;</ref><ref type="bibr" target="#b34">Torres Martins et al., 2008)</ref>. Recently, Weiss et al. (2015) stack a linear layer for the final scoring in a single model, and we extend this method to combine multiple models by stacking a linear layer on their output and hidden layers. The simple adding scheme can be viewed as adopting a final layer with specially fixed weights.</p><p>For each model to be combined, we concatenate the output layer and all hidden layers (except em- bedding layer h 0 ):</p><formula xml:id="formula_14">v all = [s, h 1 , h 2 ]</formula><p>All v all from different models are again concate- nated to form the input for the final linear layer and the final scores are obtained through a linear transformation (no bias adding):</p><formula xml:id="formula_15">v combine = [v all-o1 , v all-o2 , v all-o3 ] s combine = W combine · v combine</formula><p>We no longer update weights for the underlying neural models, and the learning of the final layer is equally training a linear model, for which struc- tured average perceptron <ref type="bibr" target="#b5">(Collins, 2002;</ref><ref type="bibr" target="#b3">Collins and Roark, 2004</ref>) is adopted for simplicity.</p><p>This ensemble scheme can be extended in sev- eral ways which might be explored in future work: (1) feed-forward network can be stacked rather than a single linear layer, (2) traditional sparse fea- tures can also be concatenated to v combine to com- bine manually specified representations with dis- tributed neural representations as in ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The proposed parsers are evaluated on English Penn Treebank (PTB) and Chinese Penn Tree- bank (CTB). Unlabeled attachment scores (UAS), labeled attachment scores (LAS) and unlabeled complete matches (CM) are the metrics. Punctu- ations 2 are ignored as in previous work <ref type="bibr" target="#b16">(Koo and Collins, 2010;</ref><ref type="bibr" target="#b42">Zhang and Clark, 2008)</ref>.</p><p>For English, we follow the splitting conven- tion for PTB3: sections 2-21 for training, 22 for developing and 23 for test. We prepare three datasets of PTB, using different conversion tools: (1) Penn2Malt <ref type="bibr">3</ref> and the head rules of <ref type="bibr" target="#b40">Yamada and Matsumoto (2003)</ref>, noted as PTB-Y&amp;M; (2) de- pendency converter in Stanford parser v3.3.0 with Stanford Basic Dependencies <ref type="bibr" target="#b6">(De Marneffe et al., 2006</ref>), noted as PTB-SD; (3) LTH Constituent- to-Dependency Conversion Tool 4 <ref type="bibr" target="#b14">(Johansson and Nugues, 2007)</ref>, noted as PTB-LTH. We use Stan- ford POS tagger ( <ref type="bibr" target="#b35">Toutanova et al., 2003</ref>) to get predicted POS tags for development and test sets, and the accuracies for their tags are 97.2% and 97.4%, respectively.</p><p>For Chinese, we adopt the splitting convention for CTB5 described in <ref type="bibr" target="#b42">(Zhang and Clark, 2008)</ref>. The dependencies (noted as CTB), are converted with the Penn2Malt converter. Gold segmentation and POS tags are used as in previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Settings</head><p>Settings of our models will be described in this sub-section, including pre-processing and initial- izations, hyper-parameters, and training details.</p><p>We ignore the words that occur less than 3 times in the training treebank and use a special token to replace them. For English parsing, we initial- ize word embeddings with word vectors trained on Wikipedia using word2vec ( <ref type="bibr" target="#b27">Mikolov et al., 2013)</ref>; all other weights and biases are initialized ran- domly with uniform distribution.</p><p>For the structures of neural models, all the em- beddings (word, POS and distances) have dimen- sions of 50. For basic local models, h 1 and h 2 are set to 200 and 100, and the local window size is set to 7. For convolutional models, a three-word-sized window for convolution is specified, and convolu- tion output dimension (number of filters) is 100. When concatenating the convolution vector (after pooling) to h 1 , it will make the first hidden layer's dimension 300.</p><p>For the training of neural network, we set the initial learning rate to 0.1 and the momentum to 0.6. After each iteration, the parser is tested on the development set and if the accuracy decreases, the learning rate will be halved. The learning rate will also be halved if no decreases of the accuracy for three epochs. We train the neural models for 12 epochs and select the one that performs best on the development set. The regularization parame- ters λ m and λ s are set to 0.0001 and 0.001. For the perceptron training of the ensemble model, only one epoch is enough based on the results of the development set.</p><p>The runtime of the model is influenced by the hyper-parameter setting. According to our ex- periments, using dual-core on 3.0 GHz i7 CPU, the training costs 6 to 15 hours for different-order models and the testing is comparably efficient as recent neural graph-parsers.  <ref type="table">Table 1</ref>: Effects of the components, on PTB-SD development set.</p><p>convolution model approximately takes up 40% of all computations. The convolution operation indeed costs more, but the lexical parts v l of the convolution do not concern the factors and are computed only once for one sentence, which makes it less computationally expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pruning</head><p>For high-order parsing, the computation cost rises in proportion to the length of the sentence, and it will be too expensive to calculate scores for all the factors. Fortunately, many edges are quite un- likely to be valid and can be pruned away using low-order models. We follow the method of <ref type="bibr" target="#b16">Koo and Collins (2010)</ref> and directly use the first-order probabilistic neural parser for pruning. We com- pute the marginal probability m(h, m) for each edge and prune away the edges whose marginal probability is below × max h m(h , m). means the pruning threshold that is set to 0.0001 for second-order. For third-order parsing, considering the computational cost, we set it to 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Analysis</head><p>This section presents experiments to verify the ef- fectiveness of the proposed methods and only the PTB-SD development set will be used in these ex- periments, which fall into three groups concerning basic models, convolutional models and ensemble ones, as shown in <ref type="table">Table 1</ref>. The first group focuses on the basic local mod- els of first order. The first two, Unlabeled and Labeled, do not use pre-training vectors for ini- tialization, while the third, Labeled+pre-training, utilizes them. The Unlabeled does not utilize the labels in training set and its model only gives one dependency score (we do not train a second stage labeling model, so the LAS of the unlabeled one is not available) and the Labeled directly pre- dicts the scores for all labels. We can see that labeled parsing not only demonstrates the conve- nience of outputting dependency relations and la- bels for once, but also obtains better parsing per- formances. Also, we observe that pre-trained word vectors bring slight improvements. Pre-trained initialization and labeled parsing will be adopted for the next two groups and the rest experiments.</p><p>Next, we explore the effectiveness of the CNN enhancement. In the four entries of this group, concatenate or replace means whether to concate- nate the sentence-level vector v g to the first hid- den layer h 1 or just replace it (just throw away the representation from basic models), add or mul means to use which way for attaching distance in- formation. Surprisingly, simple adding method surpasses the more complex multiplication-with- activation method, which might indicate that the direct activation operation may not be suitable for encoding distance information. With no surprises, the concatenating method works better because it combines both the local window-based and global sentence-level information. We also explore the influences of the convolution operations on depen- dencies of different lengths, as shown in <ref type="figure" target="#fig_1">Figure  4</ref>, the convolutional methods help the decisions of long-range dependencies generally. For the high- order parsing in the rest of this paper, we will all adopt the concatenate-add setting.</p><p>In the third group, we can see that high-order parsing brings significant performance improve- ment. For high-order parsing, three ensemble schemes are examined: no combination, adding  <ref type="table" target="#tab_5">- - - - - - - - - Fonseca and Aluísio (2015)  - - - - - - 91</ref>.6-88.9-- - - - Zhang and Zhao <ref type="formula">(2015)</ref> - - - - - - 92.52 -41.10 86.01 -31.88 Graph-Linear <ref type="bibr" target="#b16">Koo and Collins (2010)</ref> 93.04 - <ref type="formula">(2013)</ref> 93.07 - - 92.82 - - - - - - - - Ma and  93.0--48.8- <ref type="bibr" target="#b1">Chen and Manning (2014)</ref> - - - 91.8-89.6-- 92.0-90.7-- 83.9-82.4-- <ref type="bibr" target="#b8">Dyer et al. (2015)</ref> - and stacking another linear perceptron layer (with the suffixes of -nope, -adding and -perceptron re- spectively). The results show that model ensemble improves the accuracies quite a few. For third- order parsing, the no-combination method per- forms quite poorly compared to the others, which may be caused by the relative strict setting of the pruning threshold. Nevertheless, with model en- semble, the third-order models perform better than the second-order ones. Though the perceptron strategy does not work well for third-order pars- ing in this dataset, it is still more general than the simple adding method, since the latter can be seen as a special parameter setting of the former.</p><formula xml:id="formula_16">- - - - - - - - - - Martins et al.</formula><formula xml:id="formula_17">- - - - - - 87.2--37.0- Transition-NN</formula><formula xml:id="formula_18">- - 93.1-90.9-- - - - 87.2-85.7-- Weiss et al. (2015) - - - 93.99 92.05 - - - - - - - Zhou et al. (2015) 93.28 92.35 - - - - - - - - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>We show the results of two of the best proposed parsers: third-order adding (o3-adding) and third- order perceptron (o3-perceptron) methods, and compare with the reported results of some previ- ous work in <ref type="table" target="#tab_5">Table 2</ref>. We compare with three cat- egories of models: other Graph-based NN (neu- ral network) models, traditional Graph-based Lin- ear models and Transition-based NN models. For PTB, there have been several different dependency converters which lead to different sets of depen- dencies and we choose three of the most popular ones for more comprehensive comparisons. Since not all work report results on all of these depen- dencies, some of the entries might be not available.</p><p>From the comparison, we see that the pro- posed parser has output competitive performance for different dependency conversion conventions and treebanks. Compared with traditional graph- based linear models, neural models may benefit from better feature representations and more gen- eral non-linear transformations.</p><p>The results and comparisons in <ref type="table" target="#tab_5">Table 2</ref> demon- strate the proposed models can obtain comparable accuracies, which show the effectiveness of com- bining local and global features through window- based and convolutional neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>CNN has been explored in recent work of rela- tion classification ( <ref type="bibr" target="#b41">Zeng et al., 2014;</ref><ref type="bibr" target="#b51">Chen et al., 2015</ref>), which resembles the task of deciding de- pendency relations in parsing. However, relation classification usually involves labeling for given arguments and seldom needs to consider the global structure. Parsing is more complex for it needs to predict structures and the use of CNN should be incorporated with the searching algorithms.</p><p>Neural network methods have been proved ef- fective for graph-based parsing. <ref type="bibr" target="#b19">Lei et al. (2014)</ref> explore a tensor scoring method, however, it needs to combine scores from linear models and we are not able to compare with it because of dif- ferent datasets (they take datasets from CoNLL shared task). <ref type="bibr" target="#b44">Zhang and Zhao (2015)</ref> also ex- plore a probabilistic treatment, but its model may give mass to illegal trees or non-trees. <ref type="bibr" target="#b11">Fonseca and Aluísio (2015)</ref> utilize CNN for scoring edges, though only explore first-order parsing. Its model is based on head selection for each modifier and might be difficult to be extended to high-order parsing. Recently, several neural re-ranking mod- els, like Inside-Outside Recursive Neural Network ( <ref type="bibr" target="#b18">Le and Zuidema, 2014</ref>) and Recursive CNN ( <ref type="bibr" target="#b51">Zhu et al., 2015)</ref>, are utilized for capturing features with more contexts. However, re-ranking mod- els depend on the underlying base parsers, which might already miss the correct trees. Generally, the re-ranking techniques play a role of additional enhancement for basic parsing models, and there- fore they are not included in our comparisons.</p><p>The conditional log-likelihood probabilistic cri- terion utilized in this work is actually a (condi- tioned) Markov Random Field for tree structures, and it has been applied to parsing since long time ago. <ref type="bibr" target="#b15">Johnson et al. (1999)</ref> utilize the Markov Ran- dom Fields for stochastic grammars and gradient based methods are adopted for parameter estima- tions, and <ref type="bibr" target="#b12">Geman and Johnson (2002)</ref> extend this with dynamic programming algorithms for infer- ence and marginal-probability calculation. <ref type="bibr" target="#b4">Collins (2000)</ref> uses the same probabilistic treatment for re-ranking and the denominator only includes the candidate trees which can be seen as an approx- imation for the whole space of trees. <ref type="bibr" target="#b10">Finkel et al. (2008)</ref> utilize it for feature-based parsing. The probabilistic training criterion for linear graph- based dependency models have been also explored in ( <ref type="bibr" target="#b20">Li et al., 2014;</ref><ref type="bibr" target="#b23">Ma and Zhao, 2015)</ref>. How- ever, these previous methods usually exploit log- linear models utilizing sparse features for input representations and linear models for score calcu- lations, which are replaced by more sophisticated distributed representations and neural models, as shown in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This work presents neural probabilistic graph- based models for dependency parsing, together with a convolutional part which could capture the sentence-level information. With distributed vec- tors for representations and complex non-linear neural network for calculations, the model can ef- fectively capture more complex features when de- ciding the scores for sub-tree factors and exper- iments on standard treebanks show that the pro- posed techniques improve parsing accuracies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The decompositions of factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: F1 measure of different dependency lengths, on PTB-SD development set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>The calculation of the</head><label></label><figDesc></figDesc><table>Method 

UAS LAS 
CM 
Basic (first-order) 
Unlabeled 
91.53 
-
42.82 
Labeled 
92.13 89.60 45.06 
Labeled+pre-training 
92.19 89.73 45.18 
Convolutional (first-order) 
replace-add 
92.26 89.83 44.76 
replace-mul 
92.02 89.61 44.24 
concatenate-add 
92.63 90.20 46.18 
concatenate-mul 
92.33 89.83 44.94 
Higher-orders 
o2-nope 
92.85 90.51 49.65 
o2-adding 
93.47 91.13 51.41 
o2-perceptron 
93.63 91.39 51.53 
o3-nope 
92.47 90.01 49.06 
o3-adding 
93.70 91.37 53.53 
o3-perceptron 
93.51 91.20 51.76 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 : Comparisons of results on the test sets.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> Tokens whose gold POS tags are one of {&quot; &quot; : , .} for PTB or P U for CTB. 3 http://stp.lingfil.uu.se/˜nivre/research/Penn2Malt.html 4 http://nlp.cs.lth.se/software/treebank converter</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural word segmentation learning for Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multi-pooling convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume</title>
		<meeting>the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative reranking for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning</title>
		<meeting>the Seventeenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="25" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural crf parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="302" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Three new probabilistic models for dependency parsing: An exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">M</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Computational Linguistics</title>
		<meeting>the 16th International Conference on Computational Linguistics<address><addrLine>Copenhagen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-08" />
			<biblScope unit="page" from="340" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient, feature-based, conditional random field parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kleeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="959" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A deep architecture for non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erick</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Aluísio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="56" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic programming for parsing and estimation of stochastic unification-based grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="279" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Softmaxmargin crfs: Training log-linear models with cost functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL<address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="733" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extended constituent-to-dependency conversion for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Nugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Nordic Conference of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
		<respStmt>
			<orgName>University of Tartu</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimators for stochastic &quot;unification-based&quot; grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Canon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 37th Annual Meeting of the Association for Computational Linguistics<address><addrLine>College Park, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient thirdorder dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The insideoutside recursive neural network model for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="729" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Low-rank tensors for scoring dependency structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1381" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ambiguity-aware ensemble training for semisupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="457" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient innerto-outer greedy algorithm for higher-order labeled dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1322" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fourth-order dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="785" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04174</idno>
		<title level="m">Probabilistic models for high-order projective dependency parsing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Turning on the turbo: Fast third-order nonprojective turbo parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="617" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Online learning of approximate dependency parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Integrating graph-based and transition-based dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="950" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Cubic-time parsing and learning algorithms for grammatical bigram models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark A Paskin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An effective neural network model for graph-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="313" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations and syntactic parsing with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS-2010</title>
		<meeting>the NIPS-2010</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep Learning and Unsupervised Feature Learning Workshop</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stacking dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André Filipe Torres</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ENNLP</title>
		<meeting>ENNLP<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Converting continuous-space language models into n-gram language models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="845" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural network based bilingual language model growing for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="189" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning distributed word representations for bidirectional lstm recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="323" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Statistical dependency analysis with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyasu</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWPT</title>
		<meeting>IWPT</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING<address><addrLine>Dublin, Ireland, August</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A tale of two parsers: Investigating and combining graphbased and transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Combining discrete and continuous features for deterministic transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1316" to="1321" />
		</imprint>
	</monogr>
	<note>Portugal</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">High-order graph-based neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Pacific Asia Conference on Language, Information, and Computation</title>
		<meeting>the 29th Pacific Asia Conference on Language, Information, and Computation<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10" />
			<biblScope unit="page" from="114" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Semantic dependency parsing of NomBank and PropBank: An efficient integrated approach via a large-scale feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Kit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="30" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multilingual dependency learning: A huge feature engineering method to semantic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Kity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cross language dependency parsing using a bilingual lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Kit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Integrative semantic dependency parsing via efficient large-scale feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Kit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="203" to="233" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Character-level dependencies in chinese: Usefulness and learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-03" />
			<biblScope unit="page" from="879" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A neural probabilistic structuredprediction model for transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="1213" to="1222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A re-ranking model for dependency parser with recursive convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
