<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
						</author>
						<title level="a" type="main">Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="917" to="928"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1085</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a novel attention-based recurrent neural network for joint extraction of entity mentions and relations. We show that attention along with long short term memory (LSTM) network can extract semantic relations between entity mentions without having access to dependency trees. Experiments on Automatic Content Extraction (ACE) corpora show that our model significantly outperforms feature-based joint model by Li and Ji (2014). We also compare our model with an end-to-end tree-based LSTM model (SPTree) by Miwa and Bansal (2016) and show that our model performs within 1% on entity mentions and 2% on relations. Our fine-grained analysis also shows that our model performs significantly better on AGENT-ARTIFACT relations, while SPTree performs better on PHYSICAL and PART-WHOLE relations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extraction of entities and their relations from text belongs to a very well-studied family of structured prediction tasks in NLP. There are several NLP tasks such as fine-grained opinion mining ( <ref type="bibr" target="#b5">Choi et al., 2006</ref>), semantic role labeling ( <ref type="bibr" target="#b12">Gildea and Jurafsky, 2002</ref>), etc., which have a similar struc- ture; thus making it an important and a challeng- ing task.</p><p>Several methods have been proposed for entity mention and relation extraction at the sentence- level. These can be broadly categorized into - 1) pipeline models that treat the identification of entity mentions ( <ref type="bibr" target="#b31">Nadeau and Sekine, 2007)</ref> and relation classification ( <ref type="bibr" target="#b46">Zhou et al., 2005</ref>) as two separate tasks; and 2) joint models, also the more recent, which simultaneously identify the entity mention and relations ( <ref type="bibr" target="#b24">Li and Ji, 2014;</ref><ref type="bibr" target="#b30">Miwa and Sasaki, 2014</ref>). Joint models have been argued to perform better than the pipeline models as knowl- edge of the typed relation can increase the confi- dence of the model on entity extraction and vice versa.</p><p>Recurrent networks (RNNs) <ref type="bibr" target="#b11">(Elman, 1990</ref>) have recently become very popular for sequence tagging tasks such as entity extraction that in- volves a set of contiguous tokens. However, their ability to identify relations between non-adjacent tokens in a sequence, e.g., the head nouns of two entities, is less explored. For these tasks, RNNs that make use of tree structures have been deemed more suitable. <ref type="bibr" target="#b29">Miwa and Bansal (2016)</ref>, for ex- ample, propose an RNN comprised of a sequence- based long short term memory (LSTM) for en- tity identification and a separate tree-based depen- dency LSTM layer for relation classification using shared parameters between the two components. As a result, their model depends critically on ac- cess to dependency trees, restricting it to sentence- level extraction and to languages for which (good) dependency parsers exist. Also, their model does not jointly extract entities and relations; they first extract all entities and then perform relation clas- sification on all pairs of entities in a sentence.</p><p>In our previous work <ref type="bibr" target="#b21">(Katiyar and Cardie, 2016)</ref>, we address the same task in an opinion extraction context. Our LSTM-based formulation explicitly encodes distance between the head of entities into opinion relation labels. The output space of our model is quadratic in size of the entity and relation label set and we do not specifically identify the relation type. Unfortunately, adding relation type makes the output label space very sparse, making it difficult for the model to learn.</p><p>In this paper, we propose a novel RNN-based model for the joint extraction of entity mentions and relations. Unlike other models, our model does not depend on any dependency tree informa- tion. Our RNN-based model is a multi-layer bi- directional LSTM over a sequence. We encode the output sequence from left-to-right. At each time step, we use an attention-like model on the previ- ously decoded time steps, to identify the tokens in a specified relation with the current token. We also add an additional layer to our network to encode the output sequence from right-to-left and find sig- nificant improvement on the performance of rela- tion identification using bi-directional encoding.</p><p>Our model significantly outperforms the feature-based structured perceptron model of <ref type="bibr" target="#b24">Li and Ji (2014)</ref>, showing improvements on both entity and relation extraction on the ACE05 dataset. In comparison to the dependency tree- based LSTM model of <ref type="bibr" target="#b29">Miwa and Bansal (2016)</ref>, our model performs within 1% on entities and 2% on relations on ACE05 dataset. We also find that our model performs significantly better than their tree-based model on the AGENT-ARTIFACT relation, while their tree-based model performs better on PHYSICAL and PART-WHOLE relations; the two models perform comparably on all other relation types. The very competitive performance of our non-tree-based model bodes well for relation extraction of non-adjacent entities in low-resource languages that lack good parsers.</p><p>In the sections that follow, we describe related work (Section 2); our bi-directional LSTM model with attention (Section 3); the training (Section 4); the experiments on ACE dataset (Section 5); re- sults (Section 6); error analysis (Section 7) and conclusion (Section 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>RNNs <ref type="bibr" target="#b17">(Hochreiter and Schmidhuber, 1997)</ref> have been recently applied to many sequential model- ing and prediction tasks, such as machine trans- lation ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref>), named entity recognition (NER) <ref type="bibr" target="#b14">(Hammerton, 2003)</ref>, opinion mining ( <ref type="bibr" target="#b19">Irsoy and Cardie, 2014)</ref>. Variants such as adding CRF-like objec- tive on top of LSTMs have been found to produce state-of-the-art results on several sequence pre- diction NLP tasks <ref type="bibr" target="#b9">(Collobert et al., 2011;</ref><ref type="bibr" target="#b18">Huang et al., 2015;</ref><ref type="bibr" target="#b21">Katiyar and Cardie, 2016</ref>). These models assume conditional independence at the output layer whereas the model we propose in this paper does not assume any conditional indepen- dence at the output layer, allowing it to model an arbitrary distribution over output sequences.</p><p>Relation classification has been widely studied as a stand-alone task, assuming that the arguments of the relations are known in advance. There have been several models proposed including feature- based models ( <ref type="bibr" target="#b1">Bunescu and Mooney, 2005;</ref><ref type="bibr" target="#b44">Zelenko et al., 2003)</ref> and neural network based mod- els ( <ref type="bibr" target="#b34">Socher et al., 2012;</ref><ref type="bibr" target="#b10">dos Santos et al., 2015;</ref><ref type="bibr" target="#b16">Hashimoto et al., 2015;</ref><ref type="bibr">Xu et al., 2015a,b)</ref>.</p><p>For joint-extraction of entities and relations, feature-based structured prediction models ( <ref type="bibr" target="#b24">Li and Ji, 2014;</ref><ref type="bibr" target="#b30">Miwa and Sasaki, 2014)</ref>, joint inference integer linear programming models <ref type="bibr" target="#b41">(Yih and Roth, 2007;</ref><ref type="bibr" target="#b40">Yang and Cardie, 2013)</ref>, card-pyramid pars- ing ( <ref type="bibr" target="#b20">Kate and Mooney, 2010)</ref> and probabilistic graphical models ( <ref type="bibr" target="#b42">Yu and Lam, 2010;</ref><ref type="bibr" target="#b33">Singh et al., 2013</ref>) have been proposed. In contrast, we pro- pose a neural network model which does not de- pend on the availability of any features such as part of speech (POS) tags, dependency trees, etc.</p><p>Recently, <ref type="bibr" target="#b29">Miwa and Bansal (2016)</ref> proposed an end-to-end LSTM based sequence and tree- structured model. They extract entities via a se- quence layer and relations between the entities via the shortest path dependency tree network. In this paper, we try to investigate recurrent neural net- works with attention for extracting semantic rela- tions between entity mentions without using any dependency parse tree features. We also present the first neural network based joint model that can extract entity mentions and relations along with the relation type. In our previous work <ref type="bibr" target="#b21">(Katiyar and Cardie, 2016)</ref>, as explained earlier, we pro- posed a LSTM-based model for joint extraction of opinion entities and relations, but no relation types. This model cannot be directly extended to include relation types as the output space becomes sparse making it difficult for the model to learn.</p><p>Recent advances in recurrent neural network has seen the application of attention on recur- rent neural networks to obtain a representation weighted by the importance of tokens in the se- quence model. Such models have been very fre- quently used in question-answering tasks (for re- cent examples, see <ref type="bibr" target="#b3">Chen et al. (2016)</ref> and <ref type="bibr" target="#b23">Lee et al. (2016)</ref>), machine translation ( <ref type="bibr" target="#b26">Luong et al., 2015;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>, and many other NLP ap- plications. Pointer networks ( <ref type="bibr" target="#b37">Vinyals et al., 2015)</ref>, an adaptation of attention models, use these token- level weights as pointers to the input elements.  <ref type="figure">Figure 1</ref>: Gold standard annotation for an example sentence from ACE05 dataset. <ref type="bibr" target="#b45">Zhai et al. (2017)</ref>, for example, have used these for neural chunking, and <ref type="bibr" target="#b32">Nallapati et al. (2016)</ref> and <ref type="bibr" target="#b4">Cheng and Lapata (2016)</ref>, for summarization. However, to the best of our knowledge, these net- works have not been used for joint extraction of entity mentions and relations. We present first such attempt to use these attention models with re- current neural networks for joint extraction of en- tity mentions and relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Our model comprises of a multi-layer bi- directional recurrent network which learns a rep- resentation for each token in the sequence. We use the hidden representation from the top layer for joint entity and relation extraction. For each to- ken in the sequence, we output an entity tag and a relation tag. The entity tag corresponds to the entity type, whereas the relation tag is a tuple of pointers to related entities and their respective re- lation types. <ref type="figure">Figure 1</ref> shows the annotation for an example sentence from the dataset. We trans- form the relation tags from entity level to token level. For example, we separately model the re- lation "ORG-AFF" for each token in the entity "ITV News". Thus, we model the relations be- tween "ITV" and "Martin Geissler", and "News" and "Martin Geissler" separately. We employ a pointer-like network on top of the sequence layer in order to find the relation tag for each token as shown in <ref type="figure">Figure 2</ref>. At each time step, the network utilizes the information available about all output tags from the previous time steps in order to out- put the entity tag and relation tag jointly for the current token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-layer Bi-directional Recurrent Network</head><p>We use multi-layer bi-directional LSTMs for se- quence tagging because LSTMs are more capable of capturing long-term dependencies between to- kens, making it ideal for both entity mention and relation extraction. Using LSTMs, we can compute the hidden state − → h t in the forward direction and ← − h t in the backward direction for every token as below:</p><formula xml:id="formula_0">− → h t = LST M (x t , − → h t−1 ) ← − h t = LST M (x t , ← − h t+1 )</formula><p>For every token t in the subsequent layer l, we combine the representations</p><formula xml:id="formula_1">− → h l−1 t and ← − h l−1</formula><p>t from previous layer l-1 and feed it as an input. In this paper, we only use the hidden state from the last layer L for output layer and compute the top hid- den layer representation as below:</p><formula xml:id="formula_2">z t = − → V − → h (L) t + ← − V ← − h (L) t + c − → V and</formula><p>← − V are weight matrices for combining hid- den representations from the two directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Entity detection</head><p>We formulate entity detection as a sequence label- ing task using BILOU scheme similar to <ref type="bibr" target="#b24">Li and Ji (2014)</ref> and <ref type="bibr" target="#b29">Miwa and Bansal (2016)</ref>. We assign each token in the entity with the tag B appended with the entity type if it is the beginning of the en- tity, I for inside of an entity, L for the end of the entity or U if there is only one token in the entity. <ref type="figure">Figure 1</ref> shows an example of the entity tag se- quence assigned to the sentence. For each token in the sequence, we perform a softmax over all can- didate tags to output the most likely tag:</p><formula xml:id="formula_3">y t = softmax(U z t + b)</formula><p>Our network structure as shown in <ref type="figure">Figure 2</ref> also contains connections from the output y t−1 of the previous time step to the current top hidden layer. Thus our outputs are not conditionally indepen- dent from each other. In order to add connections from y t−1 , we transform this output k into a label embedding b k t−1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">. We represent each label type</head><p>Figure 2: Our network structure based on bi-directional LSTMs for joint entity and relation extraction. This snapshot shows the network when encoding the relation tag for the word "Safwan" in the sentence. The dotted lines in the figure show that top hidden layer and label embeddings for tokens is copied into relation layer. The pointers at attention layer indicate the probability distribution over tokens, the length of the pointers is used to denote the probability value.</p><p>k with a dense representation b k . We compute the output layer representations as:</p><formula xml:id="formula_4">z t = LST M ([z t ; b k t−1 ], h t−1 ) y t = softmax(U z t + b )</formula><p>We decode the output sequence from left to right in a greedy manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attention Model</head><p>We use attention model for relation extraction. At- tention models, over an encoder sequence of repre- sentations z, can compute a soft probability distri- bution p over these learned representations, where d i is the i th token in decoder sequence. These probabilities are an indication of the importance of different tokens in the encoder sequence:</p><formula xml:id="formula_5">u i t = v T tanh(W 1 z + W 2 d i ) p i t = softmax(u i t )</formula><p>v is a weight matrix for attention which transforms the hidden representations into attention scores.</p><p>We use pointer networks ( <ref type="bibr" target="#b37">Vinyals et al., 2015</ref>) in our approach, which are a variation of these at- tention models. Pointer networks interpret these p i t as the probability distribution over the input en- coding sequence and use u i t as pointers to the input elements. We can use these pointers to encode re- lation between the current token and the previous predicted tokens, making it fit for relation extrac- tion as explained in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relation detection</head><p>We formulate relation extraction also as a se- quence labeling task. For each token, we want to find the tokens in the past that the current token is related to along with its relation type. In <ref type="figure">Fig- ure 1</ref>, "Safwan" is related to the tokens "Martin" as well as "Geissler" by the relation type "PHYS". For simplicity, let us assume that there is only one previous token the current token is related to when training, i.e., "Safwan" is related to "Geissler" via PHYS relation. We can extend our approach to output multiple relations as explained in Section 4.</p><p>We use pointer networks as described in Sec-tion 3.3. At each time step, we stack the top hidden layer representations from the previous time steps z ≤t 2 and its corresponding label embeddings b ≤t . We only stack the top hidden layer representations for the tokens which were predicted as non-O's for previous time steps as shown in <ref type="figure">Figure 2</ref>. Our de- coding representation at time t is the concatena- tion of z t and b t . The attention probabilities can now be computed as below:</p><formula xml:id="formula_6">u t ≤t = v T tanh(W 1 [z ≤t ; b ≤t ] + W 2 [z t ; b t ]) p t ≤t = softmax(u t ≤t )</formula><p>Thus, p t ≤t corresponds to the probability of each token, in the sequence so far, being related to the current token at time step t. For the case of NONE relations, the token at t is related to itself.</p><p>We also want to find the type of the relations. In order to achieve this, we add an extra dimension to v corresponding to the size of relation types R space. Thus, u i t is no longer a score but a R di- mensional vector. We then take softmax over this vector of size O(|z ≤t |×R) to find the most likely tuple of pointer to the related entity and its relation type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Bi-directional Encoding</head><p>Bi-directional LSTMs have been found to be able to capture context better than plain left-to-right LSTMs, based on their performance on vari- ous NLP tasks <ref type="bibr" target="#b19">(Irsoy and Cardie, 2014</ref>). Also,  found that their perfor- mance on machine translation task improved on reversing the input sentences during training. In- spired by these developments, we experiment with bi-directional encoding at the output layer. We add another top hidden layer on Bi-LSTM in <ref type="figure">Figure 2</ref> which encodes the output sequence from right- to-left. The two encoding share the same multi- layer bi-directional LSTM except for the top hid- den layer. Thus, we have two output layers in our network which output the entity tags and re- lation tags separately. At inference time, we em- ploy heuristics to combine the output from the two directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head><p>We train our network by maximizing the log- probability of the correct entity E and relation R tag sequences jointly given the sentence S as be- low:</p><p>log p(E, R|S, θ) = 1 |S| i∈|S| log p(e i , r i |e &lt;i , r &lt;i , S, θ) = 1 |S| i∈|S| log p(e i |e &lt;i , r &lt;i ) + log p(r i |e ≤i , r &lt;i )</p><p>Thus, we can decompose our objective into the sum of log-probabilities over entity sequence and relation sequence. We use the gold entity tags while training. As shown in <ref type="figure">Figure 2</ref>, we input the label embedding from the previous time step to the top hidden layer at the current time step along with the other recurrent inputs. During training, we pass the gold label embedding to the next time step which enables better training of our model. However, at test time when the gold label is not available we use the predicted label at previous time step as input to the current step. At inference time, we can greedily decode the sequence to find the most likely entity E and rela- tion R tag sequences:</p><formula xml:id="formula_7">( E, R) = argmax E,R p(E, R)</formula><p>Since, we add another top layer to encode tag se- quences in the reverse order as explained in Sec- tion 3.5, there may be conflicts in the output. We select the positive and more confident label similar to <ref type="bibr" target="#b29">Miwa and Bansal (2016)</ref>.</p><p>Multiple Relations Our approach to relation ex- traction is different from <ref type="bibr" target="#b29">Miwa and Bansal (2016)</ref>. <ref type="bibr" target="#b29">Miwa and Bansal (2016)</ref> present each pair of enti- ties to their model for relation classification. In our approach, we use pointer networks to iden- tify the related entities. Thus, for our approach described so far if we only compute the argmax on our objective then we limit our model to output only one relation label per token. However, from our analysis of the dataset, an entity may be related to more than one entity in the sentence. Hence, we modify our objective to include multiple relations. In <ref type="figure">Figure 2</ref>, token "Safwan" is related to both to- kens "Martin" and "Geissler" of the entity "Mar- tin Geissler", hence we assign probability of 0.5 to both these tokens. This can be easily expanded to include tokens from other related entities, such that we assign equal probability 1 N to all tokens 3 depending on the number N of these related to- kens.</p><p>The log-probability for the entity part remain the same as in our objective discussed in Section 4, however we modify the relation log-probability as below:</p><p>|j:r i,j &gt;0| r i,j log p(r i,j |e ≤i , r &lt;i , S, θ) where, r i is the true distribution over relation la- bel space and r i is the softmax output from our model. From empirical analysis, we find that r i is generally sparse and hence using a cross entropy objective like this can be useful to find multiple relations. We can also use Sparsemax (Martins and Astudillo, 2016) instead of softmax which is more suitable for sparse distributions. However, we leave it for future work.</p><p>At inference time, we output all the labels with probability value above a certain threshold. We adapt this threshold based on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>We evaluate our proposed model on the two datasets from the Automatic Content Extraction (ACE) program -ACE05 and ACE04. There are 7 main entity types namely Person (PER), Or- ganization (ORG), Geographical Entities (GPE), Location (LOC), Facility (FAC), Weapon (WEA) and Vehicle (VEH). For each entity, both en- tity mentions and its head phrase are annotated. For the scope of this paper, we only use the en- tity head phrase similar to <ref type="bibr" target="#b24">Li and Ji (2014)</ref> and <ref type="bibr" target="#b29">Miwa and Bansal (2016)</ref>. Also, there are rela- tion types namely Physical (PHYS), Person-Social (PER-SOC), Organization-Affiliation (ORG-AFF), Agent-Artifact (ART), GPE-Affiliation (GPE- AFF).</p><p>ACE05 has a total of 6 relation types including PART-WHOLE. We use the same data splits as <ref type="bibr" target="#b24">Li and Ji (2014)</ref> and <ref type="bibr" target="#b29">Miwa and Bansal (2016)</ref> such that there are 351 documents for training, 80 for <ref type="bibr">3</ref> In this paper, we only identify mention heads and hence the span is limited to a few tokens. We can also include only the last token of the gold entity span in the gold probability distribution. development and the remaining 80 documents for the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACE04 has 7 relation types with an additional</head><p>Discourse (DISC) type and split ORG-AFF relation type into ORG-AFF and OTHER-AFF. We perform 5-fold cross validation similar to <ref type="bibr" target="#b2">Chan and Roth (2011)</ref> for fair comparison with the state-of-the- art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>In order to compare our system with the previous systems, we report micro F1-scores, Precision and Recall on both entities and relations similar to <ref type="bibr" target="#b24">Li and Ji (2014)</ref> and <ref type="bibr" target="#b29">Miwa and Bansal (2016)</ref>. An en- tity is considered correct if we can identify its head and the entity type correctly. A relation is con- sidered correct if we can identify the head of the argument entities and also the relation type. We also report a combined score when both argument entities and relations are correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baselines and Previous Models</head><p>We compare our approach with two previous ap- proaches. The model proposed by <ref type="bibr" target="#b24">Li and Ji (2014)</ref> is a feature-based structured perceptron model with efficient beam-search. They employ a segment-based decoder instead of token-based de- coding. Their model outperformed previous state- of-the-art pipelined models. <ref type="bibr">Miwa and Sasaki (2014) (SPTree)</ref> recently proposed a LSTM-based model with a sequence layer for entity identifi- cation, and a tree-based dependency layer which identifies relations between pairs of candidate en- tities using the shortest dependency path between them. We also employed our previous approach <ref type="bibr" target="#b21">(Katiyar and Cardie, 2016</ref>) for extraction of opin- ion entities and relations to this task. We found that the performance was not competitive with the two approaches mentioned above, performing upto 10 points lower on relations. Hence, we do not include the results in <ref type="table">Table 1</ref>. Also, <ref type="bibr" target="#b24">Li and Ji (2014)</ref> showed that the joint model performs bet- ter than the pipelined approaches. Thus, we do not include any pipeline baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Hyperparameters and Training Details</head><p>We train our model using Adadelta <ref type="bibr" target="#b43">(Zeiler, 2012)</ref> with gradient clipping. We regularize our net- work using dropout ( <ref type="bibr" target="#b35">Srivastava et al., 2014</ref>) with the drop-out rate tuned using develop- ment set. We initialized our word embeddings  <ref type="table">Table 1</ref>: Performance on ACE05 test dataset. The dashed ("-") performance numbers were missing in the original paper <ref type="bibr" target="#b29">(Miwa and Bansal, 2016</ref>).</p><p>1 We ran the system made publicly available by <ref type="bibr" target="#b29">Miwa and Bansal (2016)</ref>, on ACE05 dataset for filling in the missing values and comparing our system with theirs at fine-grained level.  with 300-dimensional word2vec ( <ref type="bibr" target="#b28">Mikolov et al., 2013</ref>) word embeddings trained on Google News dataset. We have 3 hidden layers in our network and the dimensionality of the hidden units is 100. All the weights in the network are initialized from small random uniform noise. We tune our hyper- parameters based on ACE05 development set and use them for training on ACE04 dataset. <ref type="table">Table 1</ref> compares the performance of our system with respect to the baselines on ACE05 dataset. We find that our joint model significantly outper- forms the joint structured perceptron model (Li and Ji, 2014) on both entities and relations, despite the unavailability of features such as dependency trees, POS tags, etc. However, if we compare our model to the SPTree models, then we find that their model has better recall on both entities and relations. In Section 7, we perform error analysis to understand the difference in the performance of the two models in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We also compare the performance of various en- coding schemes in <ref type="table" target="#tab_3">Table 2</ref>. We compare the bene- fits of introducing multiple relations in our objec- tive and bi-directional encoding compared to left- to-right encoding.</p><p>Multiple Relations We find that modifying our objective to include multiple relations improves the recall of our system on relations, leading to slight improvement on the overall performance on relations. However, careful tuning of the threshold may further improve precision.</p><p>Bi-directional Encoding By adding bi- directional encoding to our system, we find that we can significantly improve the performance of our system compared to left-to-right encoding. It also improves precision compared to left-to- right decoding combined with multiple relations objective.</p><p>We find that for some relations it is easier to detect them with respect to one of the entities in the entity pair. PHYS relation is easier identified with respect to GPE entity than PER entity. Thus, our bi-directional encoding of relations allows us to encode these relations with respect to both enti- ties in the relation. <ref type="table">Table 3</ref> shows the performance of our model on ACE04 dataset. We believe that tuning the hy- perparameters of our model can further improve the results on this dataset. As also pointed out by <ref type="bibr" target="#b24">Li and Ji (2014)</ref> that ACE05 has better anno- tation quality, we focused on ACE05 dataset for this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Error Analysis</head><p>In this section, we perform a fine-grained compari- son of our model with respect to the SPTree (Miwa and Bansal, 2016) model. We compare the perfor- mance of the two models with respect to entities, relation types and the distance between the rela- tion arguments and provide examples from the test set in  <ref type="table">Table 3</ref>: Performance on ACE04 test dataset. The dashed ("-") performance numbers were missing in the original paper (Miwa and Bansal, 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Entities</head><p>We find that our model has lower recall on en- tity extraction than SPTree as shown in <ref type="table">Table 1</ref>. <ref type="bibr" target="#b29">Miwa and Bansal (2016)</ref>, in one of the ablation tests on ACE05 development set, show that their model can gain upto 2% improvement in recall by entity pretraining. Since we propose a joint- model, we cannot directly apply their pretraining trick on entities separately. We leave it for future work. <ref type="bibr" target="#b24">Li and Ji (2014)</ref> mentioned in their analysis of the dataset that there were many "UNK" tokens in the test set which were never seen during train- ing. We verified the same and we hypothesize that for this reason the performance on the entities de- pends largely on the pretrained word embeddings being used. We found considerable improvements on entity recall when using pretrained word em- beddings, if available, for these "UNK" tokens. Miwa and Bansal (2016) also use additional fea- tures such as POS tags in addition to pretrained word embeddings at the input layer.   <ref type="table">Table 5</ref>: Performance based on the distance be- tween entity arguments in relations for ACE05 test dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head><p>in <ref type="table" target="#tab_6">Table 4</ref>. Interestingly, we find that the per- formance of the two models is varied over dif- ferent relation types. The dependency tree-based model significantly outperforms our joint-model on PHYS and PART-WHOLE relations, whereas our model is significantly better than tree-based model on ART relation. We show an example sen- tence (S1) in <ref type="table" target="#tab_4">Table 6</ref>, where SPTree model identi- fies the entities in ART relation correctly but fails to identify ART relation. We compare the per- formance with respect to PHYS relation in Sec- tion 7.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Distance-based Analysis</head><p>We also compare the performance of the two mod- els on relations based on the distance between the entities in a relation in <ref type="table">Table 5</ref>. We find that the performance of both the models is very low for distance greater than 7. SPTree model can iden- tify 36 relations out of 131 such relations cor- rectly, while our model can only identify 20 re- lations in this category. We manually compare the output of the two systems on these cases on several examples to understand the gain of us- ing dependency tree on longer distances. Inter- estingly, the majority of these relations belong to PHYS type, thus resulting in lower performance on PHYS as discussed in Section 7.2. We found that there were a few instances of co-reference errors as shown in S2 in <ref type="table" target="#tab_4">Table 6</ref>. Our model identifies a PHYS relation between "here" and "baghdad", whereas the gold annotation has PHYS relation be- tween "location" and "baghdad". We think that S1 :</p><p>the [ men ]PER:ART-1 held on the sinking [ vessel ]VEH:ART-1 until the [ passenger ]PER:ART-2 [ ship ]VEH:ART-2 was able...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPTree : the [ men ]PER held on the sinking [ vessel ]VEH until the [ passenger ]PER [ ship ]VEH was able to reach them.</head><p>Our Model : the [ men ]PER:ART-1 held on the sinking [ vessel ]VEH:ART-1 until the [ passenger ]PER:ART-2 [ ship ]VEH:ART-2 was able...  <ref type="table" target="#tab_4">Table 6</ref>: Examples from the dataset with label annotations from SPTree and our model for comparison. The first row for each example is the gold standard.</p><p>incorporating these co-reference information dur- ing both training and evaluation will further im- prove the performance of both systems. Another source of error that we found was the inability of our system to extract entities (lower recall) as in S3. Our model could not identify the FAC en- tity "residence". Hence, we think an improvement on entity performance via methods like pretrain- ing might be helpful in identifying more relations. For distance less than 7, we find that our model has better recall but lower precision, as expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we propose a novel attention-based LSTM model for joint extraction of entity men- tions and relations. Experimentally, we found that our model significantly outperforms feature-rich structured perceptron joint model by <ref type="bibr" target="#b24">Li and Ji (2014)</ref>. We also compare our model to an end- to-end LSTM model by <ref type="bibr" target="#b29">Miwa and Bansal (2016)</ref> which comprises of a sequence layer for entity extraction and a tree-based dependency layer for relation classification. We find that our model, without access to dependency trees, POS tags, etc performs within 1% on entities and 2% on rela- tions on ACE05 dataset. We also find that our model performs significantly better than their tree- based model on the ART relation, while their tree- based model performs better on PHYS and PART- WHOLE relations; the two models perform com- parably on all other relation types. In future, we plan to explore pretraining meth- ods for our model which were shown to improve recall on entity and relation performance by <ref type="bibr" target="#b29">Miwa and Bansal (2016)</ref>. We introduce bi-directional output encoding as well as an objective to learn multiple relations in this paper. However, this presents the challenge of combining predictions from the two directions. We use heuristics in this paper to combine the predictions. We think that using probabilistic methods to combine model predictions from both directions may further im- prove the performance. We also plan to use <ref type="bibr">Sparsemax (Martins and Astudillo, 2016</ref>) instead of Softmax for multiple relations, as the former is more suitable for multi-label classification for sparse labels.</p><p>It would also be interesting to see the effect of reranking <ref type="bibr" target="#b7">(Collins and Koo, 2005</ref>) on our joint model. We also plan to extend the identification of entities to full entity mention span instead of only the head phrase as in <ref type="bibr" target="#b25">Lu and Roth (2015)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>922</head><label>922</label><figDesc></figDesc><table>Entity 

Relation 
Entity+Relation 
Method 
P 
R 
F1 
P 
R 
F1 
P 
R 
F1 

Li and Ji (2014) 
.852 
.769 
.808 
.689 
.419 
.521 
.654 
.398 
.495 

SPTree 
.829 
.839 
.834 
-
-
-
.572 
.540 
.556 

SPTree 1 
.823 
.839 
.831 
.605 
.553 
.578 
.578 
.529 
.553 

Our Model 
.840 
.813 
.826 
.579 
.540 
.559 
.555 
.518 
.536 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Performance of different encoding methods on ACE05 dataset.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table>923 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance on different relation types 
in ACE05 test dataset. Numbers in the bracket de-
note the number of relations of each relation type 
in the test set. 

7.2 Relation Types 

We evaluate our model on different relation types 
and compare the performance with SPTree model 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>S2 : [ her ]PER research was conducted [ here ]FAC at a [ location ]FAC:PHYS1 well-known to [ u.n. ]ORG:ORG-AFF1 [ arms ]WEA [ inspectors ]PER:ORG-AFF1. 300 miles west of [ baghdad ]GPE:PHYS1.</head><label></label><figDesc></figDesc><table>SPTree : 
[ her ]PER research was conducted [ here ]GPE at a [ location ]LOC:PHYS1 well-known to u.n. [ arms ]WEA 

[ [ inspectors ] PER:PHYS1,PHY2 . 300 miles west of [ baghdad ]GPE:PHYS2. 

Our Model : 
[ her ]PER research was conducted [ here ]FAC:PHYS1 at a [ location ]GPE well-known to [ u.n. ]ORG:ORG-AFF1 [ arms ]WEA 
[ inspectors ]PER:ORG-AFF1. 300 miles west of [ baghdad ]GPE:PHYS1. 

S3 : 
... 
[ Abigail Fletcher ]PER:PHYS1 , a [ marcher ]FAC:GEN-AFF2 from [ Florida ]FAC:GEN-AFF2, said outside the 

[ president ]PER:ART3 's [ [ residence ] FAC:ART3, PHYS1 . 

SPTree : 
... 
[ Abigail Fletcher ]PER:PHYS1 , a [ marcher ]FAC:GEN-AFF2 from [ Florida ]FAC:GEN-AFF2, said outside the 

[ president ]PER:ART3 's [ [ residence ] ]FAC:ART3, PHYS1 . 

Our Model : ... [ Abigail Fletcher ]PER , a [ marcher ]FAC:GEN-AFF2 from [ Florida ]FAC:GEN-AFF2, said outside the [ president ]PER 's 
residence. 

</table></figure>

			<note place="foot" n="1"> We can also add relation label embeddings using the relation tag output from the previous time step.</note>

			<note place="foot" n="2"> The notation ≤ is used to denote the stacking of the representations from the previous time steps. Thus, if zt is a 2-dimensional matrix then z ≤t will be a 3-dimensional tensor. The size along the first dimension will now correspond to the number of 2-dimensional matrices stacked.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Qi Li and Makoto Miwa for their help with the dataset and sharing their code for analy-sis. We also thank Xilun Chen, Xanda Schofield, Yiqing Hua, Vlad Niculae, Tianze Shi and the three anonymous reviewers for their helpful feed-back and discussion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
		<idno type="doi">10.3115/1220575.1220666</idno>
		<ptr target="https://doi.org/10.3115/1220575.1220666" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA, HLT &apos;05</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting syntactico-semantic structures for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A thorough examination of the cnn/daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>Au- gust 7-12</idno>
		<ptr target="http://aclweb.org/anthology/P/P16/P16-1223.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1046" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations for opinion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<ptr target="http://www.aclweb.org/anthology/W/W06/W06-" />
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="431" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative reranking for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="70" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<idno type="doi">10.1162/0891201053630273</idno>
		<ptr target="https://doi.org/10.1162/0891201053630273" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR abs/1504.06580</idno>
		<ptr target="http://arxiv.org/abs/1504.06580" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COGNITIVE SCIENCE</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic labeling of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="288" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<idno type="doi">10.1162/089120102760275983</idno>
		<ptr target="https://doi.org/10.1162/089120102760275983" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Named entity recognition with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hammerton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003<address><addrLine>Stroudsburg, PA, USA, CONLL &apos;03</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="172" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<idno type="doi">10.3115/1119176.1119202</idno>
		<ptr target="https://doi.org/10.3115/1119176.1119202" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Task-oriented learning of word embeddings for semantic relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/K15-1027" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting>the Nineteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="268" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="doi">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bidirectional LSTM-CRF models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno>CoRR abs/1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Opinion mining with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno>Oc- tober 25-29</idno>
		<ptr target="http://aclweb.org/anthology/D/D14/D14-1080.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="720" to="728" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint entity and relation extraction using card-pyramid parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1870568.1870592" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning<address><addrLine>Stroudsburg, PA, USA, CoNLL &apos;10</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Investigating lstms for joint extraction of opinion entities and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th</title>
		<meeting>the 54th</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<ptr target="http://aclweb.org/anthology/P/P16/P16-1087.pdf" />
		<title level="m">Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning recurrent span representations for extractive question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<idno>CoRR abs/1611.01436</idno>
		<ptr target="http://arxiv.org/abs/1611.01436" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014-06-22" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint mention extraction and classification with mention hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="857" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">From softmax to sparsemax: A sparse model of attention and multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><forename type="middle">F</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Astudillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1614" to="1623" />
		</imprint>
	</monogr>
	<note>JMLR.org, ICML&apos;16</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1105" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D14/D14-1200.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A survey of named entity recognition and classification. Linguisticae Investigationes 30</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence rnns for text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR abs/1602.06023</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint inference of entities, relations, and coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaping</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="doi">10.1145/2509558.2509559</idno>
		<ptr target="https://doi.org/10.1145/2509558.2509559" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Workshop on Automated Knowledge Base Construction</title>
		<meeting>the 2013 Workshop on Automated Knowledge Base Construction<address><addrLine>New York, NY, USA, AKBC</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-813" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantic relation classification via convolutional neural networks with simple negative sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1062" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="536" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1206" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint inference for fine-grained opinion extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics, ACL<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013-08-09" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1640" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Global inference for entity and relation identification via a linear programming formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">An Introduction to Statistical Relational Learning</title>
		<editor>L. Getoor and B. Taskar</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters. Association for Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA, COLING &apos;10</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1399" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>CoRR abs/1212.5701</idno>
		<ptr target="http://arxiv.org/abs/1212.5701" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Neural models for sequence chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saloni</forename><surname>Potdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14776" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02-04" />
			<biblScope unit="page" from="3365" to="3371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="doi">10.3115/1219840.1219893</idno>
		<ptr target="https://doi.org/10.3115/1219840.1219893" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
