<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ambiguity-aware Ensemble Training for Semi-supervised Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 23-25</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Provincial Key Laboratory for Computer Information Processing Technology Soochow University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Provincial Key Laboratory for Computer Information Processing Technology Soochow University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Provincial Key Laboratory for Computer Information Processing Technology Soochow University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Ambiguity-aware Ensemble Training for Semi-supervised Dependency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="457" to="467"/>
							<date type="published">June 23-25</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1-best parse trees in previous work, our core idea is to utilize parse forest (ambiguous labelings) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data. With a conditional random field based probabilistic dependency parser, our training objective is to maximize mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings. This framework offers two promising advantages. 1) ambiguity encoded in parse forests compromises noise in 1-best parse trees. During training, the parser is aware of these ambiguous structures, and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves. 2) diverse syntactic structures produced by different parsers can be naturally compiled into forest, offering complementary strength to our single-view parser. Experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Supervised dependency parsing has made great progress during the past decade. However, it is very difficult to further improve performance * Correspondence author of supervised parsers. For example, <ref type="bibr" target="#b24">Koo and Collins (2010)</ref> and <ref type="bibr" target="#b1">Zhang and McDonald (2012)</ref> show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters ( <ref type="bibr" target="#b2">Koo et al., 2008)</ref>, subtree frequencies ( <ref type="bibr" target="#b3">Chen et al., 2009;</ref><ref type="bibr" target="#b6">Chen et al., 2013)</ref>, and word co-occurrence counts ( <ref type="bibr" target="#b5">Zhou et al., 2011;</ref><ref type="bibr" target="#b7">Bansal and Klein, 2011)</ref>. A few effective learning meth- ods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data <ref type="bibr" target="#b37">(Smith and Eisner, 2007;</ref><ref type="bibr" target="#b44">Wang et al., 2008;</ref><ref type="bibr" target="#b4">Suzuki et al., 2009</ref>). All above work leads to significant improvement on parsing accuracy.</p><p>Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training <ref type="bibr" target="#b46">(Yarowsky, 1995)</ref>, co-training <ref type="bibr" target="#b8">(Blum and Mitchell, 1998)</ref>, and tri-training (Zhou and <ref type="bibr" target="#b49">Li, 2005</ref>). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing <ref type="bibr" target="#b27">(McClosky et al., 2006;</ref><ref type="bibr" target="#b21">Huang and Harper, 2009)</ref>, self-training is shown unsuccessful for dependency parsing <ref type="bibr" target="#b39">(Spreyer and Kuhn, 2009</ref>). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. <ref type="bibr" target="#b36">Sagae and Tsujii (2007)</ref> apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. <ref type="bibr" target="#b38">Søgaard and Rishøj (2010)</ref> combine tri-training and parser ensemble to boost parsing accuracy. Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two parsers are identical. In this way, the auto- parsed unlabeled data becomes more reliable. w 0 He 1 saw 2 a 3 deer 4 riding 5 a 6 bicycle 7 in 8 the 9 park 10 . 11 <ref type="figure">Figure 1</ref>: An example sentence with an ambiguous parse forest.</p><p>However, one obvious drawback of these methods is that they are unable to exploit unlabeled data with divergent outputs from different parsers. Our experiments show that unlabeled data with identical outputs from different parsers tends to be short (18.25 words per sentence on average), and only has a small proportion of 40% (see <ref type="table" target="#tab_9">Table 6</ref>). More importantly, we believe that unlabeled data with divergent outputs is equally (if not more) useful. Intuitively, an unlabeled sentence with divergent outputs should contain some ambiguous syntactic structures (such as preposition phrase attachment) that are very hard to resolve and lead to the disagreement of different parsers. Such sentences can provide more discriminative instances for training which may be unavailable in labeled data.</p><p>To solve above issues, this paper proposes a more general and effective framework for semi-supervised dependency parsing, referred to as ambiguity-aware ensemble training. Different from traditional self/co/tri-training which only use 1-best parse trees on unlabeled data, our approach adopts ambiguous labelings, represented by parse forest, as gold-standard for unlabeled sentences. <ref type="figure">Figure 1</ref> shows an example sentence with an ambiguous parse forest. The forest is formed by two parse trees, respectively shown at the upper and lower sides of the sentence. The differences between the two parse trees are highlighted using dashed arcs. The upper tree take "deer" as the subject of "riding", whereas the lower one indicates that "he" rides the bicycle. The other difference is where the preposition phrase (PP) "in the park" should be attached, which is also known as the PP attachment problem, a notorious challenge for parsing. Reserving such uncertainty has three potential advantages. First, noise in unlabeled data is largely alleviated, since parse forest encodes only a few highly possible parse trees with high oracle score. Please note that the parse forest in <ref type="figure">Figure 1</ref> contains four parse trees after combination of the two different choices. Second, the parser is able to learn useful features from the unambiguous parts of the parse forest. Finally, with sufficient unlabeled data, it is possible that the parser can learn to resolve such uncertainty by biasing to more reasonable parse trees.</p><p>To construct parse forest on unlabeled data, we employ three supervised parsers based on different paradigms, including our baseline graph-based dependency parser, a transition-based dependency parser ( <ref type="bibr" target="#b48">Zhang and Nivre, 2011)</ref>, and a generative constituent parser <ref type="bibr" target="#b34">(Petrov and Klein, 2007)</ref>. The 1-best parse trees of these three parsers are aggre- gated in different ways. Evaluation on labeled data shows the oracle accuracy of parse forest is much higher than that of 1-best outputs of single parsers (see <ref type="table" target="#tab_2">Table 3</ref>). Finally, using a conditional random field (CRF) based probabilistic parser, we train a better model by maximizing mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings. Experimental results on both English and Chinese datasets demon- strate that the proposed ambiguity-aware ensem- ble training outperforms other entire-tree based methods such as self/co/tri-training. In summary, we make following contributions.</p><p>1. We propose a generalized ambiguity-aware ensemble training framework for semi- supervised dependency parsing, which can make better use of unlabeled data, especially when parsers from different views produce divergent syntactic structures.</p><p>2. We first employ a generative constituent pars- er for semi-supervised dependency parsing. Experiments show that the constituent parser is very helpful since it produces more diver- gent structures for our semi-supervised parser than discriminative dependency parsers.</p><p>3. We build the first state-of-the-art CRF-based dependency parser. Using the probabilistic parser, we benchmark and conduct systemat- ic comparisons among ours and all previous bootstrapping methods, including self/co/tri- training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Supervised Dependency Parsing</head><p>Given an input sentence x = w 0 w 1 ...w n , the goal of dependency parsing is to build a dependency tree as depicted in <ref type="figure">Figure 1</ref>,</p><formula xml:id="formula_0">denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 &lt; m ≤ n}, where (h, m)</formula><p>indicates a directed arc from the head word w h to the modifier w m , and w 0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream meth- ods tackle the dependency parsing problem from different perspectives but achieve comparable ac- curacy on a variety of languages. The graph- based method views the problem as finding an optimal tree from a fully-connected directed graph <ref type="bibr" target="#b30">(McDonald et al., 2005;</ref><ref type="bibr" target="#b11">Carreras, 2007;</ref><ref type="bibr" target="#b24">Koo and Collins, 2010)</ref>, while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree ( <ref type="bibr" target="#b45">Yamada and Matsumoto, 2003;</ref><ref type="bibr" target="#b32">Nivre, 2003;</ref><ref type="bibr" target="#b48">Zhang and Nivre, 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph-based Dependency Parser (GParser)</head><p>In this work, we adopt the graph-based paradigm because it allows us to naturally derive conditional probability of a dependency tree d given a sen- tence x, which is required to compute likelihood of both labeled and unlabeled data. Under the graph-based model, the score of a dependency tree is factored into the scores of small subtrees p. <ref type="figure">Figure 2</ref>: Two types of scoring subtrees in our second-order graph-based parsers.</p><formula xml:id="formula_1">Score(x, d; w) = w · f (x, d) = p⊆d Score(x, p; w) h m (a) single dependency h s (b) adjacent sibling m</formula><p>Dependency features f dep (x, h, m):</p><formula xml:id="formula_2">w h , wm, t h , tm, t h±1 , tm±1, t b , dir(h, m), dist(h, m)</formula><p>Sibling features f sib (x, h, m, s): w h , ws, wm, t h , tm, ts, t h±1 , tm±1, ts±1 dir(h, m), dist(h, m) <ref type="table">Table 1</ref>: Brief illustration of the syntactic features. t i denotes the POS tag of w i . b is an index between h and m. dir(i, j) and dist(i, j) denote the direction and distance of the dependency (i, j).</p><p>We adopt the second-order graph-based depen- dency parsing model of  as our core parser, which incorporates features from the two kinds of subtrees in <ref type="figure">Fig. 2</ref>. <ref type="bibr">1</ref> Then the score of a dependency tree is:</p><formula xml:id="formula_3">Score(x, d; w) = {(h,m)}⊆d w dep · f dep (x, h, m) + {(h,s),(h,m)}⊆d w sib · f sib (x, h, s, m)</formula><p>where f dep (x, h, m) and f sib (x, h, s, m) are the feature vectors of the two subtree in <ref type="figure">Fig. 2</ref>; w dep/sib are feature weight vectors; the dot prod- uct gives scores contributed by corresponding sub- trees.</p><p>For syntactic features, we adopt those of Bohnet (2010) which include two categories correspond- ing to the two types of scoring subtrees in <ref type="figure">Fig. 2</ref>. We summarize the atomic features used in each feature category in <ref type="table">Table 1</ref>. These atomic features are concatenated in different combinations to com- pose rich feature sets. Please refer to <ref type="table" target="#tab_5">Table 4</ref> of Bohnet <ref type="formula">(2010)</ref> for the complete feature list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CRF-based GParser</head><p>Previous work on graph-based dependency pars- ing mostly adopts linear models and perceptron based training procedures, which lack probabilis- tic explanations of dependency trees and do not need to compute likelihood of labeled training data. Instead, we build a log-linear CRF-based dependency parser, which is similar to the CRF- based constituent parser of <ref type="bibr" target="#b19">Finkel et al. (2008)</ref>. Assuming the feature weights w are known, the probability of a dependency tree d given an input sentence x is defined as:</p><formula xml:id="formula_4">p(d|x; w) = exp{Score(x, d; w)} Z(x; w) Z(x; w) = d ′ ∈Y(x) exp{Score(x, d ′ ; w)} (1)</formula><p>where Z(x) is the normalization factor and Y(x) is the set of all legal dependency trees for x. Suppose the labeled training data is</p><formula xml:id="formula_5">D = {(x i , d i )} N i=1</formula><p>. Then the log likelihood of D is:</p><formula xml:id="formula_6">L(D; w) = N i=1 log p(d i |x i ; w)</formula><p>The training objective is to maximize the log likelihood of the training data L(D). The partial derivative with respect to the feature weights w is:</p><formula xml:id="formula_7">∂L(D; w) ∂w = N i=1    f (x i , d i ) − d ′ ∈Y(x i ) p(d ′ |x i ; w)f (x i , d ′ )   </formula><p>(2) where the first term is the empirical counts and the second term is the model expectations. Since Y(x i ) contains exponentially many dependency trees, direct calculation of the second term is prohibitive. Instead, we can use the classic inside- outside algorithm to efficiently compute the model expectations within O(n 3 ) time complexity, where n is the input sentence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Ambiguity-aware Ensemble Training</head><p>In standard entire-tree based semi-supervised methods such as self/co/tri-training, automatically parsed unlabeled sentences are used as additional training data, and noisy 1-best parse trees are considered as gold-standard. To alleviate the noise, the tri-training method only uses unlabeled data on which multiple parsers from different views produce identical parse trees. However, unlabeled data with divergent syntactic structures should be more useful. Intuitively, if several parsers disagree on an unlabeled sentence, it implies that the unlabeled sentence contains some difficult syntactic phenomena which are not sufficiently covered in manually labeled data. Therefore, exploiting such unlabeled data may introduce more discriminative syntactic knowledge, largely compensating labeled training data.</p><p>To address above issues, we propose ambiguity- aware ensemble training, which can be interpreted as a generalized tri-training framework. The key idea is the use of ambiguous labelings for the purpose of aggregating multiple 1-best parse trees produced by several diverse parsers. Here, "am- biguous labelings" mean an unlabeled sentence may have multiple parse trees as gold-standard reference, represented by parse forest (see <ref type="figure">Figure  1</ref>). The training procedure aims to maximize mixed likelihood of both manually labeled and auto-parsed unlabeled data with ambiguous label- ings. For an unlabeled instance, the model is updated to maximize the probability of its parse forest, instead of a single parse tree in traditional tri-training. In other words, the model is free to distribute probability mass among the trees in the parse forest to its liking, as long as the likelihood improves <ref type="bibr">(Täckström et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Likelihood of the Unlabeled Data</head><p>The auto-parsed unlabeled data with ambiguous labelings is denoted as</p><formula xml:id="formula_8">D ′ = {(u i , V i )} M i=1</formula><p>, where u i is an unlabeled sentence, and V i is the corre- sponding parse forest. Then the log likelihood of D ′ is:</p><formula xml:id="formula_9">L(D ′ ; w) = M i=1 log   d ′ ∈V i p(d ′ |u i ; w)   where p(d ′ |u i ; w)</formula><p>is the conditional probability of d ′ given u i , as defined in Eq. (1). For an unlabeled sentence u i , the probability of its parse forest V i is the summation of the probabilities of all the parse trees contained in the forest.</p><p>Then we can derive the partial derivative of the log likelihood with respect to w:</p><formula xml:id="formula_10">∂L(D ′ ; w) ∂w = M i=1    d ′ ∈V i ˜ p(d ′ |ui, Vi; w)f (ui, d ′ ) − d ′ ∈Y(u i ) p(d ′ |ui; w)f (ui, d ′ )    (3) where˜pwhere˜ where˜p(d ′ |u i , V i ; w)</formula><p>is the probability of d ′ un-der the space constrained by the parse forest V i .</p><formula xml:id="formula_11">˜ p(d ′ |u i , V i ; w) = exp{Score(u i , d ′ ; w)} Z(u i , V i ; w) Z(u i , V i ; w) = d ′ ∈V i exp{Score(u i , d ′ ; w)}</formula><p>The second term in Eq. <ref type="formula">(3)</ref> is the same with the second term in Eq. (2). The first term in Eq. <ref type="formula">(3)</ref> can be efficiently computed by running the inside- outside algorithm in the constrained search space V i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stochastic Gradient Descent (SGD) Training</head><p>We apply L2-norm regularized SGD training to iteratively learn feature weights w for our CRF- based baseline and semi-supervised parsers. We follow the implementation in CRFsuite. <ref type="bibr">2</ref> At each step, the algorithm approximates a gradient with a small subset of the training examples, and then updates the feature weights. <ref type="bibr" target="#b19">Finkel et al. (2008)</ref> show that SGD achieves optimal test performance with far fewer iterations than other optimization routines such as L-BFGS. Moreover, it is very convenient to parallel SGD since computations among examples in the same batch is mutually independent.</p><p>Training with the combined labeled and unla- beled data, the objective is to maximize the mixed likelihood:</p><formula xml:id="formula_12">L(D; D ′ ) = L(D) + L(D ′ )</formula><p>Since D ′ contains much more instances than D (1.7M vs. 40K for English, and 4M vs. 16K for Chinese), it is likely that the unlabeled data may overwhelm the labeled data during SGD training. Therefore, we propose a simple corpus-weighting strategy, as shown in Algorithm 1, where</p><formula xml:id="formula_13">D b i,k</formula><p>is the subset of training data used in k th update and b is the batch size; η k is the update step, which is adjusted following the simulated anneal- ing procedure ( <ref type="bibr" target="#b19">Finkel et al., 2008)</ref>. The idea is to use a fraction of training data (D i ) at each iteration, and do corpus weighting by randomly sampling labeled and unlabeled instances in a certain proportion (N 1 vs. M 1 ).</p><p>Once the feature weights w are learnt, we can Algorithm 1 SGD training with mixed labeled and unlabeled data.</p><formula xml:id="formula_14">1: Input: Labeled data D = {(xi, di)} N i=1</formula><p>, and unlabeled data D ′ = {(ui, Vi)} M j=1 ; Parameters: I, N1, M1, b 2: Output: w 3: Initialization: w (0) = 0, k = 0; 4: for i = 1 to I do {iterations} 5:</p><p>Randomly</p><note type="other">select N1 instances from D and M1 instances from D ′ to compose a new dataset Di, and shuffle it. 6: Traverse Di: a small batch D b i,k ⊆ Di at one step. 7:</note><formula xml:id="formula_15">w k+1 = w k + η k 1 b ∇L(D b i,k ; w k ) 8: k = k + 1 9: end for</formula><p>parse the test data to find the optimal parse tree.</p><formula xml:id="formula_16">d * = arg max d ′ ∈Y(x) p(d ′ |x; w) = arg max d ′ ∈Y(x) Score(x, d ′ ; w)</formula><p>This can be done with the Viterbi decoding algo- rithm described in  in O(n 3 ) parsing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Forest Construction with Diverse Parsers</head><p>To construct parse forests for unlabeled data, we employ three diverse parsers, i.e., our baseline GParser, a transition-based parser (ZPar 3 ) (Zhang and Nivre, 2011), and a generative constituen- t parser (Berkeley Parser 4 ) ( <ref type="bibr" target="#b34">Petrov and Klein, 2007)</ref>. These three parsers are trained on labeled data and then used to parse each unlabeled sen- tence. We aggregate the three parsers' outputs on unlabeled data in different ways and evaluate the effectiveness through experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analysis</head><p>To verify the effectiveness of our proposed ap- proach, we conduct experiments on Penn Tree- bank (PTB) and Penn Chinese Treebank 5.1 (CT- B5). For English, we follow the popular practice to split data into training (sections 2-21), devel- opment (section 22), and test (section 23). For CTB5, we adopt the data split of ( <ref type="bibr" target="#b18">Duan et al., 2007</ref>   <ref type="table" target="#tab_1">Table 2</ref> shows the data statistics. We measure parsing performance using the s- tandard unlabeled attachment score (UAS), ex- cluding punctuation marks. For significance test, we adopt Dan Bikel's randomized parsing evalua- tion comparator <ref type="bibr" target="#b33">(Noreen, 1989)</ref>. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Parameter Setting</head><p>When training our CRF-based parsers with SGD, we use the batch size b = 100 for all experiments. We run SGD for I = 100 iterations and choose the model that performs best on development data. For the semi-supervised parsers trained with Algorithm 1, we use N 1 = 20K and M 1 = 50K for English, and N 1 = 15K and M 1 = 50K for Chinese, based on a few preliminary experiments. To accelerate the training, we adopt parallelized implementation of SGD and employ 20 threads for each run. For semi-supervised cases, one iteration takes about 2 hours on an IBM server having 2.0 GHz Intel Xeon CPUs and 72G memory.</p><p>Default parameter settings are used for training ZPar and Berkeley Parser. We run ZPar for 50 iterations, and choose the model that achieves highest accuracy on the development data. For Berkeley Parser, we use the model after 5 split- merge iterations to avoid over-fitting the train- ing data according to the manual. The phrase- structure outputs of Berkeley Parser are converted into dependency structures using the same head- finding rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Methodology Study on Development Data</head><p>Using three supervised parsers, we have many options to construct parse forest on unlabeled data. To examine the effect of different ways for forest construction, we conduct extensive methodology study on development data.  . We divide the systems into three types: 1) supervised single parsers; 2) CRF-based GParser with conventional self/co/tri-training; 3) CRF- based GParser with our approach. For the latter two cases, we also present the oracle accuracy and averaged head number per word ("Head/Word") of parse forest when applying different ways to construct forests on development datasets.</p><p>The first major row presents performance of the three supervised parsers. We can see that the three parsers achieve comparable performance on English, but the performance of ZPar is largely inferior on Chinese.</p><p>The second major row shows the results when we use single 1-best parse trees on unlabeled data. When using the outputs of GParser itself ("Unlabeled ← G"), the experiment reproduces traditional self-training. The results on both En- glish and Chinese re-confirm that self-training may not work for dependency parsing, which is consistent with previous studies <ref type="bibr" target="#b39">(Spreyer and Kuhn, 2009</ref>). The reason may be that dependency parsers are prone to amplify previous mistakes on unlabeled data during training.</p><p>The next two experiments in the second ma- jor row reimplement co-training, where another parser's 1-best results are projected into unlabeled data to help the core parser. Using unlabeled data with the results of ZPar ("Unlabeled ← Z") significantly outperforms the baseline GParser by 0.30% (93.15-82.85) on English. However, the improvement on Chinese is not significant. Using unlabeled data with the results of Berkeley Parser ("Unlabeled ← B") significantly improves parsing accuracy by 0.55% (93.40-92.85) on English and 1.06% (83.34-82.28) on Chinese. We believe the reason is that being a generative model designed for constituent parsing, Berkeley Parser is more different from discriminative dependency parsers, and therefore can provide more divergent syntactic structures. This kind of syntactic divergence is helpful because it can provide complementary knowledge from a different perspective. <ref type="bibr" target="#b40">Surdeanu and Manning (2010)</ref> also show that the diversity of parsers is important for performance improvement when integrating different parsers in the super- vised track. Therefore, we can conclude that co-training helps dependency parsing, especially when using a more divergent parser.</p><p>The last experiment in the second major row is known as tri-training, which only uses unla-  beled sentences on which Berkeley Parser and ZPar produce identical outputs ("Unlabeled ← B=Z"). We can see that with the verification of two views, the oracle accuracy is much higher than using single parsers (97.52% vs. 92.85% on English, and 95.06% vs. 82.46% on Chinese). Although using less unlabeled sentences (0.7M for English and 1.2M for Chinese), tri-training achieves comparable performance to co-training (slightly better on English and slightly worse on Chinese). The third major row shows the results of the semi-supervised GParser with our proposed approach. We experiment with different com- binations of the 1-best parse trees of the three supervised parsers. The first three experiments combine 1-best outputs of two parsers to compose parse forest on unlabeled data. "Unlabeled ← B+(Z∩G)" means that the parse forest is initialized with the Berkeley parse and augmented with the intersection of dependencies of the 1-best outputs of ZPar and GParser. In the last setting, the parse forest contains all three 1-best results.</p><p>When the parse forests of the unlabeled data are the union of the outputs of GParser and ZPar, denoted as "Unlabeled ← Z+G", each word has 1.053 candidate heads on English and 1.136 on Chinese, and the oracle accuracy is higher than using 1-best outputs of single parsers (94.97% vs. 92.85% on English, 86.66% vs. 82.46% on Chinese). However, we find that although the parser significantly outperforms the supervised GParser on English, it does not gain significant im- provement over co-training with ZPar ("Unlabeled ← Z") on both English and Chinese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combining the outputs of Berkeley Parser and</head><p>GParser ("Unlabeled ← B+G"), we get higher oracle score (96.37% on English and 89.72% on Chinese) and higher syntactic divergence (1.085 candidate heads per word on English, and 1.188 on Chinese) than "Unlabeled ← Z+G", which verifies our earlier discussion that Berkeley Pars- er produces more different structures than ZPar. However, it leads to slightly worse accuracy than co-training with Berkeley Parser ("Unlabeled ← B"). This indicates that adding the outputs of GParser itself does not help the model.</p><p>Combining the outputs of Berkeley Parser and ZPar ("Unlabeled ← B+Z"), we get the best per- formance on English, which is also significantly better than both co-training ("Unlabeled ← B") and tri-training ("Unlabeled ← B=Z") on both English and Chinese. This demonstrates that our proposed approach can better exploit unlabeled data than traditional self/co/tri-training. More analysis and discussions are in Section 4.4.</p><p>During experimental trials, we find that "Unla- beled ← B+(Z∩G)" can further boost performance on Chinese. A possible explanation is that by using the intersection of the outputs of GParser and ZPar, the size of the parse forest is better controlled, which is helpful considering that ZPar performs worse on this data than both Berkeley Parser and GParser.</p><p>Adding the output of GParser itself ("Unlabeled ← B+Z+G") leads to accuracy drop, although the oracle score is higher (96.95% on English and 91.50% on Chinese) than "Unlabeled ← B+Z". We suspect the reason is that the model is likely to distribute the probability mass to these parse trees produced by itself instead of those by Berkeley Parser or ZPar under this setting. In summary, we can conclude that our proposed ambiguity-aware ensemble training is significant- ly better than both the supervised approaches and the semi-supervised approaches that use 1-best parse trees. Appropriately composing the forest parse, our approach outperforms the best results of co-training or tri-training by 0.28% (93.78-93.50) on English and 0.92% (84.26-83.34) on Chinese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Previous Work</head><p>We adopt the best settings on development data for semi-supervised GParser with our proposed approach, and make comparison with previous results on test data. <ref type="table" target="#tab_5">Table 4</ref> shows the results.</p><p>The first major row lists several state-of-the- art supervised methods.  propose a second-order graph-based parser, but use a smaller feature set than our work. <ref type="bibr" target="#b24">Koo and Collins (2010)</ref> propose a third-order graph- based parser. <ref type="bibr" target="#b1">Zhang and McDonald (2012)</ref> ex- plore higher-order features for graph-based de- pendency parsing, and adopt beam search for fast decoding. <ref type="bibr" target="#b48">Zhang and Nivre (2011)</ref> propose a feature-rich transition-based parser. All work in the second major row adopts semi-supervised methods. The results show that our approach achieves comparable accuracy with most previous semi-supervised methods. Both <ref type="bibr" target="#b4">Suzuki et al. (2009)</ref> and <ref type="bibr" target="#b6">Chen et al. (2013)</ref> adopt the higher- order parsing model of <ref type="bibr" target="#b11">Carreras (2007)</ref>, and Suzu- ki et al. (2009) also incorporate word cluster features proposed by <ref type="bibr" target="#b2">Koo et al. (2008)</ref> in their sys- tem. We expect our approach may achieve higher performance with such enhancements, which we leave for future work. Moreover, our method may be combined with other semi-supervised ap- proaches, since they are orthogonal in method- ology and utilize unlabeled data from different perspectives.  <ref type="bibr">[joint]</ref> 82.37 <ref type="bibr" target="#b9">Bohnet and Nivre (2012)</ref>  <ref type="bibr">[joint]</ref> 81.42 <ref type="bibr" target="#b6">Chen et al. (2013)</ref>  <ref type="bibr">[higher-order]</ref>     <ref type="bibr" target="#b11">Carreras (2007)</ref>. Again, our method may be combined with their work to achieve higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>To better understand the effectiveness of our pro- posed approach, we make detailed analysis using the semi-supervised GParser with "Unlabeled ← B+Z" on English datasets. Contribution of unlabeled data with regard to syntactic divergence: We divide the unlabeled data into three sets according to the divergence of the 1-best outputs of Berkeley Parser and ZPar. The first set contains those sentences that the two parsers produce identical parse trees, denoted by "consistent", which corresponds to the setting for tri-training. Other sentences are split into two sets according to averaged number of heads per word in parse forests, denoted by "low divergence" and "high divergence" respectively. Then we train semi-supervised GParser using the three sets of unlabeled data. <ref type="table" target="#tab_9">Table 6</ref> illustrates the results and statistics. We can see that unlabeled data with identical outputs from Berkeley Parser and ZPar tends to be short sentences (18.25 words per sen-tence on average). Results show all the three sets of unlabeled data can help the parser. Especially, the unlabeled data with highly divergent struc- tures leads to slightly higher improvement. This demonstrates that our approach can better exploit unlabeled data on which parsers of different views produce divergent structures.</p><p>Impact of unlabeled data size: To under- stand how our approach performs with regards to the unlabeled data size, we train semi-supervised GParser with different sizes of unlabeled data. <ref type="figure" target="#fig_1">Fig.  3</ref> shows the accuracy curve on the test set. We can see that the parser consistently achieves higher accuracy with more unlabeled data, demonstrating the effectiveness of our approach. We expect that our approach has potential to achieve higher accuracy with more additional data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Our work is originally inspired by the work of <ref type="bibr">Täckström et al. (2013)</ref>. They first apply the idea of ambiguous labelings to multilingual parser transfer in the unsupervised parsing field, which aims to build a dependency parser for a resource- poor target language by making use of source- language treebanks. Different from their work, we explore the idea for semi-supervised dependency parsing where a certain amount of labeled training data is available. Moreover, we for the first time build a state-of-the-art CRF-based depen- dency parser and conduct in-depth comparisons with previous methods. Similar ideas of learning with ambiguous labelings are previously explored for classification <ref type="bibr" target="#b23">(Jin and Ghahramani, 2002</ref>) and sequence labeling problems ( <ref type="bibr" target="#b17">Dredze et al., 2009)</ref>. Our work is also related with the parser ensem- ble approaches such as stacked learning and re- parsing in the supervised track. Stacked learning uses one parser's outputs as guide features for another parser, leading to improved performance <ref type="bibr" target="#b31">(Nivre and McDonald, 2008;</ref><ref type="bibr" target="#b43">Torres Martins et al., 2008)</ref>. Re-parsing merges the outputs of several parsers into a dependency graph, and then apply Viterbi decoding to find a better tree <ref type="bibr" target="#b35">(Sagae and Lavie, 2006;</ref><ref type="bibr" target="#b40">Surdeanu and Manning, 2010)</ref>. One possible drawback of parser ensemble is that several parsers are required to parse the same sentence during the test phase. Moreover, our approach can benefit from these methods in that we can get parse forests of higher quality on unlabeled data <ref type="bibr" target="#b51">(Zhou, 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper proposes a generalized training framework of semi-supervised dependency parsing based on ambiguous labelings.</p><p>For each unlabeled sentence, we combine the 1-best parse trees of several diverse parsers to compose ambiguous labelings, represented by a parse forest. The training objective is to maximize the mixed likelihood of both the labeled data and the auto-parsed unlabeled data with ambiguous labelings. Experiments show that our framework can make better use of the unlabeled data, especially those with divergent outputs from different parsers, than traditional tri-training. Detailed analysis demonstrates the effectiveness of our approach. Specifically, we find that our approach is very effective when using divergent parsers such as the generative parser, and it is also helpful to properly balance the size and oracle accuracy of the parse forest of the unlabeled data.</p><p>For future work, among other possible extensions, we would like to see how our approach performs when employing more diverse parsers to compose the parse forest of higher quality for the unlabeled data, such as the easy- first non-directional dependency parser <ref type="bibr" target="#b20">(Goldberg and Elhadad, 2010</ref>) and other constituent parsers <ref type="bibr" target="#b16">(Collins and Koo, 2005;</ref><ref type="bibr" target="#b12">Charniak and Johnson, 2005;</ref><ref type="bibr" target="#b19">Finkel et al., 2008</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance of GParser with different sizes of "Unlabeled ← B+Z" on English test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>). We convert original bracketed structures into dependency structures using Penn2Malt with its default head-finding rules. For unlabeled data, we follow Chen et al. (2013) and use the BLLIP WSJ corpus (Charniak et al., 2000) for English and Xinhua portion of Chinese</figDesc><table>Train 

Dev 
Test Unlabeled 
PTB 39,832 1,700 2,416 
1.7M 
CTB5 16,091 
803 1,910 
4M 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Data sets (in sentence number).</head><label>2</label><figDesc></figDesc><table>Gigaword Version 2.0 (LDC2009T14) (Huang, 
2009) for Chinese. We build a CRF-based bigram 
part-of-speech (POS) tagger with the features de-
scribed in (Li et al., 2012), and produce POS tags 
for all train/development/test/unlabeled sets (10-
way jackknifing for training sets). The tagging ac-
curacy on test sets is 97.3% on English and 94.0% 
on Chinese. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 presents</head><label>3</label><figDesc>the 5 http://www.cis.upenn.edu/ ˜ dbikel/software.html</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser. 
 † means the corresponding parser significantly outperforms supervised parsers, and  ‡ means the result 

significantly outperforms co/tri-training at confidence level of p &lt; 0.01. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>UAS comparison on English test data. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 make comparisons with previous results UAS Supervised Li et al. (2012)</head><label>5</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><head>Table 5 : UAS comparison on Chinese test data.</head><label>5</label><figDesc></figDesc><table>Unlabeled data UAS #Sent Len Head/Word Oracle 
NULL 
92.34 
0 
-
-
-
Consistent (tri-train) 92.94 0.7M 18.25 
1.000 
97.65 
Low divergence 92.94 0.5M 28.19 
1.062 
96.53 
High divergence 93.03 0.5M 27.85 
1.211 
94.28 
ALL 
93.19 1.7M 24.15 
1.087 
96.09 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Performance of our semi-supervised 
GParser with different sets of "Unlabeled ← 
B+Z" on English test set. "Len" means averaged 
sentence length. 

on Chinese test data. Li et al. (2012) and Bohnet 
and Nivre (2012) use joint models for POS tagging 
and dependency parsing, significantly outperform-
ing their pipeline counterparts. Our approach can 
be combined with their work to utilize unlabeled 
data to improve both POS tagging and parsing 
simultaneously. Our work achieves comparable 
accuracy with Chen et al. (2013), although they 
adopt the higher-order model of </table></figure>

			<note place="foot" n="1"> Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve higher accuracy, but has much higher time cost (O(n 4 )). Our approach is applicable to these higher-order models, which we leave for future work.</note>

			<note place="foot" n="2"> http://www.chokkan.org/software/crfsuite/</note>

			<note place="foot" n="3"> http://people.sutd.edu.sg/ ˜ yue_zhang/doc/ 4 https://code.google.com/p/berkeleyparser/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the critical and insightful comments from our anonymous reviewers. This work was supported by National Natural Science Foundation of China <ref type="bibr">(Grant No. 61373095, 61333018</ref>).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semi</forename><surname>Sup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pereira</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno>higher-order] 93.04</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mcdonald</forename><surname>Zhang</surname></persName>
		</author>
		<idno>higher-order] 92.9</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koo</forename></persName>
		</author>
		<idno>higher-order] 92.02 93.16</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<idno>higher-order] 92.40 93.16</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suzuki</surname></persName>
		</author>
		<idno>higher-order,cluster] 92.70 93.79</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno>higher-order] 91.98 92.64</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<idno>higher-order] 92.76 93.77 This work 92.34 93.19</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Web-scale features for full-scale parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="693" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Conference on Computational Learning Theory</title>
		<meeting>the 11th Annual Conference on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2012</title>
		<meeting>EMNLP 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1455" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Top accuracy and fast dependency parsing is not a contradiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="89" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Experiments with a higherorder projective dependency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP/CoNLL</title>
		<meeting>EMNLP/CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Coarseto-fine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Blaheta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niyu</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">BLLIP 1987-89 WSJ Corpus Release 1, LDC2000T43. Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving dependency parsing with subtrees from auto-parsed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyotaka</forename><surname>Jun&amp;apos;ichi Kazama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Uchimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torisawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="570" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised feature transformation for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1303" to="1313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discriminative reranking for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="25" to="70" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequence learning from data with multiple labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD Workshop on Learning from Multi-Label Data</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Probabilistic models for action-based Chinese dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiangyu Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML/ECPPKDD</title>
		<meeting>ECML/ECPPKDD</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="559" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient, feature-based, conditional random field parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kleeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="959" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An efficient algorithm for easy-first non-directional dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Selftraining PCFG grammars with latent annotations across languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2009</title>
		<meeting>EMNLP 2009</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="832" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Tagged Chinese Gigaword Version 2.0, LDC2009T14. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning with multiple labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient thirdorder dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simple semi-supervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A separately passive-aggressive training algorithm for joint POS tagging and dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2012</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1681" to="1698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Effective self-training for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL</title>
		<meeting>the Human Language Technology Conference of the NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Online learning of approximate dependency parsing algorithms</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Integrating graph-based and transition-based dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="950" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An efficient algorithm for projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWPT</title>
		<meeting>IWPT</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="149" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Computer-intensive methods for testing hypotheses: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improved inference for unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Parser combination by reparsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="129" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dependency parsing and domain adaptation with LR models and parser ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL</title>
		<meeting>the CoNLL Shared Task Session of EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1044" to="1050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bootstrapping feature-rich dependency parsers with entropic priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="667" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semisupervised dependency parsing using generalized tri-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rishøj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1065" to="1073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Datadriven dependency parsing of new languages using incomplete and noisy training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathrin</forename><surname>Spreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="12" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ensemble models for dependency parsing: Cheap and good?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="649" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An empirical study of semi-supervised structured conditional models for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Target language adaptation of discriminative transfer parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1061" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stacking dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André Filipe Torres</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semi-supervised convex training for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin Iris</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="532" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Statistical dependency analysis with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyasu</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWPT</title>
		<meeting>IWPT</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Generalized higher-order dependency parsing with cube pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="320" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Tri-training: Exploiting unlabeled data using three classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1529" to="1541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploiting web-derived selectional preference to improve statistical dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1556" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">When semi-supervised learning meets ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MCS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
