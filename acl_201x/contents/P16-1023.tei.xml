<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Intrinsic Subspace Evaluation of Word Embedding Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadollah</forename><surname>Yaghoobzadeh</surname></persName>
							<email>yadollah@cis.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Intrinsic Subspace Evaluation of Word Embedding Representations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="236" to="246"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a new methodology for intrinsic evaluation of word representations. Specifically, we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria. Current intrinsic evaluations are mostly based on the overall similarity or full-space similarity of words and thus view vector representations as points. We show the limits of these point-based intrinsic evaluations. We apply our evaluation methodology to the comparison of a count vector model and several neural network models and demonstrate important properties of these models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributional word representations or embeddings are currently an active area of research in nat- ural language processing (NLP). The motivation for embeddings is that knowledge about words is helpful in NLP. Representing words as vocabulary indexes may be a good approach if large train- ing sets allow us to learn everything we need to know about a word to solve a particular task; but in most cases it helps to have a representation that contains distributional information and allows in- ferences like: "above" and "below" have similar syntactic behavior or "engine" and "motor" have similar meaning.</p><p>Several methods have been introduced to assess the quality of word embeddings. We distinguish two different types of evaluation in this paper: (i) extrinsic evaluation evaluates embeddings in an NLP application or task and (ii) intrinsic evalu- ation tests the quality of representations indepen- dent of a specific NLP task.</p><p>Each single word is a combination of a large number of morphological, lexical, syntactic, se- mantic, discourse and other features. Its em- bedding should accurately and consistently repre- sent these features, and ideally a good evaluation method must clarify this and give a way to analyze the results. The goal of this paper is to build such an evaluation.</p><p>Extrinsic evaluation is a valid methodology, but it does not allow us to understand the properties of representations without further analysis; e.g., if an evaluation shows that embedding A works bet- ter than embedding B on a task, then that is not an analysis of the causes of the improvement. There- fore, extrinsic evaluations do not satisfy our goals.</p><p>Intrinsic evaluation analyzes the generic quality of embeddings. Currently, this evaluation mostly is done by testing overall distance/similarity of words in the embedding space, i.e., it is based on viewing word representations as points and then computing full-space similarity. The assump- tion is that the high dimensional space is smooth and similar words are close to each other. Sev- eral datasets have been developed for this purpose, mostly the result of human judgement; see ( <ref type="bibr" target="#b2">Baroni et al., 2014</ref>) for an overview. We refer to these evaluations as point-based and as full-space be- cause they consider embeddings as points in the space -sub-similarities in subspaces are generally ignored.</p><p>Point-based intrinsic evaluation computes a score based on the full-space similarity of two words: a single number that generally does not say anything about the underlying reasons for a lower or higher value of full-space similarity. This makes it hard to interpret the results of point-based evaluation and may be the reason that contradic- tory results have been published; e.g., based on point-based evaluation, some papers have claimed that count-based representations perform as well as learning-based representations ( <ref type="bibr" target="#b11">Levy and Goldberg, 2014a</ref>). Others have claimed the opposite (e.g., <ref type="bibr" target="#b16">Mikolov et al. (2013)</ref>, <ref type="bibr" target="#b18">Pennington et al. (2014)</ref>, <ref type="bibr" target="#b2">Baroni et al. (2014)</ref>).</p><p>Given the limits of current evaluations, we pro- pose a new methodology for intrinsic evaluation of embeddings by identifying generic fundamen- tal criteria for embedding models that are impor- tant for representing features of words accurately and consistently. We develop corpus-based tests using supervised classification that directly show whether the representations contain the informa- tion necessary to meet the criteria or not. The fine-grained corpus-based supervision makes the sub-similarities of words important by looking at the subspaces of word embeddings relevant to the criteria, and this enables us to give direct insights into properties of representation models. <ref type="bibr" target="#b2">Baroni et al. (2014)</ref> evaluate embeddings on dif- ferent intrinsic tests: similarity, analogy, synonym detection, categorization and selectional prefer- ence. <ref type="bibr" target="#b25">Schnabel et al. (2015)</ref> introduce tasks with more fine-grained datasets. These tasks are unsu- pervised and generally based on cosine similarity; this means that only the overall direction of vec- tors is considered or, equivalently, that words are modeled as points in a space and only their full- space distance/closeness is considered. In con- trast, we test embeddings in a classification set- ting and different subspaces of embeddings are an- alyzed. <ref type="bibr" target="#b26">Tsvetkov et al. (2015)</ref> evaluate embed- dings based on their correlations with WordNet- based linguistic embeddings. However, correla- tion does not directly evaluate how accurately and completely an application can extract a particular piece of information from an embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Extrinsic evaluations are also common (cf. ( <ref type="bibr" target="#b14">Li and Jurafsky, 2015;</ref><ref type="bibr" target="#b9">Köhn, 2015;</ref><ref type="bibr" target="#b10">Lai et al., 2015)</ref>). <ref type="bibr" target="#b14">Li and Jurafsky (2015)</ref> conclude that embed- ding evaluation must go beyond human-judgement tasks like similarity and analogy. They suggest to evaluate on NLP tasks. <ref type="bibr">Köhn (2015)</ref> gives similar suggestions and also recommends the use of su- pervised methods for evaluation. <ref type="bibr" target="#b10">Lai et al. (2015)</ref> evaluate embeddings in different tasks with differ- ent setups and show the contradictory results of embedding models on different tasks. Idiosyn- crasies of different downstream tasks can affect extrinsic evaluations and result in contradictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Criteria for word representations</head><p>Each word is a combination of different proper- ties. Depending on the language, these properties include lexical, syntactic, semantic, world knowl- edge and other features. We call these properties facets. The ultimate goal is to learn representa- tions for words that accurately and consistently contain these facets. Take the facet gender (GEN) as an example. We call a representation 100% ac- curate for GEN if information it contains about GEN is always accurate; we call the representation 100% consistent for GEN if the representation of every word that has a GEN facet contains this in- formation.</p><p>We now introduce four important criteria that a representation must satisfy to represent facets ac- curately and consistently. These criteria are ap- plied across different problems that NLP applica- tions face in the effective use of embeddings.</p><p>Nonconflation. A word embedding must keep the evidence from different local contexts sepa- rate -"do not conflate" -because each context can infer specific facets of the word. Embeddings for different word forms with the same stem, like plural and singular forms or different verb tenses, are examples vulnerable to conflation because they occur in similar contexts.</p><p>Robustness against sparseness. One aspect of natural language that poses great difficulty for sta- tistical modeling is sparseness. Rare words are common in natural language and embedding mod- els must learn useful representations based on a small number of contexts.</p><p>Robustness against ambiguity. Another cen- tral problem when processing words in NLP is lexical ambiguity <ref type="bibr" target="#b4">(Cruse, 1986;</ref><ref type="bibr" target="#b29">Zhong and Ng, 2010)</ref>. Polysemy and homonymy of words can make it difficult for a statistical approach to gen- eralize and infer well. Embeddings should fully represent all senses of an ambiguous word. This criterion becomes more difficult to satisfy as dis- tributions of senses become more skewed, but a robust model must be able to overcome this.</p><p>Accurate and consistent representation of multifacetedness. This criterion addresses set- tings with large numbers of facets. It is based on the following linguistic phenomenon, a phe- nomenon that occurs frequently crosslinguistically <ref type="bibr" target="#b3">(Comrie, 1989)</ref>. (i) Words have a large number of facets, including phonetic, morphological, syn- tactic, semantic and topical properties. (ii) Each facet by itself constitutes a small part of the over- all information that a representation should cap- ture about a word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setup and results</head><p>We now design experiments to directly evaluate embeddings on the four criteria. We proceed as follows. First, we design a probabilistic context free grammar (PCFG) that generates a corpus that is a manifestation of the underlying phenomenon. Then we train our embedding models on the cor- pus. The embeddings obtained are then evaluated in a classification setting, in which we apply a lin- ear SVM <ref type="bibr" target="#b6">(Fan et al., 2008</ref>) to classify embeddings. Finally, we compare the classification results for different embedding models and analyze and sum- marize them.</p><p>Selecting embedding models. Since this paper is about developing a new evaluation methodol- ogy, the choice of models is not important as long as the models can serve to show that the proposed methodology reveals interesting differences with respect to the criteria.</p><p>On the highest level, we can distinguish two types of distributional representations. Count vectors <ref type="bibr" target="#b24">(Sahlgren, 2006;</ref><ref type="bibr" target="#b1">Baroni and Lenci, 2010;</ref><ref type="bibr" target="#b27">Turney and Pantel, 2010)</ref> live in a high- dimensional vector space in which each dimen- sion roughly corresponds to a (weighted) count of cooccurrence in a large corpus. Learned vec- tors are learned from large corpora using machine learning methods: unsupervised methods such as LSI (e.g., <ref type="bibr" target="#b5">Deerwester et al. (1990)</ref>, <ref type="bibr" target="#b12">Levy and Goldberg (2014b)</ref>) and supervised methods such as neural networks (e.g., <ref type="bibr" target="#b16">Mikolov et al. (2013)</ref>) and regression (e.g., <ref type="bibr" target="#b18">Pennington et al. (2014)</ref>). Be- cause of the recent popularity of learning-based methods, we consider one count-based and five learning-based distributional representation mod- els.</p><p>The learning-based models are: </p><formula xml:id="formula_0">(contin- 1 P (aV b|S) = 1/4 2 P (bV a|S) = 1/4 3 P (aW a|S) = 1/8 4 P (aW b|S) = 1/8 5 P (bW a|S) = 1/8 6 P (bW b|S) = 1/8 7 P (vi|V ) = 1/5 0 ≤ i ≤ 4 8 P (wi|W ) = 1/5 0 ≤ i ≤ 4</formula><p>Figure 1: Global conflation grammar. Words v i occur in a subset of the contexts of words w i , but the global count vector signatures are the same.</p><p>uous window model) ( ). These models learn word embeddings for input and tar- get spaces using neural network models.</p><p>For a given context, represented by the input space representations of the left and right neigh- bors v i−1 and v i+1 , LBL, CBOW and CWIN pre- dict the target space v i by combining the contexts. LBL combines v i−1 and v i+1 linearly with posi- tion dependent weights and CBOW (resp. CWIN) combines them by adding (resp. concatenation). SKIP and SSKIP predict the context words v i−1 or v i+1 given the input space v i . For SSKIP, con- text words are in different spaces depending on their position to the input word. In summary, CBOW and SKIP are learning embeddings using bag-of-word (BoW) models, but the other three, CWIN, SSKIP and LBL, are using position depen- dent models. We use word2vec 1 for SKIP and CBOW, wang2vec 2 for SSKIP and CWIN, and <ref type="bibr" target="#b10">Lai et al. (2015)</ref>'s implementation 3 for LBL.</p><p>The count-based model is position-sensitive PPMI, Levy and Goldberg (2014a)'s explicit vec- tor space representation model. <ref type="bibr">4</ref> For a vocabulary of size V , the representation w of w is a vector of size 4V , consisting of four parts corresponding to the relative positions r ∈ {−2, −1, 1, 2} with respect to occurrences of w in the corpus. The entry for dimension word v in the part of w cor- responding to relative position r is the PPMI (pos- itive pointwise mutual information) weight of w and v for that relative position. The four parts of the vector are length normalized. In this paper, we use only two relative positions: r ∈ {−1, 1}, so each w has two parts, corresponding to immediate left and right neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Nonconflation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grammar. The PCFG grammar shown in Fig- ure 1 generates v i words that occur in two types of contexts: a-b (line 1) and b-a (line 2); and w i</head><p>words that also occur in these two contexts (lines 4 and 5), but in addition occur in a-a (line 3) and b-b (line 6) contexts. As a result, the set of con- texts in which v i and w i occur is different, but if we simply count the number of occurrences in the contexts, then v i and w i cannot be distinguished.</p><p>Dataset. We generated a corpus of 100,000 sen- tences. Words that can occur in a-a and b-b con- texts constitute the positive class, all other words the negative class. The words v 3 , v 4 , w 3 , w 4 were assigned to the test set, all other words to the train- ing set.</p><p>Results. We learn representations of words by our six models and train one SVM per model; it takes a word representation as input and outputs +1 (word can occur in a-a/b-b) or -1 (it cannot). The SVMs trained on PPMI and CBOW repre- sentations assigned all four test set words to the negative class; in particular, w 3 and w 4 were in- correctly classified. Thus, the accuracy of clas- sification for these models (50%) was not better than random. The SVMs trained on LBL, SSKIP, SSKIP and CWIN representations assigned all four test set words to the correct class: v 3 and v 4 were assigned to the negative class and w 3 and w 4 were assigned to the positive class.</p><p>Discussion. The property of embedding mod- els that is relevant here is that PPMI is an aggre- gation model, which means it calculates aggregate statistics for each word and then computes the fi- nal word embedding from these aggregate statis- tics. In contrast, all our learning-based models are iterative models: they iterate over the corpus and each local context of a word is used as a training instance for learning its embedding.</p><p>For iterative models, it is common to use com- position of words in the context, as in LBL, CBOW and CWIN. Non-compositional iterative models like SKIP and SSKIP are also popular. Aggregation models can also use composite fea- tures from context words, but these features are too sparse to be useful. The reason that the model of <ref type="bibr" target="#b0">Agirre et al. (2009)</ref> is rarely used is precisely its inability to deal with sparseness. All widely used distributional models employ individual word oc- currences as basic features.</p><p>The bad PPMI results are explained by the fact that it is an aggregation model: the PPMI model cannot distinguish two words with the same global statistics -as is the case for, say, v 3 and w 3 . The bad result of CBOW is probably connected to its weak (addition) composition of context, although it is an iterative compositional model. Simple rep- resentation of context words with iterative updat- ing (through backpropagation in each training in- stance), can influence the embeddings in a way that SKIP and SSKIP get good results, although they are non-compositional.</p><formula xml:id="formula_1">1 P (AV B|S) = 1/2 2 P (CW D|S) = 1/2 3 P (ai|A) = 1/10 0 ≤ i ≤ 9 4 P (bi|B) = 1/10 0 ≤ i ≤ 9 5 P (ci|C) = 1/10 0 ≤ i ≤ 9 6 P (di|D) = 1/10 0 ≤ i ≤ 9 7 P (vi|V ) = 1/10 0 ≤ i ≤ 9 8 P (wi|W ) = 1/10 0 ≤ i ≤ 9 9 L = L(S) 10 ∪ {aiuibi|0 ≤ i ≤ 9} 11 ∪ {cixidi|0 ≤ i ≤ 9}</formula><p>As an example of conflation occurring in the English Wikipedia, consider this simple example. We replace all single digits by "7" in tokenization. We learn PPMI embeddings for the tokens and see that among the one hundred nearest neighbors of "7" are the days of the week, e.g., "Friday". As an example of a conflated feature consider the word "falls" occurring immediately to the right of the target word. The weekdays as well as single dig- its often have the immediate right neighbor "falls" in contexts like "Friday falls on a public holiday" and "2 out of 3 falls match" -tokenized as "7 out of 7 falls match" -in World Wrestling Entertain- ment (WWE). The left contexts of "Friday" and "7" are different in these contexts, but the PPMI model does not record this information in a way that would make the link to "falls" clear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Robustness against sparseness</head><p>Grammar. The grammar shown in <ref type="figure" target="#fig_1">Figure 2</ref> gen- erates frequent v i and rare u i in a-b contexts (lines 1 and 9); and frequent w i and rare x i in c-d con- texts (lines 2 and 10). The language generated by the PCFG on lines 1-8 is merged on lines 9-11 with the ten contexts a 0 u 0 b 0 . . . a 9 u 9 b 9 (line 9) and the ten contexts c 0 x 0 d 0 . . . c 9 x 9 d 9 (line 10); that is, each of the u i and x i occurs exactly once in the merged language L , thus modeling the phe- nomenon of sparseness.</p><p>Dataset. We generated a corpus of 100,000 sen- tences using the PCFG (lines 1-8) and added the 20 rare sentences (lines 9-11). We label all words that can occur in c-d contexts as positive and all other words as negative. The singleton words u i and x i were assigned to the test set, all other words to the training set.</p><p>Results. After learning embeddings with differ- ent models, the SVM trained on PPMI representa- tions assigned all twenty test words to the negative class. This is the correct decision for the ten u i (since they cannot occur in a c-d context), but the incorrect decision for the x i (since they can occur in a c-d context). Thus, the accuracy of classifica- tion was 50% and not better than random. The SVMs trained on learning-based representations classified all twenty test words correctly.</p><p>Discussion. Representations of rare words in the PPMI model are sparse. The PPMI represen- tations of the u i and x i only contain two nonzero entries, one entry for an a i or c i (left context) and one entry for a b i or d i (right context). Given this sparseness, it is not surprising that representations are not a good basis for generalization and PPMI accuracy is random.</p><p>In contrast, learning-based models learn that the a i , b i , c i and d i form four different distributional classes. The final embeddings of the a i after learn- ing is completed are all close to each other and the same is true for the other three classes. Once the similarity of two words in the same distribu- tional class (say, the similarity of a 5 and a 7 ) has been learned, the contexts for the u i (resp. x i ) look essentially the same to embedding models as the contexts of the v i (resp. w i ). Thus, the embeddings learned for the u i will be similar to those learned for the v i . This explains why learning-based repre- sentations achieve perfect classification accuracy.</p><p>This sparseness experiment highlights an im- portant difference between count vectors and learned vectors. Count vector models are less robust in the face of sparseness and noise be- cause they base their representations on individ- ual contexts; the overall corpus distribution is only weakly taken into account, by way of PPMI weighting. In contrast, learned vector models make much better use of the overall corpus distri- bution and they can leverage second-order effects for learning improved representations. In our ex- ample, the second order effect is that the model first learns representations for the a i , b i , c i and d i and then uses these as a basis for inferring the sim- ilarity of u i to v i and of x i to w i .</p><formula xml:id="formula_2">1 P (AV1B|S) =10/20 2 P (CW1D|S)=9/20 3 P (CW2D|S)=β·1/20 4 P (AW2B|S) =(1 − β)·1/20 5 P (ai|A) =1/10 0 ≤ i ≤ 9 6 P (bi|B) =1/10 0 ≤ i ≤ 9 7 P (ci|C) =1/10 0 ≤ i ≤ 9 8 P (di|D) =1/10 0 ≤ i</formula><note type="other">≤ 9 9 P (vi|V1) =1/50 0 ≤ i ≤ 49 10 P (wi|W1) =1/45 5 ≤ i ≤ 49 11 P (wi|W2) =1/5 0 ≤ i</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Robustness against ambiguity</head><p>Grammar. The grammar in <ref type="figure">Figure 3</ref> generates two types of contexts that we interpret as two dif- Linear SVMs were trained for the binary clas- sification task on the train set. 50 trials of this experiment were run for each of eleven values of β: β = 2 −α where α ∈ {1.0, 1.1, 1.2, . . . , 2.0}. Thus, for the smallest value of α, α = 1.0, the two senses have the same frequency; for the largest value of α, α = 2.0, the dominant sense is three times as frequent as the less frequent sense.</p><p>Results. <ref type="figure" target="#fig_4">Figure 4</ref> shows accuracy of the classi-  fication on the test set: the proportion of correctly classified words out of a total of 250 (five words each in 50 trials). All models perform well for balanced sense fre- quencies; e.g., for α = 1.0, β = 0.5, the SVMs were all close to 100% accurate in predicting that the w i can occur in a c-d context. PPMI accuracy falls steeply when α is increased from 1.4 to 1.5. It has a 100% error rate for α ≥ 1.5. Learning-based models perform better in the order CBOW (least robust), LBL, SSKIP, SKIP, CWIN (most robust). Even for α = 2.0, CWIN and SKIP are still close to 100% accurate.</p><p>Discussion. The evaluation criterion we have used here is a classification task. The classifier at- tempts to answer a question that may occur in an application -can this word be used in this con- text? Thus, the evaluation criterion is: does the word representation contain a specific type of in- formation that is needed for the application.</p><p>Another approach to ambiguity is to compute multiple representations for a word, one for each sense. We generally do not yet know what the sense of a word is when we want to use its word representation, so data-driven approaches like clustering have been used to create represen- tations for different usage clusters of words that may capture some of its senses. For example, Reisinger and Mooney (2010) and <ref type="bibr" target="#b8">Huang et al. (2012)</ref> cluster the contexts of each word and then learn a different representation for each cluster. The main motivation for this approach is the as- sumption that single-word distributional represen- tations cannot represent all senses of a word well ( <ref type="bibr" target="#b8">Huang et al., 2012</ref>). However, <ref type="bibr" target="#b14">Li and Jurafsky (2015)</ref> show that simply increasing the dimension-</p><formula xml:id="formula_3">1 P (N Fn|S) =1/4 2 P (AFa|S) =1/4 3 P (N Mn|S) =1/4 4 P (AM f |S) =1/4 5 P (ni|N ) =1/5 0 ≤ i ≤ 4 6 P (ai|A) =1/5 0 ≤ i ≤ 4 7 P (x nf i U nf i |Fn) =1/5 0 ≤ i ≤ 4 8 P (f |U nf i ) =1/2 9 P (µ(U nf i )|U nf i ) =1/2 10 P (x af i U af i |Fa) =1/5 0 ≤ i ≤ 4 11 P (f |U af i ) =1/2 12 P (µ(U af i )|U af i ) =1/2 13 P (x nm i U nm i |Mn) =1/5 0 ≤ i ≤ 4 14 P (m|U nm i ) =1/2 15 P (µ(U nm i )|U nm i )=1/2 16 P (x am i U am i |M f ) =1/5 0 ≤ i ≤ 4 17 P (m|U am i ) =1/2 18 P (µ(U am i )|U am i ) =1/2</formula><p>Figure 5: This grammar generates nouns (x n. i ) and adjectives (x a.</p><p>i ) with masculine (x .m i ) and feminine (x .f i ) gender as well as paradigm features u i . µ maps each U to one of {u 0 . . . u 4 }. µ is randomly initialized and then kept fixed.</p><p>ality of single-representation gets comparable re- sults to using multiple-representation. Our results confirm that a single embedding can be robust against ambiguity, but also show the main chal- lenge: skewness of sense distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Accurate and consistent representation of multifacetedness</head><p>Grammar. The grammar shown in <ref type="figure">Figure 5</ref> mod- els two syntactic categories, nouns and adjectives, whose left context is highly predictable: it is one of five left context words n i (resp. a i ) for nouns, see lines 1, 3, 5 (resp. for adjectives, see lines 2, 4, 6). There are two grammatical genders: feminine (corresponding to the two symbols F n and F a ) and masculine (corresponding to the two symbols M n and M a ). The four combinations of syntac- tic category and gender are equally probable (lines 1-4). In addition to gender, nouns and adjec- tives are distinguished with respect to morpholog- ical paradigm. Line 7 generates one of five fem- inine nouns (x nf i ) and the corresponding paradigm marker U nf i . A noun has two equally probable right contexts: a context indicating its gender (line 8) and a context indicating its paradigm (line 9). µ is a function that maps each U to one of five mor- phological paradigms {u 0 . . . u 4 }. µ is randomly initialized before a corpus is generated and kept fixed.</p><p>The function µ models the assignment of paradigms to nouns and adjectives. Nouns and adjectives can have different (or the same) paradigms, but for a given noun or adjective the paradigm is fixed and does not change. Lines 7- 9 generate gender and paradigm markers for fem- inine nouns, for which we use the symbols x nf i . Lines 10-18 cover the three other cases: mas- culine nouns (x nm i ), feminine adjectives (x af i ) and masculine adjectives (x am i ). Dataset. We perform 10 trials. In each trial, µ is initialized randomly and a corpus of 100,000 sentences is generated. The train set consists of the feminine nouns (x nf i , line 7) and the masculine nouns (x nm i , line 13). The test set consists of the feminine (x af i ) and masculine (x am i ) adjectives. Results. Embeddings have been learned, SVMs are trained on the binary classification task femi- nine vs. masculine and evaluated on test. There was not a single error: accuracy of classifications is 100% for all embedding models.</p><p>Discussion. The facet gender is indicated di- rectly by the distribution and easy to learn. For a noun or adjective x, we simply have to check whether f or m occurs to its right anywhere in the corpus. PPMI stores this information in two di- mensions of the vectors and the SVM learns this fact perfectly. The encoding of "f or m occurs to the right" is less direct in the learning-based rep- resentation of x, but the experiment demonstrates that they also reliably encode it and the SVM reli- ably picks it up.</p><p>It would be possible to encode the facet in just one bit in a manually designed representation. While all representations are less compact than a one-bit representation -PPMI uses two real di- mensions, learning-based models use an activation pattern over several dimensions -it is still true that most of the capacity of the embeddings is used for encoding facets other than gender: syntactic cat- egories and paradigms. Note that there are five different instances each of feminine/masculine ad- jectives, feminine/masculine nouns and u i words, but only two gender indicators: f and m. This is a typical scenario across languages: words are distinguished on a large number of morphological, grammatical, semantic and other dimensions and each of these dimensions corresponds to a small fraction of the overall knowledge we have about a given word.</p><p>Point-based tests do not directly evaluate spe- cific facets of words. In similarity datasets, there is no individual test on facets -only full- space similarity is considered. There are test cases in analogy that hypothetically evaluate spe- cific facets like gender of words, as in king- man+woman=queen. However, it does not con- sider the impact of other facets and assumes the only difference of "king" and "queen" is gen- der. A clear example that words usually differ on many facets, not just one, is the analogy: Lon- don:England ∼ Ankara:Turkey. political-capital- of applies to both, cultural-capital-of only to Lon- don:England since Istanbul is the cultural capital of Turkey.</p><p>To make our argument more clear, we designed an additional experiment that tries to evaluate gen- der in our dataset based on similarity and anal- ogy methods. In the similarity evaluation, we search for the nearest neighbor of each word and accuracy is the proportion of nearest neighbors that have the same gender as the search word. In the analogy evaluation, we randomly select triples of the form</p><formula xml:id="formula_4">&lt;x c 1 g 1 i ,x c 1 g 2 j ,x c 2 g 2 k &gt; where (c 1 , c 2 ) ∈ {(noun, adjective), (adjective, noun)} and (g 1 , g 2 ) ∈ {(masculine, feminine), (feminine, masculine) }. We then compute s = x c 1 g 1 i − x c 1 g 2 j + x c 2 g 2 k</formula><p>and identify the word whose vec- tor is closest to s where the three vectors</p><formula xml:id="formula_5">x c 1 g 1 i , x c 1 g 2 j , x c 2 g 2 k</formula><p>are excluded. If the nearest neighbor of s is of type x c 2 g 1 l , then the search is successful; e.g., for s = x nf i − x nm j + x am k , the search is suc- cessful if the nearest neighbor is feminine. We did this evaluation on the same test set for PPMI and LBL embedding models. Error rates were 29% for PPMI and 25% for LBL (similarity) and 16% for PPMI and 14% for LBL (analogy). This high er- ror, compared to 0% error for SVM classification, indicates it is not possible to determine the pres- ence of a low entropy facet accurately and consis- tently when full-space similarity and analogy are used as test criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section, we first summarize and analyze the lessons we learned through experiments in Sec- tion 4. After that, we show how these lessons are supported by a real natural-language corpus. global statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Learned lessons</head><p>(ii) Embedding learning can have different ef- fectiveness for sparse vs. non-sparse events. Thus, models of representations should be evaluated with respect to their ability to deal with sparse- ness; evaluation data sets should include rare as well as frequent words.</p><p>(iii) Our results in Section 4.3 suggest that single-representation approaches can indeed rep- resent different senses of a word. We did a classi- fication task that roughly corresponds to the ques- tion: does this word have a particular meaning? A representation can fail on similarity judgement computations because less frequent senses occupy a small part of the capacity of the representa- tion and therefore have little impact on full-space similarity values. Such a failure does not neces- sarily mean that a particular sense is not present in the representation and it does not necessarily mean that single-representation approaches per- form poor on real-world tasks. However, we saw that even though single-representations do well on balanced senses, they can pose a challenge for am- biguous words with skewed senses.</p><p>(iv) Lexical information is complex and multi- faceted. In point-based tests, all dimensions are considered together and their ability to evaluate specific facets or properties of a word is limited. The full-space similarity of a word may be high- est to a word that has a different value on a low- entropy facet. Any good or bad result on these tasks is not sufficient to conclude that the repre- sentation is weak. The valid criterion of quality is whether information about the facet is consistently and accurately stored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Extrinsic evaluation: entity typing</head><p>To support the case for sub-space evaluation and also to introduce a new extrinsic task that uses the embeddings directly in supervised classification, we address a fine-grained entity typing task.</p><p>Learning taxonomic properties or types of words has been used as an evaluation method for word embeddings <ref type="bibr" target="#b23">(Rubinstein et al., 2015)</ref>. Since available word typing datasets are quite small (cf. <ref type="bibr" target="#b2">Baroni et al. (2014)</ref>, <ref type="bibr" target="#b23">Rubinstein et al. (2015)</ref>), entity typing can be a promising alter- native, which enables to do supervised classifi- cation instead of unsupervised clustering. Enti- ties, like other words, have many properties and therefore belong to several semantic types, e.g., "Barack Obama" is a POLITICIAN, AUTHOR and AWARD WINNER. We perform entity typing by learning types of knowledge base entities from their embeddings; this requires looking at sub- spaces because each entity can belong to multiple types.</p><p>We adopt the setup of <ref type="bibr" target="#b28">Yaghoobzadeh and Schütze (2015)</ref> who present a dataset of Freebase entities; 5 there are 102 types (e.g., <ref type="bibr">POLITICIAN FOOD, LOCATION-CEMETERY)</ref> and most entities have several. More specifically, we use a multi- layer-perceptron (MLP) with one hidden layer to classify entity embeddings to 102 FIGER types. To show the limit of point-based evaluation, we also experimentally test an entity typing model based on cosine similarity of entity embeddings. To each test entity, we assign all types of the entity closest to it in the train set. We call this approach 1NN (kNN for k = 1). <ref type="bibr">6</ref> We take part of ClueWeb, which is annotated with Freebase entities using automatic annota- tion of FACC1 7 ( <ref type="bibr" target="#b7">Gabrilovich et al., 2013</ref>), as our corpus. We then replace all mentions of entities with their Freebase identifier and learn embeddings of words and entities in the same space. Our corpus has around 6 million sen- tences with at least one annotated entity. We calculate embeddings using our different models. Our hyperparameters: for learning-based mod- els: dim=100, neg=10, iterations=20, window=1, sub=10 −3 ; for PPMI: SVD-dim=100, neg=1, win- dow=1, cds=0.75, sub=10 −3 , eig=0.5. See ( <ref type="bibr" target="#b13">Levy et al., 2015</ref>) for more information about the mean- ing of hyperparameters. <ref type="table">Table 1</ref> gives results on test for all (about 60,000 entities), head (freq &gt; 100; about 12,200 enti- ties) and tail (freq &lt; 5; about 10,000 entities). The MLP models consistently outperform 1NN on 5 cistern.cis.lmu.de/figment <ref type="bibr">6</ref> We tried other values of k, but results were not better. 7 lemurproject.org/clueweb12/FACC1 243 all and tail entities. This supports our hypothe- sis that only part of the information about types that is present in the vectors can be determined by similarity-based methods that use the overall di- rection of vectors, i.e., full-space similarity.</p><p>There is little correlation between results of MLP and 1NN in all and head entities, and the correlation between their results in tail entities is high. 8 For example, for all entities, using 1NN, SKIP is 4.3% (4.1%) better, and using MLP is 1.7% (1.6%) worse than SSKIP (CWIN). The good performance of SKIP on 1NN using cosine similarity can be related to its objective function, which maximizes the cosine similarity of cooccur- ing token embeddings.</p><p>The important question is not similarity, but whether the information about a specific type ex- ists in the entity embeddings or not. Our results confirm our previous observation that a classifica- tion by looking at subspaces is needed to answer this question. In contrast, based on full-space sim- ilarity, one can infer little about the quality of em- beddings. Based on our results, SSKIP and CWIN embeddings contain more accurate and consistent information because MLP classifier gives better results for them. However, if we considered 1NN for comparison, SKIP and CBOW would be supe- rior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>We have introduced a new way of evaluating dis- tributional representation models. As an alterna- tive to the common evaluation tasks, we proposed to identify generic criteria that are important for an embedding model to represent properties of words accurately and consistently. We suggested four criteria based on fundamental characteristics of natural language and designed tests that evaluate models on the criteria. We developed this evalua- tion methodology using PCFG-generated corpora and applied it on a case study to compare different models of learning distributional representations.</p><p>While we showed important differences of the embedding models, the goal was not to do a com- prehensive comparison of them. We proposed an innovative way of doing intrinsic evaluation of embeddings. Our evaluation method gave direct insight about the quality of embeddings. Addi- tionally, while most intrinsic evaluations consider <ref type="bibr">8</ref> The spearman correlation between MLP and 1NN for all=0. <ref type="bibr">31, head=0.03, tail=0.75.</ref> word vectors as points, we used classifiers that identify different small subspaces of the full space. This is an important desideratum when designing evaluation methods because of the multifaceted- ness of natural language words: they have a large number of properties, each of which only occupies a small proportion of the full-space capacity of the embedding.</p><p>Based on this paper, there are serveral lines of investigation we plan to conduct in the future. (i) We will attempt to support our results on arti- ficially generated corpora by conducting experi- ments on real natural language data. (ii) We will study the coverage of our four criteria in evalu- ating word representations. (iii) We modeled the four criteria using separate PCFGs, but they could also be modeled by one single unified PCFG. One question that arises is then to what extent the four criteria are orthogonal and to what extent interde- pendent. A single unified grammar may make it harder to interpret the results, but may give addi- tional and more fine-grained insights as to how the performance of embedding models is influenced by different fundamental properties of natural lan- guage and their interactions.</p><p>Finally, we have made the simplifying assump- tion in this paper that the best conceptual frame- work for thinking about embeddings is that the embedding space can be decomposed into sub- spaces: either into completely orthogonal sub- spaces or -less radically -into partially "over- lapping" subspaces. Furthermore, we have made the assumption that the smoothness and robustness properties that are the main reasons why embed- dings are used in NLP can be reduced to similar- ities in subspaces. See  and  for work that makes similar assumptions.</p><p>The fundamental assumptions here are decom- posability and linearity. The smoothness proper- ties could be much more complicated. However even if this was the case, then much of the gen- eral framework of what we have presented in this paper would still apply; e.g., the criterion that a particular facet be fully and correctly represented is as important as before. But the validity of the assumption that embedding spaces can be decom- posed into "linear" subspaces should be investi- gated in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(i) vLBL (henceforth: LBL) (vectorized log-bilinear lan- guage model) (Mnih and Kavukcuoglu, 2013), (ii) SkipGram (henceforth: SKIP) (skipgram bag- of-word model), (iii) CBOW (continuous bag-of- word model (Mikolov et al., 2013), (iv) Struc- tured SkipGram (henceforth SSKIP), (Ling et al., 2015) and CWindow (henceforth CWIN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: In language L , frequent v i and rare u i occur in a-b contexts; frequent w i and rare x i occur in c-d contexts. Word representations should encode possible contexts (a-b vs. c-d) for both frequent and rare words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>≤ 4 Figure 3 :</head><label>43</label><figDesc>Figure 3: Ambiguity grammar. v i and w 5. .. w 49 occur in a-b and c-d contexts only, respectively. w 0. .. w 4 are ambiguous and occur in both contexts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ferent meanings: a-b contexts (lines 1,4) and c-d contexts (lines 2, 3). v i occur only in a-b contexts (line 1), w 5 . . . w 49 occur only in c-d contexts (line 2); thus, they are unambiguous. w 0 . . . w 4 are am- biguous and occur with probability β in c-d con- texts (line 3) and with probability (1 − β) in a-b contexts (lines 3, 4). The parameter β controls the skewedness of the sense distribution; e.g., the two senses are equiprobable for β = 0.5 and the sec- ond sense (line 4) is three times as probable as the first sense (line 3) for β = 0.25. Dataset. The grammar specified in Figure 3 was used to generate a training corpus of 100,000 sentences. Label criterion: A word is labeled posi- tive if it can occur in a c-d context, as negative oth- erwise. The test set consists of the five ambiguous words w 0 . . . w 4 . All other words are assigned to the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: SVM classification results for the ambiguity dataset. X-axis: α = − log 2 β. Y-axis: classification accuracy:</figDesc></figure>

			<note place="foot" n="1"> code.google.com/archive/p/word2vec 2 github.com/wlin12/wang2vec 3 github.com/licstar/compare 4 bitbucket.org/omerlevy/hyperwords</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study on similarity and relatedness using distributional and wordnet-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><forename type="middle">B</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kravalova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings</title>
		<meeting><address><addrLine>Boulder, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-05-31" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributional memory: A general framework for corpus-based semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="673" to="721" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language universals and linguistic typology: Syntax and morphology. Blackwell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Comrie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Cruse</surname></persName>
		</author>
		<title level="m">Lexical Semantics</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Facc1: Freebase annotation of clueweb corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference</title>
		<meeting><address><addrLine>Jeju Island</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2012-07-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What?s in an embedding? analyzing word embeddings through multilingual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Köhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="2067" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<idno>abs/1507.05523</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Do multi-sense embeddings improve natural language understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1722" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Two/too simple adaptations of word2vec for syntax problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-31" />
			<biblScope unit="page" from="1299" to="1304" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2015</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-prototype vector-space models of word meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Word embedding calculus in meaningful ultradense subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ultradense embeddings by orthogonal transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How well do distributional models capture different types of semantic knowledge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Effi</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="726" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The Word-Space Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Stockholm University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluation methods for unsupervised word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="298" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Evaluation of word vector representations by subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="2049" to="2054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res. (JAIR)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Corpus-level fine-grained entity typing using contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadollah</forename><surname>Yaghoobzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="715" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">It makes sense: A wide-coverage word sense disambiguation system for free text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2010, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Uppsala, Sweden, System Demonstrations</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07-11" />
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
