<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Information Extraction and Reasoning: A Scalable Statistical Relational Learning Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Department</orgName>
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
							<email>wcohen@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Information Extraction and Reasoning: A Scalable Statistical Relational Learning Approach</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="355" to="364"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A standard pipeline for statistical rela-tional learning involves two steps: one first constructs the knowledge base (KB) from text, and then performs the learning and reasoning tasks using probabilis-tic first-order logics. However, a key issue is that information extraction (IE) errors from text affect the quality of the KB, and propagate to the reasoning task. In this paper, we propose a statistical rela-tional learning model for joint information extraction and reasoning. More specifically , we incorporate context-based entity extraction with structure learning (SL) in a scalable probabilistic logic framework. We then propose a latent context invention (LCI) approach to improve the performance. In experiments, we show that our approach outperforms state-of-the-art baselines over three real-world Wikipedia datasets from multiple domains; that joint learning and inference for IE and SL significantly improve both tasks; that latent context invention further improves the results .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information extraction (IE) is often an early stage in a pipeline that contains non-trivial downstream tasks, such as question answering <ref type="bibr" target="#b18">(Mollá et al., 2006</ref>), machine translation ( <ref type="bibr" target="#b2">Babych and Hartley, 2003)</ref>, or other applications ( <ref type="bibr" target="#b27">Wang and Hua, 2014;</ref><ref type="bibr" target="#b15">Li et al., 2014</ref>). Knowledge bases (KBs) populated by IE techniques have also been used as an input to systems that learn rules allowing further infer- ences to be drawn from the KB ( <ref type="bibr" target="#b13">Lao et al., 2011</ref>), a task sometimes called KB completion <ref type="bibr" target="#b24">(Socher et al., 2013;</ref><ref type="bibr" target="#b31">West et al., 2014</ref>). Pipelines of this sort frequently suffer from error cascades, which reduces performance of the full system <ref type="bibr">1</ref> .</p><p>In this paper, we address this issue, and pro- pose a joint model system for IE and KB com- pletion in a statistical relational learning (SRL) setting <ref type="bibr" target="#b26">(Sutton and McCallum, 2006;</ref><ref type="bibr" target="#b10">Getoor and Taskar, 2007</ref>). In particular, we outline a system which takes as input a partially-populated KB and a set of relation mentions in context, and jointly learns: 1) how to extract new KB facts from the relation mentions, and; 2) a set of logical rules that allow one to infer new KB facts. Evaluation of the KB facts inferred by the joint system shows that the joint model outperforms its individual com- ponents. We also introduce a novel extension of this model called Latent Context Invention (LCI), which associates latent states with context features for the IE component of the model. We show that LCI further improves performance, leading to a substantial improvement over prior state-of-the-art methods for joint relation-learning and IE.</p><p>To summarize our contributions:</p><p>• We present a joint model for IE and re- lational learning in a statistical relational learning setting which outperforms universal schemas ( <ref type="bibr" target="#b22">Riedel et al., 2013</ref>), a state-of-the- art joint method;</p><p>• We incorporate latent context into the joint SRL model, bringing additional improve- ments.</p><p>In next section, we discuss related work. We describe our approach in Section 3. The details of the datasets are introduced in Section 4. We show experimental results in Section 5, discuss in Section 6, and conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In NLP, our work clearly aligns with recent work on joint models of individual text processing tasks. For example, <ref type="bibr" target="#b8">Finkel and Manning (2009)</ref> work on the problem of joint IE and parsing, where they use tree representations to combine named entities and syntactic chunks. Recently, <ref type="bibr" target="#b7">Devlin et al. (Devlin et al., 2014</ref>) use a joint neural network model for machine translation, and obtain an impressive 6.3 BLEU point improvement over a hierarchical phrase-based system.</p><p>In information extraction, weak supervi- sion ( <ref type="bibr" target="#b4">Craven et al., 1999;</ref><ref type="bibr" target="#b17">Mintz et al., 2009</ref>) is a common technique for extracting knowledge from text, without large-scale annotations. In extracting Infobox information from Wikipedia text, <ref type="bibr" target="#b34">Wu and Weld (2007;</ref> also use a similar idea. In an open IE project, <ref type="bibr" target="#b3">Banko et al. (2007)</ref> use a seed KB, and utilize weak supervision techniques to extend it. Note that weakly supervised extraction approaches can be noisy, as a pair of entities in context may be associated with one, none, or several of the possible relation labels, a property which complicates the application of distant supervision methods ( <ref type="bibr" target="#b17">Mintz et al., 2009;</ref><ref type="bibr" target="#b21">Riedel et al., 2010;</ref><ref type="bibr" target="#b11">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b25">Surdeanu et al., 2012)</ref>. <ref type="bibr" target="#b14">Lao et al. (2012)</ref> learned syntactic rules for find- ing relations defined by "lexico-semantic" paths spanning KB relations and text data. <ref type="bibr" target="#b30">Wang et al. (2015)</ref> extends the methods used by Lao et al. to learn mutually recursive relations. Recently, <ref type="bibr" target="#b22">Riedel et al. (2013)</ref> propose a matrix factoriza- tion technique for relation embedding, but their method requires a large amount of negative and unlabeled examples. <ref type="bibr" target="#b32">Weston et al. (2013)</ref> con- nect text with KB embedding by adding a scoring term, though no shared parameters/embeddings are used. All these prior works make use of text and KBs. Unlike these prior works, our method is posed in an SRL setting, using a scalable proba- bilistic first-order logic, and allows learning of re- lational rules that are mutually recursive, thus al- lowing learning of multi-step inferences. Unlike some prior methods, our method also does not re- quire negative examples, or large numbers of un- labeled examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>In this section, we first briefly review the se- mantics, inference, and learning procedures of a about(X,Z) :-handLabeled(X,Z) # base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>about(X,Z) :-sim(X,Y),about(Y,Z) # prop. sim(X,Y) :-links(X,Y) # sim,link. sim(X,Y) :- hasWord(X,W),hasWord(Y,W), linkedBy(X,Y,W) # sim,word. linkedBy(X,Y,W) :-true</head><p># by(W). newly proposed scalable probabilistic logic called ProPPR ( <ref type="bibr" target="#b28">Wang et al., 2013;</ref>). Then, we describe the joint model for information extraction and relational learning. Finally, a latent context invention theory is proposed for enhancing the performance of the joint model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ProPPR: Background</head><p>Below we will give an informal description of ProPPR, based on a small example. More formal descriptions can be found elsewhere ( <ref type="bibr" target="#b28">Wang et al., 2013)</ref>. ProPPR (for Programming with Personalized PageRank) is a stochastic extension of the logic programming language Prolog. A simple program in ProPPR is shown in <ref type="table" target="#tab_0">Table 1</ref>. Roughly speak- ing, the upper-case tokens are variables, and the ":-" symbol means that the left-hand side (the head of a rule) is implied by the conjunction of condi- tions on the right-hand size (the body). In addition to the rules shown, a ProPPR program would in- clude a database of facts: in this example, facts would take the form handLabeled(page,label), hasWord(page,word), or linkedBy(page1,page2), representing labeled training data, a document- term matrix, and hyperlinks, respectively. The condition "true" in the last rule is "syntactic sugar" for an empty body.</p><p>In ProPPR, a user issues a query, such as "about(a,X)?", and the answer is a set of possible bindings for the free variables in the query (here there is just one such varable, "X"). To answer the query, ProPPR builds a proof graph. Each node in the graph is a list of conditions R 1 , . . . , R k that remain to prove, interpreted as a conjunction. To find the children of a node R1, . . . , Rk, you look for either 1. database facts that match R 1 , in which case the appropriate variables are bound, and R 1 is removed from the list, or; 2. a rule A ← B 1 , . . . , B m with a head A that matches R 1 , in which case again the appro- priate variables are bound, and R 1 is replaced with the body of the rule, resulting in the new list B 1 , . . . , B m , R 2 , . . . , R k .</p><p>The procedures for "matching" and "appropriately binding variables" are illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. <ref type="bibr">2</ref> An empty list of conditions (written 2 in the fig- ure) corresponds to a complete proof of the ini- tial query, and by collecting the required variable bindings, this proof can be used to determine an answer to the initial query. In Prolog, this proof graph is constructed on- the-fly in a depth-first, left-to-right way, returning the first solution found, and backtracking, if re- quested, to find additional solutions. In ProPPR, however, we will define a stochastic process on the graph, which will generate a score for each node, and hence a score for each answer to the query. The stochastic process used in ProPPR is personalized PageRank ( <ref type="bibr" target="#b19">Page et al., 1998;</ref><ref type="bibr" target="#b6">Csalogny et al., 2005</ref>), also known as random-walk- with-restart. Intuitively, this process upweights solution nodes that are reachable by many short proofs (i.e., short paths from the query node.) For- mally, personalized PageRank is the fixed point of the iteration</p><formula xml:id="formula_0">p t+1 = αχ v 0 + (1 − α)W p t (1) 2</formula><p>The edge annotations will be discussed later.</p><p>where p[u] is the weight assigned to u, v 0 is the seed (i.e., query) node, χ v 0 is a vector with</p><formula xml:id="formula_1">χ v 0 [v 0 ] = 1 and χ v 0 [u] = 0 for u = v, and W is a matrix of transition probabilities, i.e., W [v, u]</formula><p>is the probability of transitioning from node u to a child node v. The parameter α is the reset proba- bility, and the transition probabilities we use will be discussed below.</p><p>Like Prolog, ProPPR's proof graph is also con- structed on-the-fly, but rather than using depth- first search, we use PageRank-Nibble, a fast ap- proximate technique for incrementally exploring a large graph from a an initial "seed" node <ref type="bibr" target="#b0">(Andersen et al., 2008)</ref>. PageRank-Nibble takes a param- eter and will return an approximationˆpapproximationˆ approximationˆp to the personalized PageRank vector p, such that each node's estimated probability is within of correct.</p><p>We close this background section with some fi- nal brief comments about ProPPR.</p><p>Scalability. ProPPR is currently limited in that it uses memory to store the fact databases, and the proof graphs constructed from them. ProPPR uses a special-purpose scheme based on sparse matrix representations to store facts which are triples, which allows it to accomodate databases with hun- dreds of millions of facts in tens of gigabytes.</p><p>With respect to run-time, ProPPR's scalabil- ity is improved by the fast approximate inference scheme used, which is often an order of mag- nitude faster than power iteration for moderate- sized problems ( <ref type="bibr" target="#b28">Wang et al., 2013</ref>). Experimen- tation and learning are also sped up because with PageRank-Nibble, each query is answered using a "small"-size O( 1 αα )-proof graph. Many opera- tions required in learning and experimentation can thus be easily parallized on a multi-core machine, by simply distributing different proof graphs to different threads.</p><p>Parameter learning. Personalized PageRank scores are defined by a transition probability matrix W , which is parameterized as follows. ProPPR allows "feature generators" to be attached to its rules, as indicated by the code after the hash- tags in the example program. <ref type="bibr">3</ref> Since edges in the proof graph correspond to rule matches, the edges can also be labeled by features, and a weighted combination of these features can be used to de- fine a total weight for each edge, which finally can be normalized used to define the transition matrix W . Learning can be used to tune these weights to data; ProPPR's learning uses a parallelized SGD method, in which inference on different examples is performed in different threads, and weight up-dates are synchronized.</p><p>Structure learning. Prior work (  has studied the problem of learning a ProPPR theory, rather than simply tuning parame- ters in an existing theory, a process called structure learning (SL). In particular,  propose a scheme called the structural gradient which scores rules in some (possibly large) user- defined space R of potential rules, which can be viewed as instantiations of rule templates, such as the ones shown in the left-hand side of <ref type="table">Table 2</ref>.</p><p>For completeness, we will summarize briefly the approach used in ( ). The space of potential rules R is defined by a "second- order abductive theory", which conceptually is an interpreter that constructs proofs using all rules in R. Each rule template is mapped to two clauses in the interpreter: one simulates the template (for any binding), and one "abduces" the specific bind- ing (facts) from the KB. Associated with the use of the abductive rule is a feature corresponding to a particular binding for the template. The gradient of these features indicates which instantiated rules can be usefully added to the theory. More details can be found in ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rule template</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ProPPR clause Structure learning (a) P(X,Y) :-R(X,Y) interp(P,X,Y) :-interp0(R,X,Y),abduce if(P,R). abduce if(P,R) :-true # f if(P,R). (b) P(X,Y) :-R(Y,X) interp(P,X,Y) :-interp0(R,Y,X),abduce ifInv(P,R). abduce ifInv(P,R) :-true # f ifInv(P,R). (c) P(X,Y) :-R1(X,Z),R2(Z,Y)</head><p>interp(P,X,Y) :-interp0(R1,X,Z),interp0(R2,Z,Y), abduce chain(P,R1,R2). abduce chain(P,R1,R2) :-true # f chain(P,R1,R2). base case for SL interpreter interp0(P,X,Y) :-rel(R,X,Y). insertion point for learned rules interp0(P,X,Y) :-any rules learned by SL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information extraction (d) R(X,Y) :-link(X,Y,W), interp(R,X,Y) :-link(X,Y,W),abduce indicates(W,R). indicates(W,R). abduce indicates(W,R) :-true #f ind1(W,R). (e) R(X,Y) :-link(X,Y,W1), interp(R,X,Y) :-link(X,Y,W1),link(X,Y,W2), link(X,Y,W2),</head><p>abduce indicates(W1,W2,R). indicates(W1,W2,R). abduce indicates(W1,W2,R) :-true #f ind2(W1,W2,R). <ref type="table">Table 2</ref>: The ProPPR template and clauses for joint structure learning and information extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent context invention (f) R(X,Y) :-latent(L), interp(R,X,Y) :-latent(L),link(X,Y,W),abduce latent(W,L,R). link(X,Y,W), abduce latent(W,L,R) :-true #f latent1(W,L,R). indicates(W,L,R) (g) R(X,Y) :-latent(L1),latent(L2) interp(R,X,Y) :-latent(L1),latent(L2),link(X,Y,W), link(X,Y,W), abduce latent(W,L1,L2,R). indicates(W,L1,L2,R) abduce latent(W,L1,L2,R) :-true #f latent2(W,L1,L2,R).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Joint Model for IE and SRL</head><p>Dataset Generation The KBs and text used in our experiments were derived from Wikipedia. Briefly, we choose a set of closely-related pages from a hand-selected Wikipedia list. These pages define a set of entities E, and a set of commonly- used Infobox relations R between these entities define a KB. The relation mentions are hyperlinks between the pages, and the features of these rela- tion mentions are words that appear nearby these links. This information is encoded in a single rela- tion link(X,Y,W), which indicates that there is hy- perlink between Wikipedia pages X to Y which is near the context word W . The Infobox relation triples are stored in another relation, rel(R,X,Y). 4 <ref type="figure" target="#fig_1">Figure 2</ref> shows an example. We first find the "European royal families" to find a list of enti-ties E. This list contains the page "Louis VI of France", the source entity, which contains an out- link to the target entity page "Philip I of France". On the source page, we can find the following text: "Louis was born in Paris, the son of Philip I and his first wife, Bertha of Holland." From Infobox data, we also may know of a relationship between the source and target entities: in this case, the tar- get entity is the parent of the source entity.</p><p>Theory for Joint IE and SL The structure learn- ing templates we used are identical to those used in prior work ( ), and are summa- rized by the clauses (a-c) in <ref type="table">Table 2</ref>. In the tem- plates in the left-hand side of the table, P , R, R1 and R2 are variables in the template, which will be bound to specific relations found to be useful in prediction. (The interpreter rules on the right- hand side are provided for completeness, and can be ignored by readers not deeply familiar with the work of ( .)</p><p>The second block of the table contains the tem- plates used for IE. For example, to understand template (d), recall that the predicate link in- dicates a hyperlink from Wikipedia page X to Y , which includes the context word W between two entities X and Y . The abductive predicate abduce indicates activates a feature template, in which we learn the degree of association of a con- text word and a relation from the training data. These rules essentially act as a trainable classi- fier which classifies entity pairs based on the hy- perlinks they that contain them, and classifies the hyperlinks according to the relation they reflect, based on context-word features.</p><p>Notice that the learner will attempt to tune word associations to match the gold rel facts used as training data, and that doing this does not require assigning labels to individual links, as would be done in a traditional distant supervision setting: instead these labels are essentially left latent in this model. Similar to "deep learning" approaches, the latent assignments are provided not by EM, but by hill-climbing search in parameter space.</p><p>A natural extension to this model is to add a bilexical version of this classifier in clause (e), where we learn a feature which conjoins word W 1, word W 2, and relation R.</p><p>Combining the clauses from (a) to (e), we de- rive a hybrid theory for joint SL and IE: the struc- ture learning section involves a second-order prob- abilistic logic theory, where it searches the rela- tional KB to form plausible first-order relational inference clauses. The information extraction sec- tion from (d) to (e) exploits the distributional sim- ilarity of contextual words for each relation, and extracts relation triples from the text, using distant supervision and latent labels for relation mentions (which in our case are hyperlinks). Training this theory as a whole trains it to perform joint reason- ing to facts for multiple relations, based on rela- tions that are known (from the partial KB) or in- ferred from the IE part of the theory. Both param- eters for the IE portion of the theory and inference rules between KB relations are learned. <ref type="bibr">5</ref> Latent Context Invention Note that so far both the IE clauses (d-e) are fully observable: there are no latent predicates or variables. Recent work ( <ref type="bibr" target="#b22">Riedel et al., 2013)</ref> suggests that learning latent representations for words improves perfor- mance in predicting relations. Perhaps this is be- cause such latent representations can better model the semantic information in surface forms, which are often ambiguous.</p><p>We call our method latent context invention (LCI), and it is inspired from literature in predi- cate invention <ref type="bibr" target="#b12">(Kok and Domingos, 2007)</ref>. <ref type="bibr">6</ref> LCI applies the idea of predicate invention to the con- text space: instead of inventing new predicates, we now invent a latent context property that captures the regularities among the similar relational lex- ical items. To do this, we introduce some addi- tional rules of the form latent(1) :-true, latent <ref type="formula">(2)</ref> :-true, etc, and allow the learner to find appro- priate weights for pairing these arbitrarily-chosen values with specific words. This is implemented by template (f) in <ref type="table">Table 2</ref>. Adding this to the joint theory means that we will learn to map surface- level lexical items (words) to the "invented" latent context values and also to relation.</p><p>Another view of LCI is that we are learning a la- tent embedding of words jointly with relations. In template (f) we model a single latent dimension, but to model higher-dimensional latent variables, we can add the clauses such as (g), which con- structs a two-dimensional latent space. Below we will call this variant method hLCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets</head><p>Using the data generation process that we de- scribed in subsection 3.2, we extract two datasets from the supercategories of "European royal fam- ilies" and "American people of English descent, and third geographic dataset using three lists: "List of countries by population", "List of largest cities and second largest cities by country" and "List of national capitals by population".</p><p>For the royal dataset, we have 2,258 pages with 67,483 source-context-target mentions, and we use 40,000 for training, and 27,483 for test- ing. There are 15 relations 7 . In the Amer- ican dataset, we have 679 pages with 11,726 mentions, and we use 7,000 for training, and 4,726 for testing. This dataset includes 30 re- lations 8 . As for the Geo dataset, there are 497 <ref type="bibr">6</ref> To give some background on this nomenclature, we note that the SL method is inspired by Cropper and Muggleton's Metagol system <ref type="bibr" target="#b5">(Cropper and Muggleton, 2014)</ref>, which in- cludes predicate invention. In principle predicates could be invented by SL, by extending the interpreter to consider "in- vented" predicate symbols as binding to its template vari- ables (e.g., P and R); however, in practice invented predi- cates leads to close dependencies between learned rules, and are highly sensitive to the level of noise in the data. <ref type="bibr">7</ref> birthPlace, child, commander, deathPlace, keyPerson, knownFor, monarch, parent, partner, predecessor, relation, restingPlace, spouse, successor, territory 8 architect, associatedBand, associatedMusicalArtist, au-pages with 43,475 mentions, and we use 30,000 for training, and 13,375 for testing. There are 10 relations <ref type="bibr">9</ref> . The datasets are freely available for download at http://www.cs.cmu.edu/ ˜ yww/data/jointIE+Reason.zip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To evaluate these methods, we use the setting of Knowledge Base completion ( <ref type="bibr" target="#b24">Socher et al., 2013;</ref><ref type="bibr" target="#b31">West et al., 2014</ref>). We ran- domly remove a fixed percentage of facts in a training knowledge base, train the learner from the partial KB, and use the learned model to pre- dict facts in the test KB. KB completion is a well- studied task in SRL, where multiple relations are often needed to fill in missing facts, and thus reconstruct the incomplete KB. Following prior work ( <ref type="bibr" target="#b22">Riedel et al., 2013;</ref><ref type="bibr" target="#b28">Wang et al., 2013</ref>), we use mean average precision (MAP) as the evalua- tion metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines</head><p>To understand the performance of our joint model, we compare with three prior methods. Struc- ture Learning (SL) includes the second-order re- lation learning templates (a-c) from <ref type="table">Table 2</ref>. In- formation Extraction (IE) includes only tem- plates (d) and (e). Markov Logic Networks (MLN) is the Alchemy's implementation 10 of Markov Logic Networks ( <ref type="bibr" target="#b20">Richardson and Domingos, 2006</ref>), using the first-order clauses learned from SL method <ref type="bibr">11</ref> . We used conjugate gradient weight learning <ref type="bibr" target="#b16">(Lowd and Domingos, 2007</ref>) with 10 iterations. Finally, Universal Schema is a state-of-the-art matrix factorization based univer- sal method for jointly learning surface patterns and relations. We used the code and parameter settings for the best-performing model (NFE) from ( <ref type="bibr" target="#b22">Riedel et al., 2013)</ref>. As a final baseline method, we considered a simpler approach to clustering context words, thor, birthPlace, child, cinematography, deathPlace, direc- tor, format, foundationOrganisation, foundationPerson, in- fluenced, instrument, keyPerson, knownFor, location, mus- icComposer, narrator, parent, president, producer, relation, relative, religion, restingPlace, spouse, starring, successor, writer 9 archipelago, capital, country, daylightSavingTimeZone, largestSettlement, leaderTitle, mottoFor, timeZone, twinCity, twinCountry 10 http://alchemy.cs.washington.edu/ <ref type="bibr">11</ref> We also experimented with Alchemy's structure learn- ing, but it was not able to generate results in 24 hours. which we called Text Clustering, which used the following template:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R(X,Y) :- clusterID(C),link(X,Y,W), cluster(C,W),related(R,W).</head><p>Here surface patterns are grouped to form latent clusters in a relation-independent fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The Effectiveness of the Joint Model</head><p>Our experimental results are shown in 3. The left- most part of the table concerns the Royal dataset. We see that the universal schema approach out- performs the MLN baseline in most cases, but ProPPR's SL method substantially improves over MLN's conjugated gradient learning method, and the universal schema approach. This is perhaps surprising, as the universal schema approach is also a joint method: we note that in our datasets, unlike the New York Times corpus used in ( <ref type="bibr" target="#b22">Riedel et al., 2013)</ref>, large numbers of unlabeled examples are not available. The unigram and bilexical IE models in ProPPR also perform well-better than SL on this data. The joint model outperforms the baselines, as well as the separate models. The dif- ference is most pronounced when the background KB gets noisier: the improvement with 10% miss- ing setting is about 1.5 to 2.3% MAP, while with 50% missing data, the absolute MAP improve- ment is from 8% to 10%.</p><p>In the next few columns of <ref type="table" target="#tab_2">Table 3</ref>, we show the KB completion results for the Geo dataset. This dataset has fewer relations, and the most com- mon one is country. The overall MAP scores are much higher than the previous dataset. MLN's re- sults are good, but still generally below the uni- versal schema method. On this dataset, the uni- versal schema method performs better than the IE only model for ProPPR in most settings. However, the ProPPRjoint model still shows large improve- ments over individual models and the baselines: the absolute MAP improvement is 22.4%.</p><p>Finally, in the rightmost columns of <ref type="table" target="#tab_2">Table 3</ref>, we see that the overall MAP scores for the Ameri- can dataset are relatively lower than other datasets, perhaps because it is the smallest of the three. The universal schema approach consistently out- performs the MLN model, but not ProPPR. On this dataset the SL-only model in ProPPR outperforms the IE-only models; however, the joint models still outperform individual ProPPR models from 1.5% to 6.4% in MAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Royal</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geo</head><p>American % missing 10% 20% 30% 40% 50% 10% 20% 30% 40% 50% 10% 20% 30% 40% 50%  The averaged training runtimes on an ordinary PC for unigram joint model on the above Royal, Geo, American datasets are 38, 36, and 29 sec- onds respectively, while the average testing times are 11, 10, and 9 seconds. For bilexical joint mod- els, the averaged training times are 25, 10, and 10 minutes respectively, whereas the testing times are 111, 28, and 26 seconds respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The Effectiveness of LCI</head><p>Finally we consider the latent context invention (LCI) approach. The last three rows of <ref type="table" target="#tab_2">Table 3</ref> show the performances of LCI and hHCI. We com- pare it here with the best previous approach, the joint IE + SL model, and text clustering approach.</p><p>For the Royal dataset, first, the LCI and hLCI models clearly improve over joint IE and SL. In noisy conditions of missing 50% facts, the biggest improvement of LCI/hLCI is 2.4% absolute MAP.</p><p>From the Geo dataset, we see that the joint mod- els and joint+latent models have similar perfor- mances in relatively clean conditions (10%-30%) facts missing. However, in noisy conditions, we the LCI and hLCI model has an advantage of be- tween 1.5% to 1.8% in absolute MAP.</p><p>Finally, the results for the American dataset show a consistent trend: again, in noisy condi- tions (missing 40% to 50% facts), the latent con- text models outperform the joint IE + SL models by 2.9% and 3.7% absolute MAP scores.</p><p>Although the LCI approach is inspired by pred- icate invention in inductive logic programming, our result is also consistent with theories of gen- eralized latent variable modeling in probabilis- tic graphical models and statistics <ref type="bibr" target="#b23">(Skrondal and Rabe-Hesketh, 2004</ref>): modeling hidden variables helps take into account the measurement (observa- tion) errors <ref type="bibr" target="#b9">(Fornell and Larcker, 1981)</ref> and results in a more robust model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussions</head><p>Compared to state-of-the-art joint models ( <ref type="bibr" target="#b22">Riedel et al., 2013</ref>) that learn the latent factor represen- tations, our method gives strong improvements in performance on three datasets with various set- tings. Our model is also trained to retrieve a target entity from a relation name plus a source entity, and does not require large samples of unlabeled or negative examples in training.</p><p>Another advantage of the ProPPR model is that they are explainable. For example, below are the features with the highest weights after joint learn- ing from the Royal dataset, written as predicates or rules:</p><p>indicates("mother",parent) indicates("king",parent) indicates("spouse",spouse) indicates("married",spouse) indicates("succeeded",successor Here we see that our model is able to learn that the keywords "mother" and "king" that are indicators of the relation parent, that the keywords "spouse" and "married" indicate the relation spouse, and the keywords "succeeded" and "son" indicate the re- lation successor. Interestingly, our joint model is also able to learn the inverse relation successor for the relation parent, as well as the similar relational predicate predecessor for parent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper, we address the issue of joint infor- mation extraction and relational inference. To be more specific, we introduce a holistic probabilis- tic logic programming approach for fusing IE con- texts with relational KBs, using locally groundable inference on a joint proof graph. We then propose a latent context invention technique that learns relation-specific latent clusterings for words. In experiments, we show that joint modeling for IE and SRL improves over prior state-of-the-art base- lines by large margins, and that the LCI model outperforms various fully baselines on three real- world Wikipedia dataset from different domains. In the future, we are interested in extending these techniques to also exploit unlabeled data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A partial proof graph for the query about(a,Z). The upper right shows the link structure between documents a, b, c, and d, and some of the words in the documents. Restart links are not shown.</figDesc><graphic url="image-1.png" coords="3,83.38,62.80,430.80,211.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The data generation example as described in subsection 3.2.</figDesc><graphic url="image-2.png" coords="4,72.00,62.81,453.55,273.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) indicates("son",successor) parent(X,Y) :-successor(Y,X) successor(X,Y) :-parent(Y,X) spouse(X,Y) :-spouse(Y,X) parent(X,Y) :-predecessor(X,Y) successor(Y,X) :-spouse(X,Y) predecessor(X,Y) :-parent(X,Y)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>A simple program in ProPPR. See text for 
explanation. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>The MAP results for KB completion on three datasets. U: unigram. B: bigram. Best result in each column is highlighted in bold.</figDesc><table></table></figure>

			<note place="foot" n="1"> For example, KBP slot filling is known for its complex pipeline, and the best overall F1 scores (Wiegand and Klakow, 2013; Angeli et al., 2014) for recent competitions are within the range of 30-40.</note>

			<note place="foot" n="3"> For instance, when matching the rule &quot;sim(X,Y) :links(X,Y)&quot; to a condition such as &quot;sim(a,X)&quot; the two features &quot;sim&quot; and &quot;link&quot; are generated; likewise when matching the rule &quot;linkedBy(X,Y,W) :-true&quot; to the condition &quot;linkedBy(a,c,sprinter)&quot; the feature &quot;by(sprinter)&quot; is generated.</note>

			<note place="foot" n="4"> In more detail, the extraction process was as follows. (1) We used a DBpedia dump of categories and hyperlink structure to find pages in a category; sometimes, this included crawling a supercategory page to find categories and then entities. (2) We used the DBpedia hyperlink graph to find the target entity pages, downloaded the most recent (2014) version of each of these pages, and collected relevant hyperlinks and anchor text, together with 80 characters of context to either side.</note>

			<note place="foot" n="5"> In in addition to finding rules which instantiate the templates, weights on these rules are also learned.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was sponsored in part by DARPA grant FA87501220342 to CMU and a Google Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Local partitioning for directed graphs using pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet Mathematics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="22" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Combining distant and partial supervision for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving machine translation quality with automatic named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Babych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International EAMT workshop on MT and other Language Technology Tools, Improving MT through other Language Technology Tools: Resources and Tools for Building MT</title>
		<meeting>the 7th International EAMT workshop on MT and other Language Technology Tools, Improving MT through other Language Technology Tools: Resources and Tools for Building MT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Open information extraction for the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2670" to="2676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Constructing biological knowledge bases by extracting information from text sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Kumlien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMB</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1999</biblScope>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Can predicate invention in meta-interpretive learning compensate for incomplete background knowl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cropper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stephen H Muggleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Inductive Logic Programming</title>
		<meeting>the 24th International Conference on Inductive Logic Programming</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards scaling fully personalized PageRank: Algorithms, lower bounds, and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kroly</forename><surname>Csalogny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dniel</forename><surname>Fogaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet Mathematics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="358" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Balzs Rcz, and Tams Sarls</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint parsing and named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="326" to="334" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluating structural equation models with unobservable variables and measurement error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claes</forename><surname>Fornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larcker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of marketing research</title>
		<imprint>
			<biblScope unit="page" from="39" to="50" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Introduction to statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statistical predicate invention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in a large scale knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reading the web with learned syntactic-semantic inference rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1017" to="1026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Weakly supervised user profile extraction from twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient weight learning for markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery in Databases: PKDD 2007</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="200" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Named entity recognition for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menno</forename><surname>Van Zaanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ALTW</title>
		<meeting>ALTW</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The PageRank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sergey Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Computer Science department, Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="107" to="136" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Generalized latent variable modeling: Multilevel, longitudinal, and structural equation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Skrondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia Rabe-Hesketh</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An introduction to conditional random fields for relational learning. Introduction to statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="93" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A semiparametric gaussian copula regression model for predicting financial risks from earnings calls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics (ACL 2014)</title>
		<meeting>the 52th Annual Meeting of the Association for Computational Linguistics (ACL 2014)<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Programming with personalized pagerank: a locally groundable first-order probabilistic logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management</title>
		<meeting>the 22nd ACM international conference on Conference on information &amp; knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2129" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structure learning via parameter learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Information and Knowledge Management (CIKM</title>
		<meeting>the 23rd ACM International Conference on Information and Knowledge Management (CIKM</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient inference and learning in a large knowledge base: Reasoning with extracted information using a locally groundable first-order probabilistic logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Journal</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge base completion via search-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on World wide web</title>
		<meeting>the 23rd international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="515" to="526" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1366" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Effective slot filling based on shallow distant supervision methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin Roth Tassilo Barth</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mittul Singh Dietrich</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIST KBP workshop</title>
		<meeting>NIST KBP workshop</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Autonomously semantifying wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel S Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixteenth ACM conference on Conference on information and knowledge management</title>
		<meeting>the sixteenth ACM conference on Conference on information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Open information extraction using wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel S Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="118" to="127" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
