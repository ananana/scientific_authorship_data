<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Preface: General Chair</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Neural Semantic Role Labeling with Dependency Path Embeddings</orgName>
								<orgName type="institution">Modeling Concept Dependencies in a Scientific Corpus</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Preface: General Chair</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Invited Talk I: Same Policy Issue, Different Portrayal: The Importance of Tone and Framing in Language Amber E. Boydstun, University of California at Davis</head><p>Many policy issues at the center of politics are relatively fixed; they tend to involve the same basic questions that do not vary over time (e.g., should abortion be legal? should we execute people convicted of horrific crimes?). Political candidates, too, are (like the rest of us) the same people year after year (e.g., Hillary Clinton in 2016 is just an older version of Hillary Clinton in 2015). Yet when citizens consider a given political item (be it a policy issue, a candidate, or something else), they tend not to perceive that item in a fixed way over time. Rather, peoples perceptions of the item tend to depend on how that item is portrayed at that moment. Policy issues and political candidates alike can be portrayed differently through the use of different visual imagery but even more pervasively through variance in the text used to describe them. In this talk, I give a general overview of the importance of issue and candidate portrayals in political communication. I outline the extensive research that has already been done trying to identify different portrayals in text. And I discuss the many opportunities available today to researchers interested in tracking issue and candidate portrayals in text and in examining the effects of issue and candidate portrayals on public attitudes and voting behavior.</p><p>Bio: Amber Boydstun is an Associate Professor of Political Science at the University of California, Davis. Her work bridges multiple disciplines, including psychology, journalism, and computer science. Her core research examines the interaction between media and politics, with a focus on how different media portray- als of the same policy issue can prompt citizens and policymakers to respond to that issue in different ways. She uses lab experiments, large-scale media studies, and manual and computational text analysis to study how issues make the news; how issues are "framed" in the news; the dynamics of "media storms"; and how media coverage can shape public opinion and public policy on issues like immigration, gun control, same- sex marriage, and capital punishment. She is author of Making the News (Chicago) and co-author of The Decline of the Death Penalty and the Discovery of Innocence (Cambridge), as well as many journal articles. Boydstun works with scholars across the globe as a member of the Comparative Agendas Project, 1 a collab- orative enterprise by political science and policy scholars to measure international government outputs. She serves on the editorial boards for the journal Political Communication, the Text as Data Association, and the Women Also Know Stuff 2 initiative. Most recently, she co-chaired the 2016 Visions in Methodology Conference. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Invited Talk II: On Distributional Semantics Mark Steedman, University of Edinburgh</head><p>The central problem in open domain-question answering from text is the problem of entailment. Given enough text, the answer is almost certain to be there, but is likely to be expressed differently than in the question-either in a paraphrase, or in a sentence that entails or implies the answer. We cannot afford to bridge this gap by open-ended theorem-proving search. Instead we need a semantics for natural language that directly supports common-sense inference, such as that arriving somewhere implies subsequently being there, and invading a country implies attacking it. We would like this semantics to be compatible with traditional logical operator semantics including quantification, negation and tense, so that not being there implies not having arrived, and not attacking implies not invading.</p><p>There have been many attempts to build such a semantics of content words by hand, from the generative semantics of the '60s to WordNet and other resources of the present. The '60s saw attempts based on gen- erative semantics, while more recently, they have exploited WordNet and other computational resources. However, such systems have been incomplete and language-specific in comparison to the vastness of human common-sense reasoning. One consequence has been renewed interest in the idea of treating the seman- tics as "hidden", to be discovered through machine learning, an idea that has its origins in the "semantic differential" of Osgood, Suci, and Tannenbaum in the '50s.</p><p>There are two distinct modern approaches to the problem of data-driven or "distributional" semantics. The first, which I will call "collocational", is the direct descendant of the semantic differential. In its most basic form, the meaning of a word is taken to be a vector in a space whose dimensions are defined by the lexicon of the language, and whose magnitude is defined by counts of those lexical items within a fixed window on the string (although in practice the dimensionality is reduced and the relation to frequency less direct). Crucially, semantic composition is defined in terms of linear algebraic operations such as vector addition.</p><p>The second approach, which I will call "denotational", defines the meaning of a word in terms of the entities (or rather their designators) that it is predicated over and the ensembles of predications over entities of the same types, obtained by machine-reading with wide coverage parsers. Semantic composition is can then be defined as an applicative system, as in traditional formal semantics.</p><p>The talk reviews recent work in both collocation-and denotation-based distributional semantics, includ- ing some hybrid approaches that interpolate grammatical features with collocational representations, or use probabilistic logics over relations whose arguments denote vectors, and asks for each what dimensions of meaning are actually being represented. It argues that the two approaches are largely orthogonal on these di- mensions. Collocational representations are good for representing ambiguity, with linear algebraic composi- tion most effective at disambiguation and representing distributional similarity. Denotational representations represent something more like a traditional compositional semantics, but one in which the primitive rela- tions correspond to those of a hidden language of logical form representing paraphrase and common-sense entailment directly.</p><p>To make this point, I will discuss recent work in which collocational distributional representations such as embeddings have been used as proxies for semantic features in models such as LSTM, to guide disam- biguation during parsing, while a lexicalized denotation-based distributional semantics is used to support inference of entailment. I will show that this hybrid approach can be applied with a number of parsing mod- xvii els, including transition-based and supertagging, to support entailment-based QA with denotation-based distributional representations. I will discuss work at Edinburgh and elsewhere in which the semantics of paraphrases is represented by a single cluster identifier and common-sense inference (derived from a learned entailment graph) is built into the lexicon and projected by syntactic derivation, rather than delegated to a later stage of inference. The method can be applied cross-linguistically, in support of machine translation. Ongoing work extends the method to extract multi-word items, light-verb constructions, and an aspect-based semantics for temporal/causal entailment, and to the creation and interrogation of Knowledge Graphs and Semantic Nets via natural language.</p><p>Bio: Mark Steedman is Professor of Cognitive Science in the School of Informatics at the University of Edinburgh. Previously, he taught as Professor in the Department of Computer and Information Science at the University of Pennsylvania, which he joined as Associate Professor in 1988, after teaching at the Universities of Warwick and Edinburgh. His PhD is in Artificial Intelligence from the University of Edinburgh. He was a Alfred P. Sloan Fellow at the University of Texas at Austin in 1980/81, and a Visiting Professor at Penn in 1986/87. He is a Fellow of the American Association for Artificial Intelligence, the British Academy, the Royal Society of Edinburgh, the Association for Computational Linguistics, and the Cognitive Science Society, and a Member of the European Academy.</p><p>His research interests cover issues in computational linguistics, artificial intelligence, computer science and cognitive science, including syntax and semantics of natural language, wide-coverage parsing and question- answering, comprehension of natural language discourse by humans and by machine, grammar-based lan- guage modeling, natural language generation, and the semantics of intonation in spoken discourse. Much of his current NLP research is addressed to probabilistic parsing and robust semantics for question-answering using the CCG grammar formalism, including the acquisition of language from paired sentences and mean- ings by child and machine. He sometimes works with colleagues in computer animation using these theories to guide the graphical animation of speaking virtual or simulated autonomous human agents. Some of his research concerns the analysis of music by humans and machines. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table of Contents</head><label>of</label><figDesc></figDesc><table>Noise reduction and targeted exploration in imitation learning for Abstract Meaning Representation 
parsing 
James Goodman, Andreas Vlachos and Jason Naradowsky . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 

Data Recombination for Neural Semantic Parsing 
Robin Jia and Percy Liang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 Together we stand: Siamese Networks for Similar Question Retrieval 
Arpita Das, Harish Yenala, Manoj Chinnakotla and Manish Shrivastava . . . . . . . . . . . . . . . . . . . . . 378 

News Citation Recommendation with Implicit and Explicit Semantics 
Hao Peng, Jing Liu and Chin-Yew Lin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388 

Grapheme-to-Phoneme Models for (Almost) Any Language 
Aliya Deri and Kevin Knight . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399 

Neural Word Segmentation Learning for Chinese 
Deng Cai and Hai Zhao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409 

Transition-Based Neural Word Segmentation 
Meishan Zhang, Yue Zhang and Guohong Fu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421 

A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data 
Adam Trischler, Zheng Ye, Xingdi Yuan, Jing He and Philip Bachman . . . . . . . . . . . . . . . . . . . . . . 432 

Combining Natural Logic and Shallow Reasoning for Question Answering 
Gabor Angeli, Neha Nayak and Christopher D. Manning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442 

Easy Questions First? A Case Study on Curriculum Learning for Question Answering 
Mrinmaya Sachan and Eric Xing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453 

Improved Representation Learning for Question Answer Matching 
Ming Tan, Cicero dos Santos, Bing Xiang and Bowen Zhou . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464 

Tables as Semi-structured Knowledge for Question Answering 
Sujay Kumar Jauhar, Peter Turney and Eduard Hovy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474 

Neural Summarization by Extracting Sentences and Words 
Jianpeng Cheng and Mirella Lapata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484 

Neural Networks For Negation Scope Detection 
Federico Fancellu, Adam Lopez and Bonnie Webber . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495 

CSE: Conceptual Sentence Embeddings based on Attention Model 
Yashen Wang, Heyan Huang, Chong Feng, Qiang Zhou, Jiahui Gu and Xiong Gao . . . . . . . . . . . 505 

DocChat: An Information Retrieval Approach for Chatbot Engines Using Unstructured Documents 
Zhao Yan, Nan Duan, Junwei Bao, Peng Chen, Ming Zhou, Zhoujun Li and Jianshe Zhou . . . . 516 </table></figure>

			<note place="foot" n="1"> http://www.comparativeagendas.info/ 2 http://womenalsoknowstuff.com/ 3 http://visionsinmethodology.org/conferences/2016-conference/ xv</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shaoping Ma, Qingsong Ma, Shuming Ma, Wolfgang Macherey, Nitin Madnani, Wolfgang Maier, Prodro-mos Malakasiotis, Suresh Manandhar, Gideon Mann, Christopher D. Manning, Diego Marcheggiani, Daniel Marcu, Marco Marelli, Eugenio Martínez Cámara, Toni Marti, Patricio Martinez-Barco, Héctor Martínez</head><note type="other">Alonso, Eugenio Martínez Cámara, Fernando Martínez Santiago, Eugenio Martínez-Cámara, Fernando Javier Martínez Santiago, André F. T. Martins, Vivien Mast, Yuichiro Matsubayashi, Takuya Matsuzaki, Jonathan May, Diana Maynard, Diana McCarthy, David McClosky, Ryan McDonald, Tara McIntosh, Kathy McKeown, Louise McNally, Beata Megyesi, Yashar Mehdad, Oren Melamud, Arul Menezes, Fandong Meng, Helen Meng, Florian Metze, Haitao Mi, Yishu Miao, Tsvetomila Mihaylova, Tomas Mikolov, Timo-thy Miller, Tristan Miller, David Mimno, Meg Mitchell, Teruhisa Misu, Yusuke Miyao, Ryosuke Miyazaki, Daichi Mochihashi, Ashutosh Modi, Marie-Francine Moens, Samaneh Moghaddam, Saif Mohammad, Karo Moilanen, Luis Gerardo Mojica de la</note></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Finding Non-Arbitrary Form-Meaning Systematicity Using String-Metric Learning for Kernel Regression E.Dario Gutierrez, Roger Levy and Benjamin Bergen 11:10-11:30 Improving Hypernymy Detection with an Integrated Path-based and Distributional Method Vered Shwartz, Yoav Goldberg and Ido Dagan Session 7B: Outstanding papers II 10:10-10:30 Multimodal Pivots for Image Caption Translation Julian Hitschler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wednesday</surname></persName>
		</author>
		<idno>50-13:30 Lunch break 13:30-15:00</idno>
	</analytic>
	<monogr>
		<title level="m">50 Harnessing Deep Neural Networks with Logic Rules Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy and Eric Xing</title>
		<editor>Systems Pei-Hao Su, Milica Gasic, Nikola Mrkši´cMrkši´c, Lina M. Rojas Barahona, Stefan Ultes, David Vandyke, Tsung-Hsien Wen and Steve Young Wednesday</editor>
		<meeting><address><addrLine>David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov and Michael Collins</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-10" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="0" to="15" />
		</imprint>
	</monogr>
	<note>ACL business meeting. open to all. 30 Coffee break Session 8A: Question answering II. short papers</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Session 8B: Word vectors III</title>
		<imprint/>
	</monogr>
	<note>short papers</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">Session 8C: Topics and discourse</title>
		<imprint/>
	</monogr>
	<note>short papers</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">Session 8D: Syntax and morphology</title>
		<imprint/>
	</monogr>
	<note>short papers</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">:00 Awards and closing session</title>
	</analytic>
	<monogr>
		<title level="m">Session 8E: Potpourri II</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="0" to="19" />
		</imprint>
	</monogr>
	<note>short papers</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
