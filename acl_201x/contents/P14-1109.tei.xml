<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Semiparametric Gaussian Copula Regression Model for Predicting Financial Risks from Earnings Calls</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213, 15213</postCode>
									<region>PA, PA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhao</forename><surname>Hua</surname></persName>
							<email>zhua@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University Pittsburgh</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213, 15213</postCode>
									<region>PA, PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Semiparametric Gaussian Copula Regression Model for Predicting Financial Risks from Earnings Calls</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1155" to="1165"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Earnings call summarizes the financial performance of a company, and it is an important indicator of the future financial risks of the company. We quantitatively study how earnings calls are correlated with the financial risks, with a special focus on the financial crisis of 2009. In particular , we perform a text regression task: given the transcript of an earnings call, we predict the volatility of stock prices from the week after the call is made. We propose the use of copula: a powerful statistical framework that separately models the uniform marginals and their complex mul-tivariate stochastic dependencies, while not requiring any prior assumptions on the distributions of the covariate and the dependent variable. By performing probability integral transform, our approach moves beyond the standard count-based bag-of-words models in NLP, and improves previous work on text regression by incorporating the correlation among local features in the form of semiparametric Gaus-sian copula. In experiments, we show that our model significantly outperforms strong linear and non-linear discriminative baselines on three datasets under various settings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Predicting the risks of publicly listed companies is of great interests not only to the traders and ana- lysts on the Wall Street, but also virtually anyone who has investments in the market ( <ref type="bibr">Kogan et al., 2009)</ref>. Traditionally, analysts focus on quantita- tive modeling of historical trading data. Today, even though earnings calls transcripts are abun- dantly available, their distinctive communicative practices <ref type="bibr" target="#b3">(Camiciottoli, 2010)</ref>, and correlations with the financial risks, in particular, future stock performances ( <ref type="bibr">Price et al., 2012)</ref>, are not well studied in the past.</p><p>Earnings calls are conference calls where a listed company discusses the financial perfor- mance. Typically, a earnings call contains two parts: the senior executives first report the oper- ational outcomes, as well as the current financial performance, and then discuss their perspectives on the future of the company. The second part of the teleconference includes a question answering session where the floor will be open to investors, analysts, and other parties for inquiries. The ques- tion we ask is that, even though each earnings call has distinct styles, as well as different speakers and mixed formats, can we use earnings calls to predict the financial risks of the company in the limited future?</p><p>Given a piece of earnings call transcript, we investigate a semiparametric approach for auto- matic prediction of future financial risk <ref type="bibr">1</ref> . To do this, we formulate the problem as a text regres- sion task, and use a Gaussian copula with prob- ability integral transform to model the uniform marginals and their dependencies. Copula mod- els ( <ref type="bibr">Schweizer and Sklar, 1983;</ref><ref type="bibr">Nelsen, 1999)</ref> are often used by statisticians <ref type="bibr">(Genest and Favre, 2007;</ref><ref type="bibr">Liu et al., 2012;</ref><ref type="bibr">Masarotto and Varin, 2012)</ref> and economists <ref type="bibr" target="#b6">(Chen and Fan, 2006</ref>) to study the bivariate and multivariate stochastic dependency among random variables, but they are very new to the machine learning ( <ref type="bibr">Ghahramani et al., 2012;</ref><ref type="bibr">Han et al., 2012;</ref><ref type="bibr">Xiang and Neville, 2013;</ref><ref type="bibr">Lopezpaz et al., 2013</ref>) and related communities <ref type="bibr" target="#b11">(Eickhoff et al., 2013)</ref>. To the best of our knowledge, even though the term "copula" is named for the resemblance to grammatical copulas in linguistics, copula models have not been explored in the NLP community. To evaluate the performance of our approach, we compare with a standard squared loss linear regression baseline, as well as strong baselines such as linear and non-linear support vector machines (SVMs) that are widely used in text regression tasks. By varying different exper- imental settings on three datasets concerning dif- ferent periods of the Great Recession from 2006- 2013, we empirically show that our approach sig- nificantly outperforms the baselines by a wide margin. Our main contributions are:</p><p>• We are among the first to formally study tran- scripts of earnings calls to predict financial risks.</p><p>• We propose a novel semiparametric Gaussian copula model for text regression.</p><p>• Our results significantly outperform standard linear regression and strong SVM baselines.</p><p>• By varying the number of dimensions of the covariates and the size of the training data, we show that the improvements over the baselines are robust across different param- eter settings on three datasets.</p><p>In the next section, we outline related work in modeling financial reports and text regression. In Section 3, the details of the semiparametric cop- ula model are introduced. We then describe the dataset and dependent variable in this study, and the experiments are shown in Section 6. We dis- cuss the results and findings in Section 7 and then conclude in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Fung et al. <ref type="bibr">(2003)</ref> are among the first to study SVM and text mining methods in the market pre- diction domain, where they align financial news articles with multiple time series to simulate the 33 stocks in the Hong Kong Hang Seng Index.</p><p>However, text regression in the financial domain have not been explored until recently. <ref type="bibr">Kogan et al. (2009)</ref> model the SEC-mandated annual re- ports, and performs linear SVM regression with -insensitive loss function to predict the mea- sured volatility. Another recent study ( <ref type="bibr">Wang et al., 2013</ref>) uses exactly the same max-margin re- gression technique, but with a different focus on the financial sentiment. Using the same dataset, <ref type="bibr">Tsai and Wang (2013)</ref> reformulate the regression problem as a text ranking problem. Note that all these regression studies above investigate the SEC-mandated annual reports, which are very dif- ferent from the earnings calls in many aspects such as length, format, vocabulary, and genre. Most recently, <ref type="bibr">Xie et al. (2013)</ref> have proposed the use of frame-level semantic features to understand fi- nancial news, but they treat the stock movement prediction problem as a binary classification task. Broadly speaking, our work is also aligned to re- cent studies that make use of social media data to predict the stock market <ref type="bibr" target="#b2">(Bollen et al., 2011;</ref><ref type="bibr">Zhang et al., 2011</ref>). Despite our financial domain, our approach is more relevant to text regression. Traditional dis- criminative models, such as linear regression and linear SVM, have been very popular in various text regression tasks, such as predicting movie rev- enues from reviews ( <ref type="bibr">Joshi et al., 2010)</ref>, under- standing the geographic lexical variation <ref type="bibr" target="#b12">(Eisenstein et al., 2010)</ref>, and predicting food prices from menus ( <ref type="bibr" target="#b4">Chahuneau et al., 2012</ref>). The advantage of these models is that the estimation of the parame- ters is often simple, the results are easy to inter- pret, and the approach often yields strong perfor- mances. While these approaches have merits, they suffer from the problem of not explicitly model- ing the correlations and interactions among ran- dom variables, which in some sense, correspond- ing to the impractical assumption of independent and identically distributed (i.i.d) of the data. For example, when bag-of-word-unigrams are present in the feature space, it is easier if one does not ex- plicitly model the stochastic dependencies among the words, even though doing so might hurt the predictive power, while the variance from the cor- relations among the random variables is not ex- plained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Copula Models for Text Regression</head><p>In NLP, many statistical machine learning meth- ods that capture the dependencies among ran- dom variables, including topic models ( <ref type="bibr" target="#b1">Blei et al., 2003;</ref><ref type="bibr">Lafferty and Blei, 2005;</ref><ref type="bibr">Wang et al., 2012)</ref>, always have to make assumptions with the under- lying distributions of the random variables, and make use of informative priors. This might be rather restricting the expressiveness of the model in some sense ( <ref type="bibr">Reisinger et al., 2010</ref>). On the other hand, once such assumptions are removed, another problem arises -they might be prone to errors, and suffer from the overfitting issue. There- fore, coping with the tradeoff between expressive- ness and overfitting, seems to be rather important in statistical approaches that capture stochastic de- pendency.</p><p>Our proposed semiparametric copula regression model takes a different perspective. On one hand, copula models <ref type="bibr">(Nelsen, 1999</ref>) seek to explicitly model the dependency of random variables by sep- arating the marginals and their correlations. On the other hand, it does not make use of any as-sumptions on the distributions of the random vari- ables, yet, the copula model is still expressive. This nice property essentially allows us to fuse distinctive lexical, syntactic, and semantic feature sets naturally into a single compact model.</p><p>From an information-theoretic point of view <ref type="bibr">(Shannon, 1948)</ref>, various problems in text analytics can be formulated as estimating the probability mass/density functions of tokens in text. In NLP, many of the probabilistic text models work in the discrete space <ref type="bibr" target="#b8">(Church and Gale, 1995;</ref><ref type="bibr" target="#b1">Blei et al., 2003</ref>), but our model is different: since the text features are sparse, we first perform kernel density estimates to smooth out the zeroing items, and then calculate the empirical cumulative distribution function (CDF) of the random variables. By doing this, we are essentially performing probability integral transform-an important statistical technique that moves beyond the count-based bag-of-words feature space to marginal cumulative density functions space. Last but not least, by using a parametric copula, in our case, the Gaussian copula, we reduce the computational cost from fully nonparametric methods, and explicitly model the correlations among the covariate and the dependent variable.</p><p>In this section, we first briefly look at the theoretical foundations of copulas, including the Sklar's theorem. Then we describe the proposed semiparametric Gaussian copula text regression model. The algorithmic implementation of our ap- proach is introduced at the end of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Theory of Copula</head><p>In the statistics literature, copula is widely known as a family of distribution function. The idea be- hind copula theory is that the cumulative distri- bution function (CDF) of a random vector can be represented in the form of uniform marginal cu- mulative distribution functions, and a copula that connects these marginal CDFs, which describes the correlations among the input random variables. However, in order to have a valid multivariate dis- tribution function regardless of n-dimensional co- variates, not every function can be used as a copula function. The central idea behind copula, there- fore, can be summarize by the Sklar's theorem and the corollary. Theorem 1 (Sklar's Theorem (1959)) Let F be the joint cumulative distribution function of n random variables X 1 , X 2 , ..., X n . Let the corresponding marginal cumulative dis- tribution functions of the random variable be</p><formula xml:id="formula_0">F 1 (x 1 ), F 2 (x 2 ), ..., F n (x n ).</formula><p>Then, if the marginal functions are continuous, there exists a unique copula C, such that</p><formula xml:id="formula_1">F (x 1 , ..., x n ) = C[F 1 (x 1 ), ..., F n (x n )]. (1)</formula><p>Furthermore, if the distributions are continuous, the multivariate dependency structure and the marginals might be separated, and the copula can be considered independent of the marginals <ref type="bibr">(Joe, 1997;</ref><ref type="bibr">Parsa and Klugman, 2011)</ref>. Therefore, the copula does not have requirements on the marginal distributions, and any arbitrary marginals can be combined and their dependency structure can be modeled using the copula. The inverse of Sklar's Theorem is also true in the following:</p><p>Corollary 1 If there exists a copula C : (0, 1) n and marginal cumulative distribution func- tions</p><formula xml:id="formula_2">F 1 (x 1 ), F 2 (x 2 ), ..., F n (x n ), then C[F 1 (x 1 ), ..., F n (x n )] defines a multivariate cumulative distribution function.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semiparametric Gaussian Copula Models</head><p>The Non-Parametric Estimation We formulate the copula regression model as fol- lows. Assume we have n random variables of text features X 1 , X 2 , ..., X n . The problem is that text features are sparse, so we need to perform non- parametric kernel density estimation to smooth out the distribution of each variable. Let f 1 , f 2 , ..., f n be the unknown density, we are interested in de- riving the shape of these functions. Assume we have m samples, the kernel density estimator can be defined as:</p><formula xml:id="formula_3">ˆ f h (x) = 1 m m i=1 K h (x − x i ) (2) = 1 mh m i=1 K x − x i h<label>(3)</label></formula><p>Here, K(·) is the kernel function, where in our case, we use the Box kernel 2 K(z):</p><formula xml:id="formula_4">K(z) = 1 2 , |z| ≤ 1,<label>(4)</label></formula><formula xml:id="formula_5">= 0, |z| &gt; 1.<label>(5)</label></formula><p>Comparing to the Gaussian kernel and other ker- nels, the Box kernel is simple, and computation- ally inexpensive. The parameter h is the band- width for smoothing 3 . Now,</p><formula xml:id="formula_6">we can derive the empiri- cal cumulative distribution functionsˆF functionsˆ functionsˆF X 1 ( ˆ f 1 (X 1 )), ˆ F X 2 ( ˆ f 2 (X 2 )), ..., ˆ F Xn ( ˆ f n (X n ))</formula><p>of the smoothed covariates, as well as the dependent variable y and its CDFˆFCDFˆ CDFˆF y ( ˆ f (y)). The empirical cumulative distribution functions are defined as:</p><formula xml:id="formula_7">ˆ F (ν) = 1 m m i=1 I{x i ≤ ν} (6)</formula><p>where I{·} is the indicator function, and ν in- dicates the current value that we are evaluating. Note that the above step is also known as prob- ability integral transform ( <ref type="bibr" target="#b10">Diebold et al., 1997)</ref>, which allows us to convert any given continuous distribution to random variables having a uniform distribution. This is of crucial importance to mod- eling text data: instead of using the classic bag-of- words representation that uses raw counts, we are now working with uniform marginal CDFs, which helps coping with the overfitting issue due to noise and data sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Parametric Copula Estimation</head><p>Now that we have obtained the marginals, and then the joint distribution can be constructed by apply- ing the copula function that models the stochastic dependencies among marginal CDFs:</p><formula xml:id="formula_8">ˆ F ( ˆ f1(X1), ..., ˆ f1(Xn), ˆ f (y)) (7) = C[ ˆ FX 1 ˆ f1(X1) , ..., ˆ FX n ˆ fn(Xn) , ˆ Fyˆfy Fyˆ Fyˆfy(y) ]<label>(8)</label></formula><p>In this work, we apply the parametric Gaussian copula to model the correlations among the text features and the label. Assume x i is the smoothed version of random variable X i , and y is the smoothed label, we have:</p><formula xml:id="formula_9">F (x1, ..., xn, y)<label>(9)</label></formula><formula xml:id="formula_10">= ΦΣ Φ −1 [Fx 1 (x1)], ..., , Φ −1 [Fx n (xn)], Φ −1 [Fy(y)]<label>(10)</label></formula><p>where Φ Σ is the joint cumulative distribution func- tion of a multivariate Gaussian with zero mean and Σ variance. Φ −1 is the inverse CDF of a standard Gaussian. In this parametric part of the model, the parameter estimation boils down to the problem of learning the covariance matrix Σ of this Gaussian copula. In this work, we perform standard maxi- mum likelihood estimation for the Σ matrix.</p><p>To calibrate the Σ matrix, we make use of the power of randomness: using the initial Σ from MLE, we generate random samples from the Gaussian copula, and then concatenate previ- ously generated joint of Gaussian inverse marginal CDFs with the newly generated random copula numbers, and re-estimate using MLE to derive the final adjusted Σ. Note that the final Σ matrix has to be symmetric and positive definite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational Complexity</head><p>One important question regarding the proposed semiparametric Gaussian copula model is the cor- responding computational complexity. This boils down to the estimation of thê Σ matrix ( <ref type="bibr">Liu et al., 2012)</ref>: one only needs to calculate the correla- tion coefficients of n(n − 1)/2 pairs of random variables. <ref type="bibr" target="#b7">Christensen (2005)</ref> shows that sort- ing and balanced binary trees can be used to cal- culate the correlation coefficients with complex- ity of O(n log n). Therefore, the computational complexity of MLE for the proposed model is O(n log n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficient Approximate Inference</head><p>In this regression task, in order to perform exact inference of the conditional probability distribution p(F y (y)|F x 1 (x 1 ), ..., F xn (x n )), one needs to solve the mean responsêresponsê</p><formula xml:id="formula_11">E(F y (y)|F x 1 (x 1 ), ..., F x 1 (x 1 )) from a joint distribution of high-dimensional Gaussian copula.</formula><p>Assume in the simple bivariate case of Gaussian copula regression, the covariance matrix Σ is:</p><formula xml:id="formula_12">Σ = Σ 11 Σ 12 Σ 22</formula><p>We can easily derive the conditional density that can be used to calculate the expected value of the CDF of the label:</p><formula xml:id="formula_13">C(F y (y)|F x 1 (x 1 ); Σ) = 1 |Σ 22 − Σ T 12 Σ −1 11 Σ 12 | 1 2 exp − 1 2 δ T [Σ 22 − Σ T 12 Σ −1 11 Σ 12 ] −1 − I δ<label>(11)</label></formula><p>where</p><formula xml:id="formula_14">δ = Φ −1 [F y (y)] − Σ T 12 Σ −1 11 Φ −1 [F x 1 (x 1 )]</formula><p>. Unfortunately, the exact inference can be in- tractable in the multivariate case, and approximate inference, such as Markov Chain Monte Carlo sampling <ref type="bibr">(Gelfand and Smith, 1990;</ref><ref type="bibr">Pitt et al., 2006</ref>) is often used for posterior inference. In this work, we propose an efficient sampling method to derive y given the text features -we sample F y (y) s.t. it maximizes the joint high-dimensional Gaussian copula density:</p><formula xml:id="formula_15">ˆ Fy(y) ≈ arg max Fy (y)∈(0,1) 1 √ det Σ exp − 1 2 ∆ T · Σ −1 − I · ∆<label>(12)</label></formula><p>where</p><formula xml:id="formula_16">∆ =      Φ −1 (F x 1 (x 1 ))</formula><p>. . .</p><formula xml:id="formula_17">Φ −1 (F xn (x n )) Φ −1 (F y (y))     </formula><p>Again, the reason why we perform approxi- mated inference is that: exact inference in the high-dimensional Gaussian copula density is non- trivial, and might not have analytical solutions, but approximate inference using maximum den- sity sampling from the Gaussian copula signifi- cantly relaxes the complexity of inference. Fi- nally, to derivê y, the last step is to compute the inverse CDF ofˆFofˆ ofˆF y (y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Algorithmic Implementation</head><p>The algorithmic implementation of our semipara- metric Gaussian copula text regression model is shown in Algorithm 1. Basically, the algorithm can be decomposed into four parts:</p><p>• Perform nonparametric Box kernel density estimates of the covariates and the dependent variable for smoothing.</p><p>• Calculate the empirical cumulative distribu- tion functions of the smoothed random vari- ables.</p><p>• Estimate the parameters (covariance Σ) of the Gaussian copula.</p><p>• Infer the predicted value of the dependent variable by sampling the Gaussian copula probability density function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets</head><p>We use three datasets 4 of transcribed quarterly earnings calls from the U.S. stock market, focus- ing on the period of the Great Recession.  <ref type="table">Table 1</ref>. (1) training data (X (tr) , y (tr) ); (2) testing data (X (te) , y (te) );</p><p>Learning:</p><formula xml:id="formula_18">for i = 1 → n dimensions do X (tr) i ← BoxKDE(X (tr) i , X (tr) i ); U (tr) i ← EmpiricalCDF (X (tr) i ); X (te) i ← BoxKDE(X (tr) i , X (te) i ); U (te) i ← EmpiricalCDF (X (te) i ); end for y (tr) ← BoxKDE(y (tr) , y (tr) ); v (tr) ← EmpiricalCDF (y (tr) ); Z (tr) ← GaussianInverseCDF ([U (tr) v (tr) ]); ˆ Σ ← CorrelationCoef f icients(Z (tr) ); r ← M ultiV ariateGaussianRandN um(0, ˆ Σ, n); Z (tr) = GaussianCDF (r); ˆ Σ ← CorrelationCoef f icients([Z (tr) Z (tr) ]);</formula><p>Inference: Note that unlike the standard news corpora in NLP or the SEC-mandated financial report, Tran- scripts of earnings call is a very special genre of text. For example, the length of WSJ docu- ments is typically one to three hundreds <ref type="bibr">(Harman, 1995)</ref>, but the averaged document length of our three earnings calls datasets is 7677. Depending on the amount of interactions in the question an- swering session, the complexities of the calls vary. This mixed form of formal statement and informal speech brought difficulties to machine learning al- gorithms.</p><formula xml:id="formula_19">for j = 1 → m instances do maxj ← 0; ˆ Y = 0; for k = 0.01 → 1 do Z (te) ← GaussianInverseCDF ([U (te) k]); pj = M ultiV ariateGaussianP DF (Z (te) , ˆ Σ) n GaussianP DF (Z (te) ) ; if pj ≥ maxj then maxj = pj; ˆ Y = k;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Measuring Financial Risks</head><p>Volatility is an important measure of the financial risk, and in this work, we focus on predicting the future volatility following the earnings teleconfer-ence call. For each earning call, we have a week of stock prices of the company after the day on which the earnings call is made. The Return of Day t is:</p><formula xml:id="formula_20">r t = x t x t−1 − 1<label>(13)</label></formula><p>where x t represents the share price of Day t, and the Measured Stock Volatility from Day t to t + τ :</p><formula xml:id="formula_21">y (t,t+τ ) = τ i=0 (r t+i − ¯ r) 2 τ<label>(14)</label></formula><p>Using the stock prices, we can use the equations above to calculate the measured stock volatility af- ter the earnings call, which is the standard measure of risks in finance, and the dependent variable y of our predictive task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>In all experiments throughout this section, we use 80-20 train/test splits on all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature sets:</head><p>We have extracted lexical, named entity, syntactic, and frame-semantics features, most of which have been shown to perform well in previous work <ref type="bibr">(Xie et al., 2013</ref>). We use the unigrams and bigrams to represent lexical features, and the Stanford part- of-speech tagger ( <ref type="bibr">Toutanova et al., 2003</ref>) to extract the lexicalized named entity and part-of-speech features. A probabilistic frame-semantics parser, SEMAFOR ( <ref type="bibr" target="#b9">Das et al., 2010)</ref>, is used to provide the FrameNet-style frame-level semantic annota- tions. For each of the five sets, we collect the top- 100 most frequent features, and end up with a total of 500 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines:</head><p>The baselines are standard squared-loss linear regression, linear kernel SVM, and non-linear (Gaussian) kernel SVM. They are all standard algorithms in regression problems, and have been shown to have outstanding performances in many recent text regression ( <ref type="bibr">Kogan et al., 2009;</ref><ref type="bibr" target="#b4">Chahuneau et al., 2012;</ref><ref type="bibr">Xie et al., 2013;</ref><ref type="bibr">Wang et al., 2013;</ref><ref type="bibr">Tsai and Wang, 2013)</ref>. We use the Statistical Toolbox's linear regression imple- mentation in Matlab, and LibSVM (Chang and Lin, 2011) for training and testing the SVM mod- els. The hyperparameter C in linear SVM, and the γ and C hyperparameters in Gaussian SVM are tuned on the training set using 10-fold cross- validation. Note that since the kernel density esti- mation in the proposed copula model is nonpara- metric, and we only need to learn the Σ in the Gaussian copula, there is no hyperparameters that need to be tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics:</head><p>Spearman's correlation <ref type="bibr">(Hogg and Craig, 1994)</ref> and Kendall's tau <ref type="bibr">(Kendall, 1938)</ref> have been widely used in many regression problems in NLP <ref type="bibr" target="#b0">(Albrecht and Hwa, 2007;</ref><ref type="bibr">Yogatama et al., 2011;</ref><ref type="bibr">Wang et al., 2013;</ref><ref type="bibr">Tsai and Wang, 2013)</ref>, and here we use them to measure the quality of predicted valuesˆyvaluesˆ valuesˆy by comparing to the vector of ground truth y. In contrast to Pearson's correlation, Spear- man's correlation has no assumptions on the rela- tionship of the two measured variables. Kendall's tau is a nonparametric statistical metric that have shown to be inexpensive, robust, and represen- tation independent <ref type="bibr">(Lapata, 2006</ref>). We also use paired two-tailed t-test to measure the statistical significance between the best and the second best approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparing to Various Baselines</head><p>In the first experiment, we compare the proposed semiparametric Gaussian copula regression model to three baselines on three datasets with all fea- tures. The detailed results are shown in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>On the pre-2009 dataset, we see that the linear re- gression and linear SVM perform reasonably well, but the Gaussian kernel SVM performs less well, probably due to overfitting. The copula model outperformed all three baselines by a wide mar- gin on this dataset with both metrics. Similar per- formances are also obtained in the 2009 dataset, where the result of linear SVM baseline falls be- hind. On the post-2009 dataset, none of results from the linear and non-linear SVM models can match up with the linear regression model, but our proposed copula model still improves over all baselines by a large margin. Comparing to second- best approaches, all improvements obtained by the copula model are statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Varying the Amount of Training Data</head><p>To understand the learning curve of our proposed copula regression model, we use the 25%, 50%, 75% subsets from the training data, and evaluate all four models. <ref type="figure" target="#fig_2">Figure 1</ref> shows the evaluation results. From the experiments on the pre-2009 dataset, we see that when the amount of training data is small (25%), both SVM models have ob- tained very impressive results. This is not surpris- ing at all, because as max-margin models, soft- margin SVM only needs a handful of examples that come with nonvanishing coefficients (support vectors) to find a reasonable margin. When in-   creasing the amount of training data to 50%, we do see the proposed copula model catches up quickly, and lead all baseline methods undoubtably at 75% training data. On the 2009 dataset, we observe very similar patterns. Interestingly, the proposed copula regression model has dominated all meth- ods for both metrics throughout all proportions of the "post-2009" earnings calls dataset, where in- stead of financial crisis, the economic recovery is the main theme. In contrast to the previous two datasets, both linear and non-linear SVMs fail to reach reasonable performances on this dataset.</p><note type="other">Method Pre-2009 2009 Post-2009 Spearman Kendall Spearman Kendall Spearman Kendall linear regression: 0.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Varying the Amount of Features</head><p>Finally, we investigate the robustness of the pro- posed semiparametric Gaussian copula regression model by varying the amount of features in the co- variate space. To do this, we sample equal amount of features from each feature set, and concatenate them into a feature vector. When increasing the amount of total features from 100 to 400, the re- sults are shown in <ref type="figure" target="#fig_3">Figure 2</ref>. On the pre-2009 dataset, we see that the gaps between the best- perform copula model and the second-best linear regression model are consistent throughout all fea- ture sizes. On the 2009 dataset, we see that the performance of Gaussian copula is aligned with the linear regression model in terms of Spearman's correlation, where the former seems to perform better in terms of Kendall's tau. Both linear and non-linear SVM models do not have any advan- tages over the proposed approach. On the post- 2009 dataset that concerns economic growth and recovery, the boundaries among all methods are very clear. The Spearman's correlation for both SVM baselines is less than 0.15 throughout all set- tings, but copula model is able to achieve 0.4 when using 400 features. The improvements of copula   <ref type="table">Table 3</ref>: Top-10 features that have positive corre- lations with stock volatility in three datasets.</p><p>model over squared loss linear regression model are increasing, when working with larger feature spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Qualitative Analysis</head><p>Like linear classifiers, by "opening the hood" to the Gaussian copula regression model, one can ex- amine features that exhibit high correlations with the dependent variable. <ref type="table">Table 3</ref> shows the top fea- tures that are positively correlated with the future stock volatility in the three datasets. On the top features from the "pre-2009" dataset, which pri- marily (82%) includes calls from 2008, we can clearly observe that the word "2008" has strong correlation with the financial risks. Interestingly, the phrase "third quarter" and its variations, not only play an important role in the model, but also highly correlated to the timeline of the financial crisis: the Q3 of 2008 is a critical period in the recession, where Lehman Brothers falls on the Sept. 15 of 2008, filing $613 billion of debt - the biggest bankruptcy in U.S. history <ref type="bibr">(Mamudi, 2008)</ref>. This huge panic soon broke out in vari- ous financial institutions in the Wall Street. On the top features from "2009" dataset, again, we see the word "2008" is still prominent in predicting fi- nancial risks, indicating the hardship and extended impacts from the center of the economic crisis.</p><p>After examining the transcripts, we found sen- tences like: "...our specialty lighting business that we discontinued in the fourth quarter of 2008...", "...the exception of fourth quarter revenue which was $100,000 below our guidance target...", and "...to address changing economic conditions and their impact on our operations, in the fourth quar- ter we took the painful but prudent step of de- creasing our headcount by about 5%...", show- ing the crucial role that Q4 of 2008 plays in 2009 earnings calls. Interestingly, after the 2008-2009 crisis, in the recovery period, we have observed new words like "revenue", indicating the "back-to- normal" trend of financial environment, and new features that predict financial volatility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussions</head><p>In the experimental section, we notice that the proposed semiparametric Gaussian copula model has obtained promising results in various setups on three datasets in this text regression task. The main questions we ask are: how is the pro- posed model different from standard text regres- sion/classification models? What are the advan- tages of copula-based models, and what makes it perform so well? One advantage we see from the copula model is that it does not require any assumptions on the marginal distributions. For example, in latent Dirichlet allocation ( <ref type="bibr" target="#b1">Blei et al., 2003)</ref>, the topic proportion of a document is always drawn from a Dirichlet(α) distribution. This is rather re- stricted, because the possible shapes from a K − 1 simplex of Dirichlet is always limited in some sense. In our copula model, instead of using some priors, we just calculate the empirical cumulative distribution function of the random variables, and model the correlation among them. This is ex- tremely practical, because in many natural lan- guage processing tasks, we often have to deal with features that are extracted from many different do- mains and signals. By applying the Probability Integral Transform to raw features in the copula model, we essentially avoid comparing apples and oranges in the feature space, which is a common problem in bag-of-features models in NLP.</p><p>The second hypothesis is about the semiparam- etirc parameterization, which contains the non- parametric kernel density estimation and the para- metric Gaussian copula regression components. The benefit of a semiparametric model is that here we are not interested in performing completely nonparametric estimations, where the infinite di- mensional parameters might bring intractability. In contrast, by considering the semiparametric case, we not only obtain some expressiveness from the nonparametric models, but also reduce the complexity of the task: we are only interested in the finite-dimensional components Σ in the Gaus- sian copula with O(n log n) complexity, which is not as computationally difficult as the com- pletely nonparametric cases. Also, by modeling the marginals and their correlations seperately, our approach is cleaner, easy-to-understand, and al- lows us to have more flexibility to model the un- certainty of data. Our pilot experiment also aligns with our hypothesis: when not performing the ker- nel density estimation part for smoothing out the marginal distributions, the performances dropped significantly when sparser features are included.</p><p>The third advantage we observe is the power of modeling the covariance of the random variables. Traditionally, in statistics, independent and identi- cally distributed (i.i.d) assumptions among the in- stances and the random variables are often used in various models, such that the correlations among the instances or the variables are often ignored. However, this might not be practical at all: in im- age processing, the "cloud" pixel of a pixel show- ing the blue sky of a picture are more likelihood to co-occur in the same picture; in natural language processing, the word "mythical" is more likely to co-occur with the word "unicorn", rather than the word "popcorn". Therefore, by modeling the cor- relations among marginal CDFs, the copula model has gained the insights on the dependency struc- tures of the random variables, and thus, the perfor- mance of the regression task is boosted.</p><p>In the future, we plan to apply the proposed approach to large datasets where millions of fea- tures and millions of instances are involved. Cur- rently we have not experienced the difficulty when estimating the Gaussian copula model, but paral- lel methods might be needed to speedup learning when significantly more marginal CDFs are in- volved. The second issue is about overfitting. We see that when features are rather noisy, we might need to investigate regularized copula models to avoid this. Finally, we plan to extend the proposed approach to text classification and structured pre- diction problems in NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this work, we have demonstrated that the more complex quarterly earnings calls can also be used to predict the measured volatility of the stocks in the limited future. We propose a novel semipara- metric Gausian copula regression approach that models the dependency structure of the language in the earnings calls. Unlike traditional bag-of- features models that work discrete features from various signals, we perform kernel density esti- mation to smooth out the distribution, and use probability integral transform to work with CDFs that are uniform. The copula model deals with marginal CDFs and the correlation among them separately, in a cleaner manner that is also flexible to parameterize. Focusing on the three financial crisis related datasets, the proposed model signif- icantly outperform the standard linear regression method in statistics and strong discriminative sup- port vector regression baselines. By varying the size of the training data and the dimensionality of the covariates, we have demonstrated that our pro- posed model is relatively robust across different parameter settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The pre-2009 dataset consists of earnings calls from the period of 2006-2008, which includes calls from the beginning of economic downturn, the outbreak of the subprime mortgage crisis, and the epidemic of collapses of large financial insti- tutions. The 2009 dataset contains earnings calls from the year of 2009, which is a period where the credit crisis spreads globally, and the Dow Jones Industrial Average hit the lowest since the begin- ning of the millennium. The post-2009 dataset in- cludes earnings calls from the period of 2010 to 2013, which concerns the recovery of global econ- omy. The detailed statistics is shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Varying the amount of training data. Left column: pre-2009 dataset. Middle column: 2009 dataset. Right column: post-2009 dataset. Top row: Spearman's correlation. Bottom row: Kendall's tau.</figDesc><graphic url="image-4.png" coords="7,77.50,323.06,156.00,132.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Varying the amount of features. Left column: pre-2009 dataset. Middle column: 2009 dataset. Right column: post-2009 dataset. Top row: Spearman's correlation. Bottom row: Kendall's tau.</figDesc><graphic url="image-10.png" coords="8,77.50,195.81,156.00,132.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparing the learning algorithms on three datasets with all features. The best result is high-
lighted in bold. * indicates p &lt; .001 comparing to the second best result. 

</table></figure>

			<note place="foot" n="1"> In this work, the risk is defined as the measured volatility of stock prices from the week following the earnings call teleconference. See details in Section 5.</note>

			<note place="foot" n="2"> It is also known as the original Parzen windows (Parzen, 1962). 3 In our implementation, we use the default h of the Box kernel in the ksdensity function in Matlab.</note>

			<note place="foot" n="4"> http://www.cs.cmu.edu/˜yww/data/earningscalls.zip Algorithm 1 A Semi-parametric Gaussian Copula Model Based Text Regression Algorithm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank Alex Smola, Barnabás Póczos, Sam Thomson, Shoou-I Yu, Zi Yang, and anonymous reviewers for their useful comments.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Regression for sentence-level mt evaluation with pseudo references</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Twitter mood predicts the stock market</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huina</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Science</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Earnings calls: Exploring an emerging financial reporting genre</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Camiciottoli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Discourse &amp; Communication</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Word salad: Relating food prices and descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lily</forename><surname>Bryan R Routledge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Scherlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Chih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Estimation of copula-based semiparametric time series models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqin</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast algorithms for the calculation of kendalls τ</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Poisson mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Probabilistic frame-semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>In Human language technologies: The 2010 annual conference of the North American chapter of the association for computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Evaluating density forecasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Francis X Diebold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony S</forename><surname>Gunther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Copulas for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arjen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevyn</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins-Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A latent variable model for geographic lexical variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
