<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="469" to="476"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Dynamic oracles provide strong supervision for training constituency parsers with exploration, but must be custom defined for a given parser&apos;s transition system. We explore using a policy gradient method as a parser-agnostic alternative. In addition to directly optimizing for a tree-level metric such as F1, policy gradient has the potential to reduce exposure bias by allowing exploration during training; moreover, it does not require a dynamic oracle for supervision. On four constituency parsers in three languages, the method substantially outperforms static oracle likelihood training in almost all settings. For parsers where a dynamic oracle is available (in-cluding a novel oracle which we define for the transition system of Dyer et al. (2016)), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many recent state-of-the-art models for con- stituency parsing are transition based, decom- posing production of each parse tree into a se- quence of action decisions <ref type="bibr">(Dyer et al., 2016;</ref><ref type="bibr">Cross and Huang, 2016;</ref><ref type="bibr" target="#b7">Liu and Zhang, 2017;</ref><ref type="bibr" target="#b17">Stern et al., 2017)</ref>, building on a long line of work in transition-based parsing <ref type="bibr" target="#b9">(Nivre, 2003;</ref><ref type="bibr" target="#b25">Yamada and Matsumoto, 2003;</ref><ref type="bibr">Henderson, 2004;</ref><ref type="bibr" target="#b26">Zhang and Clark, 2011;</ref><ref type="bibr">Chen and Manning, 2014;</ref><ref type="bibr" target="#b1">Andor et al., 2016;</ref><ref type="bibr">Kiperwasser and Goldberg, 2016)</ref>.</p><p>However, models of this type, which decom- pose structure prediction into sequential decisions, can be prone to two issues ( <ref type="bibr" target="#b12">Ranzato et al., 2016;</ref><ref type="bibr" target="#b21">Wiseman and Rush, 2016)</ref>. The first is exposure bias: if, at training time, the model only observes states resulting from correct past decisions, it will not be prepared to recover from its own mistakes during prediction. Second is the loss mismatch be- tween the action-level loss used at training and any structure-level evaluation metric, for example F1.</p><p>A large family of techniques address the ex- posure bias problem by allowing the model to make mistakes and explore incorrect states during training, supervising actions at the resulting states using an expert policy <ref type="bibr">(Daumé III et al., 2009;</ref><ref type="bibr" target="#b13">Ross et al., 2011;</ref><ref type="bibr">Choi and Palmer, 2011;</ref><ref type="bibr">Chang et al., 2015)</ref>; these expert policies are typically referred to as dynamic oracles in parsing <ref type="bibr">(Goldberg and Nivre, 2012;</ref><ref type="bibr">Ballesteros et al., 2016)</ref>. While dynamic oracles have produced substan- tial improvements in constituency parsing perfor- mance ( <ref type="bibr">Coavoux and Crabbé, 2016;</ref><ref type="bibr">Cross and Huang, 2016;</ref><ref type="bibr" target="#b17">Stern et al., 2017;</ref><ref type="bibr">González and Gómez-Rodríguez, 2018)</ref>, they must be custom designed for each transition system.</p><p>To address the loss mismatch problem, another line of work has directly optimized for structure- level cost functions <ref type="bibr">(Goodman, 1996;</ref><ref type="bibr" target="#b10">Och, 2003)</ref>. Recent methods applied to models that produce output sequentially commonly use policy gradi- ent ( <ref type="bibr" target="#b2">Auli and Gao, 2014;</ref><ref type="bibr" target="#b12">Ranzato et al., 2016;</ref><ref type="bibr" target="#b15">Shen et al., 2016</ref>) or beam search ( <ref type="bibr" target="#b22">Xu et al., 2016;</ref><ref type="bibr" target="#b21">Wiseman and Rush, 2016;</ref><ref type="bibr">Edunov et al., 2017)</ref> at training time to minimize a structured cost. These methods also reduce exposure bias through explo- ration but do not require an expert policy for su- pervision.</p><p>In this work, we apply a simple policy gra- dient method to train four different state-of-the- art transition-based constituency parsers to max- imize expected F1. We compare against training with a dynamic oracle (both to supervise explo- ration and provide loss-augmentation) where one is available, including a novel dynamic oracle that we define for the top-down transition system of <ref type="bibr">Dyer et al. (2016)</ref>.</p><p>We find that while policy gradient usually out- performs standard likelihood training, it typically underperforms the dynamic oracle-based methods -which provide direct, model-aware supervision about which actions are best to take from arbi- trary parser states. However, a substantial frac- tion of each dynamic oracle's performance gain is often recovered using the model-agnostic policy gradient method. In the process, we obtain new state-of-the-art results for single-model discrimi- native transition-based parsers trained on the En- glish PTB (92.6 F1), French Treebank (83.5 F1), and Penn Chinese Treebank Version 5.1 (87.0 F1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models</head><p>The transition-based parsers we use all decompose production of a parse tree y for a sentence x into a sequence of actions (a 1 , . . . a T ) and resulting states (s 1 , . . . s T +1 ). Actions a t are predicted se- quentially, conditioned on a representation of the parser's current state s t and parameters θ:</p><formula xml:id="formula_0">p(y|x; θ) = T t=1 p(a t | s t ; θ)<label>(1)</label></formula><p>We investigate four parsers with varying transi- tion systems and methods of encoding the current state and sentence: (1) the discriminative Recur- rent Neural Network Grammars (RNNG) parser of <ref type="bibr">Dyer et al. (2016)</ref>, (2) the In-Order parser of <ref type="bibr" target="#b7">Liu and Zhang (2017)</ref>, (3) the Span-Based parser of <ref type="bibr">Cross and Huang (2016)</ref>, and (4) the Top-Down parser of <ref type="bibr" target="#b17">Stern et al. (2017)</ref>. <ref type="bibr">1</ref> We refer to the orig- inal papers for descriptions of the transition sys- tems and model parameterizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training Procedures</head><p>Likelihood training without exploration maxi- mizes Eq. 1 for trees in the training corpus, but may be prone to exposure bias and loss mismatch (Section 1). Dynamic oracle methods are known to improve on this training procedure for a vari- ety of parsers ( <ref type="bibr">Coavoux and Crabbé, 2016;</ref><ref type="bibr">Cross and Huang, 2016;</ref><ref type="bibr" target="#b17">Stern et al., 2017;</ref><ref type="bibr">González and Gómez-Rodríguez, 2018)</ref>, supervising exploration during training by providing the parser with the best action to take at each explored state. We de- scribe how policy gradient can be applied as an oracle-free alternative. We then compare to sev- eral variants of dynamic oracle training which fo- cus on addressing exposure bias, loss mismatch, or both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Policy Gradient</head><p>Given an arbitrary cost function ∆ comparing structured outputs (e.g. negative labeled F1, for trees), we use the risk objective:</p><formula xml:id="formula_1">R(θ) = N i=1 y p(y | x (i) ; θ)∆(y, y (i) )</formula><p>which measures the model's expected cost over possible outputs y for each of the training exam-</p><formula xml:id="formula_2">ples (x (1) , y (1) ), . . . , (x (N ) , y (N ) ).</formula><p>Minimizing a risk objective has a long his- tory in structured prediction <ref type="bibr" target="#b11">(Povey and Woodland, 2002;</ref><ref type="bibr" target="#b16">Smith and Eisner, 2006;</ref><ref type="bibr" target="#b5">Li and Eisner, 2009;</ref><ref type="bibr">Gimpel and Smith, 2010</ref>) but often re- lies on the cost function decomposing according to the output structure. However, we can avoid any restrictions on the cost using reinforcement learning-style approaches ( <ref type="bibr" target="#b22">Xu et al., 2016;</ref><ref type="bibr" target="#b15">Shen et al., 2016;</ref><ref type="bibr">Edunov et al., 2017)</ref> where cost is as- cribed to the entire output structure -albeit at the expense of introducing a potentially difficult credit assignment problem.</p><p>The policy gradient method we apply is a sim- ple variant of REINFORCE <ref type="bibr" target="#b20">(Williams, 1992)</ref>. We perform mini-batch gradient descent on the gradi- ent of the risk objective:</p><formula xml:id="formula_3">R(θ) = N i=1 y p(y|x (i) )∆(y, y (i) ) log p(y|x (i) ; θ) ≈ N i=1 y∈Y(x (i) ) ∆(y, y (i) ) log p(y|x (i) ; θ)</formula><p>where Y(x (i) ) is a set of k candidate trees obtained by sampling from the model's distribution for sen- tence x (i) . We use negative labeled F1 for ∆.</p><p>To reduce the variance of the gradient estimates, we standardize ∆ using its running mean and stan- dard deviation across all candidates used so far throughout training. Following <ref type="bibr" target="#b15">Shen et al. (2016)</ref>, we also found better performance when including the gold tree y (i) in the set of k candidates Y(x (i) ), and do so for all experiments reported here. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic Oracle Supervision</head><p>For a given parser state s t , a dynamic oracle de- fines an action a * (s t ) which should be taken to incrementally produce the best tree still reachable from that state. <ref type="bibr">3</ref> Dynamic oracles provide strong supervision for training with exploration, but require custom de- sign for a given transition system. <ref type="bibr">Cross and Huang (2016)</ref> and <ref type="bibr" target="#b17">Stern et al. (2017)</ref> defined opti- mal (with respect to F1) dynamic oracles for their respective transition systems, and below we define a novel dynamic oracle for the top-down system of RNNG.</p><p>In RNNG, tree production occurs in a stack- based, top-down traversal which produces a left- to-right linearized representation of the tree using three actions: OPEN a labeled constituent (which fixes the constituent's span to begin at the next word in the sentence which has not been shifted), SHIFT the next word in the sentence to add it to the current constituent, or CLOSE the current con- stituent (which fixes its span to end after the last word that has been shifted). The parser stores opened constituents on the stack, and must there- fore close them in the reverse of the order that they were opened.</p><p>At a given parser state, our oracle does the fol- lowing:</p><p>1. If there are any open constituents on the stack which can be closed (i.e. have had a word shifted since being opened), check the top- most of these (the one that has been opened most recently). If closing it would produce a constituent from the the gold tree that has not yet been produced (which is determined by the constituent's label, span beginning po- sition, and the number of words currently shifted), or if the constituent could not be closed at a later position in the sentence to produce a constituent in the gold tree, return CLOSE.</p><p>the estimate of the risk objective's gradient; however since in the parsing tasks we consider, the gold tree has constant and minimal cost, augmenting with the gold is equivalent to jointly optimizing the standard likelihood and risk objectives, using an adaptive scaling factor for each objective that is de- pendent on the cost for the trees that have been sampled from the model. We found that including the gold candidate in this manner outperformed initial experiments that first trained a model using likelihood training and then fine-tuned using un- biased policy gradient. <ref type="bibr">3</ref> More generally, an oracle can return a set of such actions that could be taken from the current state, but the oracles we use select a single canonical action.</p><p>2. Otherwise, if there are constituents in the gold tree <ref type="bibr">which</ref> have not yet been opened in the parser state, with span beginning at the next unshifted word, OPEN the outermost of these.</p><p>3. Otherwise, SHIFT the next word.</p><p>While we do not claim that this dynamic ora- cle is optimal with respect to F1, we find that it still helps substantially in supervising exploration (Section 5).</p><p>Likelihood Training with Exploration Past work has differed on how to use dynamic ora- cles to guide exploration during oracle training ( <ref type="bibr">Ballesteros et al., 2016;</ref><ref type="bibr">Cross and Huang, 2016;</ref><ref type="bibr" target="#b17">Stern et al., 2017</ref>). We use the same sample-based method of generating candidate sets Y as for pol- icy gradient, which allows us to control the dy- namic oracle and policy gradient methods to per- form an equal amount of exploration. Likelihood training with exploration then maximizes the sum of the log probabilities for the oracle actions for all states composing the candidate trees:</p><formula xml:id="formula_4">L E (θ) = N i=1 y∈Y(x (i) ) s∈y log p(a * (s) | s)</formula><p>where a * (s) is the dynamic oracle's action for state s.</p><p>Softmax Margin Softmax margin loss (Gimpel and Smith, 2010; Auli and Lopez, 2011) addresses loss mismatch by incorporating task cost into the training loss. Since trees are decomposed into a sequence of local action predictions, we cannot use a global cost, such as F1, directly. As a proxy, we rely on the dynamic oracles' action-level su- pervision.</p><p>In all models we consider, action probabilities (Eq. 1) are parameterized by a softmax function</p><formula xml:id="formula_5">p M L (a | s t ; θ) ∝ exp(z(a, s t , θ))</formula><p>for some state-action scoring function z. The softmax-margin objective replaces this by</p><formula xml:id="formula_6">p SM M (a | s t ; θ) ∝ exp(z(a, s t , θ) + ∆(a, a * t ))<label>(2)</label></formula><p>We use ∆(a, a * t ) = 0 if a = a * t and 1 otherwise. This can be viewed as a "soft" version of the max- margin objective used by <ref type="bibr">Stern</ref>   Softmax Margin with Exploration Finally, we train using a combination of softmax margin loss augmentation and exploration. We perform the same sample-based candidate generation as for policy gradient and likelihood training with explo- ration, but use Eq. 2 to compute the training loss for candidate states. For those parsers that have a dynamic oracle, this provides a means of training that more directly provides both exploration and cost-aware losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We compare the constituency parsers listed in Sec- tion 2 using the above training methods. Our experiments use the English PTB ( <ref type="bibr" target="#b8">Marcus et al., 1993)</ref>, French Treebank ( <ref type="bibr" target="#b0">Abeillé et al., 2003)</ref>, and Penn Chinese Treebank (CTB) Version 5.1 ( <ref type="bibr" target="#b24">Xue et al., 2005</ref>).</p><p>Training To compare the training procedures as closely as possible, we train all models for a given parser in a given language from the same randomly-initialized parameter values.</p><p>We train two different versions of the RNNG model: one model using size 128 for the LSTMs and hidden states (following the original work), and a larger model with size 256. We perform evaluation using greedy search in the Span-Based and Top-Down parsers, and beam search with beam size 10 for the RNNG and In-Order parsers. We found that beam search improved performance for these two parsers by around 0.1-0.3 F1 on the development sets, and use it at inference time in every setting for these two parsers.</p><p>In our experiments, policy gradient typically re- quires more epochs of training to reach perfor- mance comparable to either of the dynamic oracle- based exploration methods. <ref type="figure" target="#fig_0">Figure 1</ref> gives a typi- cal learning curve, for the Top-Down parser on En- glish. We found that policy gradient is also more sensitive to the number of candidates sampled per sentence than either of the other exploration meth- ods, with best performance on the development set usually obtained with k = 10 for k ∈ {2, 5, 10} (where k also counts the sentence's gold tree, in- cluded in the candidate set). See Appendix A in the supplemental material for the values of k used.</p><p>Tags, Embeddings, and Morphology We largely follow previous work for each parser in our use of predicted part-of-speech tags, pretrained word embeddings, and morphological features.</p><p>All parsers use predicted part-of-speech tags as part of their sentence representations. For En- glish and Chinese, we follow the setup of <ref type="bibr">Cross and Huang (2016)</ref>: training the Stanford tagger ( <ref type="bibr" target="#b18">Toutanova et al., 2003</ref>) on the training set of each parsing corpus to predict development and test set tags, and using 10-way jackknifing to predict tags for the training set.</p><p>For French, we use the predicted tags and mor- phological features provided with the SPMRL dataset ( <ref type="bibr" target="#b14">Seddah et al., 2014</ref>). We modified the publicly released code for all parsers to use pre- dicted morphological features for French. We fol- low the approach outlined by <ref type="bibr">Cross and Huang (2016)</ref> and <ref type="bibr" target="#b17">Stern et al. (2017)</ref> for representing morphological features as learned embeddings, and use the same dimensions for these embeddings as in their papers. For RNNG and In-Order, we similarly use 10-dimensional learned embeddings for each morphological feature, feeding them as LSTM inputs for each word alongside the word and part-of-speech tag embeddings.</p><p>For RNNG and the In-Order parser, we use the same word embeddings as the original papers for English and Chinese, and train 100-dimensional word embeddings for French using the structured skip-gram method of <ref type="bibr" target="#b6">Ling et al. (2015)</ref> on French Wikipedia. <ref type="table" target="#tab_3">Table 1</ref> compares parser F1 by training procedure for each language. Policy gradient improves upon likelihood training in 14 out of 15 cases, with im- provements of up to 1.5 F1. One of the three dy- namic oracle-based training methods -either like- lihood with exploration, softmax margin (SMM), or softmax margin with exploration -obtains bet- ter performance than policy gradient in 10 out of 12 cases. This is perhaps unsurprising given the strong supervision provided by the dynamic ora- cles and the credit assignment problem faced by 91.4 (+0.4) 81.4 (-0.1) 83.5 (+0.2) likelihood+explore * 91.3 (+0.3) 81.2 (-0.3) 83.5 (+0.2) SMM * 91.3 (+0.3) 81.5 (+0.0) 83.7 (+0.4) SMM+explore * 91.5 (+0.5) 81.7 (+0.2) 84.0 (+0.7) Top-Down ( <ref type="bibr" target="#b17">Stern et al., 2017</ref> 91.6 (+0.2) 83.3 (+0.1) 84.7 (+0.2) likelihood+explore * 92.1 (+0.7) 83.0 (-0.2) 85.5 (+1.0) SMM * 91.5 (+0.1) 82.8 (-0.4) 83.6 (-0.9) SMM+explore * 92.1 (+0.7) 83.5 (+0.3) 85.0 (+0.5) RNNG Discriminative, Size 256 likelihood 91.7 83.1 84.5 policy gradient 92.3 (+0.7) 83.2 (+0.1) 85.6 (+1.1) likelihood+explore 92.6 (+0.9) 82.9 <ref type="bibr">(-0.2)</ref> 86.0 (+1.5) In-Order ( <ref type="bibr" target="#b7">Liu and Zhang, 2017</ref>) L&amp;Z 91.8 - 86.1 likelihood 91.6 82.7 85.5 policy gradient 92.2 (+0.6) 83.3 (+0.6) 87.0 (+1.5) policy gradient. However, a substantial fraction of this performance gain is recaptured by policy gra- dient in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>While likelihood training with exploration us- ing a dynamic oracle more directly addresses ex- ploration bias, and softmax margin training more directly addresses loss mismatch, these two phe- nomena are still entangled, and the best dynamic oracle-based method to use varies. The effective- ness of the oracle method is also likely to be influ- enced by the nature of the dynamic oracle avail- able for the parser. For example, the oracle for RNNG lacks F1 optimality guarantees, and soft- max margin without exploration often underper- forms likelihood for this parser. However, explo- ration improves softmax margin training across all parsers and conditions. Although results from likelihood training are mostly comparable between RNNG-128 and the larger model RNNG-256 across languages, policy gradient and likelihood training with exploration both typically yield larger improvements in the larger models, obtaining 92.6 F1 for English and 86.0 for Chinese (using likelihood training with exploration), although results are slightly higher for the policy gradient and dynamic oracle-based methods for the smaller model on French (includ- ing 83.5 with softmax margin with exploration). Finally, we observe that policy gradient also pro- vides large improvements for the In-Order parser, where a dynamic oracle has not been defined.</p><p>We note that although some of these results (92.6 for English, 83.5 for French, 87.0 for Chi- nese) are state-of-the-art for single model, dis- criminative transition-based parsers, other work on constituency parsing achieves better perfor- mance through other methods. Techniques that combine multiple models or add semi-supervised data ( <ref type="bibr" target="#b19">Vinyals et al., 2015;</ref><ref type="bibr">Dyer et al., 2016;</ref><ref type="bibr">Choe and Charniak, 2016;</ref><ref type="bibr" target="#b4">Kuncoro et al., 2017;</ref><ref type="bibr" target="#b7">Liu and Zhang, 2017;</ref><ref type="bibr">Fried et al., 2017)</ref> are orthog- onal to, and could be combined with, the single- model, fixed training data methods we explore. Other recent work ( <ref type="bibr">Gaddy et al., 2018;</ref><ref type="bibr" target="#b3">Kitaev and Klein, 2018)</ref> obtains comparable or stronger per- formance with global chart decoders, where train- ing uses loss augmentation provided by an ora- cle. By performing model-optimal global infer- ence, these parsers likely avoid the exposure bias problem of the sequential transition-based parsers we investigate, at the cost of requiring a chart de- coding procedure for inference.</p><p>Overall, we find that although optimizing for F1 in a model-agnostic fashion with policy gradi- ent typically underperforms the model-aware ex- pert supervision given by the dynamic oracle train- ing methods, it provides a simple method for con- sistently improving upon static oracle likelihood training, at the expense of increased training costs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: English development set F1 by training epoch, comparing likelihood training with two exploration variants for the Top-Down parser.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>et al. (2017) for training without exploration, but retains a locally- normalized model that we can use for sampling- based exploration.</figDesc><table>10 
20 
30 
40 
50 
epoch 

89.0 

89.5 

90.0 

90.5 

91.0 

91.5 

92.0 

development F1 

likelihood 
likelihood + explore, k=10 
policy gradient, k=10 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Test set F1 by training procedure, and 
in comparison to past work using the same mod-
els. Improvements over likelihood training are 
indicated in parentheses, with the highest results 
among the training procedures compared here in 
bold.  *  : training uses a dynamic oracle;  † : past 
work using a global scoring model (all models we 
train here are locally-normalized). 

</table></figure>

			<note place="foot" n="1"> Stern et al. (2017) trained their model using a nonprobabilistic, max-margin objective. For comparison to the other models and to allow training with policy gradient, we create a locally-normalized probabilistic variant of their model by applying a softmax function to the predicted scores for each action.</note>

			<note place="foot" n="2"> Including the gold tree in the set of candidates does bias</note>

			<note place="foot">Michael Auli and Adam Lopez. 2011. Training a loglinear parser with loss functions via softmax-margin. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11, pages 333-343, Stroudsburg, PA, USA. Association for Computational Linguistics. Miguel Ballesteros, Yoav Goldberg, Chris Dyer, and</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Building a Treebank for French</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Abeillé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Clément</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Toussenel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer</publisher>
			<pubPlace>Netherlands, Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2442" to="2452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Decoder integration and expected BLEU training for recurrent neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="313" to="327" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Constituency parsing with a self-attentive encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01052</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What do recurrent neural network grammars learn about syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1249" to="1258" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">First-and secondorder expectation semirings with applications to minimum-risk training on translation forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="40" to="51" />
		</imprint>
	</monogr>
	<note>EMNLP &apos;09. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Two/too simple adaptations of word2vec for syntax problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1299" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">In-order transition-based constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="413" to="424" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An efficient algorithm for projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Parsing Technologies (IWPT)</title>
		<meeting>the 8th International Workshop on Parsing Technologies (IWPT)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Minimum phone error and i-smoothing for improved discriminative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Philip C Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">105</biblScope>
		</imprint>
	</monogr>
	<note>2002 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<title level="m">Sequence level training with recurrent neural networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="627" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Introducing the spmrl 2014 shared task on parsing morphologically-rich languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages</title>
		<meeting>the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="109" />
		</imprint>
	</monogr>
	<note>Dublin City University</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1683" to="1692" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Minimum risk annealing for training log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions</title>
		<meeting>the COLING/ACL 2006 Main Conference Poster Sessions<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="787" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A minimal span-based neural constituency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>NAACL &apos;03. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reinforcement Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1296" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Expected f-measure training for shiftreduce parsing with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenduan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen Christopher</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North</title>
		<meeting>the 2016 Conference of the North</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The penn chinese treebank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Statistical dependency analysis with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyasu</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Parsing Technologies (IWPT)</title>
		<meeting>the 8th International Workshop on Parsing Technologies (IWPT)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Syntactic processing using the generalized perceptron and beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="151" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
