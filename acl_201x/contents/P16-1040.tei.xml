<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transition-Based Neural Word Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Heilongjiang University</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
							<email>ghfu@hotmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Heilongjiang University</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transition-Based Neural Word Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="421" to="431"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Character-based and word-based methods are two main types of statistical models for Chinese word segmentation, the former exploiting sequence labeling models over characters and the latter typically exploiting a transition-based model, with the advantages that word-level features can be easily utilized. Neural models have been exploited for character-based Chi-nese word segmentation, giving high accuracies by making use of external character embeddings, yet requiring less feature engineering. In this paper, we study a neu-ral model for word-based Chinese word segmentation, by replacing the manually-designed discrete features with neural features in a word-based segmentation framework. Experimental results demonstrate that word features lead to comparable performances to the best systems in the literature , and a further combination of discrete and neural features gives top accuracies.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Statistical word segmentation methods can be cat- egorized character-based <ref type="bibr" target="#b35">(Xue, 2003;</ref><ref type="bibr" target="#b27">Tseng et al., 2005</ref>) and word-based <ref type="bibr" target="#b1">(Andrew, 2006;</ref><ref type="bibr" target="#b38">Zhang and Clark, 2007)</ref> approaches. The former casts word segmentation as a sequence labeling problem, us- ing segmentation tags on characters to mark their relative positions inside words. The latter, in con- trast, ranks candidate segmented outputs directly, extracting both character and full-word features.</p><p>An influential character-based word segmenta- tion model ( <ref type="bibr" target="#b21">Peng et al., 2004;</ref><ref type="bibr" target="#b27">Tseng et al., 2005</ref>) uses B/I/E/S labels to mark a character as the be- ginning, internal (neither beginning nor end), end and only-character (both beginning and end) of a word, respectively, employing conditional random field (CRF) to model the correspondence between the input character sequence and output label se- quence. For each character, features are extracted from a five-character context window and a two- label history window. Subsequent work explores different label sets ( <ref type="bibr" target="#b45">Zhao et al., 2006</ref>), feature sets (Shi and <ref type="bibr" target="#b22">Wang, 2007)</ref> and semi-supervised learn- ing ( <ref type="bibr" target="#b23">Sun and Xu, 2011)</ref>, reporting state-of-the-art accuracies.</p><p>Recently, neural network models have been in- vestigated for the character tagging approach. The main idea is to replace manual discrete features with automatic real-valued features, which are de- rived automatically from distributed character rep- resentations using neural networks. In particular, convolution neural network 1 ( <ref type="bibr" target="#b47">Zheng et al., 2013)</ref>, tensor neural network ( <ref type="bibr" target="#b20">Pei et al., 2014</ref>), recur- sive neural network <ref type="bibr" target="#b4">(Chen et al., 2015a</ref>) and long- short-term-memory (LSTM) <ref type="bibr" target="#b5">(Chen et al., 2015b)</ref> have been used to derive neural feature represen- tations from input word sequences, which are fed into a CRF inference layer.</p><p>In this paper, we investigate the effectiveness of word embedding features for neural network seg- mentation using transition-based models. Since it is challenging to integrate word features to the CRF inference framework of the existing step action buffer(· · · w−1w0) queue(c0c1 · · · ) 0 -</p><formula xml:id="formula_0">φ 中 国 · · · 1 SEP 中 国 外 · · · 2 APP 中国 外 企 · · · 3 SEP 中国 外 企 业 · · · 4 APP 中国 外企 业 务 · · · 5 SEP 中国 外企 业 务 发 · · · 6 APP 中国 外企 业务 发 展 · · · 7 SEP · · · 业务 发 展 迅 速 8 APP · · · 业务 发展 迅 速 9 SEP · · · 发展 迅 速 10 APP · · · 发展 迅速 φ</formula><p>Figure 2: Segmentation process of "中 国 (Chi- nese) 外 企 (foreign company) 业 务 (busi- ness) 发展 (develop) 迅速 (quickly)".</p><p>character-based methods, we take inspiration from word-based discrete segmentation instead. In par- ticular, we follow <ref type="bibr" target="#b38">Zhang and Clark (2007)</ref>, using the transition-based framework to decode a sen- tence from left-to-right incrementally, scoring par- tially segmented results using both character-level and word-level features. Beam-search is applied to reduce error propagation and large-margin train- ing with early-update ( <ref type="bibr" target="#b6">Collins and Roark, 2004</ref>) is used for learning from inexact search. We replace the discrete word and character fea- tures of <ref type="bibr" target="#b38">Zhang and Clark (2007)</ref> with word and character embeddings, respectively, and change their linear model into a deep neural network. Following <ref type="bibr" target="#b47">Zheng et al. (2013)</ref> and <ref type="bibr" target="#b5">Chen et al. (2015b)</ref>, we use convolution neural networks to achieve local feature combination and LSTM to learn global sentence-level features, respectively. The resulting model is a word-based neural seg- menter that can leverage rich embedding features. Its correlation with existing work on Chinese seg- mentation is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Results on standard benchmark datasets show the effectiveness of word embedding features for neural segmentation. Our method achieves state- of-the-art results without any preprocess based on external knowledge such as Chinese idioms of <ref type="bibr" target="#b4">Chen et al. (2015a)</ref> and <ref type="bibr" target="#b5">Chen et al. (2015b)</ref>. We re- lease our code under GPL for research reference. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Baseline Discrete Model</head><p>We exploit the word-based segmentor of Zhang and Clark (2011) as the baseline system. It in- crementally segments a sentence using a transition system, with a state holding a partially-segmented sentence in a buffer s and ordering the next incom- ing characters in a queue q. Given an input Chi- nese sentence, the buffer is initially empty and the queue contains all characters of the sentence, a se- quence of transition actions are used to consume characters in the queue and build the output sen- tence in the buffer. The actions include:</p><p>• Append (APP), which removes the first character from the queue, and appends it to the last word in the buffer;</p><p>• Separate (SEP), which moves the first character of the queue onto the buffer as a new (sub) word.</p><p>Given the input sequence of characters "中国 外 企 业 务 发 展 迅 速" (The business of foreign company in China develops quickly), the correct output can be derived using action sequence "SEP APP SEP APP SEP APP SEP APP SEP APP", as shown in <ref type="figure">Figure 2</ref>.</p><p>Search. Based on the transition system, the de- coder searches for an optimal action sequence for a given sentence. Denote an action sequence as A = a 1 · · · a n . We define the score of A as the total score of all actions in the sequence, which is computed by:</p><formula xml:id="formula_1">score(A) = a∈A score(a) = a∈A w · f (s, q, a),</formula><p>where w is the model parameters, f is a feature ex- traction function, s and q are the buffer and queue of a certain state before the action a is applied. The feature templates are shown in <ref type="table">Table 1</ref>, which are the same as <ref type="bibr" target="#b39">Zhang and Clark (2011)</ref>. These base features include three main source of information. First, characters in the front of the queue and the end of the buffer are used for scor- ing both separate and append actions (e.g. c 0 ). Second, words that are identified are used to guide separate actions (e.g. w 0 ). Third, relevant infor- mation of identified words, such as their lengths and first/last characters are utilized for additional features (e.g. len(w −1 )).</p><p>We follow <ref type="bibr" target="#b39">Zhang and Clark (2011)</ref> in using beam-search for decoding, shown in Algorith 1, where Θ is the set of model parameters. Initially the beam contains only the initial state. At each step, each state in the beam is extended by apply- ing both SEP and APP, resulting in a set of new states, which are scored and ranked. The top B are APP, SEP w−1, w−1w−2, w−1c0, w−2len(w−1)  <ref type="table">Table 1</ref>: Feature templates for the baseline model, where w i denotes the word in the buffer, c i de- notes the character in the queue, as shown in <ref type="figure">Fig- ure 2</ref>, start(.), end(.) and len(.) denote the first, last character and length of a word, respectively.</p><formula xml:id="formula_2">Algorithm 1 Beam-search decoding. function DECODE(c 1 · · · c n , Θ) agenda ← { (φ, c 1 · · · c n , score=0.0) } for i in 1 · · · n beam ← { } for cand in agenda new ← SEP(cand, c i , Θ) ADDITEM(beam, new) new ← APP(cand, c i , Θ) ADDITEM(beam, new) agenda ← TOP-B(beam, B) best ← BESTITEM(agenda) w 1 · · · w m ← EXTRACTWORDS(best)</formula><p>used as the beam states for the next step. The same process replaces until all input character are pro- cessed, and the highest-scored state in the beam is taken for output. Online leaning with max-margin is used, which is given in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Transition-Based Neural Model</head><p>We use a neural network model to replace the discrete linear model for scoring transition action sequences. For better comparison between dis- crete and neural features, the overall segmentation framework of the baseline is used, which includes the incremental segmentation process, the beam- search decoder and the training process integrated with beam-search ( <ref type="bibr" target="#b39">Zhang and Clark, 2011</ref>). In ad- dition, the neural network scorer takes the simi- lar feature sources as the baseline, which includes character information over the input, word infor- mation of the partially constructed output, and the history sequence of the actions that have been ap- plied so far.</p><p>The overall architecture of the neural scorer is shown in <ref type="figure" target="#fig_3">Figure 3</ref>. Given a certain state All the three feature vectors are used scoring the SEP action. For APP, on the other hand, we use only the character and action features r c and r a because the last word w 0 in the buffer is a partial word. Formally, given r w , r c , r a , the action scores are computed by:</p><formula xml:id="formula_3">score(SEP) score(APP) · · · h sep · · · h app · · · r c · · · r w · · · r a word sequence character sequence action sequence RNN RNN RNN · · · w −1 w 0 · · · c −1 c 0 c 1 · · · · · · a</formula><formula xml:id="formula_4">score(SEP) = w sep h sep score(APP) = w app h app where h sep = tanh(W sep [r w , r c , r a ] + b sep ) h app = tanh(W app [r c , r a ] + b app ) W sep , W app , b sep , b app , w sep , w app are model pa- rameters.</formula><p>The neural networks take the embedding forms of words, characters and actions as input, for ex- tracting r w , r c and r a , respectively. We exploit the LSTM-RNN structure (Hochreiter and Schmidhu- ber, 1997), which can better capture non-local syn- tactic and semantic information from a sequential input, yet reducing gradient explosion or dimin- ishing during training.</p><p>In general, given a sequence of input vectors x 0 · · · x n , the LSTM-RNN computes a sequence of hidden vectors h 0 · · · h n , respectively, with each h i being determined by the input x i and the previous hidden vector h i−1 . A cell structure ce is ... ... used to carry long-term memory information over the history h 0 · · · h i for calculating h i , and infor- mation flow is controlled by an input gate ig, an output gate og and a forget gate fg. Formally, the calculation of h i using h i−1 and x i is:</p><formula xml:id="formula_5">w i−1 ...... ... x w i ...... (a) word representation ... a i ... a i−1 ...... ... x a i ...... (b) action representation ... ⊕ ... c i , c i−1 c i ... ⊕ ... c i−1 , c i−2 c i−1 ... ⊕ ...</formula><formula xml:id="formula_6">ig i = σ(W ig x i + U ig h i−1 + V ig ce i−1 + b ig ) fg i = σ(W f g x i + U f g h i−1 + V f g ce i−1 + b f g ) ce i = fg i ce i−1 + ig i tanh(W ce x i + U ce h i−1 + b ce ) og i = σ(W og x i + U og h i−1 + V og ce i + b og ) h i = og i tanh(ce i ),</formula><p>where U, V, W, b are model parameters, and de- notes Hadamard product. When used to calculate r w , r c and r a , the gen- eral LSTM structure above is given different input sequences x 0 · · · x n , according to the word, char- acter and action sequences, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input representation</head><p>Words. Given a word w, we use a looking-up ma- trix E w to obtain its embedding e w (w). The ma- trix can be obtained through pre-training on large size of auto segmented corpus. As shown in <ref type="figure" target="#fig_5">Fig- ure 4(a)</ref>, we use a convolutional neural layer upon a two-word window to obtain · · · x w −1 x w 0 for the LSTM for r w , with the following formula:</p><formula xml:id="formula_7">x w i = tanh W w [e w (w i−1 ), e w (w i )] + b w</formula><p>Actions. We represent an action a with an em- bedding e a (a) from a looking-up table E a , and apply the similar convolutional neural network to obtain · · · x a −1 x a 0 for r a , as shown in <ref type="figure" target="#fig_5">Figure 4</ref>(b).</p><p>Given the input action sequence · · · a −1 a 0 , the x a i is computed by:</p><formula xml:id="formula_8">x a i = tanh W a [e a (a i−1 ), e a (a i )] + b a</formula><p>Characters. We make embeddings for both char- acter unigrams and bigrams by looking-up ma- trixes E c and E bc , respectively, the latter being shown to be useful by <ref type="bibr" target="#b20">Pei et al. (2014)</ref>. For each character c i , the unigram embedding e c (c i ) and the bigram embedding e bc (c i−1 c i ) are con- catenated, before being given to a CNN with a convolution size of 5. For the character sequence</p><formula xml:id="formula_9">· · · c −1 c 0 c 1 · · · of a given state (s, q), we compute its input vectors · · · x c −1 x c 0 x c 1 · · ·</formula><p>for the LSTM for r c by:</p><formula xml:id="formula_10">x c i = tanh W c [e c (c i−2 ) ⊕ e bc (c i−3 c i−2 ), · · · , e c (c i ) ⊕ e bc (c i−1 c i ), · · · , e c (c i+2 ) ⊕ e bc (c i+1 c i+2 )] + b c</formula><p>For all the above input representations, the looking-up tables E w , E a , E c , E bc and the weights W w , W a , W c , b w , b a , b c are model pa- rameters. For calculating r w and r a , we apply the LSTMs directly over the sequences · · · x w −1 x w 0 and · · · x a −1 x a 0 for words and actions, and use the out- puts h w 0 and h a 0 for r w and r a , respectively. For calculating r c , we further use a bi-directional ex- tension of the original LSTM structure. In partic- ular, the base LSTM is applied to the input char- acter sequence both from left to right and from right to left, leading to two hidden node sequences</p><formula xml:id="formula_11">· · · h cL −1 h cL 0 h cL 1 · · · and · · · h cR −1 h cR 0 h cR 1 · · · , re- spectively.</formula><p>For the current character c 0 , h cL 0 and h cR 0 are concatenated to form the final vector r c . This is feasible because the character sequence is input and static, and previous work has demon- strated better capability of bi-directional LSTM for modeling sequences (Yao and Zweig, 2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Integrating discrete features</head><p>Our model can be extended by integrating the baseline discrete features into the feature layer. In particular,</p><formula xml:id="formula_12">score(SEP) = w sep (h sep ⊕ f sep ) score(APP) = w app (h app ⊕ f app ),</formula><p>where f sep and f app represent the baseline sparse vector for SEP and APP features, respectively, and ⊕ denotes the vector concatenation operation. </p><note type="other">2 Max-margin training with early- update. function TRAIN</note><formula xml:id="formula_13">(c 1 · · · c n , a g 1 · · · a g n , Θ) agenda ← { (φ, c 1 · · · c n , score=0.0) } for i in 1 · · · n beam ← { } for cand in agenda new ← SEP(cand, c i , Θ) if {a g i = SEP} new.score += η ADDITEM(beam, new) new ← APP(cand, c i , Θ) if {a g i = APP} new.score += η ADDITEM(beam, new) agenda ← TOP-B(beam, B) if {ITEM(a g 1 · · · a g i ) / ∈ agenda} Θ = Θ − f BESTITEM(agenda) Θ = Θ + f ITEM((a g 1 · · · a g i ) return if {ITEM(a g 1 · · · a g n ) = BESTITEM(agenda)} Θ = Θ − f BESTITEM(agenda) Θ = Θ + f ITEM((a g 1 · · · a g n )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head><p>To train model parameters for both the discrete and neural models, we exploit online learning with early-update as shown in Algorithm 2. A max- margin objective is exploited, 3 which is defined as:</p><formula xml:id="formula_14">L(Θ) = 1 K K k=1 l(A g k , Θ) + λ 2 Θ 2 l(A g k , Θ) = max A score(A k , Θ) + η · δ(A k , A g k ) − score(A g k , Θ),</formula><p>where Θ is the set of all parameters, {A g k } K n=1 are gold action sequences to segment the training cor- pus, A k is the model output action sequence, λ is a regularization parameter and η is used to tune the loss margins.</p><p>For the discrete models, f (·) denotes the fea- tures extracted according to the feature templates in <ref type="table">Table 1</ref>. For the neural models, f (·) denotes the corresponding h sep and h app . Thus only the output layer is updated, and we further use back- propagation to learn the parameters of the other layers ( <ref type="bibr" target="#b14">LeCun et al., 2012</ref>). We use online Ada- 3 <ref type="bibr" target="#b48">Zhou et al. (2015)</ref> find that max-margin training did not yield reasonable results for neural transition-based parsing, which is different from our findings. One likely reason is that when the number of labels is small max-margin is effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CTB60</head><p>PKU <ref type="table" target="#tab_1">MSR   Training  #sent  23k  17k  78k  #word  641k  1,</ref>     <ref type="table" target="#tab_2">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Development Results</head><p>To better understand the word-based neural mod- els, we perform several development experiments.</p><p>All the experiments in this section are conducted on the CTB6 development dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Embeddings and beam size</head><p>We study the influence of beam size on the base- line and neural models. Our neural model has two choices of using pre-trained word embeddings. We can either fine-tune or fix the embeddings dur- ing training. In case of fine-tuning, only words in the training data can be learned, while embeddings of out-of-vocabulary (OOV) words could not be used effectively. <ref type="bibr">5</ref> In addition, following <ref type="bibr" target="#b10">Dyer et al. (2015)</ref> we randomly set words with frequency 1 in the training data as the OOV words in order to learn the OOV embedding, while avoiding over- fitting. If the pretrained word embeddings are not fine-tuned, we can utilize all word embeddings. <ref type="figure" target="#fig_8">Figure 5</ref> shows the development results, where the training curve of the discrete baseline is shown in <ref type="figure" target="#fig_8">Figure 5</ref> On the other hand, with fine-tuning, the results are different. The model with beam size 1 gives better accuracies compared to the other models with the same beam size. However, as the beam size increases, the performance increases very lit- tle. The results are consistent with <ref type="bibr" target="#b10">Dyer et al. (2015)</ref>, who find that beam-search improves the results only slightly on dependency parsing. When a beam size of 16 is used, this model performs the <ref type="bibr">5</ref> We perform experiments using random initialized word embeddings as well when fine-tune is used, which is a fully supervised model. The performance is slightly lower.   worst compared with the discrete model and the neural model without fine-tuning. This is likely because the fine-tuning of embeddings leads to overfitting of in-vocabulary words, and underfit- ting over OOV words. Based on the observation, we exploit fixed word embeddings in our final models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Feature ablation</head><p>We conduct feature ablation experiments to study the effects of the word, character unigram, charac- ter bigram and action features to the neural model. The results are shown in <ref type="table" target="#tab_4">Table 4</ref>. Word features are particularly important to the model, without which the performance decreases by 4.5%. The effects of the character unigram, bigram and ac- tion features are relatively much weaker. <ref type="bibr">6</ref> This demonstrates that in the word-based incremental search framework, words are the most crucial in- formation to the neural model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Integrating discrete features</head><p>Prior work has shown the effectiveness of integrat- ing discrete and neural features for several NLP tasks <ref type="bibr" target="#b28">(Turian et al., 2010</ref>  2013; <ref type="bibr" target="#b9">Durrett and Klein, 2015;</ref>. We investigate the usefulness of such inte- gration to our word-based segmentor on the devel- opment dataset. We study it by two ways. First, we compare the error distributions between the discrete and the neural models. Intuitively, differ- ent error distributions are necessary for improve- ments by integration. We draw a scatter graph to show their differences, with the (x, y) values of each point denoting the F-measure scores of the two models with respect to sentences, respectively. As shown in <ref type="figure" target="#fig_10">Figure 6</ref>, the points are rather disper- sive, showing the differences of the two models.</p><p>Further, we directly look at the results after in- tegration of both discrete and neural features. As shown in <ref type="table" target="#tab_4">Table 4</ref>, the integrated model improves the accuracies from 95.45% to 96.30%, demon- strating that the automatically-induced neural fea- tures contain highly complementary information to the manual discrete features. <ref type="table">Table 6</ref> shows the final results on CTB6 test dataset. For thorough comparison, we implement discrete, neural and combined character-based models as well. <ref type="bibr">7</ref> In particular, the character-based discrete model is a CRF tagging model using char- acter unigrams, bigrams, trigrams and tag transi- tions ( <ref type="bibr" target="#b27">Tseng et al., 2005</ref>), and the character-based neural model exploits a bi-directional LSTM layer to model character sequences 8 and a CRF layer for <ref type="bibr">7</ref> The code is released for research reference under GPL at https://github.com/SUTDNLP/NNSegmentation. <ref type="bibr">8</ref> We use a concatenation of character unigram and bigram embeddings at each position as the input to LSTM, because our experiments show that the character bigram embeddings are useful, without which character-based neural models are significantly lower than their discrete counterparts. <ref type="formula">(2009)</ref> 95.2 97.3 <ref type="table">Table 6</ref>: Main results on PKU and MSR test datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Final Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><note type="other">PKU MSR our word-based models discrete 95.1 97.3 neural 95.1 97.0 combined 95.7 97.7 character-based models discrete 94.9 96.8 neural 94.4 97.2 combined 95.4 97.2 other models Cai and Zhao (2016) 95.5 96.5 Ma and Hinrichs (2015) 95.1 96.6 Pei et al. (2014) 95.2 97.2 Zhang et al. (2013a) 96.1 97.5 Sun et al. (2012) 95.4 97.4 Zhang and Clark (2011) 95.1 97.1 Sun (2010) 95.2 96.9 Sun et al.</note><p>output <ref type="bibr" target="#b5">(Chen et al., 2015b)</ref>. <ref type="bibr">9</ref> The combined model uses the same method for integrating discrete and neural features as our word-based model. The word-based models achieve better perfor- mances than character-based models, since our model can exploit additional word information learnt from large auto-segmented corpus. We also compare the results with other models. <ref type="bibr" target="#b30">Wang et al. (2011)</ref> is a semi-supervised model that exploits word statistics from auto-segmented raw corpus, which is similar with our combined model in using semi-supervised word information. We achieve slightly better accuracies. <ref type="bibr" target="#b44">Zhang et al. (2014)</ref> is a joint segmentation, POS-tagging and dependency parsing model, which can exploit syntactic infor- mation.</p><p>To compare our models with other state-of-the- art models in the literature, we report the perfor- mance on the PKU and MSR datasets also. 10 Our combined model gives the best result on the MSR dataset, and the second best on PKU. The method of <ref type="bibr" target="#b42">Zhang et al. (2013a)</ref> gives the best performance on PKU by co-training on large-scale data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Error Analysis</head><p>To study the differences between word-based and character-based neural models, we conduct error analysis on the test dataset of CTB60. First, we examine the error distribution on individual sentences. <ref type="figure" target="#fig_11">Figure 7</ref> shows the F-measure val- ues of each test sentence by word-and character- based neural models, respectively, where the x- axis value denotes the F-measure value of the word-based neural model, and the y-axis value de- notes its performance of the character-based neu- ral model. We can see that the majority scat- ter points are off the diagonal line, demonstrating strong differences between the two models. This results from the differences in feature sources. Second, we study the F-measure distribution of the two neural models with respect to sen- tence lengths. We divide the test sentences into ten bins, with bin i denoting sentence lengths in [5 * (i − 1), 5 * i]. <ref type="figure">Figure 8</ref> shows the results. Ac- cording to the figure, we observe that word-based neural model is relatively weaker for sentences with length in <ref type="bibr">[5,</ref><ref type="bibr">10]</ref>, while can better tackle long sentences.</p><p>Third, we compare the two neural models by their capabilities of modeling words with different lengths. <ref type="figure" target="#fig_12">Figure 9</ref> shows the results. The perfor- mances are lower for words with lengths beyond 2, and the performance drops significantly for words with lengths over 3. Overall, the word-based neu- ral model achieves comparable performances with the character-based model, but gives significantly better performances for long words, in particular when the word length is over 3. This demonstrates the advantage of word-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Xue (2003) was the first to propose a character- tagging method to Chinese word segmentation, us- ing a maximum entropy model to assign B/I/E/S tags to each character in the input sentence sepa- rately. <ref type="bibr" target="#b21">Peng et al. (2004)</ref> showed that better results can be achieved by global learning using a CRF model. This method has been followed by most subsequent models in the literature ( <ref type="bibr" target="#b27">Tseng et al., 2005;</ref><ref type="bibr" target="#b46">Zhao, 2009;</ref><ref type="bibr" target="#b25">Sun et al., 2012</ref>). The most effective features have been character unigrams, bigrams and trigrams within a five-character win- dow, and a bigram tag window. Special characters such as alphabets, numbers and date/time charac- ters are also differentiated for extracting features. <ref type="bibr" target="#b47">Zheng et al. (2013)</ref> built a neural network seg- mentor, which essentially substitutes the manual discrete features of <ref type="bibr" target="#b21">Peng et al. (2004)</ref>, with dense real-valued features induced automatically from character embeddings, using a deep neural net- work structure <ref type="bibr" target="#b7">(Collobert et al., 2011)</ref>. A tag tran- sition matrix is used for inference, which makes the model effectively. Most subsequent work on neural segmentation followed this method, im- proving the extraction of emission features by us- ing more complex neural network structures. <ref type="bibr" target="#b18">Mansur et al. (2013)</ref> experimented with embed- dings of richer features, and in particular charac-ter bigrams. <ref type="bibr" target="#b20">Pei et al. (2014)</ref> used a tensor neu- ral network to achieve extensive feature combi- nations, capturing the interaction between charac- ters and tags. <ref type="bibr" target="#b4">Chen et al. (2015a)</ref> used a recur- sive network structure to the same end, extract- ing more combined features to model complicated character combinations in a five-character win- dow. <ref type="bibr" target="#b5">Chen et al. (2015b)</ref> used a LSTM model to capture long-range dependencies between charac- ters in a sentence. <ref type="bibr" target="#b34">Xu and Sun (2016)</ref> proposed a dependency-based gated recursive neural network to efficiently integrate local and long-distance fea- tures. The above methods are all character-based models, making no use of full word information. In contrast, we leverage both character embed- dings and word embeddings for better accuracies.  <ref type="bibr" target="#b16">Liu et al., 2016;</ref><ref type="bibr" target="#b3">Cai and Zhao, 2016)</ref>, which are different from our work in the basic frame- work. For instance, <ref type="bibr" target="#b16">Liu et al. (2016)</ref> follow An- drew (2006) using a semi-CRF for structured in- ference.</p><p>We followed the global learning and beam- search framework of <ref type="bibr" target="#b39">Zhang and Clark (2011)</ref> in building a word-based neural segmentor. The main difference between our model and that of <ref type="bibr" target="#b39">Zhang and Clark (2011)</ref> is that we use a neural network to induce feature combinations directly from character and word embeddings. In addi- tion, the use of a bi-directional LSTM allows us to leverage non-local information from the word se- quence, and look-ahead information from the in- coming character sequence. The automatic neu- ral features are complementary to the manual dis- crete features of <ref type="bibr" target="#b39">Zhang and Clark (2011)</ref>. We show that our model can accommodate the inte- gration of both types of features. This is similar in spirit to the work of Sun (2010) and <ref type="bibr" target="#b31">Wang et al. (2014)</ref>, who integrated features of character-based and word-based segmentors.</p><p>Transition-based framework with beam search has been widely exploited in a number of other NLP tasks, including syntactic parsing ( <ref type="bibr" target="#b40">Zhang and Nivre, 2011;</ref><ref type="bibr" target="#b49">Zhu et al., 2013)</ref>, information ex- traction ( <ref type="bibr" target="#b15">Li and Ji, 2014</ref>) and the work of joint models <ref type="bibr" target="#b43">(Zhang et al., 2013b;</ref><ref type="bibr" target="#b44">Zhang et al., 2014)</ref>. Recently, the effectiveness of neural features has been studied for this framework. In the natural language parsing community, it has achieved great success. Representative work includes <ref type="bibr" target="#b48">Zhou et al. (2015)</ref>, <ref type="bibr" target="#b33">Weiss et al. (2015)</ref>, <ref type="bibr" target="#b32">Watanabe and Sumita (2015)</ref> and <ref type="bibr" target="#b0">Andor et al. (2016)</ref>. In this work, we apply the transition-based neural framework to Chinese segmentation, in order to exploit word- level neural features such as word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed a word-based neural model for Chi- nese segmentation, which exploits not only char- acter embeddings as previous work does, but also word embeddings pre-trained from large scale corpus. The model achieved comparable per- formances compared with a discrete word-based baseline, and also the state-of-the-art character- based neural models in the literature. We fur- ther demonstrated that the model can utilize dis- crete features conveniently, resulting in a com- bined model that achieved top performances com- pared with previous work. Finally, we conducted several comparisons to study the differences be- tween our word-based model with character-based neural models, showing that they have different er- ror characteristics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Word segmentation methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scorer for the neural transition-based Chinese word segmentation model. We denote the last word in the buffer as w 0 , the next incoming character as c 0 in the queue in consistent with Figure 2, and the last applied action as a 0. configuration (s, q), we use three separate recurrent neural networks (RNN) to model the word sequence · · · w −1 w 0 , the character sequence · · · c −1 c 0 c 1 · · · , and the action sequence · · · a −1 a 0 , respectively, resulting in three dense real-valued vectors {r w , r c and r a }, respectively. All the three feature vectors are used scoring the SEP action. For APP, on the other hand, we use only the character and action features r c and r a because the last word w 0 in the buffer is a partial word. Formally, given r w , r c , r a , the action scores are computed by:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Input representations of LSTMS for r a (actions) r w (words) and r c (characters).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Accuracies against the training epoch using beam sizes 1, 2, 4, 8 and 16, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) and the curve of the neural model without and with fine tuning are shown in 5(b) and 5(c), respectively. The performance increases with a larger beam size in all settings. When the beam increases into 16, the gains levels out. The results of the discrete model and the neural model without fine-tuning are highly similar, showing the useful- ness of beam-search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Sentence accuracy comparisons for the discrete and the neural models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Sentence accuracy comparisons for word-and character-based neural models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: F-measure against word length, where the boxes with red dots denote the performances of word-based neural model, and the boxes with blue slant lines denote character-based neural model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>For</head><label></label><figDesc>word-based segmentation, Andrew (2006) used a semi-CRF model to integrate word fea- tures, Zhang and Clark (2007) used a percep- tron algorithm with inexact search, and Sun et al. (2009) used a discriminative latent variable model to make use of word features. Recently, there have been several neural-based models us- ing word-level embedding features (Morita et al., 2015;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics of datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : Hyper-parameter values in our model.</head><label>3</label><figDesc></figDesc><table>Grad (Duchi et al., 2011) to minimize the objec-
tive function for both the discrete and neural mod-
els. All the matrix and vector parameters are ini-
tialized by uniform sampling in (−0.01, 0.01). 

5 Experiments 

5.1 Experimental Settings 

Data. We use three datasets for evaluation, 
namely CTB6, PKU and MSR. The CTB6 corpus 
is taken from Chinese Treebank 6.0, and the PKU 
and MSR corpora can be obtained from Bake-
Off 2005 (Emerson, 2005). We follow Zhang et 
al. (2014), splitting the CTB6 corpus into train-
ing, development and testing sections. For the 
PKU and MSR corpora, only the training and test 
datasets are specified and we randomly split 10% 
of the training sections for development. Table 1 
shows the overall statistics of the three datasets. 
Embeddings. We use word2vec 4 to pre-train 
word, character and bi-character embeddings on 
Chinese Gigaword corpus (LDC2011T13). In or-
der to train full word embeddings, the corpus is 
segmented automatically by our baseline model. 
Hyper-parameters. The hyper-parameter values 
are tuned according to development performances. 
We list their final values in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Feature experiments. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>;</head><label></label><figDesc></figDesc><table>Wang and Manning, Models 

P 
R 
F 
word-based models 
discrete 
95.29 95.26 95.28 
neural 
95.34 94.69 95.01 
combined 
96.11 95.79 95.95 
character-based models 
discrete 
95.38 95.12 95.25 
neural 
94.59 94.92 94.76 
combined 
95.63 95.60 95.61 
other models 
Zhang et al. (2014) 
N/A 
N/A 
95.71 
Wang et al. (2011) 
95.83 95.75 95.79 
Zhang and Clark (2011) 95.46 94.78 95.13 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 : Main results on CTB60 test dataset.</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> The term in this paper is used to denote the neural network structure with convolutional layers, which is different from the typical convolution neural network that has a pooling layer upon convolutional layers (Krizhevsky et al., 2012).</note>

			<note place="foot" n="2"> https://github.com/SUTDNLP/NNTransitionSegmentor.</note>

			<note place="foot" n="4"> http://word2vec.googlecode.com/</note>

			<note place="foot" n="6"> In all our experiments, we fix the character unigram and bigram embeddings, because fine-tuning of these embeddings results in little changes.</note>

			<note place="foot" n="9"> Bi-directional LSTM is slightly better than a single leftright LSTM used in Chen et al. (2015b). 10 The results of Chen et al. (2015a) and Chen et al. (2015b) are not listed, because they take a preprocessing step by replacing Chinese idioms with a uniform symbol in their test data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers, Yijia Liu and Hai Zhao for their constructive comments, which help to improve the final paper. This work is sup-ported by National Natural Science Foundation of China (NSFC) under grant 61170148, Natu-ral Science Foundation of Heilongjiang Province (China) under grant No.F2016036, the Singapore Ministry of Education (MOE) AcRF Tier 2 grant T2MOE201301 and SRG ISTD 2012 038 from Singapore University of Technology and Design. Yue Zhang is the corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A hybrid markov/semi-markov conditional random field for sequence segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">Proceedings of the 2006 Conference on EMNLP</title>
		<meeting>the 2006 Conference on EMNLP<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural word segmentation learning for Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2016</title>
		<meeting>ACL 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gated recursive neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53nd ACL</title>
		<meeting>the 53nd ACL</meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="1744" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory neural networks for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 EMNLP</title>
		<meeting>the 2015 EMNLP</meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1197" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume</title>
		<meeting>the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural crf parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53nd ACL</title>
		<meeting>the 53nd ACL</meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="302" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53nd ACL</title>
		<meeting>the 53nd ACL</meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The second international chinese word segmentation bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Emerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Second SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="123" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Yann A Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><forename type="middle">B</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="9" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring segment representations for neural segmentation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate linear-time chinese word segmentation via embedding matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Hinrichs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53nd ACL</title>
		<meeting>the 53nd ACL</meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="1733" to="1743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature-based neural language model and chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mairgup</forename><surname>Mansur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Joint Conference on Natural Language Processing</title>
		<meeting>the Sixth International Joint Conference on Natural Language Processing<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1271" to="1277" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Morphological analysis for unsegmented languages using recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Morita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on EMNLP</title>
		<meeting>the 2015 Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2292" to="2297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maxmargin tensor neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd ACL</title>
		<meeting>the 52nd ACL<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Chinese segmentation and new word detection using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangfang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Coling</title>
		<meeting>Coling<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-08-23" />
			<biblScope unit="page" from="562" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A dual-layer crfs based joint decoding method for cascaded segmentation and labeling tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1707" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Enhancing chinese word segmentation using unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on EMNLP</title>
		<meeting>the 2011 Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="970" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A discriminative latent variable chinese segmenter with hybrid word/character information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaozhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2009</title>
		<meeting>NAACL 2009</meeting>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="56" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th ACL</title>
		<meeting>the 50th ACL</meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Word-based and character-based word segmentation models: Comparison and combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2010: Posters</title>
		<imprint>
			<date type="published" when="2010-08" />
			<biblScope unit="page" from="1211" to="1219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A conditional random field word segmenter for sighan bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihsin</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth SIGHAN workshop</title>
		<meeting>the fourth SIGHAN workshop</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="168" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Effect of non-linear deep architecture in sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Joint Conference on Natural Language Processing</title>
		<meeting>the Sixth International Joint Conference on Natural Language Processing<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1285" to="1291" />
		</imprint>
	</monogr>
	<note>October. Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving chinese word segmentation and pos tagging with semi-supervised methods using large auto-analyzed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Jun&amp;apos;ichi Kazama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th IJCNLP</title>
		<meeting>5th IJCNLP<address><addrLine>Chiang Mai, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="309" to="317" />
		</imprint>
	</monogr>
	<note>Yujie Zhang, and Kentaro Torisawa</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Two knives cut better than one: Chinese word segmentation with dual decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Voigt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd ACL</title>
		<meeting>the 52nd ACL<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="193" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transitionbased neural constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd ACL</title>
		<meeting>the 53rd ACL</meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="1169" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd ACL</title>
		<meeting>the 53rd ACL</meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="323" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dependency-based gated recursive neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Chinese word segmentation as character tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Linguistics and Chinese Language Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence neural net models for grapheme-to-phoneme conversion</title>
		<idno type="arXiv">arXiv:1506.00196</idno>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Chinese segmentation with a word-based perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th ACL</title>
		<meeting>the 45th ACL<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="840" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Syntactic processing using the generalized perceptron and beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="151" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th ACL</title>
		<meeting>the 49th ACL</meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Combining discrete and continuous features for deterministic transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 EMNLP</title>
		<meeting>the 2015 EMNLP</meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1316" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exploring representations from unlabeled data with co-training for Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mairgup</forename><surname>Mansur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP 2013</title>
		<meeting>the EMNLP 2013<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="311" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Chinese parsing exploiting characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st ACL</title>
		<meeting>the 51st ACL</meeting>
		<imprint>
			<publisher>August</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Character-level chinese dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd ACL</title>
		<meeting>the 52nd ACL<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1326" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Effective tag set selection in chinese word segmentation via conditional random field modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ning</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PACLIC</title>
		<meeting>PACLIC</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Character-level dependencies in chinese: Usefulness and learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL</title>
		<meeting>the EACL<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-03" />
			<biblScope unit="page" from="879" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep learning for Chinese word segmentation and POS tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on EMNLP</title>
		<meeting>the 2013 Conference on EMNLP<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A neural probabilistic structuredprediction model for transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd ACL</title>
		<meeting>the 53rd ACL</meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="1213" to="1222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fast and accurate shiftreduce constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st ACL</title>
		<meeting>the 51st ACL</meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="434" to="443" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
