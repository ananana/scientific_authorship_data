<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Encoding Distributional Semantics into Triple-Based Knowledge Ranking for Document Enrichment</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Encoding Distributional Semantics into Triple-Based Knowledge Ranking for Document Enrichment</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="524" to="533"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Document enrichment focuses on retrieving relevant knowledge from external resources , which is essential because text is generally replete with gaps. Since conventional work primarily relies on special resources , we instead use triples of Subject, Predicate, Object as knowledge and incorporate distributional semantics to rank them. Our model first extracts these triples automatically from raw text and converts them into real-valued vectors based on the word semantics captured by Latent Dirich-let Allocation. We then represent these triples, together with the source document that is to be enriched, as a graph of triples, and adopt a global iterative algorithm to propagate relevance weight from source document to these triples so as to select the most relevant ones. Evaluated as a ranking problem, our model significantly out-performs multiple strong baselines. Moreover , we conduct a task-based evaluation by incorporating these triples as additional features into document classification and enhances the performance by 3.02%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Document enrichment is the task of acquiring rel- evant background knowledge from external re- sources for a given document. This task is essen- tial because, during the writing of text, some ba- sic but well-known information is usually omitted by the author to make the document more concise. For example, Baghdad is the capital of Iraq is omitted in <ref type="figure">Figure 1a</ref>. A human will fill these gaps automatically with the background knowledge in his mind. However, the machine lacks both the necessary background knowledge and the ability to select. The task of document enrichment is pro- posed to tackle this problem, and has been proved helpful in many NLP tasks such as web search ( <ref type="bibr" target="#b18">Pantel and Fuxman, 2011)</ref>, coreference resolu- tion ( <ref type="bibr" target="#b2">Bryl et al., 2010)</ref>, document cluster ( <ref type="bibr" target="#b11">Hu et al., 2009</ref>) and entity disambiguation <ref type="bibr" target="#b24">(Sen, 2012)</ref>.</p><p>We can classify previous work into two classes according to the resources they rely on. The first line of work uses Wikipedia, the largest on-line en- cyclopedia, as a resource and introduces the con- tent of Wikipedia pages as external knowledge <ref type="bibr" target="#b4">(Cucerzan, 2007;</ref><ref type="bibr" target="#b12">Kataria et al., 2011;</ref><ref type="bibr" target="#b8">He et al., 2013</ref>). Most research in this area relies on the text similarity ( <ref type="bibr" target="#b29">Zheng et al., 2010;</ref><ref type="bibr" target="#b9">Hoffart et al., 2011</ref>) and structure information ( <ref type="bibr" target="#b13">Kulkarni et al., 2009;</ref><ref type="bibr" target="#b24">Sen, 2012;</ref><ref type="bibr" target="#b8">He et al., 2013</ref>) between the mention and the Wikipedia page. Despite the apparent suc- cess of these methods, most Wikipedia pages con- tain too much information, most of which is not relevant enough to the source document, and this causes a noise problem. Another line of work tries to improve the accuracy by introducing ontolo- gies <ref type="bibr" target="#b6">(Fodeh et al., 2011;</ref><ref type="bibr" target="#b14">Kumar and Salim, 2012)</ref> and structured knowledge bases such as WordNet ( <ref type="bibr" target="#b17">Nastase et al., 2010)</ref>, which provide semantic in- formation about words such as synonym ) and antonym <ref type="bibr" target="#b22">(Sansonnet and Bouchet, 2010)</ref>. However, these methods primarily rely on special resources constructed with supervision or even manually, which are difficult to expand and in turn limit their applications in practice.</p><p>In contrast, we wish to seek the benefits of both coverage and accuracy from a better representa- tion of background knowledge: triples of Subject, Predicate, Object (SPO). According to <ref type="bibr" target="#b10">Hoffart et al. (2013)</ref>, these triples, such as LeonardCohen, wasBornIn, Montreal, can be extracted automat- ically from Wikipedia and other sources, which is compatible with the RDF data model <ref type="bibr" target="#b25">(Staab and Studer, 2009)</ref>. Moreover, by extracting these Global Ranking lobal anking S 1 : The coalition may never know if Iraqi president Saddam Hussein survived a U.S. air strike yesterday. S 2 : A B-1 bomber dropped four 2,000-pound bombs on a building in a residential area of Baghdad . </p><formula xml:id="formula_0">k 2 :</formula><p>(a) Source document: air strike aiming at Saddam in Baghdad</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Ranking</head><p>Global Ranking S 1 : The coalition may never know if Iraqi president Saddam Hussein survived a U.S. air strike yesterday.</p><p>S 2 : A B-1 bomber dropped four 2,000-pound bombs on a building in a residential area of Baghdad . </p><formula xml:id="formula_1">k 2 :</formula><p>(b) Two omitted relevant pieces of background knowledge <ref type="figure">Figure 1</ref>: An example of document enrichment: A source document about a U.S. air strike omit- ting two important pieces of background knowl- edge which are acquired by our framework.</p><p>triples from multiple sources, we also get better coverage. Therefore, one can expect that this rep- resentation is helpful for better document enrich- ment by incorporating both accuracy and cover- age. In fact, there is already evidence that this representation is helpful. <ref type="bibr" target="#b28">Zhang et al. (2014)</ref> pro- posed a triple-based document enrichment frame- work which uses triples of SPO as background knowledge. They first proposed a search engine- based method to evaluate the relatedness between every pair of triples, and then an iterative propa- gation algorithm was introduced to select the most relevant triples to a given source document (see Section 2), which achieved a good performance. However, to evaluate the semantic relatedness between two triples, <ref type="bibr" target="#b28">Zhang et al. (2014)</ref> primar- ily relied on the text of triples and used search engines, which makes their method difficult to re-implement and in turn limits its application in practice. Moreover, they did not carry out any task-based evaluation, which makes it uncertain whether their method will be helpful in real appli- cations. Therefore, we instead use topic models, especially Latent Dirichlet Allocation (LDA), to encode distributional semantics of words and con- vert every triple into a real-valued vector, which is then used to evaluate the relatedness between a pair of triples. We then incorporate these triples into the given source document and represent them together as a graph of triples. Then a modified it- erative propagation is carried out over the entire graph to select the most relevant triples of back- ground knowledge to the given source document.</p><p>To evaluate our model, we conduct two series of experiments: (1) evaluation as a ranking problem, and (2) task-based evaluation. We first treat this task as a ranking problem which inputs one doc- ument and outputs the top N most-relevant triples of background knowledge. Second, we carry out a task-based evaluation by incorporating these rele- vant triples acquired by our model into the origi- nal model of document classification as additional features. We then perform a direct comparison be- tween the classification models with and without these triples, to determine whether they are help- ful or not. On the first series of experiments, we achieve a MAP of 0.6494 and a P@N of 0.5597 in the best situation, which outperforms the strongest baseline by 5.87% and 17.21%. In the task-based evaluation, the enriched model derived from the triples of background knowledge performs better by 3.02%, which demonstrates the effectiveness of our framework in real NLP applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The most closely related work in this area is our own ( <ref type="bibr" target="#b28">Zhang et al., 2014)</ref>, which used the triples of SPO as background knowledge. In that work, we first proposed a triple graph to represent the source document and then used a search engine- based iterative algorithm to rank all the triples. We describe this work in detail below.</p><p>Triple graph <ref type="bibr" target="#b28">Zhang et al. (2014)</ref> proposed the triple graph as a document representation, where the triples of SPO serve as nodes, and the edges between nodes indicate their semantic relatedness. There are two kinds of nodes in the triple graph:</p><p>(1) source document nodes (sd-nodes), which are triples extracted from source documents, and (2) background knowledge nodes (bk-nodes), which are triples extracted from external sources. Both of them are extracted automatically with Reverb, a well-known Open Information Extraction system (Etzioni et al., 2011). There are also two kinds of edges: (1) an edge between a pair of sd-nodes, and (2) an edge between one sd-node and another bk-node, both of which are unidirectional. In the original representation, there are no edges between two bk-nodes because they treat the bk-nodes as recipients of relevance weight only. In this paper, we modify this setup and connect every pair of bk- nodes with an edge, so the bk-nodes serve as in- termediate nodes during the iterative propagation process and contribute to the final performance too as shown in our experiments (see Section 5.1).</p><p>Relevance evaluation To compute the weight of a edge, <ref type="bibr" target="#b28">Zhang et al. (2014)</ref> evaluate the seman- tic relatedness between two nodes with a search engine-based method. They first convert every node, which is a triple of SPO, into a query by combining the text of Subject and Object together. Then for every pair of nodes t i and t j , they con- struct three queries: p, q, and p ∩ q, which corre- spond to the queries of t i , t j , and t j ∩ t j , the com- bination of t i and t j . All these queries are put into a search engine to get H(p), H(q), and H(p ∩ q), the numbers of returned pages for query p, p, and p ∩ q. Then the WebJaccard Coefficient ( <ref type="bibr" target="#b1">Bollegala et al., 2007</ref>) is used to evaluate r(i, j), the related- ness between t i and t j , according to Formula 1.</p><formula xml:id="formula_2">r(i, j) = WebJaccard(p, q) =    0 if H(p ∩ q) ≤ C H(p ∩ q) H(p) + H(q) − H(p ∩ q)</formula><p>otherwise.</p><p>(1)</p><p>Using r(i, j), <ref type="bibr" target="#b28">Zhang et al. (2014)</ref> further define p(i, j), the probability of t i and t j propagating to each other, as shown in Formula 2. Here N is the set of all nodes, and δ (i, j) denotes whether an edge exists between two nodes or not.</p><formula xml:id="formula_3">p(i, j) = r(i, j) × δ (i, j) ∑ n∈N r(n, j) × δ (n, j)<label>(2)</label></formula><p>Iterative propagation Considering that the source document D is represented as a graph of sd-nodes, so the relevance of background knowl- edge t b to D is naturally converted into that of t b to the graph of sd-nodes. <ref type="bibr" target="#b28">Zhang et al. (2014)</ref> evalu- ate this relevance by propagating relevance weight from sd-nodes to t b iteratively. After convergence, the relevance weight of t b will be treated as the fi- nal relevance to D. There are in total n × n pairs of nodes, and their p(i, j) are stored in a matrix P. <ref type="bibr" target="#b28">Zhang et al. (2014)</ref> use W = (w 1 , w 2 , . . . , w n ) to de- note the relevance weights of nodes, where w i in- dicates the relevance of t i to D. At the beginning, each w i of bk-nodes is initialized to 0, and each that of sd-nodes is initialized to its importance to D. Then W is updated to W after every iteration according to Formula 3. They keep updating the weights of both sd-nodes and bk-nodes until con- vergence and do not distinguish them explicitly.</p><formula xml:id="formula_4">W = W × P = W ×     p(1, 1) p(1, 2) . . . p(1, n) p(2, 1) p(2, 2) . . . p(2, n) . . . . . . . . . . . . p(n, 1) p(n, 2) . . . p(n, n)     (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>The key idea behind this work is that every doc- ument is composed of several units of informa- tion, which can be extracted into triples automat- ically. For every unit of background knowledge b, the more units that are relevant to b and the more relevant they are, the more relevant b will be to the source document. Based on this intu- ition, we first present both source document infor- mation and background knowledge together as a document-level triple graph as illustrated in Sec- tion 2. Then we use LDA to capture the distribu- tional semantics of a triple by representing it as a vector of distributional probabilities over k topics and evaluate the relatedness between two triples with cosine-similarity. Finally, we propose a mod- ified iterative process to propagate the relevance score from the source document information to the background knowledge and select the top n rele- vant ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoding distributional semantics</head><p>LDA LDA is a popular generative probabilistic model, which was first introduced by <ref type="bibr" target="#b0">Blei et al. (2003)</ref>. LDA views every document as a mixture over underlying topics, and each topic as a distri- bution over words. Both the document-topic and the topic-word distributions are assumed to have a Dirichlet prior. Given a set of documents and a number of topics, the model returns θ d , the topic distribution for each document d, and φ z , the word distribution for every topic z. LDA assumes the following generative process for each document in a corpus D:</p><formula xml:id="formula_5">1. Choose N ∼ Poisson(ξ ). 2. Choose θ ∼ Dir(α).</formula><p>(a) Choose a topic z n ∼ Multinomial(θ ).</p><p>(b) Choose a word w n from p(w n |z n , β ) con- ditioned on the topic z n .</p><p>Here the dimensionality k of the Dirichlet distribu- tion (and thus the dimensionality of the topic vari- able z) is assumed to be known and fixed; θ is a k- dimensional Dirichlet random variable, where the parameter α is a k-vector with components α i &gt; 0; and the β indicates the word probabilities over topics, which is a matrix with β i j = p(w j = 1|z i = 1). <ref type="figure" target="#fig_1">Figure 2</ref> shows the representation of LDA as a probabilistic graphical model with three levels. There are two corpus-level parameters α and β , which are assumed to be sampled once in the pro- cess of generating a corpus; one document-level variable θ d , which is sampled once per document; and two word-level variables z dn and w dn , which are sampled once for each word in each document. We employ the publicly available implementa- tion of LDA, JGibbLDA2 1 ( <ref type="bibr" target="#b19">Phan et al., 2008)</ref>, which has two main execution methods: param- eter estimation (model building) and inference for new data (classification of a new document).</p><p>Relevance evaluation Given a set of documents and the number of topics k, LDA will return φ z , the word distribution over the topic z. So for every word w n , we get k distributional probabilities over k topics. We use p w n z i to denote the probability that w n appears in the i th topic z i , where i ≤ k, z i ∈ Z, the set of k topics. Then we combine these k possibilities together as a real-valued vector v w n to represent w n as shown in Formula 4.</p><formula xml:id="formula_6">v w n = (p w n z 1 , p w n z 2 , . . . , p w n z k )<label>(4)</label></formula><p>After getting the vectors of words, we employ an intuitive method to compute the vector of a triple t, by accumulating all the corresponding vectors of words appearing in t according to For- mula 5. Considering that the elements of this newly generated vector indicate the distributional probabilities of t over k topics, we then normalize it according to Formula 6 so that its elements sum to 1. This gives us v t , the real-valued vector of triple t, which captures its distributional probabil- ities over k topics. Here t corresponds to a triple of background knowledge or of source document, p tz i indicates the possibility of t to appear in the i th topic z i , and w n ∈ t means that w n appears in t.</p><formula xml:id="formula_7">p tz i = ∑ w n ∈t p w n z i (5) v t = (p tz 1 , p tz 2 , . . . , p tz k ) ∑ k i=1 p tz i<label>(6)</label></formula><p>Using the vectors of triples, we can easily com- pute the semantic relatedness between a pair of triples as their cosine-similarity according to For- mula 7. Here A, B correspond to the real-valued vectors of two triples, r(A, B) denotes their se- mantic relatedness, and k is the number of topics, which is also the length of A (or B). A high value of r(A, B) usually indicates a close relatedness be- tween A and B, and thus a higher probability of propagating to each other in the following modi- fied iterative propagation illustrated in Section 3.2.</p><formula xml:id="formula_8">r(A, B) = cos(A, B) = AB AB = ∑ k i=1 A i B i ∑ k i=1 (A i ) 2 ∑ k i=1 (B i ) 2<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modified iterative propagation</head><p>In this part, we propose a modified iterative prop- agation based ranking model to select the most- relevant triples of background knowledge. There are three primary modifications to the original model of <ref type="bibr" target="#b28">Zhang et al. (2014)</ref>, all of which are shown more powerful in our experiments. First of all, the original model ( <ref type="bibr" target="#b28">Zhang et al., 2014</ref>) does not reset the relevance weight of sd- nodes after every iteration. This results in a contin- ued decrease of the relevance weight of sd-nodes, which weakens the effect of sd-nodes during the iterative propagation and in turn affects the fi- nal performance. To tackle this problem, we de- crease the relevance weight of bk-nodes and in- crease that of sd-nodes according to a fixed ratio after every iteration, so as to ensure that the to- tal weight of sd-nodes is always higher than that of bk-nodes. Note that although the relevance weights of bk-nodes are changed after the redis- tribution, the corresponding ranking of them is not changed because the redistribution is carried out  over all nodes accordingly. In our experiments, we tried different ratios and finally chose 10:1, with sd-nodes corresponding to 10 and bk-nodes to 1, which achieved the best performance. In addition, we also modify the triple graph, the representation of a document illustrated in Section 2, by connecting every pair of bk-nodes with an edge, which is not allowed in the original model. This modification was motivated by the intuition that the relatedness between bk-nodes also con- tributes to the better evaluation of relevance to the source document, because the bk-nodes can serve as the intermediate nodes during the iterative prop- agation over the entire graph. <ref type="figure" target="#fig_0">Figure 3</ref> shows an example, where the bk-node John Lennon is close to both the sd-node Beatles and to another bk- node Yoko Ono, so the relatedness between two bk-nodes John Lennon and Yoko Ono helps in bet- ter evaluation of the relatedness between the bk- node Yoko Ono and the sd-node Beatles.</p><p>We also modify the definition of p(i, j), the probability of two nodes t i and t j propagating to each other. <ref type="bibr" target="#b28">Zhang et al. (2014)</ref> compute this prob- ability according to Formula 2, which highlights the number of neighbors, but weakens the related- ness between nodes, due to the normalization. For instance, if a node t x has only one neighbor t y , no matter how low their relatedness is, their p(x, y) will still be equal to 1 in the original model, while another node with two equally but closely related neighbors will only get a probability of 0.5 for each neighbor. We modify this setup by removing the normalization process and computing p(i, j) as the relatedness between t i and t j directly, which is evaluated according to Formula 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Encoding background knowledge into document classification</head><p>In this part, we demonstrate that the introduction of relevant knowledge could be helpful to real NLP applications. In particular, we choose the document classification task as a demonstration, which aims to classify documents into predefined categories automatically <ref type="bibr" target="#b23">(Sebastiani, 2002</ref>). We choose this task for two reasons: (1) This task has witnessed a booming interest in the last 20 years, due to the increased availability of docu- ments in digital form and the ensuing need to orga- nize them, so it is important in both research and application. <ref type="formula" target="#formula_3">(2)</ref> The state-of-the-art performance of this task is achieved by a series of topic model- based methods, which rely on the same model as we do, but make use of source document informa- tion only. However, there is always some omitted information and relevant knowledge, which can- not be captured from the source document. In- tuitively, the recovery of this information will be helpful. If we can improve the performance by in- troducing extra background knowledge into exist- ing framework of document classification, we can inference naturally that the improvement benefits from the introduction of this knowledge.</p><p>Traditional methods primarily use topic models to represent a document as a topic vector. Then a SVM classifier takes this vector as input and out- puts the class of the document. In this work, we propose a new framework for document classifica- tion to incorporate extra knowledge. Given a doc- ument to be classified, we select the top N most- relevant triples of background knowledge with our model introduced in Section 3, all of which are represented as vectors of v t = (p tz 1 , p tz 2 , . . . , p tz k ). Then we combine these N triples as a new vec- tor v t , which is then incorporated into the original framework of document classification. Another SVM classifier takes v t , together with the original features extracted from the source document, as input and outputs the category of the source doc- ument. To combine N triples as one, we employ an intuitive method by computing the average of N corresponding vectors in every dimension.</p><p>One possible problem is how to decide N, the number of triples to be introduced. We first intro- duce a fixed amount of triples for every document. Moreover, we also select the triples according to their relevance weight to the source document (see Section 3.2) by setting a threshold of relevance weight first and selecting the triples whose weights are higher than the threshold. We further discuss the impact of different thresholds in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To evaluate our model, we conduct two series of experiments: (1) We first treat this task as a ranking problem, which takes a document as in- put and outputs the ranked triples of background knowledge, and evaluate the ranking performance by computing the scores of MAP and P@N. (2) We also conduct a task-based evaluation, where document classification (see Section 4) is chosen as a demonstration, by enriching the background knowledge to the original framework as additional features and performing a direct comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation as a ranking problem</head><p>Data preparation The data is composed of two parts: source documents and background knowl- edge. For source documents, we use a publicly available Chinese corpus which consists of 17,199 documents and 13,719,428 tokens extracted from Internet news 2 including 9 topics: Finance, IT, Health, Sports, Travel, Education, Jobs, Art, Mil- itary. We then randomly but equally select 600 articles as the set of source documents from 9 top- ics without data bias. We use all the other 16,599 documents of the same corpus as the source of background knowledge, and then introduce a well- known Chinese open source tool ( <ref type="bibr" target="#b3">Che et al., 2010)</ref> to extract the triples of background knowledge from the raw text automatically. So the back- ground knowledge also distributes evenly across the same 9 topics. We use the same tool to extract the triples of source documents too.</p><p>Baseline systems As <ref type="bibr" target="#b28">Zhang et al. (2014)</ref> argued, it is difficult to use the methods in traditional ranking tasks, such as information retrieval <ref type="bibr" target="#b15">(Manning et al., 2008</ref>) and entity linking <ref type="bibr" target="#b7">(Han et al., 2011;</ref><ref type="bibr" target="#b24">Sen, 2012)</ref>, as baselines in this task, because our model takes triples as basic input and thus lacks some crucial information such as link struc- ture. For better comparison, we implement three methods as baselines, which have been proved ef- fective in relevance evaluation: (1) Vector Space Model (VSM), (2) Word Embedding (WE), and (3) Latent Dirichlet Allocation (LDA). Note that our model captures the distributional semantics of triples with LDA, while WE serves as a baseline only, where the word embeddings are acquired over the same corpus mentioned previously with <ref type="bibr">2</ref> http://www.sogou.com/labs/dl/c.html the publicly available tool word2vec 3 .</p><p>Here we use t i , D, and w i to denote a triple of background knowledge, a source document, and the relevance of t i to D. For VSM, we represent both t i and D with a tf-idf scheme first <ref type="bibr" target="#b21">(Salton and McGill, 1986)</ref> and compute w i as their cosine- similarity. For WE, we first convert both t i and the triples extracted from D into real-valued vectors with WE and then compute w i by accumulating all the cosine-similarities between t i and every triple from D. For LDA, we represent t i as a vector with our model introduced in Section 3.1 and get the vector of D directly with LDA. Then we evaluate their relevance of t i to D by computing the cosine- similarity of two corresponding vectors.</p><p>Moreover, to determine whether our modified iterative propagation is helpful or not, we also compare our full model (Ours) against a simpli- fied version without iterative propagation (Ours- S). In Ours-S, we represent both t i and the triples extracted from D as real-valued vectors with our model introduced in Section 3.1. Then we com- pute w i by accumulating all the cosine-similarities between t i and the triples extracted from D. For all the baselines, we rank the triples of background knowledge according to w i , their relevance to D.</p><p>Experimental setup Previous research relies on manual annotation to evaluate the ranking perfor- mance ( <ref type="bibr" target="#b28">Zhang et al., 2014</ref>), which costs a lot, and in which it is difficult to get high consistency. In this paper, we carry out an automatic evalua- tion. The corpus we used consists of 9 different classes, from which we extract triples of back- ground knowledge. So correspondingly, there will be 9 sets of triples too. Then we randomly select 200 triples from every class and mix 200 × 9 = 1800 triples together as S, the set of triples of background knowledge. For every document D to be enriched, our model selects the top N most- relevant triples from S and returns them to D as enrichments. We treat a triple t i selected by our model as positive only if t i is extracted from the same class as D. We evaluate the performance of our model with two well-known criteria in ranking problem: MAP and P@N ( <ref type="bibr" target="#b27">Voorhees et al., 2005</ref>). Statistically significant differences of performance are determined using the two-tailed paired t-test computed at a 95% confidence level based on the average performance per source document.  <ref type="table">Table 1</ref>: The performance evaluated as a ranking task. Here Ours corresponds to our full model, while Ours-S is a simplified version of our model without iterative propagation (see Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model MAP 5 P@5 MAP 10 P@10</head><p>Results The performance of multiple models is shown in <ref type="table">Table 1</ref>. Overall, our full model Ours outperforms all the baseline systems significantly in every metric. When evaluating the top 10 triples with the highest relevance weight, our framework outperforms the best baseline LDA by 4.4% in MAP and by 3.91% in P@N. When evaluating the top 5 triples, our framework performs even better and significantly outperforms the best baseline by 5.87% in MAP and by 17.</p><formula xml:id="formula_9">21% in P@N.</formula><p>To analyze the results further, Ours-S, the sim- plified version of our model without iterative propagation, outperforms two strong baselines VSM and WE, which indicates the effectiveness of encoding distributional semantics. However, the performance of this simplified model is not as good as that of LDA, because Ours-S evaluates the relevance with simple accumulation, which fails to capture the relatedness between multiple triples from the source document. We tackle this prob- lem by incorporating the modified iterative propa- gation over the entire triple graph into Ours, which achieves the best performance. One possible prob- lem is why WE has a poor performance, the reason of which lies in the setup of our evaluation, where we label positive and negative instances according to the class information of triples and documents. This is better fit for topic model-based methods.</p><p>Discussion We further analyze the impact of the three modifications we made to the original model (see Section 3.2). We first focus on the impact of decreasing the relevance weight of bk-nodes and increasing that of sd-nodes after every itera- tion. As mentioned previously, we change their relevance weight according to a fixed ratio, which is important to the performance. <ref type="figure">Figure 4</ref> shows the performance of models with different ratios. With any increase of the ratio, our model improves its performance in every metric, which shows the   <ref type="figure">Figure 4</ref>: The performance of our model with dif- ferent ratios between sd-nodes and bk-nodes.</p><p>effectiveness of this setup. The performance re- mains stable from the value of 10:1, which is thus chosen as the final value in our experiments. We then turn to the other two modifications about the edges between bk-nodes and the setup of propaga- tion probability. <ref type="table">Table 2</ref> shows the performance of our full model and the simplified models without these two modifications. With the edges between bk-nodes, our model improves the performance by 1.48% in MAP 5 and by 1.82% in P@5. With the modified iterative propagation, we achieve a even greater improvement of 13.99% in MAP 5 and 24.27% in P@5. All these improvements are sta- tistically significant, which indicates the effective- ness of these modifications to the original model.  <ref type="table">Table 2</ref>: The performance of our full model (Full) and two simplified models without modifications:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>(1) without edges between bk-nodes (Full−bb), (2) without the newly proposed definition of prop- agation probability between nodes (Full−p).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Task-based evaluation</head><p>Data preparation To carry out the task-based evaluation, we use the same Chinese corpus as that in previous experiments, which consists of 17,199 documents extracted from Internet news in 9 top- ics. We also use the same tool ( <ref type="bibr" target="#b3">Che et al., 2010)</ref>   <ref type="table" target="#tab_0">Table 3</ref>: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours) and without (others) background knowledge.</p><p>without background knowledge to evaluate the im- pact of introducing background knowledge.</p><p>Baseline systems We first illustrate two base- lines without background knowledge based on VSM and LDA. For VSM, the test document D is represented as a bag of words, where the word distribution over candidate topics is trained on the same corpus mentioned previously. Then we evaluate the similarity between D and a candi- date topic with cosine-similarity directly, where the topic with the highest similarity will be chosen as the final class. We use two setups: (1) VSM- one-hot represents a word as 1 if it appears in a document or topic, or 0 if not. (2) VSM-tf-idf rep- resents a word as the value of tf-idf. For LDA, we re-implement the state-of-the-art system as an- other baseline, which represents D as a topic vec- tor v d in the parameter estimation step, and then introduces a SVM classifier to take v d as input and decide the final class in the inference step. We also evaluate the impact of knowledge qual- ity by proposing two different models to introduce background knowledge: our full model introduced in Section 3 (Ours), and a simplified version of our model without iterative propagation (Ours-S). They have different performances on introducing background knowledge as shown in previous ex- periments (see Section 5.1). We then conduct a di- rect comparison between the document classifica- tion models with these conditions, whose differing performances demonstrates the impact of different qualities of background knowledge on this task. <ref type="table" target="#tab_0">Table 3</ref> shows the results. We use P, R, F to evaluate the performance, which are computed as the micro-average over 9 topics. Both models with background knowledge (LDA+SVM+Ours- S, LDA+SVM+Ours) outperform systems without knowledge, which shows that the introduction of background knowledge helps in better classifica-   <ref type="figure">Figure 5</ref>: The performance of document classifica- tion models with different thresholds. The knowl- edge whose relevance weight to the source docu- ment exceeds the threshold will be introduced as background knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>tion of documents. The system with the simpli- fied version of our model without iterative prop- agation (LDA+SVM+Ours-S) achieves a F-value of 0.8501, which outperforms the other baselines without knowledge too. Moreover, the system with our full model (LDA+SVM+Ours) achieves the best performance, a F-value of 0.8691, and outperforms the best baseline LDA+SVM signif- icantly. This shows that introducing better qual- ity of background knowledge is helpful to the bet- ter classification of documents. Statistical signif- icance is also verified using the two-tailed paired t-test computed at a 95% confidence level based on the results of classification over the test set.</p><p>Discussion One important question here is how much background knowledge to include. As men- tioned in Section 4, we have tried two different solutions: (1) introducing a fixed amount of back- ground knowledge for every document, and (2) setting a threshold and selecting knowledge whose relevance weight exceeds the threshold. The re- sults are shown in <ref type="table">Table 4</ref>, where the systems with threshold outperform that with fixed amount, which shows that the threshold helps in better in- troduction of background knowledge.  <ref type="table">Table 4</ref>: The performance of document classifica- tion with the full model (Ours) and the simplified model (Ours-S) to introduce knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>We also evaluate the impact of different thresh- olds as shown in <ref type="figure">Figure 5</ref>. The performance keeps improving as the threshold increases up to 6.4 and becomes steady from 6.4 to 6.7, while it begins to decline sharply from 6.7. This is reasonable be- cause at the beginning, as the threshold increases, we recall more background knowledge and pro- vide more information. However, with the further increase of the threshold, we introduce more noise, which decreases the performance. In our experi- ments, we choose 6.4 as the final threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>This study encodes distributional semantics into the triple-based background knowledge ranking model ( <ref type="bibr" target="#b28">Zhang et al., 2014</ref>) for better document enrichment. We first use LDA to represent ev- ery triple as a real-valued vector, which is used to evaluate the relatedness between triples, and then propose a modified iterative propagation model to rank all the triples of background knowledge. For evaluation, we conduct two series of experiments: (1) evaluation as ranking problem, and (2) task- based evaluation, especially for document classifi- cation. In the first set of experiments, our model outperforms multiple strong baselines based on VSM, LDA, and WE. In the second set of exper- iments, our full model with background knowl- edge outperforms the state-of-the-art systems sig- nificantly. Moreover, we also explore the impact of knowledge quality and show its importance.</p><p>In our future work, we wish to explore a better way to encode distributional semantics by propos- ing a modified LDA for better triples representa- tion. In addition, we also want to explore the ef- fect of introducing background knowledge in con- junction with other NLP tasks, especially with dis- course parsing <ref type="bibr" target="#b16">(Marcu, 2000;</ref><ref type="bibr" target="#b20">Pitler et al., 2009</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>S 3 :</head><label>3</label><figDesc>They had received intelligence reports that senior officials were meeting there, possibly including Saddam Hussein and his sons . Iraq Baghdad Saddam Hussein Capital hasChild Qusay Hussein k 1 :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graphical representation of LDA. The boxes represents replicates, where the inner box represents the repeated choice of N topics and words within a document, while the outer one represents the repeated generation of M documents.</figDesc><graphic url="image-1.png" coords="4,90.53,63.21,181.25,80.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The edge between two bk-nodes helps in the better evaluation of relatedness between the bk-node Yoko Ono and the sd-node Beatles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>S 3 : They had received intelligence reports that senior officials were meeting there, possibly including Saddam Hussein and his sons .</head><label>3</label><figDesc></figDesc><table>Iraq 

Baghdad 

Saddam Hussein 

Capital 

hasChild 
Qusay Hussein 

k 1 : 

</table></figure>

			<note place="foot">* This work was partly done while the first author was visiting University of Toronto.</note>

			<note place="foot" n="1"> http://jgibblda.sourceforge.net/</note>

			<note place="foot" n="3"> https://code.google.com/p/word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank our colleagues for their great help. This work was partly sup-ported by National Natural Science Foundation of China via grant 61133012, the National 863 Leading Technology Research Project via grant 2015AA015407, and the National Natural Science Foundation of China Surface Project via grant 61273321.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Measuring semantic similarity between words using web search engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web</title>
		<meeting>the 16th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="757" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using background knowledge to support coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volha</forename><surname>Bryl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Giuliano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Serafini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kateryna</forename><surname>Tymoshenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on ECAI 2010: 19th European Conference on Artificial Intelligence</title>
		<meeting>the 2010 Conference on ECAI 2010: 19th European Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="759" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ltp: A Chinese language technology platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale named entity disambiguation based on Wikipedia data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silviu Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="708" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Open information extraction: The second generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janara</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename><surname>Mausam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second international joint conference on Artificial IntelligenceVolume Volume One</title>
		<meeting>the Twenty-Second international joint conference on Artificial IntelligenceVolume Volume One</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On ontology-driven document clustering using core semantic features. Knowledge and Information Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samah</forename><surname>Fodeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Punch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang-Ning</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="395" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Collective entity linking in web text: a graph-based method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="765" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning entity representation for entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Amir</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilyana</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="782" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Yago2: a spatially and temporally enhanced knowledge base from Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="28" to="61" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploiting Wikipedia as external knowledge for document clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caimei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eun</forename><forename type="middle">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 15th International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="389" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Entity disambiguation with hierarchical topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saurabh S Kataria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sengamedu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 17th International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1037" to="1045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Collective annotation of Wikipedia entities in web text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayali</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 15th International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic multi document summarization approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaya</forename><surname>Yogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomie</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The rhetorical parsing of unrestricted texts: A surface-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="395" to="448" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wikinet: A very large scale multi-lingual concept network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivi</forename><surname>Nastase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Börschinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cäcilia</forename><surname>Zirn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anas</forename><surname>Elghafari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 7th International Conference on Language Resources and Evaluation</title>
		<meeting>eeding of the 7th International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Jigs and lures: Associating web queries with structured entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Fuxman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to classify short and sparse text &amp; web with hidden topics from largescale data collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Hieu</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le-Minh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susumu</forename><surname>Horiguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on World Wide Web</title>
		<meeting>the 17th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic sense prediction for implicit discourse relations in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="683" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Introduction to Modern Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>McGrawHill, Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Extraction of agent psychological behaviors from glosses of WordNet personality adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Sansonnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Bouchet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 8th European Workshop on Multi-Agent Systems (EUMAS10)</title>
		<meeting>of the 8th European Workshop on Multi-Agent Systems (EUMAS10)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Machine learning in automated text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Collective context-aware topic models for entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on World Wide Web</title>
		<meeting>the 21st International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Handbook on Ontologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Staab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudi</forename><surname>Studer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Incorporated</title>
		<imprint>
			<publisher>Springer Publishing Company</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A WordNet-based near-synonyms and similar-looking word learning system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koun-Tem</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Min</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Chi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Technology &amp; Society</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="134" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">TREC: Experiment and evaluation in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><forename type="middle">K</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">63</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Triple based background knowledge ranking for document enrichment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="917" to="927" />
		</imprint>
	</monogr>
	<note>Dublin City University and Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to link entities with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="483" to="491" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
