<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Larger-Context Language Modelling with Recurrent Neural Network *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Wang</surname></persName>
							<email>t.wang@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Data Science</orgName>
								<orgName type="department" key="dep2">Courant Institute of Mathematical Sciences and Center for Data Science</orgName>
								<orgName type="institution" key="instit1">New York University</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
							<email>kyunghyun.cho@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Data Science</orgName>
								<orgName type="department" key="dep2">Courant Institute of Mathematical Sciences and Center for Data Science</orgName>
								<orgName type="institution" key="instit1">New York University</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Larger-Context Language Modelling with Recurrent Neural Network *</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1319" to="1329"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this work, we propose a novel method to incorporate corpus-level discourse information into language modelling. We call this larger-context language model. We introduce a late fusion approach to a recurrent language model based on long short-term memory units (LSTM), which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other. Through the evaluation on four corpora (IMDB, BBC, Penn TreeBank, and Fil9), we demonstrate that the proposed model improves per-plexity significantly. In the experiments, we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM. By analyzing the trained larger-context language model, we discover that content words, including nouns, adjectives and verbs, benefit most from an increasing number of context sentences. This analysis suggests that larger-context language model improves the unconditional language model by capturing the theme of a document better and more easily.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of language modelling is to estimate the probability distribution of various linguistic units, e.g., words, sentences <ref type="bibr" target="#b28">(Rosenfeld, 2000</ref>). Among the earliest techniques were count-based n-gram language models which intend to assign the prob- ability distribution of a given word observed af- * Recently, ( <ref type="bibr" target="#b18">Ji et al., 2015</ref>) independently proposed a sim- ilar approach.</p><p>ter a fixed number of previous words. Later <ref type="bibr" target="#b1">Bengio et al. (2003)</ref> proposed feed-forward neural language model, which achieved substantial im- provements in perplexity over count-based lan- guage models. <ref type="bibr">Bengio et al.</ref> showed that this neu- ral language model could simultaneously learn the conditional probability of the latest word in a se- quence as well as a vector representation for each word in a predefined vocabulary.</p><p>Recently recurrent neural networks have be- come one of the most widely used models in lan- guage modelling ( <ref type="bibr" target="#b23">Mikolov et al., 2010)</ref>. Long short-term memory unit (LSTM, <ref type="bibr" target="#b17">Hochreiter and Schmidhuber, 1997</ref>) is one of the most common recurrent activation function. Architecturally, the memory state and output state are explicitly sep- arated by activation gates such that the vanish- ing gradient and exploding gradient problems de- scribed in <ref type="bibr" target="#b2">Bengio et al. (1994)</ref> is avoided. Moti- vated by such gated model, a number of variants of RNNs (e.g. <ref type="bibr">Cho et al. (GRU, 2014b)</ref>, <ref type="bibr">Chung et al. (GF-RNN, 2015)</ref>) have been designed to eas- ily capture long-term dependencies.</p><p>When modelling a corpus, these language mod- els assume the mutual independence among sen- tences, and the task is often reduced to as- signing a probability to a single sentence. In this work, we propose a method to incorporate corpus-level discourse dependency into neural lan- guage model. We call this larger-context lan- guage model. It models the influence of con- text by defining a conditional probability in the form of P (w n |w 1:n−1 , S), where w 1 , ..., w n are words from the same sentence, and S represents the context which consists a number of previous sentences of arbitrary length.</p><p>We evaluated our model on four different cor- pora <ref type="bibr">(IMDB, BBC, Penn TreeBank, and Fil9)</ref>. Our experiments demonstrate that the proposed larger-context language model improve perplex-ity for sentences, significantly reducing per-word perplexity compared to the language models with- out context information. Further, through Part-Of- Speech tag analysis, we discovered that content words, including nouns, adjectives and verbs, ben- efit the most from increasing number of context sentences. Such discovery led us to the conclu- sion that larger-context language model improves the unconditional language model by capturing the theme of a document.</p><p>To achieve such improvement, we proposed a late fusion approach, which is a modification to the LSTM such that it better incorporates the dis- course context from preceding sentences. In the experiments, we evaluated the proposed approach against early fusion approach with various num- bers of context sentences, and demonstrated the late fusion is superior to the early fusion approach.</p><p>Our model explores another aspect of context- dependent recurrent language model. It is novel in that it also provides an insightful way to feed information into LSTM unit, which could benefit all encoder-decoder based applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Statistical Language Modelling with Recurrent Neural Network</head><p>Given a document D = (S 1 , S 2 , . . . , S L ) which consists of L sentences, statistical language mod- elling aims at computing its probability P (D). It is often assumed that each sentence in the whole document is mutually independent from each other:</p><formula xml:id="formula_0">P (D) ≈ L l=1 P (S l ).<label>(1)</label></formula><p>We call this probability (before approximation) a corpus-level probability. Under this assumption of mutual independence among sentences, the task of language modelling is often reduced to assigning a probability to a single sentence P (S l ). A sentence S l = (w 1 , w 2 , . . . , w T l ) is a variable-length sequence of words or tokens. By assuming that a word at any location in a sentence is largely predictable by preceding words, we can rewrite the sentence probability into</p><formula xml:id="formula_1">P (S) = T l t=1 p(w t |w &lt;t ),<label>(2)</label></formula><p>where w &lt;t denotes all the preceding words. We call this a sentence-level probability.</p><p>This rewritten probability expression can be ei- ther directly modelled by a recurrent neural net- work ( <ref type="bibr" target="#b23">Mikolov et al., 2010)</ref> or further approxi- mated as a product of n-gram conditional proba- bilities such that</p><formula xml:id="formula_2">P (S) ≈ T l t=1 p(w t |w t−1 t−(n−1) ),<label>(3)</label></formula><p>where w t−1 t−(n−1) = (w t−(n−1) , . . . , w t−1 ). The latter is called n-gram language modelling.</p><p>A recurrent language model is composed of two functions-transition and output functions. The transition function reads one word w t and updates its hidden state such that</p><formula xml:id="formula_3">h t = φ (w t , h t−1 ) ,<label>(4)</label></formula><p>where h 0 is an all-zero vector. φ is a recurrent activation function. For more details on widely- used recurrent activation units, we refer the reader to ( <ref type="bibr" target="#b19">Jozefowicz et al., 2015;</ref><ref type="bibr" target="#b12">Greff et al., 2015)</ref>. At each timestep, the output function computes the probability over all possible next words in the vocabulary V . This is done by</p><formula xml:id="formula_4">p(w t+1 = w |w t 1 ) ∝ exp (g w (h t )) .<label>(5)</label></formula><p>g is commonly an affine transformation:</p><formula xml:id="formula_5">g(h t ) = W o h t + b o ,</formula><p>where W o ∈ R |V |×d and b o ∈ R |V | . The whole model is trained by maximizing the log-likelihood of a training corpus often using stochastic gradient descent with backpropagation through time (see, e.g., <ref type="bibr" target="#b29">Rumelhart et al., 1988)</ref>.</p><p>This conventional approach to statistical lan- guage modelling often treats every sentence in a document to be independent from each other This is often due to the fact that downstream tasks, such as speech recognition and machine translation, are done sentence-wise. In this paper, we ask how strong an assumption this is, how much impact this assumption has on the final language model qual- ity and how much gain language modelling can get by making this assumption less strong.</p><p>Long Short-Term Memory Here let us briefly describe a long short-term memory unit which is widely used as a recurrent activation function φ (see Eq. <ref type="formula" target="#formula_3">(4)</ref>) for language modelling (see, e.g., <ref type="bibr" target="#b10">Graves, 2013)</ref>.</p><p>A layer of long short-term memory (LSTM) unit consists of three gates and a single memory cell. They are computed by</p><formula xml:id="formula_6">i t =σ (W i x t + U i h t−1 + b i ) o t =σ (W o x t + U o h t−1 + b o ) f t =σ (W f x t + U f h t−1 + b f ) ,</formula><p>where σ is a sigmoid function. x t is the input at time t. The memory cell is computed by</p><formula xml:id="formula_7">c t = f t c t−1 + i t tanh (W c x + U c h t−1 + b c ) ,</formula><p>where is an element-wise multiplication. This adaptive leaky integration of the memory cell al- lows the LSTM to easily capture long-term depen- dencies in the input sequence.</p><p>The output, or the activation of this LSTM layer, is then computed by h t = o t tanh(c t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Larger-Context Language Modelling</head><p>In this paper, we aim not at improving the sentence-level probability estimation P (S) (see Eq. <ref type="formula" target="#formula_1">(2)</ref>) but at improving the corpus-level prob- ability P (D) from Eq. (1) directly. One thing we noticed at the beginning of this work is that it is not necessary for us to make the assumption of mutual independence of sentences in a corpus. Rather, similarly to how we model a sentence probability, we can loosen this assumption by</p><formula xml:id="formula_8">P (D) ≈ L l=1 P (S l |S l−1 l−n ),<label>(6)</label></formula><p>where S l−1 l−n = (S l−n , S l−n+1 , . . . , S l−1 ). n de- cides on how many preceding sentences each con- ditional sentence probability conditions on, sim- ilarly to what happens with a usual n-gram lan- guage modelling.</p><p>From the statistical modelling's perspective, es- timating the corpus-level language probability in Eq. (6) is equivalent to build a statistical model that approximates</p><formula xml:id="formula_9">P (S l |S l−1 l−n ) = T l t=1 p(w t |w &lt;t , S l−1 l−n ),<label>(7)</label></formula><p>similarly to Eq. (2). One major difference from the existing approaches to statistical language mod- elling is that now each conditional probability of a next word is conditioned not only on the preced- ing words in the same sentence, but also on the n − 1 preceding sentences.</p><p>A conventional, count-based n-gram language model is not well-suited due to the issue of data sparsity. In other words, the number of rows in the table storing n-gram statistics will explode as the number of possible sentence combinations grows exponentially with respect to both the vocabulary size, each sentence's length and the number of context sentences.</p><p>Either neural or recurrent language modelling however does not suffer from this issue of data sparsity. This makes these models ideal for mod- elling the larger-context sentence probability in Eq. <ref type="formula" target="#formula_9">(7)</ref>. More specifically, we are interested in adapting the recurrent language model for this.</p><p>In doing so, we answer two questions in the following subsections. First, there is a question of how we should represent the context sentences S l−1 l−n . We consider two possibilities in this work. Second, there is a large freedom in how we build a recurrent activation function to be conditioned on the context sentences. We also consider two alter- natives in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Context Representation</head><p>A sequence of preceding sentences can be repre- sented in many different ways. Here, let us de- scribe two alternatives we test in the experiments.</p><p>The first representation is to simply bag all the words in the preceding sentences into a single vec- tor s ∈ [0, 1] |V | . Any element of s corresponding to the word that exists in one of the preceding sen- tences will be assigned the frequency of that word, and otherwise 0. This vector is multiplied from left by a matrix P which is tuned together with all the other parameters: p = Ps. We call this repre- sentation p a bag-of-words (BoW) context.</p><p>Second, we try to represent the preceding con- text sentences as a sequence of bag-of-words. Each bag-of-word s j is the bag-of-word represen- tation of the j-th context sentence, and they are put into a sequence (s l−n , . . . , s l−1 ). Unlike the first BoW context, this allows us to incorporate the or- der of the preceding context sentences.</p><p>This sequence of BoW vectors are read by a recurrent neural network which is separately from the one used for modelling a sentence (see Eq. (4).) We use LSTM units as recurrent acti- vations, and for each context sentence in the se- quence, we get z t = φ (x t , z t−1 ) , for t = l − n, . . . , l − 1. We set the last hidden state z l−1 of this context recurrent neural network as the con-text vector p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention-based Context Representation</head><p>The sequence of BoW vectors can be used in a bit dif- ferent way from the above. Instead of a unidi- rectional recurrent neural network, we first use a bidirectional recurrent neural network to read the sequence. The forward recurrent neural network reads the sequence as usual in a forward direction, and the reverse recurrent neural network in the op- posite direction. The hidden states from these two networks are then concatenated for each context sentence in order to form a sequence of annotation vectors (z l−n , . . . , z l−1 ).</p><p>Unlike the other approaches, in this case, the context vector p differs for each word w t in the current sentence, and we denote it by p t . The con- text vector p t for the t-th word is computed as the weighted sum of the annotation vectors:</p><formula xml:id="formula_10">p t = l−1 l =l−n α t,l z l ,</formula><p>where the attention weight α t,l is computed by</p><formula xml:id="formula_11">α t,l = exp score (z l , h t ) l−1 k=l−n exp score (z k , h t ) .</formula><p>h t is the hidden state of the recurrent language model of the current sentence from Eq. (5). The scoring function score(z l , h t ) returns a relevance score of the l -th context sentence w.r.t. h t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conditional LSTM</head><p>Early Fusion Once the context vector p is com- puted from the n preceding sentences, we need to feed this into the sentence-level recurrent language model. One most straightforward way is to simply consider it as an input at every time step such that</p><formula xml:id="formula_12">x = E w t + W p p,</formula><p>where E is the word embedding matrix that trans- forms the one-hot vector of the t-th word into a continuous word vector. We call this approach an early fusion of the context.</p><p>Late Fusion In addition to this approach, we propose here a modification to the LSTM such that it better incorporates the context from the pre- ceding sentences (summarized by p t .) The ba- sic idea is to keep dependencies within the sen- tence being modelled (intra-sentence dependen- cies) and those between the preceding sentences We let the memory cell c t of the LSTM to model intra-sentence dependencies. This simply means that there is no change to the existing for- mulation of the LSTM.</p><p>The inter-sentence dependencies are reflected on the interaction between the memory cell c t , which models intra-sentence dependencies, and the context vector p, which summarizes the n pre- ceding sentences. We model this by first comput- ing the amount of influence of the preceding con- text sentences as</p><formula xml:id="formula_13">r t = σ (W r (W p p) + W r c t + b r ) .</formula><p>This vector r t controls the strength of each of the elements in the context vector p. This amount of influence from the n preceding sentences is decided based on the currently captured intra- sentence dependency structures and the preceding sentences.</p><p>This controlled context vector r t (W p p) is used to compute the output of the LSTM layer:</p><formula xml:id="formula_14">h t = o t tanh (c t + r t (W p p)) .</formula><p>This is illustrated in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>.</p><p>We call this approach a late fusion, as the ef- fect of the preceding context is fused together with the intra-sentence dependency structure in the later stage of the recurrent activation.</p><p>Late fusion is a simple, but effective way to mitigate the issue of vanishing gradient in corpus- level language modelling. By letting the context representation flow without having to pass through saturating nonlinear activation functions, it pro- vides a linear path through which the gradient for the context flows easily. <ref type="bibr">, 2012)</ref> proposed an approach, called context-dependent recurrent neural network language model, very similar to the proposed approach here. The basic idea of their approach is to use a topic distribution, represented as a vector of probabilities, of previous n words when computing the hidden state of the recurrent neural network each time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><note type="other">Context-dependent Language Model This possibility of extending a neural or recurrent language modeling to incorporate larger context was explored earlier. Especially, (Mikolov and Zweig</note><p>There are three major differences in the pro- posed approach from the work by <ref type="bibr" target="#b26">Mikolov and Zweig (2012)</ref>. First, the goal in this work is to explicitly model preceding sentences to bet- ter approximate the corpus-level probability (see Eq. (6)) rather than to get a better context of the current sentence. Second, <ref type="bibr" target="#b26">Mikolov and Zweig (2012)</ref> use an external method, such as latent Dirichlet allocation ( <ref type="bibr" target="#b3">Blei et al., 2003)</ref> or latent se- mantics analysis <ref type="bibr" target="#b8">(Dumais, 2004</ref>) to extract a fea- ture vector, whereas we learn the whole model, in- cluding the context vector extraction, end-to-end. Third, we propose a late fusion approach which is well suited for the LSTM units which have re- cently been widely adopted many works involv- ing language models (see, e.g., <ref type="bibr" target="#b31">Sundermeyer et al., 2015)</ref>. This late fusion is later shown to be supe- rior to the early fusion approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dialogue Modelling with Recurrent Neural</head><p>Networks A more similar model to the pro- posed larger-context recurrent language model is a hierarchical recurrent encoder decoder (HRED) proposed recently by <ref type="bibr" target="#b30">Serban et al. (2015)</ref>. The HRED consists of three recurrent neural networks to model a dialogue between two people from the perspective of one of them, to which we refer as a speaker. If we consider the last utterance of the speaker, the HRED is a larger-context recurrent language model with early fusion.</p><p>Aside the fact that the ultimate goals differ (in their case, dialogue modelling and in our case, document modelling), there are two technical dif- ferences. First, they only test with the early fusion approach. We show later in the experiments that the proposed late fusion gives a better language modelling quality than the early fusion. Second, we use a sequence of bag-of-words to represent the preceding sentences, while the HRED a sequence of sequences of words. This allows the HRED to potentially better model the order of the words in each preceding sentence, but it increases computa- tional complexity (one more recurrent neural net- work) and decreases statistical efficient (more pa- rameters with the same amount of data.)</p><p>Skip-Thought Vectors Perhaps the most simi- lar work is the skip-thought vector by <ref type="bibr" target="#b21">Kiros et al. (2015)</ref>. In their work, a recurrent neural network is trained to read a current sentence, as a sequence of words, and extract a so-called skip-thought vec- tor of the sentence. There are two other recurrent neural networks which respectively model preced- ing and following sentences. If we only con- sider the prediction of the following sentence, then this model becomes a larger-context recurrent lan- guage model which considers a single preceding sentence as a context.</p><p>As with the other previous works we have dis- cussed so far, the major difference is in the ulti- mate goal of the model. <ref type="bibr" target="#b21">Kiros et al. (2015)</ref> fully focused on using their model to extract a good, generic sentence vector, while in this paper we are focused on obtaining a good language model. There are less major technical differences. First, the skip-thought vector model conditions only on the immediate preceding sentence, while we ex- tend this to multiple preceding sentences. Second, similarly to the previous works by <ref type="bibr" target="#b26">Mikolov and Zweig (2012)</ref>, the skip-thought vector model only implements early fusion.</p><p>Neural Machine Translation Neural machine translation is another related approach (Forcada and˜Necoand˜ and˜Neco, 1997; <ref type="bibr" target="#b20">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr">Cho et al., 2014b;</ref><ref type="bibr" target="#b32">Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>). In neural machine transla- tion, often two recurrent neural networks are used. The first recurrent neural network, called an en- coder, reads a source sentence, represented as a sequence of words in a source language, to form a context vector, or a set of context vectors. The other recurrent neural network, called a decoder, then, models the target translation conditioned on this source context. This is similar to the proposed larger-context re- current language model, if we consider the source sentence as a preceding sentence in a corpus. The major difference is in the ultimate application, ma- chine translation vs. language modelling, and technically, the differences between neural ma- chine translation and the proposed larger-context language model are similar to those between the HRED and the larger-context language model.</p><p>Context-Dependent Question-Answering Mod- els Context-dependent question-answering is a task in which a model is asked to answer a ques- tion based on the facts from a natural language paragraph. The question and answer are often for- mulated as filling in a missing word in a query sentence ( <ref type="bibr" target="#b16">Hill et al., 2015)</ref>. This task is closely related to the larger-context language model we proposed in this paper in the sense that its goal is to build a model to learn</p><formula xml:id="formula_15">p(q k |q &lt;k , q &gt;k , D),<label>(8)</label></formula><p>where q k is the missing k-th word in a query Q, and q &lt;k and q &gt;k are the context words from the query. D is the paragraph containing facts about this query. It is explicitly constructed so that the query q does not appear in the paragraph D.</p><p>It is easy to see the similarity between Eq. (8) and one of the conditional probabilities in the r.h.s. of Eq. <ref type="formula" target="#formula_9">(7)</ref>. By replacing the context sen- tences S l−1 l−n in Eq. <ref type="formula" target="#formula_9">(7)</ref> with D in Eq. (8) and con- ditioning w t on both the preceding and follow- ing words, we get a context-dependent question- answering model. In other words, the pro- posed larger-context language model can be used for context-dependent question-answering, how- ever, with computational overhead. The overhead comes from the fact that for every possible answer the conditional probability completed query sen- tence must be evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Models</head><p>There are six possible combinations of the pro- posed methods. First, there are two ways of rep- resenting the context sentences; (1) bag-of-words (BoW) and (2) a sequence of bag-of-words (Se- qBoW), from Sec. 3.1. There are two separate ways to incorporate the SeqBoW; (1) with atten- tion mechanism (ATT) and (2) without it. Then, there are two ways of feeding the context vector into the main recurrent language model (RLM); (1) early fusion (EF) and (2) late fusion (LF), from Sec. 3.2. We will denote them by 1. RLM-BoW-EF-n 2. RLM-SeqBoW-EF-n 3. RLM-SeqBoW-ATT-EF-n 4. RLM-BoW-LF-n 5. RLM-SeqBoW-LF-n 6. RLM-SeqBoW-ATT-LF-n n denotes the number of preceding sentences to have as a set of context sentences. We test four different values of n; 1, 2, 4 and 8.</p><p>As a baseline, we also train a recurrent language model without any context information. We refer to this model by RLM. Furthermore, we also re- port the result with the conventional, count-based n-gram language model with the modified Kneser- Ney smoothing with <ref type="bibr">KenLM (Heafield et al., 2013)</ref>.</p><p>Each recurrent language model uses 1000 LSTM units and is trained with Adadelta <ref type="bibr" target="#b34">(Zeiler, 2012)</ref> to maximize the log-likelihood;</p><formula xml:id="formula_16">L(θ) = 1 K K k=1 log p(S k |S k−1 k−n ).</formula><p>We early-stop training based on the validation log-likelihood and report the perplexity on the test set using the best model according to the validation log-likelihood.</p><p>We use only those sentences of length up to 50 words when training a recurrent language model for the computational reason. For KenLM, we used all available sentences in a training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Datasets</head><p>We evaluate the proposed larger-context language model on three different corpora. For detailed statistics, see <ref type="table">Table 1</ref>.</p><p>IMDB Movie Reviews A set of movie reviews is an ideal dataset to evaluate many different settings of the proposed larger-context language models, because each review is highly likely of a single theme (the movie under review.) A set of words or the style of writing will be well deter- mined based on the preceding sentences.</p><p>We use the IMDB Movie Review Corpus (IMDB) prepared by <ref type="bibr" target="#b22">Maas et al. (2011)</ref>. <ref type="bibr">1</ref> This cor- pus has 75k training reviews and 25k test reviews. We use the 30k most frequent words in the training corpus for recurrent language models.</p><p>BBC Similarly to movie reviews, each new ar- ticle tends to convey a single theme. We use the BBC corpus prepared by <ref type="bibr" target="#b11">Greene and Cunningham (2006)</ref>. <ref type="bibr">2</ref> Unlike the IMDB corpus, this corpus contains news articles which are almost always written in a formal style. By evaluating the pro- posed approaches on both the IMDB and BBC corpora, we can tell whether the benefits from larger context exist in both informal and formal languages. We use the 10k most frequent words in the training corpus for recurrent language models. Both with the IMDB and BBC corpora, we did not do any preprocessing other than tokenization. <ref type="bibr">3</ref> Penn Treebank We evaluate a normal recurrent language model, count-based n-gram language model as well as the proposed RLM-BoW-EF-n and RLM-BoW-LF-n with varying n = 1, 2, 4, 8 on the Penn Treebank Corpus. We preprocess the corpus according to <ref type="bibr" target="#b25">(Mikolov et al., 2011</ref>) and use a vocabulary of 10k words from the training cor- pus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fil9</head><p>Fil9 is a cleaned Wikipedia corpus, consist- ing of approximately 140M tokens, and is pro- vided on Matthew Mahoney's website. <ref type="bibr">4</ref> We tok- enized the corpus and used the 44k most frequent words in the training corpus for recurrent language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Analysis</head><p>Corpus-level Perplexity We evaluated the mod- els, including all the proposed approaches (RLM- {BoW,SeqBoW}-{ATT,∅}-{EF,LF}-n), on the IMDB corpus. In <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>, we see three ma- jor trends. First, RLM-BoW, either with the early fusion or late fusion, outperforms both the count-based n-gram and recurrent language model (LSTM) regardless of the number of context sen- tences. Second, the improvement grows as the number n of context sentences increases, and this is most visible with the novel late fusion. Lastly, we see that the RLM-SeqBoW does not work well regardless of the fusion type (RLM-SeqBow- EF not shown), while the attention-based model (RLM-SeqBow-ATT) outperforms all the others. After observing that the late fusion clearly outperforms the early fusion, we evaluated only RLM-{BoW,SeqBoW}-{ATT}-LF-n's on the other two corpora.</p><formula xml:id="formula_17">(i) RLM-BoW-LF (ii) RLM-SeqBoW-ATT-LF (a) IMDB (b) BBC (b) Penn Treebank</formula><p>On the other two corpora, PTB and BBC, we observed a similar trend of RLM-SeqBoW- ATT-LF-n and RLM-BoW-LF-n outperforming the two conventional language models, and that this trend strengthened as the number n of the con- text sentences grew. We also observed again that the RLM-SeqBoW-ATT-LF outperforms RLM- SeqBoW-LF and RLM-BoW in almost all the cases.</p><p>Observing the benefit of RLM-SeqBoW-ATT- LF, we evaluated only such model on Fil9 to val- idate its performance on large corpus. Similar to the results on all three previous corpora, we con- tinue to observe the advantage of RLM-SeqBoW- ATT-LF-n on Fil9 corpus.</p><p>From these experiments, the benefit of allow- ing larger context to a recurrent language model is clear, however, with the right choice of the context representation (see Sec. 3.1) and the right mech- anism for feeding the context information to the recurrent language model (see Sec. 3.2.) In these experiments, the sequence of bag-of-words repre- sentation with attention mechanism, together with the late fusion was found to be the best choice in all four corpora.</p><p>One possible explanation on the failure of the SeqBoW representation with a context recurrent neural network is that it is simply difficult for the context recurrent neural network to compress mul- tiple sentences into a single vector. This difficulty in training a recurrent neural network to com- press a long sequence into a single vector has been observed earlier, for instance, in neural machine translation ( <ref type="bibr" target="#b4">Cho et al., 2014a</ref>). Attention mech- anism, which was found to avoid this problem in machine translation ( <ref type="bibr" target="#b0">Bahdanau et al., 2014)</ref>, is found to solve this problem in our task as well.</p><p>Perplexity per Part-of-Speech Tag Next, we attempted at discovering why the larger-context recurrent language model outperforms the uncon- ditional one. In order to do so, we computed the perplexity per part-of-speech (POS) tag.</p><p>We used the Stanford log-linear part-of-speech tagger (Stanford POS Tagger, <ref type="bibr" target="#b33">Toutanova et al., 2003)</ref> to tag each word of each sentence in the cor- pora. <ref type="bibr">5</ref> We then computed the perplexity of each word and averaged them for each tag type sepa- rately. Among the 36 POS tags used by the Stan- ford POS Tagger, we looked at the perplexities of the ten most frequent tags <ref type="figure">(NN, IN, DT</ref>, JJ, RB, NNS, VBZ, VB, PRP, CC), of which we combined NN and NNS into a new tag Noun and VB and VBZ into a new tag Verb.</p><p>We show the results using the RLM-BoW- LF and RLM-SeqBoW-ATT-LF on three corpora- IMDB, BBC and Penn Treebank-in <ref type="figure" target="#fig_2">Fig. 3</ref>. We observe that the predictability, measured by the perplexity (negatively correlated), grows most for nouns (Noun) and adjectives (JJ) as the number of context sentences increases. They are followed by verbs (Verb). In other words, nouns, adjec- <ref type="table">Training  930,139  21M  37,207  890K  42,068  888K  6,619,098  115M  Validation  152,987  3M  1,998  49K  3,370  70K  825,919  14M  Test  151,987  3M  2,199  53K  3,761  79K</ref> 827,416 14M <ref type="table">Table 1</ref>: Statistics of IMDB, BBC, Penn TreeBank and Fil9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMDB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BBC Penn TreeBank Fil9 # Sentences # Words # Sentences # Words # Sentences # Words # Sentences # Words</head><p>tives and verbs are the ones which become more predictable by a language model given more con- text. We however noticed the relative degradation of quality in coordinating conjunctions (CC), de- terminers (DT) and personal pronouns (PRP). It is worthwhile to note that nouns, adjectives and verbs are open-class, content, words, and con- junctions, determiners and pronouns are closed- class, function, words (see, e.g., <ref type="bibr" target="#b27">Miller, 1999</ref>). The functions words often play grammatical roles, while the content words convey the content of a sentence or discourse, as the name indicates. From this, we may carefully conclude that the larger- context language model improves upon the con- ventional, unconditional language model by cap- turing the theme of a document, which is reflected by the improved perplexity on "content-heavy" open-class words <ref type="bibr" target="#b6">(Chung and Pennebaker, 2007)</ref>. In our experiments, this came however at the ex- pense of slight degradation in the perplexity of function words, as the model's capacity stayed same (though, it is not necessary.)</p><p>This observation is in line with a recent find- ing by <ref type="bibr" target="#b16">Hill et al. (2015)</ref>. They also observed sig- nificant gain in predicting open-class, or content, words when a question-answering model, includ- ing humans, was allowed larger context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we proposed a method to improve language model on corpus-level by incorporating larger context. Using this model results in the im- provement in perplexity on the IMDB, BBC, Penn Treebank and Fil9 corpora, validating the advan- tage of providing larger context to a recurrent lan- guage model.</p><p>From our experiments, we found that the se- quence of bag-of-words with attention is better than bag-of-words for representing the context sentences (see Sec. 3.1), and the late fusion is better than the early fusion for feeding the con- text vector into the main recurrent language model (see <ref type="bibr">Sec. 3.2)</ref>. Our part-of-speech analysis re- vealed that content words, including nouns, adjec- tives and verbs, benefit most from an increasing number of context sentences. This analysis sug- gests that larger-context language model improves perplexity because it captures the theme of a doc- ument better and more easily.</p><p>To explore the potential of such a model, there are several aspects in which more research needs to be done. First, the four datasets we used in this paper are relatively small in the context of lan- guage modelling, therefore the proposed larger- context language model should be evaluated on larger corpora. Second, more analysis, beyond the one based on part-of-speech tags, should be con- ducted in order to better understand the advantage of such larger-context models. Lastly, it is impor- tant to evaluate the impact of the proposed larger- context models in downstream tasks such as ma- chine translation and speech recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Proposed fusion methods and the current sent (inter-sentence dependencies) separately from each other. We let the memory cell c t of the LSTM to model intra-sentence dependencies. This simply means that there is no change to the existing formulation of the LSTM. The inter-sentence dependencies are reflected on the interaction between the memory cell c t , which models intra-sentence dependencies, and the context vector p, which summarizes the n preceding sentences. We model this by first computing the amount of influence of the preceding context sentences as</figDesc><graphic url="image-2.png" coords="4,307.28,205.69,207.35,155.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Corpus-level perplexity on (a) IMDB, (b) Penn Treebank, (c) BBC and (d) Fil9. The countbased 5-gram language models with Kneser-Ney smoothing respectively resulted in the perplexities of 110.20, 148, 127.32 and 65.21, and are not shown here.</figDesc><graphic url="image-7.png" coords="7,312.38,224.28,213.17,141.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Perplexity per POS tag on the (a) IMDB, (b) BBC and (c) Penn Treebank corpora.</figDesc><graphic url="image-13.png" coords="8,361.15,152.73,122.45,78.40" type="bitmap" /></figure>

			<note place="foot" n="1"> http://ai.stanford.edu/ ˜ amaas/data/ sentiment/</note>

			<note place="foot" n="2"> http://mlg.ucd.ie/datasets/bbc.html 3 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ tokenizer/tokenizer.perl</note>

			<note place="foot" n="4"> http://mattmahoney.net/dc/textdata</note>

			<note place="foot" n="5"> http://nlp.stanford.edu/software/ tagger.shtml</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is done as a part of the course DS-GA 1010-001 Independent Study in Data Science at the Center for Data Science, New York Univer-sity. KC thanks Facebook, Google (Google Fac-ulty Award 2016) and NVidia (GPU Center of <ref type="bibr">Excellence 2015</ref><ref type="bibr">Excellence-2016</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Pascal Vincent, and Christian Janvin</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult. Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent dirichlet allocation. the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<title level="m">Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014b. Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The psychological functions of function words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cindy</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="343" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gated feedback recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2067" to="2075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Latent semantic analysis. Annual review of information science and technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumais</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="188" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recursive hetero-associative memories for translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mikel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón P ˜</forename><surname>Forcada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biological and Artificial Computation: From Neuroscience to Technology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="453" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Practical solutions to the problem of diagonal dominance in kernel document clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pádraig</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd International Conference on Machine learning (ICML&apos;06)</title>
		<meeting>23rd International Conference on Machine learning (ICML&apos;06)</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.04069</idno>
		<title level="m">Lstm: A search space odyssey</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bulgaria</forename><surname>Sofia</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="690" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk`kočisk`y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03340</idno>
		<title level="m">Teaching machines to read and comprehend</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03962</idno>
		<title level="m">Document context language models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, and Sanja Fidler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06726</idno>
	</analytic>
	<monogr>
		<title level="m">Skip-thought vectors</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernock`cernock`y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2010</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<biblScope unit="page" from="1045" to="1048" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">HonzaČernock`yHonzaˇHonzaČernock`HonzaČernock`y, and Sanjeev Khudanpur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011-01" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
	<note>Extensions of recurrent neural network language model</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SLT</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On knowing a word</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of psychology</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two decades of statistical language modeling: where do we go from here</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.04808</idno>
		<title level="m">Hierarchical neural network generative models for movie dialogues</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">From feedforward to recurrent lstm neural networks for language modeling. Audio, Speech, and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schluter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="517" to="529" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Featurerich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
		<meeting>the 2003 Conference of the North American Chapter</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: An adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
