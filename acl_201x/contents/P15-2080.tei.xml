<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">User Based Aggregation for Biterm Topic Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizheng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">User Based Aggregation for Biterm Topic Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="489" to="494"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Biterm Topic Model (BTM) is designed to model the generative process of the word co-occurrence patterns in short texts such as tweets. However, two aspects of BTM may restrict its performance: 1) user individualities are ignored to obtain the corpus level words co-occurrence patterns; and 2) the strong assumptions that two co-occurring words will be assigned the same topic label could not distinguish background words from topical words. In this paper, we propose Twitter-BTM model to address those issues by considering user level personalization in BTM. Firstly, we use user based biterms aggregation to learn user specific topic distribution. Secondly, each user&apos;s preference between background words and topical words is estimated by incorporating a background topic. Experiments on a large-scale real-world Twitter dataset show that Twitter-BTM outperforms several state-of-the-art baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, short texts are increasingly preva- lent due to the explosive growth of online social media. For example, about 500 million tweets are published per day on Twitter 1 , one of the most popular online social networking services. Proba- bilistic topic models ( <ref type="bibr" target="#b0">Blei et al., 2003)</ref> are broadly used to uncover the hidden topics of tweets, s- ince the low-dimensional semantic representation is crucial for many applications, such as prod- uct recommendation ( <ref type="bibr" target="#b21">Zhao et al., 2014</ref>), hashtag recommendation ( <ref type="bibr" target="#b11">Ma et al., 2014</ref>), user interest tracking ( <ref type="bibr" target="#b13">Sasaki et al., 2014</ref>), sentiment analysis ( <ref type="bibr" target="#b14">Si et al., 2013)</ref>. However, the scarcity of context and the noisy words restrict LDA and its variations in topic modeling over short texts.</p><p>Previous works model topic distribution at three different levels for tweets: 1) document, the standard LDA assumes each document is associated with a topic distribution ( <ref type="bibr" target="#b7">Godin et al., 2013;</ref><ref type="bibr" target="#b10">Huang, 2012)</ref>. LDA and its variations suffer from context sparsity in each tweet. 2) user, user based aggregation is utilized to alleviate the sparsity problem in short texts ( <ref type="bibr" target="#b17">Weng et al., 2010;</ref><ref type="bibr" target="#b9">Hong and Davison, 2010)</ref>. In these models, all the tweets of the same user are aggregated together as a pseudo document based on the observation that the tweets written by the same user are more similar. 3) corpus, BTM <ref type="bibr" target="#b18">(Yan et al., 2013)</ref> assumes that all the biterms (co-occurring word pairs) are generated by a corpus level topic distribution to benefit from the global rich word co-occurrence patterns.</p><p>As far as we know, how to incorporate user factor into BTM has not been studied yet. User based aggregation has proven effective for LDA. But unfortunately, our preliminary experiments in- dicate that simple user-based aggregation for BTM will generate incoherent topics. To distinguish be- tween commonly used words (e.g., good, people, etc) and topical words (e.g., food, travel, etc), a background topic is often incorporated into the topic models. <ref type="bibr" target="#b20">Zhao et al. (2011)</ref> use a back- ground topic in Twitter-LDA to distill discrimi- native words in tweets. <ref type="bibr" target="#b13">Sasaki et al. (2014)</ref> re- duce the perplexity of Twitter-LDA by estimating the ratio between choosing background words and topical words for each user. They both make a very strong assumption that one tweet only covers one topic. <ref type="bibr" target="#b19">Yan et al. (2015)</ref> use a background topic to distinguish between common biterms and bursty biterms, which need external data to evaluate the burstiness of each biterm as prior knowledge. Un- like those above, we incorporate a background topic to absorb non-discriminative common words in each biterm. And we also estimate the user's preference between common words and topical words. Our new model is named as Twitter-BTM, which combines user based aggregation and the background topic in BTM. Finally, experiments on a Twitter dataset show that Twitter-BTM not only can discover more coherent topics but also can give more accurate topic representation of tweets compared with several state-of-the-art baselines.</p><p>We organize the rest of the paper as follows. Section 2 gives a brief review for BTM. Section 3 introduces our Twitter-BTM model and its im- plementation. Section 4 describes experimental results on a large-scale Twitter dataset. Finally, Section 5 contains a conclusion and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BTM</head><p>There are two major differences between BTM and LDA ( <ref type="bibr" target="#b18">Yan et al., 2013</ref>). For one thing, con- sidering a topic is a mixture of highly correlated words, which implies that they often occur togeth- er in the same document, BTM models the gen- erative process of the word co-occurrence patterns directly. Thus a document made up of n words will be converted to C 2 n biterms. For another, LDA and its variants suffer from the severe data sparsity in short documents. BTM uses global co-occurrence patterns to model the topic distribution over corpus level instead of document level.</p><p>The graphical representation of BTM ( <ref type="bibr" target="#b18">Yan et al., 2013</ref>) is shown in <ref type="figure">Figure 1</ref>(a). It assumes that the whole corpus is associated with a distri- butions θ over K topics drawn from a Dirichlet prior Dir(α). And each topic t is associated with a multinomial distribution φ t over a vocabulary of V unique words drawn from a Dirichlet pri- or Dir(β). The generative process for a corpus which consists of N B biterms B = {b 1 , ..., b N B }, where b i = (w i 1 , w i 2 ), is as follows:</p><formula xml:id="formula_0">1 For each topic t=1,...,T (a) Draw φ t ∼ Dir(β) 2 For the whole tweets collection (a) Draw θ ∼ Dir(α) 3 For each biterm b = 1,...,N B (a) Draw z b ∼ M ulti(θ) (b) Draw w b,1 , w b,2 ∼ M ulti(φ z b )</formula><p>In the above process, z b is the topic assign- ment latent variable of biterm b. To infer the parameters φ and θ, collapsed Gibbs sampling</p><formula xml:id="formula_1">θ α NB 2 w z β K Φ k θ α Nu U 2 γ y w z π β K Φ k Φ B (a) BTM (b) Twitter-BTM Figure 1: Graphical representation of (a) BTM, (b) Twitter-BTM</formula><p>algorithm ( <ref type="bibr" target="#b8">Griffiths and Steyvers, 2004</ref>) is used for approximate inference.</p><p>Compared with the strong assumption that a short document only covers a single topic ( <ref type="bibr" target="#b4">Diao et al., 2012;</ref><ref type="bibr" target="#b5">Ding et al., 2013</ref>), BTM makes a looser assumption that two words will be assigned the same topic label if they have co-occurred. Thus a short document could cover more than one topic, which is more close to the reality. But this assump- tion causes another issue, those commonly used words and those topical words are treated equally. Obviously it is inappropriate to assign same topic label to those words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Twitter-BTM</head><p>In this Section, we introduce our Twitter-BTM model. <ref type="figure">Figure 1(b)</ref> shows the graphical represen- tation of Twitter-BTM. The generative process of Twitter-BTM is as follows:</p><formula xml:id="formula_2">1 Draw φ B ∼ Dir(β) 2 For each topic t=1,...,T (a) Draw φ t ∼ Dir(β) 3 For each user u=1,...,U (a) Draw θ u ∼ Dir(α), π u ∼ Beta(γ) (b) For each biterm b = 1,...,N u (i) Draw z u,b ∼ M ulti(θ u ) (ii) For each word n = 1,2 (A) Draw y u,b,n ∼ Bern(π u ) (B) if y u,b,n = 0 Draw w u,b,n ∼ M ulti(φ B ) if y u,b,n = 1 Draw w u,b,n ∼ M ulti(φ z u,b )</formula><p>In the above process, user u's topic interest θ u is a multinomial distribution over K topics drawn from a Dirichlet prior Dir(α). The background topic B is associated with a multinomial distribu- tion φ B drawn from a Dirichlet prior Dir(β).</p><p>The assumption that each user has a different prefer- ence between topical words and background word- s is shown to be effective in ( <ref type="bibr" target="#b13">Sasaki et al., 2014</ref>). We adopt this assumption in Twitter-BTM. User u's preference is represented as a Bernoulli distri- bution with parameter π u drawn from a beta prior Beta(γ). N u is the number of biterms of user u, z u,b is the topic assignment latent variable of user u's biterm b. For user u and his/her biterm b, n=1 or 2, we use a latent variable y u,b,n to indicate the word type of the word w b,n . When y u,b,n = 1, w b,n is generated from topic z u,b . When y u,b,n = 0, w b,n is generated from the background topic B.</p><p>We adopt collapsed Gibbs Sampling to estimate the parameters. Because of the limitations of s- pace, we leave out the details about the sampling algorithm. Since we can't get a document's distri- bution over topics from the parameters estimated by Twitter-BTM directly, we utilize the following formula ( <ref type="bibr" target="#b18">Yan et al., 2013</ref>) to infer the topic distri- bution of document d. Given a document d whose author is user u:</p><formula xml:id="formula_3">P (z = t|d) = N b i P (z = t|bi)P (bi|d)<label>(1)</label></formula><p>Now the problem is converted to how to estimate P (b i |d) and P (z = t|b i ). P (b i |d) is estimated by empirical distribution in d:</p><formula xml:id="formula_4">P (bi|d) = N b i N b<label>(2)</label></formula><p>where N b i is the number of biterm b i occurred in d, N b is the total number of biterms in d. We can apply Bayes' rule to compute P (z = t|b i ) via following expression:</p><formula xml:id="formula_5">θ u t π u φ B w i,1 + (1 − π u )φ t w i,1 π u φ B w i,2 + (1 − π u )φ t w i,2 k θ u k π u φ B w i,1 + (1 − π u )φ k w i,1 π u φ B w i,2 + (1 − π u )φ k w i,2<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this Section, we describe our experiments car- ried on a Twitter dataset collected form 10th Jun, 2009 to 31st Dec, 2009. Stop words and words occur less than 5 times are removed. We also filter tweets which only have one or two words. All letters are converted into lower case. The dataset is divided into two parts. The first part whose statis- tics is shown in <ref type="table">Table 1</ref> is used for training. The second part which consists of 22,496,107 tweets is used as the external dataset in topic coherence evaluation task in Section 4.1. We compare the performance of Twitter-BTM with five baselines:</p><p>• LDA-U, user based aggregation is applied before training LDA.</p><p>• Twitter-LDA (Zhao et al., 2011), which makes a strong assumption that a tweet only covers one topic.</p><p>• TwitterUB-LDA ( <ref type="bibr" target="#b13">Sasaki et al., 2014</ref>), an im- proved version of Twitter-LDA, which mod- els the user level preference between topical words and background words.</p><p>• BTM ( <ref type="bibr" target="#b18">Yan et al., 2013)</ref>, the Biterm Topic Model.</p><p>• BTM-U, a simplified version of Twitter-BTM without background topic.</p><p>For all the above models, we use symmetric Dirichlet priors. The hyperparameters are set as follows: for all the models, we set α = 50/K, β = 0.01; for Twitter-LDA, TwitterUB-LDA and Twitter-BTM, we set γ = 0.5. We run Gibbs sampling for 400 iterations. Perplexity metric is not used in our experiments since it is not a suitable evaluation metric for BTM <ref type="bibr" target="#b3">(Cheng et al., 2014</ref>). The first reason is that BTM and LDA optimize different likelihood. The second reason is that topic models which have bet- ter perplexity may infer less semantically topics ( <ref type="bibr" target="#b2">Chang et al., 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DataSet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Topic Coherence</head><p>We use PMI-Score ( <ref type="bibr" target="#b12">Newman et al., 2010</ref>) to quan- titatively evaluate the quality of topic component.  <ref type="table">100  method  Top5  Top10  Top20  Top5  Top10  Top20  LDA-U</ref> 2.83±0.07 1.93±0.06 1.40±0.04 3.11±0.09 1.89±0.09 1.15±0.04 Twitter-LDA 2.58±0.04 1.90±0.03 1.39±0.03 2.97±0.20 1.98±0.09 1.44±0.06 TwitterUB-LDA 2.57±0.05 1.87±0.07 1.45±0.04 3.07±0.11 2.05±0.05 1.45±0.05 BTM 2.88±0.14 2.01±0.09 1.44±0.08 3.25±0.14 2.13±0.06 1.49±0.06 BTM-U 2.92±0.10 1.89±0.05 1.33±0.04 3.03±0.07 1.95±0.05 1.34±0.07 Twitter-BTM 3.04±0.10 2.05±0.08 1.47±0.05 3.27±0.12 2.15±0.08 1.48±0.05 <ref type="table">Table 2</ref>: PMI-Score of different topic models Equation (4) defines PMI (Pointwise Mutual In- formation) for two words w i and w j :</p><formula xml:id="formula_6">P M I(wi, wj) = log P (wi, wj) + P (wi)P (wj)<label>(4)</label></formula><p>is an extremely small constant ( <ref type="bibr" target="#b15">Stevens et al., 2012)</ref>, which is equal to 10 −12 in this paper. The word probabilities and the co-occurrence proba- bilities are computed on the large-scale external dataset empirically. Here we use the second part Twitter dataset as the external dataset. Then for a topic t and its top T words ranked by topic-word probability φ t w , the PMI-Score of topic t is defined as follow:</p><formula xml:id="formula_7">P M I − Score(t) = 1 T (T − 1) 1≤i&lt;j≤T P M I(wi, wj)<label>(5)</label></formula><p>The model's PMI-Score is defined as the mean of all the topics' PMI-Score. <ref type="table">Table 2</ref> shows the average results over 10 runs of different models. When K = 50, Twitter-BTM outperforms all other models significantly. When K = 100, The PMI-Score of BTM and Twitter-BTM are very close. BTM-U is worse than BTM, the reason may be that each user's biterm sets provide extremely limited words co-occurring information. <ref type="table" target="#tab_2">Table 3</ref> shows top 10 words of topic "food" learned by BTM, BTM-U and Twitter-BTM when K = 50. We use italic fonts to indicate back- ground words labeled by human judgement. Com- pared with BTM and BTM-U, Twitter-BTM can rank those background words at lower level. It demonstrates that representative words learned by Twitter-BTM are more coherent and meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Document Representation</head><p>Topic models are powerful dimension reduction methods for texts. Given a tweet d, we can in- fer its probability distribution over K topics with  </p><formula xml:id="formula_8">d = [P (z = 1|d), ..., P (z = K|d)]<label>(6)</label></formula><p>We use document classification task (Cheng et al., 2014) and document clustering task ( <ref type="bibr" target="#b6">Duan et al., 2012)</ref> to measure the quality of the docu- ments' topic proportions. Tweets in Twitter have no explicit label information. But some tweets are labeled by one or more hashtags (a type of label whose form is "#keyword") manually by its author to indicate the topic the tweets involve. We follow previous works <ref type="bibr" target="#b3">(Cheng et al., 2014;</ref><ref type="bibr" target="#b16">Wang et al., 2014</ref>) and use hashtags as the tweets' labels. <ref type="table" target="#tab_3">Table 4</ref> lists 38 frequent (at least appears in 100 tweets ) hashtags relating to certain topic or event manually selected in our dataset. We choose those tweets which contain only one of these hashtags appear in <ref type="table" target="#tab_3">Table 4</ref> from our o- riginal data in the following experiments. When we infer a tweet's topic distribution, the hashtag is ignored. Because it doesn't make sense to use the label information to construct the feature vector directly.</p><p>We classify these selected tweets by Random Forest classifier <ref type="bibr" target="#b1">(Breiman, 2001</ref>) implemented in aaliyah afghanistan beatcancer birding blogtalkradio digguser dmv dontyouhate fact giladshalit gno gov green haiku healthcare honduras india iranelection jazz jesus krp lgbt mindsetshift nfl nn oink rhoa slaughterhouse socialmedia tech travel trueblood vegan vegas voss weeklyfitnesschallenge wordpress yyj   <ref type="figure" target="#fig_1">Figure 2</ref>. With the increase of the topic number K, all the models' accuracies are tending to increase. BTM is worse than all other models, which confirms the effectiveness of user based aggregation. Twitter-BTM and BTM- U always outperform LDA-U, Twitter-LDA and TwitterUB-LDA. Twitter-BTM's accuracy is a lit- tle higher than BTM-U, which demonstrates that the background topic is helpful to capture more accurate topic representation of documents. We adopt k-means algorithm implemented in sklearn python module as our clustering method. The number of cluster is set to 38. Consider- ing we have the knowledge of ground truth class assignments of each tweet, and Adjusted Rand Index (ARI) and Normalized Mutual Information are used as cluster validation indices in our exper- iments. As shown in <ref type="figure">Figure 3</ref> and <ref type="figure" target="#fig_3">Figure 4</ref>, The higher ARI and NMI value indicate that Twitter- BTM outperform other models. And BTM per- forms worse than all other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we investigate the problem of topic modeling over short texts with user factor. Us-  er individualities are sacrificed to obtain the cor- pus level words co-occurrence patterns in BTM. However, unlike LDA, simple user based aggre- gation will reduce the topic coherence for BTM. To address this problem, we propose Twitter-BTM which loosens the inappropriate assumption that two co-occurring words must have same topic la- bel made in BTM by leveraging user based ag- gregation and incorporating a background topic in BTM. The experimental results show that Twitter- BTM substantially outperforms BTM.</p><p>In the future, we plan to study the influence of other factors such as temporal information to BTM and its variants.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of classification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 3: Performance of clustering (ARI)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Top 10 words of topic food 

equation (1). Thus d can be represented as a topic 
probability vector: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Hashtags selected for evaluation 

</table></figure>

			<note place="foot" n="1"> See https://about.Twitter.com/company</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent dirichlet allocation. the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Btm: Topic modeling over short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finding bursty topics from microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feida</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="536" to="544" />
		</imprint>
	</monogr>
	<note>ACL (1)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning topical translation model for microblog hashtag suggestion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoye</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 2013, Proceedings of the 23rd International Joint Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ranktopic: Ranking based topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiming</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="211" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using topic models for twitter hashtag recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Godin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Slavkovikj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wesley</forename><surname>De Neve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van De Walle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web companion</title>
		<meeting>the 22nd international conference on World Wide Web companion</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="593" to="596" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Empirical study of topic modeling in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Social Media Analytics, SOMA &apos;10</title>
		<meeting>the First Workshop on Social Media Analytics, SOMA &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="80" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic hashtag recommendation for microblogs using topic-specific translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoye Ding Qi Zhang Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Conference on Computational Linguistics</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page">265</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tagging your tweets: A probabilistic modeling of hashtag annotation in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="999" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic evaluation of topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online topic model for twitter considering dynamics of user interests and topic trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Furuhashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empircal Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empircal Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1977" to="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploiting topic based twitter sentiment for stock prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotie</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploring topic coherence over many models and many topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Kegelmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Andrzejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Buttler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="952" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hashtag graph based topic model for tweet mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jishi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Data Mining, ICDM</title>
		<meeting><address><addrLine>Shenzhen, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-14" />
			<biblScope unit="page" from="1025" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Twitterrank: finding topic-sensitive influential twitterers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A biterm topic model for short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
		<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1445" to="1456" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A probabilistic model for bursty topic discovery in microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Comparing twitter and traditional media using topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="338" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">We know what you want to buy: a demographicbased system for product recommendation on microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Xin Wayne Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1935" to="1944" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
