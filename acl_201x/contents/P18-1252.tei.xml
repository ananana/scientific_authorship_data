<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supervised Treebank Conversion: Data and Approaches</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhou</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Alibaba Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Alibaba Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Supervised Treebank Conversion: Data and Approaches</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2706" to="2716"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2706</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Treebank conversion is a straightforward and effective way to exploit various heterogeneous treebanks for boosting parsing accuracy. However, previous work mainly focuses on unsuper-vised treebank conversion and makes little progress due to the lack of manually labeled data where each sentence has two syntactic trees complying with two different guidelines at the same time, referred as bi-tree aligned data. In this work, we for the first time propose the task of supervised treebank conversion. First, we manually construct a bi-tree aligned dataset containing over ten thousand sentences. Then, we propose two simple yet effective treebank conversion approaches (pat-tern embedding and treeLSTM) based on the state-of-the-art deep biaffine parser. Experimental results show that 1) the two approaches achieve comparable conversion accuracy, and 2) treebank conversion is superior to the widely used multi-task learning framework in multiple treebank exploitation and leads to significantly higher parsing accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised Treebank Conversion: Data and Approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Xinzhou Jiang 2 * , Bo Zhang 2 , Zhenghua Li 1,2 , Min Zhang 1,2 , Sheng Li 3 , Luo Si</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Treebank conversion is a straightfor- ward and effective way to exploit vari- ous heterogeneous treebanks for boost- ing parsing accuracy. However, previ- ous work mainly focuses on unsuper- vised treebank conversion and makes little progress due to the lack of man- ually labeled data where each sentence has two syntactic trees complying with two different guidelines at the same time, referred as bi-tree aligned data.</p><p>In this work, we for the first time propose the task of supervised treebank conversion. First, we manually con- struct a bi-tree aligned dataset contain- ing over ten thousand sentences. Then, we propose two simple yet effective treebank conversion approaches (pat- tern embedding and treeLSTM) based on the state-of-the-art deep biaffine parser. Experimental results show that 1) the two approaches achieve com- parable conversion accuracy, and 2) treebank conversion is superior to the widely used multi-task learning frame- work in multiple treebank exploitation and leads to significantly higher pars- ing accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>During the past few years, neural network based dependency parsing has achieved sig- nificant progress and outperformed the tra- ditional discrete-feature based parsing <ref type="bibr" target="#b4">(Chen and Manning, 2014;</ref><ref type="bibr" target="#b9">Dyer et al., 2015</ref>; Zhou * The first two (student) authors make equal contributions to this work. Zhenghua is the correspondence author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Treebanks</head><p>#Tok Grammar Sinica ( <ref type="bibr" target="#b5">Chen et al., 2003)</ref> 0.36M</p><p>Case grammar CTB ( <ref type="bibr" target="#b27">Xue et al., 2005)</ref> 1.62M Phrase structure TCT ( <ref type="bibr" target="#b31">Zhou, 2004)</ref> 1.00M Phrase structure PCT <ref type="bibr" target="#b28">(Zhan, 2012)</ref> 0.90M Phrase structure HIT-CDT ( ) 0.90M Dependency structure PKU-CDT ( <ref type="bibr" target="#b24">Qiu et al., 2014)</ref> 1.40M Dependency structure <ref type="table" target="#tab_0">Table 1</ref>: Large-scale Chinese treebanks (token number in million).</p><p>et al., <ref type="bibr" target="#b0">Andor et al., 2016</ref>). Most re- markably, <ref type="bibr" target="#b8">Dozat and Manning (2017)</ref> propose a simple yet effective deep biaffine parser that further advances the state-of-the-art accuracy by large margin. As reported, their parser out- performs the state-of-the-art discrete-feature based parser of <ref type="bibr" target="#b1">Bohnet and Nivre (2012)</ref> by 0.97 (93.76% − 92.79%) on the English WSJ data and 6.87 (85.38% − 78.51%) on the Chinese CoNLL-2009 data, respectively. Kindly note that all these results are obtained by training parsers on a single treebank.</p><p>Meanwhile, motivated by different syntactic theories and practices, major languages in the world often possess multiple large-scale hetero- geneous treebanks, e.g., <ref type="bibr">Tiger (Brants et al., 2002</ref>) and TüBa-D/Z ( <ref type="bibr" target="#b26">Telljohann et al., 2004</ref>) treebanks for German, Talbanken <ref type="bibr" target="#b10">(Einarsson, 1976)</ref> and Syntag <ref type="bibr">(Järborg, 1986)</ref> treebanks for Swedish, ISST ( <ref type="bibr" target="#b20">Montemagni et al., 2003)</ref> and TUT 1 treebanks for Italian, etc. <ref type="table" target="#tab_0">Ta- ble 1</ref> lists several large-scale Chinese tree- banks. In this work, we take HIT-CDT as a case study. Our next-step plan is to annotate bi-tree aligned data for PKU-CDT and then convert PKU-CDT to our guideline. For non-dependency treebanks, the straight-forward choice is to convert such treebanks to dependency treebanks based on heuris- tic head-finding rules. The second choice is to directly extend our proposed approaches by adapting the patterns and treeLSTMs for non-dependency structures, which should be straightforward as well.</p><p>Considering the high cost of treebank con- struction, it has always been an interesting and attractive research direction to exploit various heterogeneous treebanks for boosting parsing performance. Though under different linguistic theories or annotation guidelines, the treebanks are painstakingly developed to capture the syntactic structures of the same language, thereby having a great deal of com- mon grounds.</p><p>Previous researchers have proposed two ap- proaches for multi-treebank exploitation. On the one hand, the guiding-feature method projects the knowledge of the source-side tree- bank into the target-side treebank, and uti- lizes extra pattern-based features as guid- ance for the target-side parsing, mainly for the traditional discrete-feature based pars- ing ( ). On the other hand, the multi-task learning method simultaneously trains two parsers on two treebanks and uses shared neural network parameters for repre- senting common-ground syntactic knowledge ( <ref type="bibr" target="#b11">Guo et al., 2016)</ref>. <ref type="bibr">2</ref> Regardless of their ef- fectiveness, while the guiding-feature method fails to directly use the source-side treebank as extra training data, the multi-task learning method is incapable of explicitly capturing the structural correspondences between two guidelines. In this sense, we consider both of them as indirect exploitation approaches.</p><p>Compared with the indirect approaches, treebank conversion aims to directly convert a source-side treebank into the target-side guideline, and uses the converted treebank as extra labeled data for training the target- side model. Taking the example in <ref type="figure" target="#fig_0">Figure 1</ref>, the goal of this work is to convert the under tree that follows the HIT-CDT guideline ) into the upper one that follows our new guideline. However, due to the lack 2 Johansson (2013) applies the feature-sharing approach of Daumé III (2007) for multiple treebank exploitation, which can be regarded as a simple discrete-feature variant of multi-task learning. $ Grandma asks me quickly go to school of bi-tree aligned data, in which each sentence has two syntactic trees following the source- side and target-side guidelines respectively, most previous studies are based on unsuper- vised treebank conversion ( <ref type="bibr" target="#b21">Niu et al., 2009</ref>) or pseudo bi-tree aligned data ( <ref type="bibr" target="#b32">Zhu et al., 2011;</ref><ref type="bibr" target="#b15">Li et al., 2013)</ref>, making very limited progress. In this work, we for the first time propose the task of supervised treebank conversion.</p><p>The key motivation is to better utilize a large- scale source-side treebank by constructing a small-scale bi-tree aligned data. In summary, we make the following contributions.</p><p>(1) We have manually annotated a high- quality bi-tree aligned data containing over ten thousand sentences, by re- annotating the HIT-CDT treebank according to a new guideline.</p><p>(2) We propose a pattern embedding conver- sion approach by retrofitting the indirect guiding-feature method of  to the direct conversion scenario, with several substantial extensions.</p><p>(3) We propose a treeLSTM conversion ap- proach that encodes the source-side tree at a deeper level than the shallow pattern embedding approach.</p><p>Experimental results show that 1) the two conversion approaches achieve nearly the same conversion accuracy, and 2) direct treebank conversion is superior to indirect multi-task learning in exploiting multiple treebanks in methodology simplicity and performance, yet with the cost of manual annotation. We release the annotation guideline and the newly annotated data in http://hlt.suda.edu.cn/ index.php/SUCDT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Annotation of Bi-tree Aligned Data</head><p>The key issue for treebank conversion is that sentences in the source-side and target-side treebanks are non-overlapping.</p><p>In other words, there lacks a bi-tree aligned data in which each sentence has two syntactic trees complying with two guidelines as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Consequently, we cannot train a supervised conversion model to directly learn the structural correspondences between the two guidelines. To overcome this obstacle, we construct a bi-tree aligned data of over ten thousand sentences by re-annotating the publicly available dependency-structure HIT- CDT treebank according to a new annotation guideline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Annotation</head><p>Annotation guideline.</p><p>Unlike phrase- structure treebank construction with very detailed and systematic guidelines <ref type="bibr" target="#b27">(Xue et al., 2005;</ref><ref type="bibr" target="#b31">Zhou, 2004</ref>), previous works on Chinese dependency-structure annotation only briefly describe each relation label with a few concrete examples. For example, the HIT-CDT guideline contains 14 relation labels and illustrates them in a 14-page document.</p><p>The UD (universal dependencies) project 3 releases a more detailed language-generic guideline to facilitate cross-linguistically consistent annotation, containing 37 relation labels.</p><p>However, after in-depth study, we find that the UD guideline is very useful and comprehensive, but may not be completely compact for realistic annotation of Chinese-specific syntax. After many months' investigation and trial, we have developed a systematic and detailed annotation guideline for Chinese dependency treebank construction. Our 60-page guideline employs 20 relation labels and gives detailed illustrations for annotation, in order to improve consistency and quality.</p><p>Please refer to <ref type="bibr" target="#b12">Guo et al. (2018)</ref> for the details of our guideline, including detailed discussions on the correspondences and differ- ences between the UD guideline and ours.</p><p>3 http://universaldependencies.org Partial annotation. To save annotation effort, we adopt the idea of <ref type="bibr" target="#b17">Li et al. (2016)</ref> and only annotate the most uncertain (difficult) words in a sentence. For simplicity, we directly use their released parser and produce the un- certainty results of all HLT-CDT sentences via two-fold jack-knifing. First, we select 2, 000 most difficult sentences of lengths <ref type="bibr">[5,</ref><ref type="bibr">10]</ref> for full annotation <ref type="bibr">4</ref> . Then, we select 3, 000 most difficult sentences of lengths <ref type="bibr">[10,</ref><ref type="bibr">20]</ref> from the remaining data for 50% annotation. Finally, we select 6, 000 most difficult sentences of lengths <ref type="bibr">[5,</ref><ref type="bibr">25]</ref> for 20% annotation from the remaining data. The difficulty of a sentence is computed as the averaged difficulty of its selected words.</p><p>Annotation platform. To guarantee an- notation consistency and data quality, we build an online annotation platform to sup- port strict double annotation and subsequent inconsistency handling. Each sentence is dis- tributed to two random annotators. If the two submissions are not the same (inconsistent dependency or relation label), a third expert annotator will compare them and decide a single answer.</p><p>Annotation process. We employ about 20 students in our university as part-time annotators. Before real annotation, we first give a detailed talk on the guideline for about two hours. Then, the annotators spend several days on systematically studying our guideline. Finally, they are required to annotate 50 test- ing sentences on the platform. If the submis- sion is different from the correct answer, the annotator receives an instant feedback for self- improvement. Based on their performance, about 10 capable annotators are chosen as experts to deal with inconsistent submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Statistics and Analysis</head><p>Consistency statistics. Compared with the final answers, the overall accuracy of all annotators is 87.6%. Although the overall inter-annotator dependency-wise consistency rate is 76.5%, the sentence-wise consistency rate is only 43.7%. In other words, 56.3% (100 − 43.7) sentences are further checked by a third expert annotator. This shows how difficult it is to annotate syntactic structures and how important it is to employ strict double annotation to guarantee data quality.</p><p>Annotation time analysis. As shown in <ref type="table" target="#tab_3">Table 2</ref>, the averaged sentence length is 15.4 words in our annotated data, among which 4.7 words (30%) are partially annotated with their heads. According to the records of our annotation platform, each sentence requires about 3 minutes in average, including the annotation time spent by two annotators and a possible expert. The total cost of our data annotation is about 550 person-hours, which can be completed by 20 full-time annotators within 4 days. The most cost is spent on quality control via two-independent annota- tion and inconsistency handling by experts. This is in order to obtain very high-quality data. The cost is reduced to about 150 person- hours without such strict quality control.</p><p>Heterogeneity analysis. In order to un- derstand the heterogeneity between our guide- line and the HIT-CDT guideline, we analyze the 36, 348 words with both-side heads in the train data, as shown in <ref type="table" target="#tab_3">Table 2</ref>. The con- sistency ratio of the two guidelines is 81.69% (UAS), without considering relation labels. By mapping each relation label in HIT-CDT (14 in total) to a single label of our guideline (20 in total), the maximum consistency ratio is 73.79% (LAS). The statistics are similar for the dev/test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Indirect Multi-task Learning</head><p>Basic parser. In this work, we build all the approaches over the state-of-the-art deep bi- affine parser proposed by <ref type="bibr" target="#b8">Dozat and Manning (2017)</ref>. As a graph-based dependency parser, it employs a deep biaffine neural network to compute the scores of all dependencies, and uses viterbi decoding to find the highest- scoring tree. <ref type="figure" target="#fig_1">Figure 2</ref> shows how to score a dependency i ← j. <ref type="bibr">5</ref> First, the biaffine parser applies multi-layer bidirectional sequential LSTMs (biSeqLSTM) to encode the input sentence. The word/tag embeddings e w k and e t k are concatenated as the input vector at w k .</p><formula xml:id="formula_0">r H k = MLP H ( h seq k ) r D k = MLP D ( h seq k )<label>(1)</label></formula><p>where r H k is the representation vector of w k as a head word, and r D k as a dependent. Finally, the score of the dependency i ← j is computed via a biaffine operation.</p><formula xml:id="formula_1">score(i ← j) = [ r D i 1 ] T W b r H j<label>(2)</label></formula><p>During training, the original biaffine parser uses the local softmax loss. For each w i and its head w j , its loss is defined as − log e score(i←j) ∑ k e score(i←k) . Since our training data is partially annotated, we follow <ref type="bibr" target="#b17">Li et al. (2016)</ref> and employ the global CRF loss <ref type="bibr" target="#b18">(Ma and Hovy, 2017)</ref> for better utilization of the data, leading to consistent accuracy gain.</p><p>Multi-task learning aims to incorporate labeled data of multiple related tasks for im- proving performance <ref type="bibr" target="#b6">(Collobert and Weston, 2008)</ref>. <ref type="bibr" target="#b11">Guo et al. (2016)</ref> apply multi-task learning to multi-treebank exploitation based on the neural transition-based parser of <ref type="bibr" target="#b9">Dyer et al. (2015)</ref>, and achieve higher improvement than the guiding-feature approach of .</p><p>Based on the state-of-the-art biaffine parser, this work makes a straightforward extension to realize multi-task learning. We treat the source-side and target-side parsing as two individual tasks. The two tasks use shared pa- rameters for word/tag embeddings and multi- layer biSeqLSTMs to learn common-ground syntactic knowledge, use separate parameters for the MLP and biaffine layers to learn task- specific information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Direct Treebank Conversion</head><p>Task definition. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, given an input sentence x, treebank conversion aims to convert the under source-side tree d src to the upper target-side tree d tgt . Therefore, the main challenge is how to make full use of the given d src to guide the construction of d tgt . Specifically, under the biaffine parser framework, the key is to utilize d src as guid- ance for better scoring an arbitrary target-side dependency i ← − j.</p><p>In this paper, we try to encode the struc- tural information of i and j in d src as a dense vector from two representation levels, thus leading to two approaches, i.e., the shallow pattern embedding approach and the deep treeLSTM approach. The dense vectors are then used as extra inputs of the MLP layer to obtain better word representations, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Pattern Embedding Approach</head><p>In this subsection, we propose the pattern em- bedding conversion approach by retrofitting the indirect guiding-feature method of  to the direct conversion scenario, with several substantial extensions.</p><p>The basic idea of  is to use extra guiding features produced by the source- side parser. First, they train the source parser P arser src on the source-side treebank. Then, they use P arser src to parse the target-side treebank, leading to pseudo bi-tree aligned data. Finally, they use the predictions of P arser src as extra pattern-based guiding fea- tures and build a better target-side parser P arser tgt .</p><p>The original method of  is proposed for traditional discrete-feature based parsing, and does not consider the relation labels in d src . In this work, we make a few useful extensions for more effective utilization of d src .</p><p>• We further subdivide their "else" pattern into four cases according to the length of the path from w i to w j in d src . The left part of <ref type="figure" target="#fig_1">Figure 2</ref> shows all 9 patterns.</p><p>• We use the labels of w i and w j in d src , denoted as l i and l j .</p><p>• Inspired by the treeLSTM approach, we also consider the label of w a , the lowest common ancestor (LCA) of w i and w j , denoted as l a .</p><p>Our pattern embedding approach works as follows. Given i ← j, we first decide its pattern type according to the structural re- lationship between w i and w j in d src , denoted as p i←j . For example, if w i and w j are both the children of a third word w k in d src , then p i←j = "sibling". <ref type="figure" target="#fig_1">Figure 2</ref> shows all 9 patterns.</p><p>Then, we embed p i←j into a dense vector e p i←j through a lookup operation in order to fit into the biaffine parser. Similarly, the three labels are also embedded into three dense vectors, i.e., e l i , e l j , e la .</p><p>The four embeddings are combined as r pat i←j to represent the structural information of w i and w j in d src .</p><formula xml:id="formula_2">r pat i←j = e p i←j ⊕ e l i ⊕ e l j ⊕ e la<label>(3)</label></formula><p>Finally, the representation vector r pat i←j and the top-layer biSeqLSTM outputs are concate- nated as the inputs of the MLP layer.</p><formula xml:id="formula_3">r D i,i←j = MLP D ( r seq i ⊕ r pat i←j ) r H j,i←j = MLP H ( r seq j ⊕ r pat i←j )<label>(4)</label></formula><p>Through r pat i←j , the extended word representa- tions, i.e., r D i,i←j and r H j,i←j , now contain the structural information of w i and w j in d src .</p><p>The remaining parts of the biaffine parser is unchanged. The extended r D i,i←j and r H j,i←j are fed into the biaffine layer to compute a more reliable score of the dependency i ← j, with the help of the guidance of d src .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The TreeLSTM Approach</head><p>Compared with the pattern embedding ap- proach, our second conversion approach em- ploys treeLSTM to obtain a deeper represen- tation of i ← j in the source-side tree d src . <ref type="bibr" target="#b25">Tai et al. (2015)</ref> first propose treeLSTM as a generalization of seqLSTM for encoding tree- structured inputs, and show that treeLSTM is more effective than seqLSTM on the semantic relatedness and sentiment classification tasks. <ref type="bibr" target="#b19">Miwa and Bansal (2016)</ref> compare three treeL- STM variants on the relation extraction task and show that the SP-tree (shortest path) treeLSTM is superior to the full-tree and subtree treeLSTMs.</p><p>In this work, we employ the SP-tree treeL- STM of Miwa and Bansal (2016) for our treebank conversion task. Our preliminary experiments also show the SP-tree treeLSTM outperforms the full-tree treeLSTM, which is consistent with Miwa and Bansal. We did not implement the in-between subtree treeLSTM.  Given w i and w j and their LCA w a , the SP- tree is composed of two paths, i.e., the path from w a to w i and the path from w a to w j , as shown in the right part of <ref type="figure" target="#fig_1">Figure 2</ref>.</p><formula xml:id="formula_4">Biaffine score(i ← j) consistent: i ← j grand: i ← k ← j sibling: i ← k → j reverse: i → j</formula><p>Different from the shallow pattern embed- ding approach, the treeLSTM approach runs a bidirectional treeLSTM through the SP-tree, in order to encode the structural information of w i and w j in d src at a deeper level. The top- down treeLSTM starts from w a and accumu- lates information until w i and w j , whereas the bottom-up treeLSTM propagates information in the opposite direction.</p><p>Following <ref type="bibr" target="#b19">Miwa and Bansal (2016)</ref>, we stack our treeLSTM on top of the biSeqLSTM layer of the basic biaffine parser, instead of directly using word/tag embeddings as inputs. For example, the input vector for w k in the treeL- STM is x k = h seq k ⊕ e l k , where h seq k is the top- level biSeqLSTM output vector at w k , and l k is the label between w k and its head word in d src , and e l k is the label embedding.</p><p>In the bottom-up treeLSTM, an LSTM node computes a hidden vector based on the com- bination of the input vector and the hidden vectors of its children in the SP-tree. The right part of <ref type="figure" target="#fig_1">Figure 2</ref> and Eq. <ref type="formula" target="#formula_5">(5)</ref> illustrate the computation at w a .</p><formula xml:id="formula_5">˜ h a = ∑ k∈C(a) h k i a = σ ( U (i) x a + V (i) ˜ h a + b (i) ) f a,k = σ ( U (f ) x a + V (f ) h k + b (f ) ) o a = σ ( U (o) x a + V (o) ˜ h a + b (o) ) u a = tanh ( U (u) x a + V (u) ˜ h a + b (u) ) c a = i a ⊙ u a + ∑ k∈C(a) f a,k ⊙ c k h a = o a ⊙ tanh ( c a )<label>(5)</label></formula><p>where C(a) means the children of w a in the SP-tree, and f a,k is the forget vector for w a 's child w k . The top-down treeLSTM sends information from the root w a to the leaves w i and w j . An LSTM node computes a hidden vector based on the combination of its input vector and the hidden vector of its single preceding (father) node in the SP-tree.</p><p>After performing the biTreeLSTM, we fol- low <ref type="bibr" target="#b19">Miwa and Bansal (2016)</ref> and use the com- bination of three output vectors to represent the structural information of w i and w j in d src , i.e., the output vectors of w i and w j in the top- down treeLSTM, and the output vector of w a #Sent #Tok (HIT) #Tok (our)   in the bottom-up treeLSTM.</p><formula xml:id="formula_6">r tree i←j = h ↓ i ⊕ h ↓ j ⊕ h ↑ a<label>(6)</label></formula><p>Similar to Eq. (4) for the pattern embed- ding approach, we concatenate r tree i←j with the output vectors of the top-layer biSeqLSTM, and feed them into MLP H/D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Settings</head><p>Data. We randomly select 1, 000/2, 000 sen- tences from our newly annotated data as the dev/test datasets, and the remaining as train. <ref type="table" target="#tab_3">Table 2</ref> shows the data statistics after removing some broken sentences (ungrammat- ical or wrongly segmented) discovered during annotation. The "#tok (our)" column shows the number of tokens annotated according to our guideline. Train-HIT contains all sen- tences in HIT-CDT except those in dev/test, among which most sentences only have the HIT-CDT annotations.</p><p>Evaluation. We use the standard labeled attachment score (LAS, UAS for unlabeled) to measure the parsing and conversion accuracy.</p><p>Implementation. In order to more flexibly realize our ideas, we re-implement the baseline biaffine parser in C++ based on the lightweight neural network library of . On the Chinese CoNLL-2009 data, our parser achieves 85.80% in LAS, whereas the origi- nal tensorflow-based parser 6 achieves 85.54% (85.38% reported in their paper) under the same parameter settings and external word embedding.</p><p>Hyper-parameters. We follow most pa- rameter settings of <ref type="bibr" target="#b8">Dozat and Manning (2017)</ref>. The external word embedding dictionary is trained on Chinese Gigaword (LDC2003T09) with GloVe ( <ref type="bibr" target="#b23">Pennington et al., 2014</ref>  efficiency, we use two biSeqLSTM layers in- stead of three, and reduce the biSeqLSTM output dimension (300) and the MLP output dimension (200). For the conversion approaches, the source- side pattern/label embedding dimensions are 50 (thus |r pat i←j | = 200), and the treeLSTM output dimension is 100 (thus |r tree i←j | = 300). During training, we use 200 sentences as a data batch, and evaluate the model on the dev data every 50 batches (as an epoch). Training stops after the peak LAS on dev does not increase in 50 consecutive epochs.</p><p>For the multi-task learning approach, we randomly sample 100 train sentences and 100 train-HIT sentences to compose a data batch, for the purpose of corpus weighting.</p><p>To fully utilize train-HIT for the conversion task, the conversion models are built upon multi-task learning, and directly reuse the embeddings and biSeqLSTMs of the multi- task trained model without fine-tuning. <ref type="table" target="#tab_5">Table 3</ref> shows the conversion accuracy on the test data. As a strong baseline for the conversion task, the multi-task trained target-side parser ("multi-task") does not use d src during both training and evaluation. In contrast, the conversion approaches use both the sentence x and d src as inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results: Treebank Conversion</head><p>Compared with "multi-task", the two pro- posed conversion approaches achieve nearly the same accuracy, and are able to dramat- ically improve the accuracy with the extra guidance of d src . The gain is 7.58 (82.09 − 74.51) in LAS for the treeLSTM approach.</p><p>It is straightforward to combine the two conversion approaches. We simply concate- nate h seq i/j with both r pat i←j and r tree i←j before feed- ing into MLP H/D . However, the "combined" model leads to no further improvement. This indicates that although the two approaches try  to encode the structural information of w i and w j in d src from different perspectives, the re- sulted representations are actually overlapping instead of complementary, which is contrary to our intuition that the treeLSTM approach should give better and deeper representations than the shallow pattern embedding approach.</p><p>We have also tried several straightforward modifications to the standard treeLSTM in Eq. <ref type="formula" target="#formula_5">(5)</ref>, but found no further improvement.</p><p>We leave further exploration of better treeL- STMs and model combination approaches as future work.</p><p>Feature ablation results are presented in <ref type="table" target="#tab_7">Table 4</ref> to gain more insights on the two proposed conversion approaches.</p><p>In each experiment, we remove a single component from the full model to learn its individual contribution.</p><p>For the pattern embedding approach, all proposed extensions to the basic pattern-based approach of  are useful. Among the three labels, the embedding of l i is the most useful and its removal leads to the highest LAS drop of 0. <ref type="bibr">88 (82.03 − 81.15)</ref>. This is reasonable considering that 81.69% dependencies are consistent in the two guide- lines, as discussed in the heterogeneity analysis of Section 2.2. Removing all three labels decreases UAS by 0.73 (86.66−85.93) and LAS by 1.95 (82.03 − 80.08), demonstrating that the source-side labels are highly correlative with the target-side labels, and therefore very helpful for improving LAS.</p><p>For the treeLSTM approach, the source-side labels in d src are also very useful, improving UAS by 0. <ref type="bibr">49 (86.69 − 86.20)</ref> and LAS by 1.53 (82.09 − 80.56).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results: Utilizing Converted Data</head><p>Another important question to be answered is whether treebank conversion can lead to higher parsing accuracy than multi-task learn- ing. In terms of model simplicity, treebank conversion is better because eventually the target-side parser is trained directly on an enlarged homogeneous treebank unlike the multi-task learning approach that needs to simultaneously train two parsers on two het- erogeneous treebanks. <ref type="table" target="#tab_9">Table 5</ref> shows the empirical results. Please kindly note that the parsing accuracy looks very low, because the test data is partially annotated and only about 30% most uncertain (difficult) words are manually labeled with their heads according to our guideline, as discussed in Section 2.1.</p><p>The first-row, "single" is the baseline target- side parser trained on the train data.</p><p>The second-row "single (hetero)" refers to the source-side heterogeneous parser trained on train-HIT and evaluated on the target-side test data. Since the similarity between the two guidelines is high, as discussed in Section 2.2, the source-side parser achieves even higher UAS by 0.21 (76.20 − 75.99) than the baseline target-side parser trained on the small-scale train data. The LAS is obtained by mapping the HIT-CDT labels to ours (Section 2.2).</p><p>In the third row, "multi-task" is the target- side parser trained on train &amp; train-HIT with the multi-task learning approach. It significantly outperforms the baseline parser by <ref type="bibr">4.30 (74.51 − 70.21</ref>) in LAS. This shows that the multi-task learning approach can effectively utilize the large-scale train-HIT to help the target-side parsing.</p><p>In the fourth row, "single (large)" is the ba- sic parser trained on the large-scale converted train-HIT (homogeneous). We employ the treeLSTM approach to convert all sentences in train-HIT into our guideline. <ref type="bibr">7</ref> We can see that   <ref type="bibr" target="#b22">Noreen (1989)</ref>.  the single parser trained on the converted data significantly outperforms the parser in the multi-task learning approach by <ref type="bibr">1.32 (75.83 − 74.51</ref>) in LAS.</p><p>In summary, we can conclude that treebank conversion is superior to multi-task learning in multi-treebank exploitation for its simplicity and better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results on fully annotated data</head><p>We randomly divided the newly annotated data into train/dev/test, so the test set has a mix of 100%, 50% and 20% annotated sentences. To gain a rough estimation of the performance of different approaches on fully annotated data, we give the results in <ref type="table" target="#tab_11">Table  6</ref>. We can see that all the models achieve much higher accuracy on the portion of fully annotated data than on the whole test data as shown in <ref type="table" target="#tab_5">Table 3</ref> and 5, since the dependencies to be evaluated are the most difficult ones in a sentence for the portion of partially annotated data. Moreover, the conversion model can achieve over 90% LAS thanks to the guidance of the source-side HIT-CDT tree. Please also note that there would still be a slight bias, because those fully annotated sentences are chosen as the most difficult ones according to the parsing model but are also very short ( <ref type="bibr">[5,</ref><ref type="bibr">10]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this work, we for the first time propose the task of supervised treebank conversion by constructing a bi-tree aligned data of over ten thousand sentences. We design two simple yet effective conversion approaches based on the state-of-the-art deep biaffine parser. Results show that 1) the two approaches achieves nearly the same conversion accuracy; 2) relation labels in the source-side tree are very helpful for both approaches; 3) treebank conversion is more effective in multi-treebank exploitation than multi-task learning, and achieves significantly higher parsing accuracy.</p><p>In future, we would like to advance this work in two directions: 1) proposing more effective conversion approaches, especially by exploring the potential of treeLSTMs; 2) constructing bi-tree aligned data for other treebanks and exploiting all available single-tree and bi-tree labeled data for better conversion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of treebank conversion from the source-side HIT-CDT tree (under) to the target-side our-CDT tree (upper).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Computation of score(i ← j) in our proposed conversion approaches. Without the source-side tree d src , the baseline uses the basic r D i and r H j (instead of r D i,i←j and r H j,i←j ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>3 1 . Institute of Artificial Intelligence, Soochow University, Suzhou, China 2. School of Computer Science and Technology, Soochow University, Suzhou, China</head><label>1</label><figDesc></figDesc><table>{xzjiang, bzhang17}@stu.suda.edu.cn, {zhli13,minzhang}@suda.edu.cn 
3. Alibaba Inc., Hangzhou, China 

{lisheng.ls,luo.si}@alibaba-inc.com 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Data statistics. Kindly note that 
sentences in train are also in train-HIT. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 : Conversion accuracy on test data.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Feature ablation for the conversion 
approaches. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Parsing accuracy on test data. 
LAS difference between any two systems is 
statistically significant (p &lt; 0.005) according 
to Dan Bikel s randomized parsing evaluation 
comparer for significance test </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Results on the fully annotated 372 sentences of the test data.</figDesc><table></table></figure>

			<note place="foot" n="1"> http://www.di.unito.it/~tutreeb/</note>

			<note place="foot" n="4"> Punctuation marks are ruled out and unannotated.</note>

			<note place="foot" n="5"> The score computation of the relation labels is analogous, but due to space limitation, we refer readers to Dozat and Manning (2017) for more details. Then, the output vector of the top-layer biSeqLSTM at w k , denoted as h seq k , is fed into two separate MLPs to get two lowerdimensional representation vectors.</note>

			<note place="foot" n="7"> For each sentence in train, which is already partially annotated, the conversion model actually completes the partial target-side tree into a full tree via constrained decoding. As shown by the results in Li et al. (2016), since the most difficult dependencies are known and given to the model, the parsing accuracy will be much higher than the traditional parsing without constraints.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anony-mous reviewers for the helpful comments. We are greatly grateful to all participants in data annotation for their hard work. We also thank Guodong Zhou and Wenliang Chen for the helpful discussions, and Meishan Zhang for his help on the re-implementation of the Biaffine Parser. This work was supported by National Natural Science Foundation of China (Grant No. 61525205, 61502325 61432013), and was also partially supported by the joint research project of Alibaba and Soochow University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2442" to="2452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2012</title>
		<meeting>EMNLP 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1455" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The TIGER treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Dipper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Lezius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Treebanks and Linguistic Theory</title>
		<meeting>the Workshop on Treebanks and Linguistic Theory</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="24" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chinese Dependency Treebank 1.0 (LDC2012T05)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Philadelphia: Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sinica treebank: Design criteria,representational issues and implementation, chapter 13</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keh-Jiann</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Ching</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng-Yi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Jan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Churen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Ming</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependecy parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Talbankens skriftspråkskonkordans</title>
		<imprint>
			<date type="published" when="1976" />
		</imprint>
		<respStmt>
			<orgName>Department of Scandinavian Languages, Lund University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A universal framework for inductive transfer parsing across multi-typed treebanks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data annotation guideline of chinese dependency syntax for multi-domain and multi-source texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chinese Information Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Manual for syntaggning. Department of Linguistic Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerker</forename><surname>Järborg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
		<respStmt>
			<orgName>University of Gothenburg</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training parsers on incompatible treebanks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="127" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Iterative transformation of annotation guidelines for constituency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lü</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="591" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting multiple treebanks for parsing with quasisynchronous grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Active learning for dependency parsing with partial annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural probabilistic model for non-projective mst parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="59" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Building the italian syntactic semantic treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simonetta</forename><surname>Montemagni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Barsotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Battista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicoletta</forename><surname>Calzolari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ornella</forename><surname>Corazzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Zampolli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Fanciulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Massetani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remo</forename><surname>Raffaelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Teresa</forename><surname>Pazienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Saracino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Zanzotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Mana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Pianesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolfo</forename><surname>Delmonte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Building and Using Syntactically Annotated Corpora</title>
		<editor>Anne Abeille</editor>
		<meeting><address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploiting heterogeneous treebanks for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Computer-intensive methods for testing hypotheses: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-view chinese treebanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Likun</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="257" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long shortterm memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Tüba-D/Z treebank: Annotating German with a context-free backbone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Telljohann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Hinrichs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kbler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the Fourth International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="2229" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Penn Chinese Treebank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Engineering</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="207" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The application of treebank to assist Chinese grammar instruction: a preliminary investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Technology and Chinese Language Teaching</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="16" to="29" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Libn3l:a lightweight package for neural nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="225" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A neural probabilistic structured-prediction model for transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1213" to="1222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Annotation scheme for Chinese treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chinese Information Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Better automatic treebank conversion using a feature-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghan</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="715" to="719" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
