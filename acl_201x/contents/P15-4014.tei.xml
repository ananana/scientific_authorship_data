<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SACRY: Syntax-based Automatic Crossword puzzle Resolution sYstem</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015. c 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianni</forename><surname>Barlacchi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Information Engineering</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<postCode>38123</postCode>
									<settlement>Povo (TN)</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ALT, Qatar Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">Hamad Bin Khalifa University</orgName>
								<address>
									<postCode>5825</postCode>
									<settlement>Doha</settlement>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SACRY: Syntax-based Automatic Crossword puzzle Resolution sYstem</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL-IJCNLP 2015 System Demonstrations</title>
						<meeting>ACL-IJCNLP 2015 System Demonstrations <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="79" to="84"/>
							<date type="published">July 26-31, 2015. c 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we present our Crossword Puzzle Resolution System (SACRY), which exploits syntactic structures for clue reranking and answer extraction. SACRY uses a database (DB) containing previously solved CPs in order to generate the list of candidate answers. Additionally, it uses innovative features, such as the answer position in the rank and aggregated information such as the min, max and average clue reranking scores. Our system is based on WebCrow, one of the most advanced systems for automatic crossword puzzle resolution. Our extensive experiments over our two million clue dataset show that our approach highly improves the quality of the answer list, enabling the achievement of unprecedented results on the complete CP resolution tasks, i.e., accuracy of 99.17%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Crossword Puzzles (CPs) are the most famous lan- guage games played around the world. The auto- matic resolution of CPs is an open challenge for the artificial intelligence (AI) community, which mainly employs AI techniques for filling the puz- zle grid with candidate answers. Basic approaches try to optimize the overall probability of correctly filling the grid by exploiting the likelihood of each candidate answer, while satisfying the grid con- straints.</p><p>Previous work ( <ref type="bibr" target="#b3">Ernandes et al., 2005</ref>) clearly suggests that providing the solver with an accurate list of answer candidates is an important step for the CP resolution task. These can be retrieved us- ing (i) the Web, (ii) Wikipedia, (iii) dictionaries or lexical databases like WordNet or, (iv) most im- portantly, recuperated from the DBs of previously solved CP. Indeed, CPs are often created reusing the same clues of past CPs, and thus querying the DB with the target clue allows for recuperating the same (or similar) clues of the target one. It is in- teresting to note that, for this purpose, all previous automatic CP solvers use standard DB techniques, e.g., SQL Full-Text query. Existing systems for automatic CP resolution, such as Proverb ( <ref type="bibr" target="#b9">Littman et al., 2002</ref>) and Dr. Fill <ref type="bibr" target="#b7">(Ginsberg, 2011</ref>), use sev- eral different modules for generating candidate an- swer lists. These are merged and used for defining a Constraint Satisfaction Problem, resolved by the CP solver.</p><p>Our CP system, SACRY, is based on innovative QA methods for answering CP clues. We employ (i) state-of-the-art IR techniques to retrieve the correct answer by querying the DB of previously solved CPs, (ii) learning to rank methods based on syntactic structure of clues and structural ker- nels to improve the ranking of clues that can po- tentially contain the answers and (iii) an aggrega- tion algorithm for generating the final list contain- ing unique candidate answers. We implemented a specific module based on these approaches and we plugged it into an automatic CP solver, namely WebCrow ( <ref type="bibr" target="#b3">Ernandes et al., 2005</ref>). The latter is one of the best systems for CP resolution and it has been kindly made available by the authors.</p><p>We tested our models on a dataset containing more than two million clues and their associated answers. This dataset is an interesting resource that we will make available to the research com- munity. It can be used for tasks such as: (i) simi- lar clue retrieval/reranking, which focuses on im- proving the rank of clues c i retrieved by a search engine, and (ii) answer reranking, which targets the list of a c i , i.e., their aggregated clues. We tested SACRY on an end-to-end task by solving ten crossword puzzles provided by two of the most famous CP editors from The New York Times and the Washington Post. SACRY obtained an impres- sive CP resolution accuracy of 99.17%. In the reminder of this paper, Sec. 2 introduces WebCrow and its architecture. Our models for similar clues retrieval and answers reranking are described in Sec. 3 while Sec. 4 illustrates our ex- periments. Finally, the conclusions and directions for future work are presented in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The WebCrow Architecture</head><p>Our approaches can be used to generate accurate candidate lists that CP solvers can exploit to im- prove their overall accuracy. Therefore, the qual- ity of our methods can be also implicitly evalu- ated on the final resolution task. For this purpose, we use an existing CP solver, namely WebCrow, which is rather modular and accurate and it has been kindly made available by the authors. Its architecture is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. In the fol- lowing, we briefly introduce the database module of WebCrow, which is the one that we substituted with ours.</p><p>WebCrow's solving process can be divided in two phases. In the first phase, the input list of clues activate a set of answer search modules, which produce several lists of possible solutions for each clue. These lists are then merged by a specific Merger component, which uses the con- fidence values of the lists and the probability that a candidate in a list is correct. Eventually, a sin- gle list of answers with their associated probabil- ities is built for each input clue. In the second phase WebCrow fills the crossword grid by solving a constraint-satisfaction problem. WebCrow se- lects a single answer from each merged list of can- didates, trying to satisfy the imposed constraints. The goal of this phase is to find an admissible so- lution that maximizes the number of correctly in- serted words. It is done using an adapted version of the WA* algorithm <ref type="bibr" target="#b14">(Pohl, 1970)</ref> for CP resolu- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CrossWord Database module (CWDB)</head><p>Gathering clues contained in previously published CPs is essential for solving new puzzles. A large portion of clues in new CPs has usually already ap- peared in the past. Clues may share similar word- ing or may be restated in a very different way. Therefore, it is important to identify the clues that have the same answer. WebCrow uses three differ- ent modules to retrieve clues identical or similar to a given clue from the database: the CWDB- EXACT module, which retrieves DB clues that match exactly with a target clue, and weights them by the frequency they have in the clue collec- tion. The CWDB-PARTIAL module, which uses the MySQL's partial matching function, query ex- pansion and positional term distances to compute clue-similarity scores, along with the Full-Text search functions. The CWDB-DICTIO module, which simply returns the full list of words of cor- rect length, ranked by their number of occurrences in the initial list.</p><p>We outperform the previous approach by apply- ing learning-to-rank algorithms based on SVMs and tree kernels on clue lists generated by state- of-the-art passage retrieval systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Crossword Puzzle Database (CPDB) Module</head><p>WebCrow creates answer lists by retrieving clues from the DB of previously solved crosswords. It simply uses the classical SQL operators and full-text search. We instead index the DB clues and their answers with the open source search engine Lucene ( <ref type="bibr" target="#b10">McCandless et al., 2010)</ref>, using the state-of-the-art BM25 retrieval model. This alone significantly improves the quality of the re- trieved clue list, which is further refined by apply- ing reranking. The latter consists in promoting the clues that potentially have the same answer of the query clue. We designed a relatively complex pipeline shown in <ref type="figure">Fig. 2</ref>. We build a training set using some training clues for querying our search en- gine, which retrieves correct and incorrect can- didates from the indexed clues. At classification time, the new clues are used as a search query and the retrieved similar candidate are reranked by our models. The next sections show our approach for building rerankers that can exploit structures for solving the ineffectiveness of the simple word rep- resentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Reranking framework based on Kernels</head><p>The basic architecture of our reranking framework is relatively simple: it uses a standard preference kernel reranking approach and is similar to the one proposed in <ref type="bibr" target="#b15">(Severyn and Moschitti, 2012</ref>) for QA tasks. However, we modeled different kernels suit- able for clue retrieval.</p><p>The framework takes a query clue and retrieves a list of related candidate clues using a search en- gine (applied to the CPDB), according to some similarity criteria. Then, the query and the can- didates are processed by an NLP pipeline. Our pipeline is built on top of the UIMA framework ( <ref type="bibr" target="#b5">Ferrucci and Lally, 2004</ref>) and contains many text analysis components. The components used for our specific tasks are: the tokenizer 1 , sentence detector 1 , lemmatizer 1 , part-of-speech (POS) tag- ger 1 and chunker 2 .</p><p>The annotations produced by these standard processors are input to our components that extract structures as well as traditional features for rep- resenting clues. This representation is employed to train kernel-based rerankers for reordering the candidate lists provided by a search engine. Since the syntactic parsing accuracy can impact the qual- ity of our structures and consequently the accuracy of our learning to rank algorithms, we preferred to use shallow syntactic trees over full syntactic rep- resentations.</p><p>In the reranker we used the Partial Tree Kernel (PTK) <ref type="bibr" target="#b13">(Moschitti, 2006</ref>). Given an input tree, it generates all possible connected tree fragments, e.g., sibling nodes can also be separated and be part of different tree fragments. In other words, a fragment (which is a feature) is any possible tree path, from whose nodes other tree paths can de- part. Thus, it can generate a very rich feature space resulting in higher generalization ability.</p><p>We combined the structural features with other traditional ones. We used the following groups: iKernels features (iK), which include similarity features and kernels applied intra-pairs, i.e., be- tween the query and the retrieved clues: -Syntactic similarities, i.e., cosine similarity mea- sures computed on n-grams (with n = 1, 2, 3, 4) of word lemmas and part-of-speech tags. -Kernel similarities, i.e., string kernels and tree kernels applied to structural representations.</p><p>DKPro Similarity (DKP), which defines features used in the Semantic Textual Similarity (STS) challenge. These are encoded by the UKP <ref type="bibr">Lab (Bär et al., 2013</ref> <ref type="bibr" target="#b6">Gabrilovich and Markovitch, 2007)</ref>, which rep- resents documents as weighted vectors of con- cepts learned from Wikipedia, WordNet and Wik- tionary. -Lexical Substitution <ref type="bibr" target="#b2">(Biemann, 2013)</ref>. A super- vised word sense disambiguation system is used to substitute a wide selection of high-frequency En- glish nouns with generalizations. Resnik and ESA features are computed on the transformed text.</p><note type="other">): -Longest common substring measure and Longest common subsequence measure. They determine the length of the longest substring shared by two text segments. -Running-Karp-Rabin Greedy String Tiling. It provides a similarity between two sentences by counting the number of shuffles in their subparts. -Resnik similarity. The WordNet hierarchy is used to compute a measure of semantic related- ness between concepts expressed in the text. The aggregation algorithm in (Mihalcea et al., 2006) is applied to extend the measure from words to sentences. -Explicit Semantic Analysis (ESA) similarity (</note><p>WebCrow features (WC), which are the similar- ity measures computed on the clue pairs by We- bCrow (using the Levenshtein distance) and the Search Engine score. Kernels for reranking, given a query clue q c and two retrieved clues c 1 , c 2 , we can rank them by using a reranking model, namely (RR). It uses two pairs P = p 1 q , p 2 q and P = p 1 q , p 2 q , the member of each pair are clues from the same list generated by q and q , respectively. In this case, we use the kernel, K RR (P, P ) = P T K(q, c 1 , q , c 1 ) + P T K(q, c 2 , q , c 2 ) − P T K(q, c 1 , q , c 2 ) − P T K(q, c 2 , q , c 1 ), which corresponds to the scalar product between the vectors,</p><formula xml:id="formula_0">φ(p 1 q ) − φ(p 2 q ) · φ(p 1 q ) − φ(p 2 q ) ,</formula><p>in the fragment vector space generated by PTK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning to rank aggregated answers</head><p>Groups of similar clues retrieved from the search engine can be associated with the same answers.</p><p>Since each clue receives a score from the reranker, a strategy to combine the scores is needed. We <ref type="figure">Figure 2</ref>: The architecture of our system aim at aggregating clues associated with the same answer and building meaningful features for such groups. For this purpose, we train an SVM rank with each candidate answer a c i represented with features derived from all the clues c i associated with such answer, i.e., we aggregate them using standard operators such as average, min. and max.</p><p>We model an answer a using its set of clues C a = {c i : a c i = a} in SVM rank . The feature vector associated with a must contains informa- tion from all c ∈ C a . Thus, we designed novel aggregated features that we call AVG: (i) we av- erage the feature values used for each clue by the first reranker, i.e., those described in Sec. 3.1 as well as the scores produced by the clue reranker. More specifically, we compute their sum, average, maximum and minimum values. (ii) We also add the term frequency of the answer word in CPDB.</p><p>Additionally, we model the occurrences of the answer instance in the list by means of positional features: we use n features, where n is the size of our candidate list (e.g., 10). Each feature cor- responds to the positions of each candidate and it is set to the reranker score if c i ∈ C a (i.e., for the target answer candidate) and 0 otherwise. We call such features (POS). For example, if an an- swer candidate is associated with clues appearing at positions 1 and 3 of the list, Feature 1 and Fea- ture 3 will be set to the score calculated from the reranker. We take into account the similarity be- tween the answer candidate and the input clues using a set of features, derived from word embed- dings ( <ref type="bibr" target="#b12">Mikolov et al., 2013</ref>). These features con- sider (i) the similarities between the clues in a pair, (ii) the target clue and the candidate answer and (iii) the candidate clue and the candidate answer. They are computed summing the embedding vec- tors of words and computing the cosine similarity. This way we produce some evidence of semantic relatedness. We call such features (W2V).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generating probabilities for the solver</head><p>After the aggregation and reranking steps we have a set of unique candidate answers ordered by their reranking scores. Using the latter in WebCrow generally produces a decrease of its accuracy since it expects probabilities (or values ranging from 0 to 1). The summed votes or the scores produced by the reranker can have a wider range and can also be negative. Therefore, we apply logistic re- gression (LGR) to learn a mapping between the reranking scores and values ranging from 0 to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In our experiments we compare our approach with WebCrow both on ranking candidate answers and on the end-to-end CP resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Database of previously resolved CPs</head><p>The most commonly used databases of clues con- tain both single clues taken from various cross- words <ref type="bibr" target="#b7">(Ginsberg, 2011</ref>) and entire crossword puz- zle ( <ref type="bibr" target="#b4">Ernandes et al., 2008)</ref>. They refer to relatively clean pairs of clue/answer and other crossword re- lated information such as date of the clue, name of the CP editor and difficulty of the clue (e.g., clues taken from the CPs published on The Sunday newspaper are the most difficult). Unfortunately, they are not publicly available.</p><p>Therefore, we compiled a crossword corpus combining (i) CP downloaded from the Web 3 and (ii) the clue database provided by Otsys <ref type="bibr">4</ref> . We re- moved duplicates, fill-in-the-blank clues (which are better solved by using other strategies) and clues representing anagrams or linguistic games. We collected over 6.3 millions of clues, published by many different American editors. Although this is a very rich database, it contains many du- plicates and non-standard clues, which introduce significant noise in the dataset. For this reason we created a compressed dataset of 2,131,034 unique and standard clues, with associated answers. It ex- cludes the fill-in-the-blank clues mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>To train our models, we adopted SVM-light-TK 5 , which enables the use of the Partial Tree Kernel (PTK) <ref type="bibr" target="#b13">(Moschitti, 2006</ref>) in SVM-light <ref type="bibr" target="#b8">(Joachims, 2002)</ref>, with default parameters. We applied a polynomial kernel of degree 3 to the explicit fea- ture vectors (FV). To measure the impact of the rerankers as well as the CWDB module, we used well-known metrics for assessing the accuracy of QA and retrieval systems, i.e.: Recall at differ- ent ranks (R@1, 5, 20, 50, 100), Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP). R@1 is the percentage of questions with a cor- rect answer ranked at the first position. MRR is computed as follows:</p><formula xml:id="formula_1">M RR = 1 |Q| |Q| q=1 1 rank(q)</formula><p>, where rank(q) is the position of the first correct answer in the candidate list. For a set of queries Q, MAP is the mean over the average precision scores for each query: 1 Q Q q=1 AveP (q). To measure the complete CP resolution task, we use the accuracy over the entire words filling a CP grid (one wrong letter causes the entire definition to be incorrect).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Clue reranking experiments</head><p>Given an input clue BM25 retrieves a list of 100 clues. On the latter, we tested our different mod- els for clue reranking. For space constraints, we only report a short summary of our experiments: kernel-based rerankers combined with traditional features (PTK+FV) relatively improve standard IR by 16%. This is an interesting result as in ( <ref type="bibr" target="#b1">Barlacchi et al., 2014</ref>), the authors showed that standard IR greatly improves on the DB methods for clue retrieval, i.e., they showed that BM25 relatively improves on SQL by about 40% in MRR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Answer aggregation and reranking</head><p>Reranking clues is just the first step as the solver must be fed with the list of unique answers. Thus, we first used our best model (i.e., PTK+FV) for clue reranking to score the answers of a separate set, i.e., our answer reranking training set. Then, we used these scores to train an additional reranker for aggregating identical answers. The aggrega- tion module merges clues sharing the same answer into a single instance.</p><p>Tab. 1 shows the results for several answer reranking models tested on a development set: the first row shows the accuracy of the answer list pro- duces by WebCrow. The second row reports the accuracy of our model using a simple voting strat- egy, i.e., the score of the clue reranker is used as a vote for the target candidate answer. The third row applies Logistic Regression (LGR) to transform the SVM reranking scores in probabili- ties. It uses Lucene score for the candidate answer as well as the max and min scores of the entire list. From the fourth column, the answer reranker is trained using SVM rank using FV, AVG, POS, W2V and some of their combinations. We note that: (i) voting the answers using the raw score im-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation of the CP resolution</head><p>In order to test the effectiveness of our method, we evaluated the resolution of full CP. We selected five crosswords from The New York Times newspa- per and other five from the Washington Post. <ref type="figure" target="#fig_1">Fig. 3</ref> shows the average resolution accuracy over the ten CP of the original WebCrow compared to We- bCrow using our reranked lists. We ran the solver by providing it with lists of different size. We note that our model consistently outperforms We- bCrow. This means that the lists of candidate an- swers generated by our models help the solver, which in turn fills the grid with higher accuracy. In particular, our CP system achieves an average accuracy of 99.17%, which makes it competitive with international CP resolution challenges. Additionally, WebCrow achieves the highest ac- curacy when uses the largest candidate lists (both original or reranked) but a large list size negatively impacts on the speed of the solver, which in a CP competition is critical to beat the other com- petitors (if participants obtain the same score, the solving time decides who is ranked first). Thus, our approach also provide a speedup as the best accuracy is reached for just 50 candidates (in con- trast with the 100 candidates needed by the origi- nal WebCrow).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Final Remarks</head><p>In this paper, we describe our system SACRY for automatic CP resolution. It is based on  modeling rerankers for clue retrieval from DBs. SACRY achieves a higher accuracy than We- bCrow. SACRY uses rerankers based on SVMs and structural kernels, where the latter are applied to robust shallow syntactic structures. Our struc- tural models applied to clue reranking enable us to learn clue paraphrasing by exploiting relational syntactic structures representing pairs of clues.</p><p>We collected the biggest clue dataset ever, which can be also used for QA tasks since it is composed by pairs of clue/answer. The dataset includes 2,131,034 unique pairs of clue/answers, which we are going to make available to the re- search community. The experiments show that our methods improve the quality of the lists generated by WebCrow by 14% in MRR. When used in We- bCrow solver with its best setting, its resolution er- ror relatively decreases by 50%, achieving almost a perfect resolution accuracy, i.e., 99.17%. In the future, we would like to release the solver to allow researchers to contribute to the project and make the system even more competitive.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of WebCrow</figDesc><graphic url="image-1.png" coords="2,73.71,62.81,221.34,186.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average accuracy over 10 CPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Answer reranking on the dev. set. 

proves WebCrow but the probabilities computed 
by LGR perform much better, i.e., about 2 per-
cent points better than Raw voting and 4.5 points 
better than WebCrow; (ii) the SVM rank aggrega-
tion model can provide another additional point, 
when positional features and standard feature vec-
tors are used along with aggregated and W2C fea-
tures. (iii) The overall relative improvement of 
14% over WebCrow is promising for improving 
the end-to-end CP resolution task. 

</table></figure>

			<note place="foot" n="1"> http://nlp.stanford.edu/software/ corenlp.shtml 2 http://cogcomp.cs.illinois.edu/page/ software_view/13</note>

			<note place="foot" n="3"> http://www.crosswordgiant.com 4 http://www.otsys.com/clue 5 http://disi.unitn.it/moschitti/ Tree-Kernel.htm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the EC project CogNet, 671625 (H2020-ICT-2014-2, Research and Innovation action). This research is part of the Interactive sYstems for Answer Search (IYAS) project, conducted by the Arabic Language Tech-nologies (ALT) group at Qatar Computing Re-search Institute (QCRI) within the Hamad Bin Khalifa University and Qatar Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dkpro similarity: An open source framework for text similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bär</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to rank answer candidates for automatic resolution of crossword puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianni</forename><surname>Barlacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Creating a system for lexical substitutions from scratch using crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lang. Resour. Eval</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="122" />
			<date type="published" when="2013-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Webcrow: A web-based system for crossword solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Ernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI 05</title>
		<meeting>of AAAI 05<address><addrLine>Menlo Park, Calif</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1412" to="1417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A web-based agent challenges human experts on crosswords</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Ernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">29</biblScope>
			<pubPlace>AI Magazine</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Uima: An architectural approach to unstructured information processing in the corporate research environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="327" to="348" />
			<date type="published" when="2004-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Computing semantic relatedness using wikipediabased explicit semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dr.fill: Crosswords and an implemented solver for singly weighted csps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">L</forename><surname>Ginsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="851" to="886" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGKDD</title>
		<meeting>ACM SIGKDD<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A probabilistic approach to solving crossword puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="23" to="55" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccandless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Hatcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otis</forename><surname>Gospodnetic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lucene in Action</title>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Manning Publications Co</publisher>
		</imprint>
	</monogr>
	<note>Second Edition: Covers Apache Lucene 3.0.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Corpus-based and knowledge-based measures of text semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient convolution kernels for dependency and constituent syntactic trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Heuristic search viewed as path finding in a graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Pohl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page" from="193" to="204" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structural relationships for large-scale learning of answer re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR</title>
		<meeting>ACM SIGIR<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
