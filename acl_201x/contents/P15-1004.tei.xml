<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Statistical Machine Translation Features with Multitask Tensor Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
							<email>jdevlin@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<addrLine>One Microsoft Way</addrLine>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">BBN Technologies</orgName>
								<address>
									<addrLine>10 Moulton St</addrLine>
									<postCode>02138</postCode>
									<settlement>Raytheon, Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Statistical Machine Translation Features with Multitask Tensor Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="31" to="41"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a three-pronged approach to improving Statistical Machine Translation (SMT), building on recent success in the application of neural networks to SMT. First, we propose new features based on neural networks to model various non-local translation phenomena. Second, we augment the architecture of the neural network with tensor layers that capture important higher-order interaction among the network units. Third, we apply multitask learning to estimate the neural network parameters jointly. Each of our proposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and Chinese-English translation over a state-of-the-art system that already includes neural network features.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have gen- erally taken one of two approaches. They ei- ther develop neural network-based features that are used to score hypotheses generated from tra- ditional translation grammars ( <ref type="bibr" target="#b32">Sundermeyer et al., 2014;</ref><ref type="bibr" target="#b10">Devlin et al., 2014;</ref><ref type="bibr" target="#b0">Auli et al., 2013;</ref><ref type="bibr" target="#b19">Le et al., 2012;</ref><ref type="bibr" target="#b25">Schwenk, 2012</ref>), or they implement the whole translation process as a single neu- ral network ( <ref type="bibr" target="#b1">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b33">Sutskever et al., 2014</ref>). The latter approach, sometimes re- ferred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated * * Research conducted when the author was at BBN. by phrase-based or hierarchical translation rules. This paper adopts the former approach, as n-best scores from state-of-the-art SMT systems often suggest that these systems can still be significantly improved with better features.</p><p>We build on <ref type="bibr" target="#b10">(Devlin et al., 2014</ref>) who proposed a simple yet powerful feedforward neural network model that estimates the translation probability conditioned on the target history and a large win- dow of source word context. We take advantage of neural networks' ability to handle sparsity, and to infer useful abstract representations automati- cally. At the same time, we address the challenge of learning the large set of neural network param- eters. In particular,</p><p>• We develop new Neural Network Features to model non-local translation phenomena related to word reordering. Large fully- lexicalized contexts are used to model these phenomena effectively, making the use of neural networks essential. All of the features are useful individually, and their combination results in significant improvements (Section 2).</p><p>• We use a Tensor Neural Network Architecture ( <ref type="bibr" target="#b35">Yu et al., 2012</ref>) to automatically learn com- plex pairwise interactions between the net- work nodes. The introduction of the tensor hidden layer results in more powerful fea- tures with lower model perplexity and signif- icantly improved MT performance for all of neural network features (Section 3).</p><p>• We apply Multitask Learning (MTL) <ref type="bibr" target="#b5">(Caruana, 1997</ref>) to jointly train related neural net- work features by sharing parameters. This allows parameters learned for one feature to benefit the learning of the other features. This results in better trained models and achieves additional MT improvements (Section 4). We apply the resulting Multitask Tensor Net- works to the new features and to existing ones, obtaining strong experimental results over the strongest previous results of <ref type="bibr" target="#b10">(Devlin et al., 2014</ref>). We obtain improvements of +2.5 BLEU points for Arabic-English and +1.8 BLEU points for Chinese-English on the DARPA BOLT Web Fo- rum condition. We also obtain improvements of +2.7 BLEU point for Arabic-English and +1.9 BLEU points for Chinese-English on the NIST Open12 test sets over the best previously pub- lished results in <ref type="bibr" target="#b10">(Devlin et al., 2014</ref>). Both the tensor architecture and multitask learning are gen- eral techniques that are likely to benefit other neu- ral network features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">New Non-Local SMT Features</head><p>Existing SMT features typically focus on local in- formation in the source sentence, in the target hy- pothesis, or both. For example, the n-gram lan- guage model (LM) predicts the next target word by using previously generated target words as con- text (local on target), while the lexical translation model (LTM) predicts the translation of a source word by taking into account surrounding source words as context (local on source).</p><p>In this work, we focus on non-local transla- tion phenomena that result from non-monotone re- ordering, where local context becomes non-local on the other side. We propose a new set of power- ful MT features that are motivated by this simple idea. To facilitate the discussion, we categorize the features into hypothesis-enumerating features that estimates a probability for each generated target word (e.g., n-gram language model), and source- enumerating features that estimates a probability for each source word (e.g., lexical translation).</p><p>More concretely, we introduce a) Joint Model with Offset Source Context (JMO), a hypothesis enumerating feature that predicts the next target word the source context affiliated to the previous target words; and b) Translation Context Model (TCM), a source-enumerating feature that predicts the context of the translation of a source word rather than the translation itself. These two mod- els extend pre-existing features: the Joint (lan- guage and translation) Model (JM) of <ref type="bibr" target="#b10">(Devlin et al., 2014</ref>) and the LTM respectively respectively. We use a large lexicalized context for there fea- tures, making the choice of implementing them as neural networks essential. We also present neural- network implementations of pre-existing source- enumerating features: lexical translation, orien- tation and fertility models. We obtain additional gains from using tensor networks and multitask learning in the modeling and training of all the fea- tures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hypothesis-Enumerating Features</head><p>As mentioned, hypothesis-enumerating features score each word in the hypothesis, typically by conditioning it on a context of n-1 previous tar- get words as in the n-gram language model. One recent such model, the joint model of <ref type="bibr" target="#b10">Devlin et al. (2014)</ref> achieves large improvements to the state- of-the-art SMT by using a large context window of 11 source words and 3 target words. The Joint Model with Offset Source Context (JMO) is an extension of the JM that uses the source words affiliated with the n-gram target history as con- text. The source contexts of JM and JMO over- lap highly when the translation is monotone, but are complementary when the translation requires word reordering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Joint Model with Offset Source Context</head><p>Formally, JMO estimates the probability of the tar- get hypothesis E conditioned on the source sen- tence F and a target-to-source affiliation A:</p><formula xml:id="formula_0">P (E|F, A) ≈ |E| i=1 P (e i |e i−n+1 i−1 , C a i−k = f a i−k +m a i−k −m )</formula><p>where e i is the word being predicted; e i−n+1 i−1 is the string of n − 1 previously generated words; C a i−k to the source context of m source words around f a i−k , the source word affiliated with e i−k . We refer to k as the offset parameter. We use the def- inition of word affiliation introduced in <ref type="bibr" target="#b10">Devlin et al. (2014)</ref>. When no source context is used, the model is equivalent to an n-gram language model, while an offset parameter of k = 0 reduces the model to the JM of <ref type="bibr" target="#b10">Devlin et al. (2014)</ref>.</p><p>When k &gt; 0, the JMO captures non-local con- text in the prediction of the next target word. More specifically, e i−k and e i , which are local on the target side, are affiliated to f a i−k and f a i which may be distant from each other on the source side due to non-monotone translation, even for k = 1. The offset model captures reordering constraints by encouraging the predicted target word e i to fit well with the previous affiliated source word f a i−k and its surrounding words. We implement a sep- arate feature for each value of k, and later train them jointly via multitask learning. As our ex- periments in Section 5.2.1 confirm, the history- affiliated source context results in stronger SMT improvement than just increasing the number of surrounding words in JM. <ref type="figure">Fig. 1</ref> illustrates the difference between JMO and JM. Assuming n = 3 and m = 1, then JM estimates P (e 5 |e 4 , e 3 , C a 5 = {f 6 , f 7 , f 8 }). On the other hand, for k = 1 , JMO k=1 estimates P (e 5 |e 4 , e 3 , C a 4 = {f 8 , f 9 , f 10 }).</p><formula xml:id="formula_1">f 9 f 5 . . . e 5 e 6 e 4 e 7 e 3 . . .</formula><p>. . .</p><formula xml:id="formula_2">C 7 = C a 5 . . . f 6 f 7 f 8</formula><p>Figure 1: Example to illustrate features. f 9 5 is the source segment, e 7 3 is the corresponding transla- tion and lines refer to the alignment. We show hypothesis-enumerating features that look at f 7 and source-enumerating features that look at e 5 . We surround the source words affiliated with e 5 and its n-gram history with a bracket, and sur- round the source words affiliated with the history of e 5 with squares.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Source-Enumerating Features</head><p>Source-Enumerating Features iterate over words in the source sentence, including unaligned words, and assign it a score depending on what as- pect of translation they are modeling. A source- enumerating feature can be formulated as follows:</p><formula xml:id="formula_3">P (E|F, A) ≈ |F | j=1 P (Y j |C j = f j+m j−m )</formula><p>where C a j is the source context (similar to the hypothesis-enumerating features above) and Y j is the label being predicted by the feature. We first describe pre-existing source-enumerating fea- tures: the lexical translation model, the orientation model and the fertility model, and then discuss a new feature: Translation Context Model (TCM), which is an extension of the lexical translation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Pre-existing Features</head><p>Lexical Translation model (LTM) estimates the probability of translating a source word f j to a tar- get word l(f j ) = e b j given a source context C j , b j ∈ B is the source-to-target word affiliation as defined in <ref type="bibr" target="#b10">(Devlin et al., 2014</ref>). When f j is trans- lated to more than one word, we arbitrarily keep the left-most one. The target word vocabulary V is extended with a N U LL token to accommodate unaligned source words. Orientation model (ORI) describes the proba- bility of orientation of the translation of phrases surrounding a source word f j relative to its own translation. We follow ( <ref type="bibr" target="#b26">Setiawan et al., 2013</ref>) in modeling the orientation of the left and right phrases of f j with maximal orientation span (the longest neighboring phrase consistent with align- ment), which we denote by L j and R j respec-</p><formula xml:id="formula_4">tively. Thus, o(f j ) = o L j (f j ), o R j (f j ), where o L j and o R j refer to the orientation of L j and R j respectively. For unaligned f j , we set o(f j ) = o L j (R j ), the orientation of R j with respect to L j .</formula><p>Fertility model (FM) models the probability that a source word f j generates φ(f j ) words in the hypothesis. Our implemented model only dis- tinguishes between aligned and unaligned source words (i.e., φ(f j ) ∈ {0, 1}). The generalization of the model to account for multiple values of φ(f i ) is straightforward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Translation Context Model</head><p>As with JMO in Section 2.1.1, we aim to cap- ture translation phenomena that appear local on the target hypothesis but non-local on the source side. Here, we do so by extending the LTM feature to predict not only the translated word e b j , but also its surrounding context.</p><p>For- mally, we model</p><formula xml:id="formula_5">P (l(f j )|C j ), where l(f j ) = e b j −d , · · · , e b j , · · · e b j +d is the hypothesis word window around e b j . In practice, we decompose TCM further into +d d =−d P (e b j +d |C j )</formula><p>and imple- mented each as a separate neural network-based feature. Note that TCM is equivalent to the LTM when d = 0. Because of word reordering, a given hypothesis word in l(f j ) might not be affiliated with f j or even to the words in C j . TCM can model non-local information in this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Combined Model</head><p>Since the feature label is undefined for unaligned source words, we make the model hierarchical, based on whether the source word is aligned or not, and thus arrive at the following formulation:</p><formula xml:id="formula_6">P (l(f j )) · P (ori(f j )) · P (φ(f j )) =            P (φ p (f j ) = 0) · P (o L j (R j )) P (φ p (f j ) ≥ 1) · +d d =−d P (e b j +d ) ·P (o L j (f j ), o R j (f j ))</formula><p>We dropped the common context (C j ) for readabil- ity.</p><p>We reuse <ref type="figure">Fig. 1</ref> to illustrate the source- enumerating features. Assuming d = 1, the scores associated with f 7 are P (φ(f 7 ) ≥ 1|C 7 ) for the FM; P (e 4 |C 7 ) · P (e 5 |C 7 ) · P (e 6 )|C 7 ) for the TCM; and</p><formula xml:id="formula_7">P (o(f 7 ) = o L 7 (f 7 ) = RA, o R 7 (f 7 ) = RA)</formula><p>for the ORI(RA refers to Reverse Adjacent). L 7 and R 7 (i.e. f 6 and f 9 8 respectively), the longest neighboring phrase of f 7 , are translated in reverse order and adjacent to e 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tensor Neural Networks</head><p>The second part of this work improves SMT by improving the neural network architecture. Neural Networks derive their strength from their ability to learn a high-level representation of the input auto- matically from data. This high-level representa- tion is typically constructed layer by layer through a weighted sum linear operation and a non-linear activation function. With sufficient training data, neural networks often achieve state-of-the-art per- formance on many tasks. This stands in sharp con- trast to other algorithms that require tedious man- ual feature engineering. For the features presented in this paper, the context words are fed to the net- work network with minimal engineering.</p><p>We further strengthen the network's ability to learn rich interactions between its units by intro- ducing tensors in the hidden layers. The multi- plicative property of the tensor bares a close re- semblance to collocation of context words which are useful in many natural language processing tasks.</p><p>In conventional feedforward neural networks, the output of hidden layer l is produced by mul- tiplying the output vector from the previous layer with a weight matrix (W l ) and then applying the activation function σ to the product. Tensor Neu- ral Networks generalize this formulation by using a tensor U l of order 3 for the weights. The output of node k in layer l is computed as follows:</p><formula xml:id="formula_8">h l [k] = σ h l−1 · U l [k] · h T l−1 where U l [k]</formula><p>, the k-th slice of U l , is a square ma- trix.</p><p>In our implementation, we follow ( <ref type="bibr" target="#b35">Yu et al., 2012;</ref><ref type="bibr" target="#b15">Hutchinson et al., 2013</ref>) and use a low-rank approximation of</p><formula xml:id="formula_9">U l [k] = Q l [k] · R l [k] T , where Q l [k], R l [k] ∈ R n×r .</formula><p>The output of node k be- comes:</p><formula xml:id="formula_10">h l [k] = σ h l−1 · Q l [k] · R l [k] T · h T l−1</formula><p>In our experiments, we choose r = 1, and also apply the non-linear activation function σ distribu- tively. We arrive at the following three equations for computing the hidden layer outputs (0 &lt; l &lt; L):</p><formula xml:id="formula_11">v l = σ (h l−1 · Q l ) v l = σ (h l−1 · R l ) h l = v l ⊗ v l</formula><p>where h l−1 is double-projected to v l and v l , and the two projections are merged using the Hadamard element-wise product operator ⊗.</p><p>This formulation allows us to use the same in- frastructure of the conventional neural networks by projecting the previous layer to two different spaces of the same dimensions, then multiply- ing them element-wise. The only component that is different from conventional feedforward neural networks is the multiplicative function, which is trivially differentiable with respect to the learnable parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multitask Learning</head><p>The third part of this paper addresses the challenge of effectively learning a large number of neural network parameters without overfitting. The chal- lenge is even larger for tensor network since they practically doubles the number of parameters. In this section, we propose to apply Multitask Learn- ing (MTL) to partially address this issue. We im- plement MTL as parameter sharing among the net- works. This effectively reduces the number of pa- rameters, and more importantly, it takes advan- tage of parameters learned for one feature to better learn the parameters of the other features. Another way of looking at this is that MTL facilitates reg- ularization through learning the other tasks. MTL is suitable for SMT features as they model different but closely related aspects of the same translation process. MTL has long been used by the wider machine learning community <ref type="bibr" target="#b5">(Caruana, 1997</ref>) and more recently for natural language pro- cessing <ref type="bibr" target="#b7">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b8">Collobert et al., 2011</ref>). The application of MTL to ma- chine translation, however, has been much less re- stricted, which is rather surprising since SMT fea- tures arise from the same translation task and are naturally related.</p><formula xml:id="formula_12">Input h 1 h 2 Input h 1 ⊗ v 1 v 1 h 2 ⊗ v 2 v 2 Input h 1 ⊗ v 1 v 1 h 1 2 ⊗ v 1 2 v 1 2 h M 2 ⊗ v M 2 v M 2 · · · · · · W 1 W 2 Q 1 R 1 R 2 Q 2 R 1 Q 1 Q 1 2 R 1 2 Q M 2 R M 2 (a) (b)<label>(c)</label></formula><p>We apply MTL for the features described in Section 2. We design all the features to also share the same neural network architecture (in this case, the tensor architecture described in Section 3) and the same input, thus resulting in two large neural networks: one for the hypothesis-enumerating fea- tures and another for the source-enumerating ones. This simplifies the implementation of MTL. Us- ing this setup, it is possible to vary the number of shared hidden layers t from 0 (only sharing the embedding layer) to L − 1 (sharing all the layers except the output). Note that in principle MTL is applicable to other set of networks that have differ- ent architecture or even different input set. With MTL, the training procedure is the same as that of standard neural networks.</p><p>We use the back propagation algorithm, and use as the loss function the product of likelihood of each feature 1 : <ref type="bibr">1</ref> In this and in the other parts of the paper, we add the normalization regularization term described in <ref type="bibr" target="#b10">(Devlin et al., 2014</ref>) to the loss function to avoid computing the normaliza- tion constant at model query/decoding time.</p><formula xml:id="formula_13">Loss = i M j log (P (Y j (X i )))</formula><p>where X i is the training sample and Y j is one of the M models trained. We use the sum of log like- lihoods since we assume that the features are inde- pendent. <ref type="figure" target="#fig_0">Fig. 3(c)</ref> illustrates MTL between M models sharing the input embedding layer and the first hidden layer (t = 1) compared to the separately- trained conventional feedforward neural network and tensor neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We demonstrate the impact of our work with ex- tensive MT experiments on Arabic-English and Chinese-English translation for the DARPA BOLT Web Forum and the NIST OpenMT12 conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline MT System</head><p>We run our experiments using a state-of-the-art string-to-dependency hierarchical decoder <ref type="bibr" target="#b27">(Shen et al., 2010</ref>). The baseline we use includes a set of powerful features as follow:</p><p>• Forward and backward rule probabilities Neural Network Lexical Translation Model (NNLTM) <ref type="bibr" target="#b10">(Devlin et al., 2014</ref>) As shown, our baseline system already includes neural network-based features. NNJM, NNLTM and use two hidden layers with 500 units and use embedding of size 200 for each input.</p><p>We use the MADA-ARZ tokenizer <ref type="bibr" target="#b13">(Habash et al., 2013</ref>) for Arabic word tokenization. For Chi- nese tokenization, we use a simple longest-match- first lexicon-based approach. We align the training data using GIZA++ <ref type="bibr" target="#b21">(Och and Ney, 2003)</ref>. For tun- ing the weights of MT features including the new features, we use iterative k-best optimization with an ExpectedBLEU objective function ( <ref type="bibr" target="#b23">Rosti et al., 2010)</ref>, and decode the test sets after 5 tuning iter- ation. We report the lower-cased BLEU and TER scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">BOLT Discussion Forum</head><p>The bulk of our experiments is on the BOLT Web Discussion Forum domain, which uses data col- lected by the LDC. The parallel training data con- sists of all of the high-quality NIST training cor- pora, plus an additional 3 million words of trans- lated forum data. The tuning and test sets consist of roughly 5000 segments each, with 2 indepen- dent references for Arabic and 3 for Chinese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Effects of New Features</head><p>We first look at the effects of the proposed features compared to the baseline system. <ref type="table">Table 1</ref> summa- rizes the primary results of the Arabic-English and Chinese-English experiments for the BOLT condi- tion. We show the experimental results related to hypothesis-enumerating features (HypEn) in rows S 2 -S 5 , those related to source-enumerating fea- tures (SrcEn) in rows S 6 -S 9 , and the combination of the two in row S 10 . For all the features, we set the source context length to m = 5 (11-word win- dow). For JM and JMO, we set the target context length to n = 4. For the offset parameter k of JMO, we use values 1 to 3. For TCM, we model one word around the translation (d = 1). Larger values of d did not result in further gains. The baseline is comparable to the best results of <ref type="bibr" target="#b10">(Devlin et al., 2014</ref>).</p><p>In rows S 3 to S 5 , we incrementally add a model with different offset source context, from k = 1 to k = 3. For AR-EN, adding JMOs with differ- ent offset source context consistently yields pos- itive effects in BLEU score, while in ZH-EN, it yields positive effects in TER score. Utilizing all offset source contexts "+JMO k≤3 " (row S 5 ) yields around 0.9 BLEU point improvement in AR-EN and around 0.3 BLEU in ZH-EN compared to the baseline. The JMO consistently provides bet- ter improvement compared to a larger JM con- text (row S 2 ), validating our hypothesis that using offset source context captures important non-local context.</p><p>Rows S 6 to S 9 present the improvements that result from implementing pre-existing source- enumerating SMT features as neural networks, and highlight the contribution of our translation context model (TCM). This set of experiments is orthogonal to the HypEn experiments (rows S 2 - S 5 ). Each pre-existing model has a modest pos- itive cumulative effect on both BLEU and TER. We see this result as further confirming the cur- rent trend of casting existing SMT features as neu- ral network since our baseline already contains such features. The next row present the results of adding the translation context model, with one word surrounding the translation (d = 1). As shown, TCM yields a positive effect of around 0.5 BLEU and TER improvements in AR-EN and around 0.2 BLEU and TER improvements in ZH- EN.</p><p>Separately, the set of source-enumerating fea- tures and the set of target-enumerating features produce around 1.1 to 1.2 points BLEU gain in AR-EN and 0.3 to 0.5 points BLEU gain in ZH- EN. The combination of the two sets produces a complementary gain in addition to the gains of the individual models as Row (S 10 ) shows. The com- bined gain improves to 1.5 BLEU points in AR- EN and 0.7 BLEU points in ZH-EN.  <ref type="table">Table 1:</ref> MT results of various model combination in BLEU and in TER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Effects of Tensor Network and Multitask Learning</head><p>We first analyze the impact of tensor architecture and MTL intrinsically by reporting the models' average log-likelihood on the validation sets (a subset of the test set) in <ref type="table">Table 2</ref>. As mentioned, we group the models to HypEn (JM and JMO k≤3 ) and SrcEn (LTM, ORI,FERT and TCM) as we perform MTL on these two groups. Likelihood of these two groups in the previous subsection are in col- umn "NN" (for Neural Network), which serves as a baseline. The application of the tensor architec- ture improves their likelihood as shown in column "Tensor" for both languages and models.  <ref type="table">Table 2</ref>: Sum of the average log-likelihood of the models in HypEn and SrcEn. t = 0 refers to MTL that shares only the embedding layer, while t = 1 shares the first hidden layer as well. L refers to the network's depth. Higher value is better.</p><formula xml:id="formula_14">Feat. Independent MTL NN Tensor t = 0 t = 1 L = 2 L =</formula><p>The likelihoods of the MTL-related experi- ments are in columns with "MTL" header. We present two set of results. In the first set (col- umn "MTL,t=0,L=2"), we run MTL for features from column "Tensor" by sharing the embedding layer only (t = 0). This allows us to isolate the impact of MTL in the presence of Tensors. Column "MTL,t=1,l=3" corresponds to the exper- iment that produces the best intrinsic result, where each model uses Tensors with three hidden lay- ers (500x500x500, l = 3) and the models share the embedding and the first hidden layers (t = 1). MTL consistently gives further intrinsic gain com- pared to tensors. More sharing provides an extra gain for SrcEn as shown in the last column. Note that we only experiment with different l and t for SrcEn and not for HypEn because the models in HypEn have different input sets. In our experi- ments, further sharing and more hidden layers re- sulted in no further gain. In total, we see a consis- tent positive effect in intrinsic evaluation from the tensor networks and multitask learning.</p><p>Moving on to MT evaluation, we summarize the experiments showing the impact of Tensors and MTL in <ref type="table" target="#tab_3">Table 3</ref>. For MTL, we use L = 3, t = 2 since it gives the best intrinsic score. Employing tensors instead of regular neural networks gives a significant and consistent positive impact for all models and language pairs. For the system with the baseline features, we use the tensor architec- ture for both the joint model and the lexical trans- lation model of Devlin et al. resulting in an im- provement of around 0.7 BLEU points, and show- ing the wide applicability of the tensor architec- ture. On top of this improved baseline, we also ob- serve an improvement of the same scale for other models (column "Tensor"), except for HypEn fea- tures in AR-EN experiment. Moving to MTL ex- periments, we see improvements, especially from SrcEn features. MTL gives around 0.5 BLEU point improvement for AR-EN and around 0.4 BLEU point for ZH-EN. When we employ both HypEn and SrcEn together, MTL gives around 0.4 BLEU point in AR-EN and 0.2 BLEU point in ZH-EN. In total, our work results in an improve- ment of 2.5 BLEU point for AR-EN and 1.8 for BLEU point in ZH-EN on top of the best results in ( <ref type="bibr" target="#b10">Devlin et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">NIST OpenMT12</head><p>Our NIST system is compatible with the OpenMT12 constrained track, which consists of 10M words of high-quality parallel training for Arabic, and 25M words for Chinese. The n-gram LM is trained on 5B words of data from the En- glish GigaWord. For test, we use the "Arabic-To- English Original Progress Test" (1378 segments) and "Chinese-to-English Original Progress Test + OpenMT12 Current Test" (2190 segments), which consists of a mix of newswire and web data. All test segments have 4 references. Our tuning set contains 5000 segments, and is a mix of the MT02-05 eval set as well as additional held-out parallel data from the training corpora. We report the experiments for the NIST con- dition in <ref type="table" target="#tab_4">Table 4</ref>. In particular, we investigate the impact of deploying our new features (column "Feat") and demonstrate the effects of the ten- sor architecture (column "Tensor") and multitask learning (column "MTL"). As shown the results are inline with the BOLT condition where we ob- serve additive improvements from adding our new features, applying tensor network and multitask learning. On Arabic-English, we see a gain of 2.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Our work is most closely related to <ref type="bibr" target="#b10">Devlin et al. (2014)</ref>. They use a simple feedforward neural network to model two important MT features: A joint language and translation model, and a lex- ical translation model. They show very large improvements on Arabic-English and Chinese- English web forum and newswire baselines. We improve on their work in 3 aspects. First, we model more features using neural networks, in- cluding two novel ones: a joint model with off- set source context and a translation context model. Second, we enhance the neural network architec- ture by using tensor layers, which allows us to model richer interactions. Lastly, we improve the performance of the individual features by training them using multitask learning. In the remainder of this section, we describe previous work relat- ing to the three aspect of our work, namely MT modeling, neural network architecture and model learning.</p><p>The features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the origi- nal IBM models ( <ref type="bibr" target="#b3">Brown et al., 1993)</ref>: lexical translation, reordering, word fertility, and lan- guage models. Of particular relevance to our work are approaches that incorporate context-sensitivity into the models <ref type="bibr" target="#b4">(Carpuat and Wu, 2007)</ref>, formu- late reordering as orientation prediction task <ref type="bibr" target="#b34">(Tillman, 2004</ref>) and that use neural network language models ( <ref type="bibr" target="#b2">Bengio et al., 2003;</ref><ref type="bibr" target="#b24">Schwenk, 2010;</ref><ref type="bibr" target="#b25">Schwenk, 2012)</ref>, and incorporate source-side con- text into them <ref type="bibr" target="#b10">(Devlin et al., 2014;</ref><ref type="bibr" target="#b0">Auli et al., 2013;</ref><ref type="bibr" target="#b19">Le et al., 2012;</ref><ref type="bibr" target="#b25">Schwenk, 2012)</ref>.</p><p>Approaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representa- tion of the source sentence, we follow <ref type="bibr" target="#b10">(Devlin et al., 2014</ref>) in using a window around the affiliated source word. To name some other approaches, <ref type="bibr" target="#b0">Auli et al. (2013)</ref> uses latent semantic analysis and source sentence embeddings learned from the re- current neural network; <ref type="bibr" target="#b32">Sundermeyer et al. (2014)</ref> take the representation from a bidirectional LSTM recurrent neural network <ref type="bibr">;</ref><ref type="bibr" target="#b16">and Kalchbrenner and Blunsom (2013)</ref> employ a convolutional sentence model. For target context, recent work has tried to look beyond the classical n-gram history. ( <ref type="bibr" target="#b0">Auli et al., 2013;</ref><ref type="bibr" target="#b32">Sundermeyer et al., 2014</ref>) consider an unbounded history, at the expense of making their model only applicable for N-best rescoring. Another recent line of research ( <ref type="bibr" target="#b1">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b33">Sutskever et al., 2014</ref>) departs more rad- ically from conventional feature-based SMT and implements the MT system as a single neural net- work. These models use a representation of the whole input sentence.</p><p>We use a feedforward neural network in this work. Besides feedforward and recurrent net-works, other network architectures that have been applied to SMT include convolutional networks <ref type="bibr" target="#b17">(Kalchbrenner et al., 2014</ref>) and recursive networks <ref type="bibr" target="#b30">(Socher et al., 2011</ref>). The simplicity of feedfor- ward networks works to our advantage. More specifically, due to the absence of a feedback loop, the feedforward architecture allows us to treat individual decisions independently, which makes parallelization of the training easy and the query- ing the network at decoding time straightforward. The use of tensors in the hidden layers strengthens the neural network model, allowing us to model more complex feature interactions like colloca- tion, which has been long recognized as impor- tant information for many NLP tasks (e.g. word sense disambiguation ( <ref type="bibr" target="#b20">Lee and Ng, 2002)</ref>). The tensor formulation we use is similar to that of ( <ref type="bibr" target="#b35">Yu et al., 2012;</ref><ref type="bibr" target="#b15">Hutchinson et al., 2013)</ref>. Ten- sor Neural Networks have a wide application in other field, but have only been recently applied in NLP ( <ref type="bibr" target="#b31">Socher et al., 2013;</ref><ref type="bibr" target="#b22">Pei et al., 2014</ref>). To our knowledge, our work is the first to use tensor networks in SMT.</p><p>Our approach to multitask learning is related to work that is often labeled joint training or transfer learning. To name a few of these works, <ref type="bibr" target="#b12">Finkel and Manning (2009)</ref> successfully train name en- tity recognizers and syntactic parsers jointly, and <ref type="bibr" target="#b28">Singh et al. (2013)</ref> train models for coreference resolution, named entity recognition and relation extraction jointly. Both efforts are motivated by the minimization of cascading errors. Our work is most closely related to <ref type="bibr" target="#b7">Collobert and Weston (2008;</ref><ref type="bibr" target="#b8">Collobert et al. (2011)</ref>, who apply multi- task learning to train neural networks for multi- ple NLP models: part-of-speech tagging, semantic role labeling, named-entity recognition and lan- guage model variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper argues that a relatively simple feedfor- ward neural network can still provides significant improvement to Statistical Machine Translation (SMT). We support this argument by presenting a multi-pronged approach that addresses modeling, architectural and learning aspects of pre-existing neural network-based SMT features. More con- cretely, we paper present a new set of neural network-based SMT features to capture important translation phenomena, extend feedforward neu- ral network with tensor layers, and apply multi- task learning to integrate the SMT features more tightly. Empirically, all our proposals successfully produce an improvement over state-of-the-art ma- chine translation system for Arabic-to-English and Chinese-to-English and for both BOLT web fo- rum and NIST conditions. Building on the suc- cess of this paper, we plan to develop other neural- network-based features, and to also relax the lim- iteation of current rule extraction heuristics by generating translations word-by-word.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 (</head><label>3</label><figDesc>b) illustrates the tensor ar- chitecture for two hidden layers. The tensor network can learn collocation fea- tures more easily. For example, it can learn a col- location feature that is activated only if h l−1 [i] col- locates with h l−1 [j] by setting U l [k][i][j] to some positive number. This results in SMT improve- ments as we describe in Section 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The network architecture for (a) a conventional feedforward neural network, (b) tensor hidden layers, and (c) multitask learning with M features that share the embedding and first hidden layers (t = 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>Contextual lexical smoothing (Devlin, 2009) • 5-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) • Neural Network Joint Model (NNJM) and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>7</head><label>7</label><figDesc></figDesc><table>Feature set 

AR-EN 
ZH-EN 
NN Tensor MTL NN Tensor MTL 
R 1 : Baseline Features 
43.2 
43.9 
-30.2 
30.8 
-
R 2 : R 1 + HypEn 
44.4 
44.4 44.5 30.5 
31.5 31.3 
R 3 : R 1 + SrcEn 
44.3 
44.9 45.5 30.7 
31.5 31.9 
R 4 : R 1 + HypEn + SrcEn 44.7 
45.3 45.7 30.9 
31.8 32.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Experimental results to investigate the effects of the new features, DTN and MTL. The top 
part shows the BOLT results, while the bottom part shows the NIST results. The best results for each 
conditions and each language-pair are in bold. The baselines are in italics. . 

Base. Feat Tensor MTL 
AR-EN 
53.7 
55.4 55.9 
56.4 
mixed-case 51.8 
53.1 53.7 
54.1 
ZH-EN 
36.6 
37.8 38.2 
38.5 
mixed-case 34.4 
35.5 35.9 
36.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Experimental results for the NIST condi-
tion. Mixed-case scores are also reported. Base-
lines are in italics. 

BLEU point and on Chinese-English, we see a 1.9 
BLEU point gain. We also report the mixed-cased 
BLEU scores for comparison with previous best 
published results, i.e. Devlin et al. (2014) report 
52.8 BLEU for Arabic-English and 34.7 BLEU for 
Chinese-English. Thus, our results are around 1.3-
1.4 BLEU point better. Note that they use addi-
tional rescoring features but we do not. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by DARPA/I2O Contract No. HR0011-12-C-0014 under the BOLT Pro-gram. The views, opinions, and/or findings con-tained in this article are those of the author and should not be interpreted as representing the of-ficial views or policies, either expressed or im-plied, of the Defense Advanced Research Projects Agency or the Department of Defense.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint language and translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA,</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1044" to="1054" />
		</imprint>
	</monogr>
	<note>October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving statistical machine translation using word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">001 new features for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="218" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning, ICML &apos;08</title>
		<meeting>the 25th International Conference on Machine Learning, ICML &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Traitbased hypothesis selection for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Matsoukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="528" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Lexical features for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint parsing and named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="326" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Morphological analysis and disambiguation for dialectal arabic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="426" to="432" />
		</imprint>
	</monogr>
	<note>Ramy Eskander, and Nadi Tomeh</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Factored soft source syntactic constraints for hierarchical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="556" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tensor deep stacking networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA, October</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Continuous space translation models with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Son</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoong Keok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the ACL-02 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
	<note>EMNLP &apos;02. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Maxmargin tensor neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BBN system description for WMT10 system combination task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT/MetricsMATR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Continuous-space language models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prague Bull. Math. Linguistics</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="137" to="146" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Continuous space translation models for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING (Posters)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1071" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two-neighbor orientation model with cross-boundary global contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1264" to="1274" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">String-to-dependency statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="649" to="671" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint inference of entities, relations, and coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaping</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Workshop on Automated Knowledge Base Construction, AKBC &apos;13</title>
		<meeting>the 2013 Workshop on Automated Knowledge Base Construction, AKBC &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Language and translation model adaptation using comparable corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="857" to="866" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Parsing Natural Scenes and Natural Language with Recursive Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning (ICML)</title>
		<meeting>the 26th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Translation modeling with bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="14" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A unigram orientation model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL 2004: Short Papers</title>
		<editor>Daniel Marcu Susan Dumais and Salim Roukos</editor>
		<meeting><address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-05-02" />
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Large vocabulary speech recognition using deep tensor neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<editor>INTERSPEECH. ISCA</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
