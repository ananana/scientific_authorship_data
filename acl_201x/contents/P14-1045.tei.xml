<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predicting the relevance of distributional semantic similarity with contextual information</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Muller</surname></persName>
							<email>philippe.muller@irit.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clémentine</forename><surname>Adam</surname></persName>
							<email>clementine.adam@univ-tlse2.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">IRIT</orgName>
								<orgName type="department" key="dep2">Cécile Fabre CLLE</orgName>
								<orgName type="institution" key="instit1">Toulouse University</orgName>
								<orgName type="institution" key="instit2">Université Paul Sabatier</orgName>
								<address>
									<addrLine>118 Route de Narbonne</addrLine>
									<postCode>31062</postCode>
									<settlement>Toulouse Cedex 04</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">CLLE</orgName>
								<orgName type="institution" key="instit1">Toulouse University</orgName>
								<orgName type="institution" key="instit2">Université Toulouse-Le Mirail</orgName>
								<address>
									<addrLine>5 alles A. Machado</addrLine>
									<postCode>31058</postCode>
									<settlement>Toulouse Cedex</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Toulouse University</orgName>
								<orgName type="institution" key="instit2">Université Toulouse-Le Mirail</orgName>
								<address>
									<addrLine>5 alles A. Machado</addrLine>
									<postCode>31058</postCode>
									<settlement>Toulouse Cedex</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Predicting the relevance of distributional semantic similarity with contextual information</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="479" to="488"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Using distributional analysis methods to compute semantic proximity links between words has become commonplace in NLP. The resulting relations are often noisy or difficult to interpret in general. This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains, but instead of considering it in abstracto, we focus on pairs of words in context. In a discourse , we are interested in knowing if the semantic link between two items is a by-product of textual coherence or is irrelevant. We first set up a human annotation of semantic links with or without contex-tual information to show the importance of the textual context in evaluating the relevance of semantic similarity, and to assess the prevalence of actual semantic relations between word tokens. We then built an experiment to automatically predict this relevance , evaluated on the reliable reference data set which was the outcome of the first annotation. We show that in-document information greatly improve the prediction made by the similarity level alone.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the work presented in this paper is to improve distributional thesauri, and to help evalu- ate the content of such resources. A distributional thesaurus is a lexical network that lists semantic neighbours, computed from a corpus and a simi- larity measure between lexical items, which gen- erally captures the similarity of contexts in which the items occur. This way of building a seman- tic network has been very popular since <ref type="bibr" target="#b19">(Grefenstette, 1994;</ref><ref type="bibr" target="#b22">Lin, 1998)</ref>, even though the nature of the information it contains is hard to define, and its evaluation is far from obvious. A distributional thesaurus includes a lot of "noise" from a seman- tic point of view, but also lists relevant lexical pairs that escape classical lexical relations such as syn- onymy or hypernymy.</p><p>There is a classical dichotomy when evaluat- ing NLP components between extrinsic and in- trinsic evaluations <ref type="bibr" target="#b21">(Jones, 1994)</ref>, and this applies to distributional thesauri <ref type="bibr" target="#b11">(Curran, 2004;</ref><ref type="bibr" target="#b28">Poibeau and Messiant, 2008)</ref>. Extrinsic evaluations mea- sure the capacity of a system in which a resource or a component to evaluate has been used, for in- stance in this case information retrieval (van der <ref type="bibr" target="#b32">Plas, 2008)</ref> or word sense disambiguation <ref type="bibr" target="#b33">(Weeds and Weir, 2005</ref>). Intrinsic evaluations try to mea- sure the resource itself with respect to some hu- man standard or judgment, for instance by com- paring a distributional resource with respect to an existing synonym dictionary or similarity judg- ment produced by human subjects <ref type="bibr" target="#b26">(Pado and Lapata, 2007;</ref><ref type="bibr" target="#b3">Baroni and Lenci, 2010)</ref>. The short- comings of these methods have been underlined in ( <ref type="bibr" target="#b4">Baroni and Lenci, 2011)</ref>. Lexical resources designed for other objectives put the spotlight on specific areas of the distributional thesaurus. They are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities, which exceeds the lim- its of classical lexical relations, even though re- searchers have tried to collect equivalent resources manually, to be used as a gold standard <ref type="bibr" target="#b34">(Weeds, 2003;</ref><ref type="bibr" target="#b5">Bordag, 2008;</ref><ref type="bibr" target="#b2">Anguiano et al., 2011</ref>). One advantage of distributional similarities is to exhibit a lot of different semantic relations, not necessar- ily standard lexical relations. Even with respect to established lexical resources, distributional ap- proaches may improve coverage, complicating the evaluation even more.</p><p>The method we propose here has been de- signed as an intrinsic evaluation with a view to validate semantic proximity links in a broad per-spective, to cover what ( <ref type="bibr" target="#b25">Morris and Hirst, 2004)</ref> call "non classical lexical semantic relations". For instance, agentive relations (author/publish, author/publication) or associative relations (ac- tor/cinema) should be considered. At the same time, we want to filter associations that can be considered as accidental in a semantic perspective (e.g. flag and composer are similar because they appear a lot with nationality names). We do this by judging the relevance of a lexical relation in a context where both elements of a lexical pair oc- cur. We show not only that this improves the relia- bility of human judgments, but also that it gives a framework where this relevance can be predicted automatically. We hypothetize that evaluating and filtering semantic relations in texts where lexical items occur would help tasks that naturally make use of semantic similarity relations, but assessing this goes beyond the present work.</p><p>In the rest of this paper, we describe the re- source we used as a case study, and the data we collected to evaluate its content (section 2). We present the experiments we set up to automatically filter semantic relations in context, with various groups of features that take into account informa- tion from the corpus used to build the thesaurus and contextual information related to occurrences of semantic neighbours 3). Finally we discuss some related work on the evaluation and improve- ment of distributional resources (section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Evaluation of lexical similarity in context 2.1 Data</head><p>We use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in <ref type="bibr" target="#b6">(Bourigault, 2002</ref>) from a structured model ( <ref type="bibr" target="#b3">Baroni and Lenci, 2010)</ref>, i.e. using syntactic con- texts. In this approach, contexts are triples (gover- nor,relation,dependent) derived from syntactic de- pendency structures. Governors and dependents are verbs, adjectives and nouns. Multiword units are available, but they form a very small subset of the resulting neighbours. Base elements in the thesaurus are of two types: arguments (depen- dents' lemma) and predicates (governor+relation). This is to keep the predicate/argument distinction since similarities will be computed between pred- icate pairs or argument pairs, and a lexical item can appear in many predicates and as an argument (e.g. interest as argument, interest for as one pred- icate). The similarity of distributions was com- puted with Lin's score <ref type="bibr" target="#b22">(Lin, 1998</ref>). We will talk of lexical neighbours or distribu- tional neighbours to label pairs of predicates or ar- guments, and in the rest of the paper we consider only lexical pairs with a Lin score of at least 0.1, which means about 1.4M pairs. This somewhat arbitrary level is an a priori threshold to limit the resulting database, and it is conservative enough not to exclude potential interesting relations. The distribution of scores is given <ref type="figure" target="#fig_0">figure 1</ref>; 97% of the selected pairs have a score between 0.1 and 0.29. To ease the use of lexical neighbours in our ex- periments, we merged together predicates that in- clude the same lexical unit, a posteriori. Thus there is no need for a syntactic analysis of the con- text considered when exploiting the resource, and sparsity is less of an issue 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Annotation</head><p>In order to evaluate the resource, we set up an an- notation in context: pairs of lexical items are to be judged in their context of use, in texts where they occur together. To verify that this method- ology is useful, we did a preliminary annotation to contrast judgment on lexical pairs with or with- out this contextual information. Then we made a larger annotation in context once we were assured of the reliability of the methodology.</p><p>For the preliminary test, we asked three annota- tors to judge the similarity of pairs of lexical items without any context (no-context), and to judge the <ref type="bibr">[...]</ref> Le ventre de l'impala de même que ses l` evres et sa queue sont blancs. Il faut aussi mentionner leurs lignes noires uniques ` a chaque individu au bout des oreilles , sur le dos de la queue et sur le front. Ces lignes noires sont très utiles aux impalas puisque ce sont des signes qui leur permettent de se reconnaitre entre eux. Ils possèdent aussi des glandes sécrétant des odeurs sur les pattesarrì eres et sur le front. Ces odeurs permettentégalementpermettent´permettentégalement aux individus de se reconnaitre entre eux. Il a ´ egalement des coussinets noirs situés, ` a larrì ere de ses pattes . Les impalas mâles et femelles ont une morphologie différente. En effet, on peut facilement distinguer un mâle par ses cornes en forme de S qui mesurent de 40à40`40à 90 cm de long.</p><p>Les impalas vivent dans les savanes o` u l' herbe (courte ou moyenne) abonde. Bien qu'ils apprécient la proximité d'une source d'eau, celle-ci n'est généralement pas essentielle aux impalas puisqu'ils peuvent se satisfaire de l'eau contenue dans l' herbe qu'ils consomment. Leur environnement est relativement peu accidenté et n'est composé que d' herbes , de buissons ainsi que de quelques arbres.</p><p>[...] <ref type="figure">Figure 2</ref>: Example excerpt during the annotation of lexical pairs: annotators focus on a target item (here corne, horn, in blue) and must judge yellow words (pending: oreille/queue, ear/tail), either validating their relevance (green words: pattes, legs) or rejecting them (red words: herbe, grass). The text describes the morphology of the impala, and its habitat. similarity of pairs presented within a paragraph where they both occur (in context). The three an- notators were linguists, and two of them <ref type="formula">(1 and</ref> 3) knew about the resource and how it was built. For each annotation, 100 pairs were randomly se- lected, with the following constraints:</p><p>• for the no-context annotation, candidate pairs had a Lin score above 0.2, which placed them in the top 14% of lexical neighbours with re- spect to the similarity level.</p><p>• for the in context annotation, the only con- straint was that the pairs occur in the same paragraph somewhere in the corpus used to build the resource. The example paragraph was chosen at random.</p><p>The guidelines given in both cases were the same: "Do you think the two words are seman- tically close ? In other words, is there a seman- tic relation between them, either classical (syn- onymy, hypernymy, co-hyponymy, meronymy, co- meronymy) or not (the relation can be paraphrased but does not belong to the previous cases) ?"</p><p>For the pre-test, agreement was rather moderate without context (the average of pairwise kappas was .46), and much better with a context (aver- age = .68), with agreement rates above 90%. This seems to validate the feasability of a reliable anno- tation of relatedness in context, so we went on for a larger annotation with two of the previous anno- tators.</p><p>For the larger annotation, the protocol was slightly changed: two annotators were given 42 full texts from the original corpus where lexical neighbours occurred. They were asked to judge the relation between two items types, regardless of the number of occurrences in the text. This time there was no filtering of the lexical pairs beyond the 0.1 threshold of the original resource. We fol- lowed the well-known postulate ( <ref type="bibr" target="#b18">Gale et al., 1992</ref>) that all occurrences of a word in the same dis- course tend to have the same sense ("one sense per discourse"), in order to decrease the annotator workload. We also assumed that the relation be- tween these items remain stable within the docu- ment, an arguably strong hypothesis that needed to be checked against inter-annotator agreement be- fore beginning the final annotation . It turns out that the kappa score (0.80) shows a better inter- annotator agreement than during the preliminary test, which can be explained by the larger context given to the annotator (the whole text), and thus more occurrences of each element in the pair to judge, and also because the annotators were more experienced after the preliminary test. Agreement measures are summed-up table 1. An excerpt of an example text, as it was presented to the annotators, is shown figure 2.</p><p>Overall, it took only a few days to annotate 9885 pairs of lexical items. Among the pairs that were presented to the annotators, about 11% were judged as relevant by the annotators. It is not easy to decide if the non-relevant pairs are just noise, or context-dependent associations that were not present in the actual text considered (for pol- ysemy reasons for instance), or just low-level as- sociations. An important aspect is thus to guar- antee that there is a correlation between the sim-   <ref type="table">Table 1</ref>: Inter-annotator agreements with Cohen's Kappa for contextual and non-contextual annotations. N1, N2, N3 were annotators during the pre-test; expert annotation was made on a different dataset from the same corpus, only with the full discourse context.</p><p>ilarity score (Lin's score here), and the evaluated relevance of the neighbour pairs. Pearson corre- lation factor shows that Lin score is indeed sig- nificantly correlated to the annotated relevance of lexical pairs, albeit not strongly (r = 0.159).</p><p>The produced annotation 2 can be used as a ref- erence to explore various aspects of distributional resources, with the caveat that it is as such a bit dependent on the particular resource used. We nonetheless assume that some of the relevant pairs would appear in other thesauri, or would be of in- terest in an evaluation of another resource.</p><p>The first thing we can analyse from the anno- tated data is the impact of a threshold on Lin's score to select relevant lexical pairs. The resource itself is built by choosing a cut-off which is sup- posed to keep pairs with a satisfactory similar- ity, but this threshold is rather arbitrary. <ref type="figure" target="#fig_2">Figure  3</ref> shows the influence of the threshold value to se- lect relevant pairs, when considering precision and recall of the pairs that are kept when choosing the threshold, evaluated against the human annotation of relevance in context. In case one wants to opti- mize the F-score (the harmonic mean of precision and recall) when extracting relevant pairs, we can see that the optimal point is at .24 for a threshold of .22 on Lin's score. This can be considered as a baseline for extraction of relevant lexical pairs, to which we turn in the following section. dataset to set up a supervised classification exper- iment in order to automatically predict the rele- vance of a semantic link in a given discourse. We present now the list of features that were used for the model. They can be divided in three groups, according to their origin: they are computed from the whole corpus, gathered from the distributional resource, or extracted from the considered text which contains the semantic pair to be evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Features</head><p>For each pair neighbour a /neighbour b , we com- puted a set of features from Wikipedia (the corpus used to derive the distributional similarity): We first computed the frequencies of each item in the corpus, f req a and f req b , from which we derive</p><p>• f req min , f req max : the min and max of f req a and f req b ;</p><p>• f req × : the combination of the two, or log(f req a × f req b )</p><p>We also measured the syntagmatic association of neighbour a and neighbour b , with a mutual infor- mation measure <ref type="bibr" target="#b9">(Church and Hanks, 1990)</ref>, com- puted from the cooccurrence of two tokens within the same paragraph in Wikipedia. This is a rather large window, and thus gives a good coverage with respect to the neighbour database (70% of all pairs). A straightforward parameter to include to pre- dict the relevance of a link is of course the simi- larity measure itself, here Lin's information mea- sure. But this can be complemented by additional information on the similarity of the neighbours, namely:</p><p>• each neighbour productivity : prod a and prod b are defined as the numbers of neighbours of respectively neighbour a and neighbour b in the database (thus related to- kens with a similarity above the threshold), from which we derive three features as for frequencies: the min, the max, and the log of the product. The idea is that neighbours whith very high productivity give rise to less reliable relations.</p><p>• the ranks of tokens in other related items neighbours: rank a−b is defined as the rank of neighbour a among neighbours of neighbour b ordered with respect to Lin's score; rang b−a is defined similarly and again we consider as features the min, max and log-product of these ranks.</p><p>We add two categorial features, of a more linguis- tic nature:</p><p>• cats is the pair of part-of-speech for the re- lated items, e.g. to distinguish the relevance of NN or VV pairs.</p><p>• predarg is related to the predicate/argument distinction: are the related items predicates or arguments ?</p><p>The last set of features derive from the occur- rences of related tokens in the considered dis- courses:</p><p>First, we take into account the frequencies of items within the text, with three features as before: the min of the frequencies of the two related items, the max, and the log-product. Then we consider a tf·idf ( <ref type="bibr" target="#b30">Salton et al., 1975</ref>) measure, to evaluate the specificity and arguably the importance of a word Feature Description</p><formula xml:id="formula_0">f req min min(f req a , f req b ) f req max max(f req a , f req b ) f req × log(f req a × f req b ) im im = log P (a,b) P (a)·P (b) lin Lin's score rank min min(rank a−b , rank b−a ) rank max max(rank a−b , rank b−a ) rank × log(rank a−b × rank b−a ) prod min min(prod a , prod b ) prod max max(prod a , prod b ) prod × log(prod a × prod b ) cats</formula><p>neighbour pos pair predarg predicate or argument</p><formula xml:id="formula_1">f reqtxt min min(f reqtxt a , f reqtxt b ) f reqtxt max max(f reqtxt a , f reqtxt b ) f reqtxt × log(f reqtxt a × f reqstxt b ) tf·ipf tf·ipf (neighbour a )×tf·ipf (neighbour b ) copr ph</formula><p>copresence in a sentence copr para copresence in a paragraph sd smallest distance between neighbour a and neighbour b gd highest distance between neighbour a and neighbour b ad average distance between neighbour a and neighbour b prodtxt min min(prod a , prod b ) prodtxt max max(prod a , prod b ) prodtxt × log(prod a × prod b ) cc belong to the same lexical connected component in a document or within a document. Several vari- ants of tf·idf have been proposed to adapt the mea- sure to more local areas in a text with respect to the whole document. For instance ( <ref type="bibr" target="#b12">Dias et al., 2007)</ref> propose a tf·isf (term frequency · inverse sentence frequency), for topic segmentation. We similarly defined a tf·ipf measure based on the frequency of a word within a paragraph with respect to its fre- quency within the text. The resulting feature we used is the product of this measure for neighbour a and neighbour b . A few other contextual features are included in the model: the distances between pairs of related items, instantiated as:</p><p>• distance in words between occurrences of re- lated word types:</p><p>-minimal distance between two occur- rences (sd) -maximal distance between two occur- rences (gd) -average distance (ad) ;</p><p>• boolean features indicating whether neighbour a and neighbour b appear in the same sentence (copr s ) or the same paragraph (copr para ).</p><p>Finally, we took into account the network of re- lated lexical items, by considering the largest sets of words present in the text and connected in the database (self-connected components), by adding the following features:</p><p>• the degree of each lemma, seen as a node in this similarity graph, combined as above in minimal degree of the pair, maximal de- gree, and product of degrees (prodtxt min , prodtxt max , prodtxt × ). This is the number of pairs (present in the text) where a lemma appears in.</p><p>• a boolean feature cc saying whether a lexi- cal pair belongs to a connected component of the text, except the largest. This reflects the fact that a small component may concern a lexical field which is more specific and thus more relevant to the text. <ref type="figure" target="#fig_3">Figure 4</ref> shows examples of self-connected components in an excerpt of the page on Go- rille (gorilla), e.g. the set {pelage, dos, four- rure} (coat, back, fur).</p><p>The last feature is probably not entirely indepen- dent from the productivity of an item, or from the tf.ipf measure. <ref type="table" target="#tab_1">Table 2</ref> sums up the features used in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model</head><p>Our task is to identify relevant similarities between lexical items, between all possible related pairs, and we want to train an inductive model, a clas- sifier, to extract the relevant links. We have seen that the relevant/not relevant classification is very imbalanced, biased towards the "not relevant" cat- egory (about 11%/89%), so we applied methods dedicated to counter-balance this, and will focus on the precision and recall of the predicted rele- vant links. Following a classical methodology, we made a 10-fold cross-validation to evaluate robustly the performance of the classifiers. We tested a few popular machine learning methods, and report on two of them, a naive bayes model and the best method on our dataset, the Random Forest clas- sifier <ref type="bibr" target="#b7">(Breiman, 2001</ref>). Other popular methods (maximum entropy, SVM) have shown slightly in- ferior combined F-score, even though precision and recall might yield more important variations. As a baseline, we can also consider a simple threshold on the lexical similarity score, in our case Lin's measure, which we have shown to yield the best F-score of 24% when set at 0.22.</p><p>To address class imbalance, two broad types of methods can be applied to help the model focus on the minority class. The first one is to resam- ple the training data to balance the two classes, the second one is to penalize differently the two classes during training when the model makes a mistake (a mistake on the minority class being made more costly than on the majority class). We tested the two strategies, by applying the classical Smote method of ( <ref type="bibr" target="#b8">Chawla et al., 2002</ref>) as a kind of resampling, and the ensemble method Meta- Cost of <ref type="bibr" target="#b13">(Domingos, 1999</ref>) as a cost-aware learn- ing method. Smote synthetizes and adds new in- stances similar to the minority class instances and is more efficient than a mere resampling. Meta- Cost is an interesting meta-learner that can use any classifier as a base classifier. We used Weka's implementations of these methods <ref type="bibr" target="#b16">(Frank et al., 2004</ref>), and our experiments and comparisons are thus easily replicated on our dataset, provided with this paper, even though they can be improved by Le gorille est après le bonobo et le chimpanzé , du point de vue génétique , l' animal le plus proche de l' humain . Cette parenté a ´ eté confirmée par les similitudes entre les chromosomes et les groupes sanguins . Notre génome ne diffère que de 2 % de celui du gorille . Redressés , les gorilles atteignent une taille de 1,75 m` etre , mais ils sont en fait un peu plus grands car ils ont les genoux fléchis . L' envergure des bras dépasse la longueur du corps et peut atteindre 2,75 m` etres . Il existe une grande différence de masse entre les sexes : les femelles p` esent de 90à90`90à 150 kilogrammes et les mâles jusqu' ` a 275. En captivité ,particulì erement bien nourris , ils atteignent 350 kilogrammes . Le pelage dépend du sexe et de l' ˆ age . Chez les mâles les plusâgésplusˆplusâgés se développe sur le dos une fourrure gris argenté , d' o` u leur nom de "dos argentés" . Le pelage des gorilles de montagne est particulì erement long et soyeux . Comme tous les anthropodes , les gorilles sont dépourvus de queue . Leur anatomie est puissante , le visage et les oreilles sont glabres et ils présentent des torus supra-orbitaires marqués . refinements of these techniques. We chose the following settings for the different models: naive bayes uses a kernel density estimation for numer- ical features, as this generally improves perfor- mance. For Random Forests, we chose to have ten trees, and each decision is taken on a randomly chosen set of five features. For resampling, Smote advises to double the number of instances of the minority class, and we observed that a bigger re- sampling degrades performances. For cost-aware learning, a sensible choice is to invert the class ra- tio for the cost ratio, i.e. here the cost of a mistake on a relevant link (false negative) is exactly 8.5 times higher than the cost on a non-relevant link (false positive), as non-relevant instances are 8.5 times more present than relevant ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>We are interested in the precision and recall for the "relevant" class. If we take the best simple classifier (random forests), the precision and re- call are 68.1% and 24.2% for an F-score of 35.7%, and this is significantly beaten by the Naive Bayes method as precision and recall are more even (F- score of 41.5%). This is already a big improve- ment on the use of the similarity measure alone (24%). Also note that predicting every link as rel- evant would result in a 2.6% precision, and thus a 5% F-score. The random forest model is signifi- cantly improved by the balancing techniques: the overall best F-score of 46.3% is reached with Ran- dom Forests and the cost-aware learning method. <ref type="table" target="#tab_3">Table 3</ref> sums up the scores for the different con- figurations, with precision, recall, F-score and the confidence interval on the F-score. We analysed the learning curve by doing a cross-validation on reduced set of instances (from 10% to 90%); F1- scores range from 37.3% with 10% of instances and stabilize at 80%, with small increment in ev- ery case.</p><p>The filtering approach we propose seems to yield good results, by augmenting the similarity built on the whole corpus with signals from the lo- cal contexts and documents where related lexical items appear together.</p><p>To try to analyse the role of each set of fea- tures, we repeated the experiment but changed the set of features used during training, and results are shown table 4 for the best method (RF with cost- aware learning).</p><p>We can see that similarity-related features (mea- sures, ranks) have the biggest impact, but the other ones also seem to play a significant role. We can draw the tentative conclusion that the quality of distributional relations depends on the contextual- izing of the related lexical items, beyond just the similarity score and the ranks of items as neigh- bours of other items.   <ref type="table">Table 4</ref>: Impact of each group of features on the best scores (%) : the lowest the results, the bigger the impact of the removed group of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>Our work is related to two issues: evaluating dis- tributional resources, and improving them. Eval- uating distributional resources is the subject of a lot of methodological reflection <ref type="bibr" target="#b29">(Sahlgren, 2006</ref>), and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evalua- tions. In extrinsic evaluations, models are evalu- ated against benchmarks focusing on a single task or a single aspect of a resource: either discrimina- tive, TOEFL-like tests ( <ref type="bibr" target="#b17">Freitag et al., 2005</ref>), anal- ogy production <ref type="bibr" target="#b31">(Turney, 2008)</ref>, or synonym selec- tion <ref type="bibr" target="#b34">(Weeds, 2003;</ref><ref type="bibr" target="#b2">Anguiano et al., 2011;</ref><ref type="bibr" target="#b14">Ferret, 2013;</ref><ref type="bibr" target="#b10">Curran and Moens, 2002</ref>). In intrin- sic evaluations, associations norms are used, such as the 353 word-similarity dataset ( <ref type="bibr" target="#b15">Finkelstein et al., 2002</ref>), e.g. ( <ref type="bibr" target="#b26">Pado and Lapata, 2007;</ref><ref type="bibr" target="#b0">Agirre et al., 2009)</ref>, or specifically designed test cases, as in ( <ref type="bibr" target="#b4">Baroni and Lenci, 2011</ref>). We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an "existential" view of semantic relatedness.</p><p>As for improving distributional thesauri, out- side of numerous alternate approaches to the construction, there is a body of work focusing on improving an existing resource, for instance reweighting context features once an initial the- saurus is built <ref type="bibr" target="#b35">(Zhitomirsky-Geffet and Dagan, 2009)</ref>, or post-processing the resource to filter bad neighbours or re-ranking neighbours of a given target <ref type="bibr" target="#b14">(Ferret, 2013)</ref>. They still use "essential" evaluation measures (mostly synonym extraction), although the latter comes close to our work since it also trains a model to detect (intrinsically) bad neighbours by using example sentences with the words to discriminate. We are not aware of any work that would try to evaluate differently seman- tic neighbours according to the context they ap- pear in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a method to reliably evaluate distri- butional semantic similarity in a broad sense by considering the validation of lexical pairs in con- texts where they both appear. This helps cover non classical semantic relations which are hard to eval- uate with classical resources. We also presented a supervised learning model which combines global features from the corpus used to built a distribu- tional thesaurus and local features from the text where similarities are to be judged as relevant or not to the coherence of a document. It seems from these experiments that the quality of distri- butional relations depends on the contextualizing of the related lexical items, beyond just the simi-larity score and the ranks of items as neighbours of other items. This can hopefully help filter out lex- ical pairs when word lexical similarity is used as an information source where context is important: lexical disambiguation ( <ref type="bibr" target="#b24">Miller et al., 2012</ref>), topic segmentation ( <ref type="bibr" target="#b20">Guinaudeau et al., 2012)</ref>. This can also be a preprocessing step when looking for sim- ilarities at higher levels, for instance at the sen- tence level ( <ref type="bibr" target="#b23">Mihalcea et al., 2006</ref>) or other macro- textual level <ref type="bibr" target="#b1">(Agirre et al., 2013)</ref>, since these are always aggregation functions of word similarities. There are limits to what is presented here: we need to evaluate the importance of the level of noise in the distributional neighbours database, or at least the quantity of non-semantic relations present, and this depends on the way the database is built. Our starting corpus is relatively small compared to cur- rent efforts in this framework. We are confident that the same methodology can be followed, even though the quantitative results may vary, since it is independent of the particular distributional the- saurus we used, and the way the similarities are computed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Histogram of Lin scores for pairs considered.</figDesc><graphic url="image-1.png" coords="2,307.28,249.67,240.09,169.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Annotators</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Precision and recall on relevant links with respect to a threshold on the similarity measure (Lin's score)</figDesc><graphic url="image-2.png" coords="4,307.28,243.07,196.43,138.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A few connected lexical components of the similarity graph, projected on a text, each in a different color. The groups are, in order of appearance of the first element: {genetic, close, human}, {similarity, kinship}, {chromosome, genome}, {male, female}, {coat, back, fur}, {age/N, aged/A}, {ear, tail, face}. The text describes the gorilla species, more particularly its morphology. Gray words are other lexical elements in the neighbour database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Summary of features used in the super- vised model, with respect to two lexical items a and b. The first group is corpus related, the second group is related to the distributional database, the third group is related to the textual context. Freq is related to the frequencies in the corpus, Freqtext the frequencies in the considered text.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Classification scores (%) on the relevant class. CI is the confidence interval on the F-score (RF 
= Random Forest, NB= naive bayes). 

Features 
Prec. Recall F-score 

all 
40.4 
54.3 
46.3 
all − corpus feat. 
37.4 
52.8 
43.8 
all − similarity feat. 
36.1 
49.5 
41.8 
all − contextual feat. 36.5 
54.8 
43.8 

</table></figure>

			<note place="foot" n="1"> Whenever two predicates with the same lemma have common neighbours, we average the score of the pairs.</note>

			<note place="foot" n="3"> Experiments: predicting relevance in context The outcome of the contextual annotation presented above is a rather sizeable dataset of validated semantic links, and we showed these linguistic judgments to be reliable. We used this 2 Freely available here http://www.irit.fr/ ˜ Philippe.Muller/resources.html.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study on similarity and relatedness using distributional and wordnetbased approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravalova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pas¸capas¸ca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">*sem 2013 shared task: Semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalezagirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity</title>
		<meeting>the Main Conference and the Shared Task: Semantic Textual Similarity<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="32" to="43" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FreDist: Automatic construction of distributional thesauri for French</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Anguiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Actes de la 18eme conférence sur le traitement automatique des langues naturelles</title>
		<meeting>s de la 18eme conférence sur le traitement automatique des langues naturelles</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="119" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributional memory: A general framework for corpus-based semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="673" to="721" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">How we BLESSed distributional semantic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GEMS 2011</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A comparison of co-occurrence and similarity measures as simulations of context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bordag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>Alexander F. Gelbukh</editor>
		<imprint>
			<biblScope unit="volume">4919</biblScope>
			<biblScope unit="page" from="52" to="63" />
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">UPERY : un outil d&apos;analyse distributionnelle tendue pour la construction d&apos;ontologies partir de corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bourigault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Actes de la 9e confrence sur le Traitement Automatique de la Langue Naturelle</title>
		<meeting>s de la 9e confrence sur le Traitement Automatique de la Langue Naturelle<address><addrLine>Nancy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Random forests. Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Smote: Synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res. (JAIR)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improvements in automatic thesaurus extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition</title>
		<meeting>the ACL-02 Workshop on Unsupervised Lexical Acquisition</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">From distributional to semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Topic segmentation algorithms for text summarization and passage retrieval: an exhaustive evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elsa</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José Gabriel Pereira</forename><surname>Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd national conference on Artificial intelligence</title>
		<meeting>the 22nd national conference on Artificial intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1334" to="1339" />
		</imprint>
	</monogr>
	<note>AAAI&apos;07</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Metacost: A general method for making classifiers cost-sensitive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
		<editor>Usama M. Fayyad, Surajit Chaudhuri, and David Madigan, editors, KDD</editor>
		<imprint>
			<date type="published" when="1999" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identifying bad semantic neighbors for improving distributional thesauri</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Ferret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="561" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Placing search in context: the concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Weka 3.3: Data mining software in java</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Len</forename><surname>Trigg</surname></persName>
		</author>
		<ptr target="www.cs.waikato.ac.nz/ml/weka/" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">New experiments in distributional representations of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayne</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Blume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Byrnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmond</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadik</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Rohwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">One sense per discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th DARPA Speech and Natural Language Workshop</title>
		<meeting>the 4th DARPA Speech and Natural Language Workshop<address><addrLine>New-York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="233" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Explorations in automatic thesaurus discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grefenstette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Kluwer Academic Pub</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhancing lexical cohesion measure with confidence measures, semantic relations and language model interpolation for multimedia spoken content topic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Guinaudeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Sébillot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="90" to="104" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards better NLP system evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Sparck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jones</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference</title>
		<meeting>the Human Language Technology Conference</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="102" to="107" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An information-theoretic definition of similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Machine Learning</title>
		<meeting>the 15th International Conference on Machine Learning<address><addrLine>Madison</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="296" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Corpus-based and knowledge-based measures of text semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st national conference on Artificial intelligence</title>
		<meeting>the 21st national conference on Artificial intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page" from="775" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using distributional similarity for lexical expansion in knowledge-based word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2012 Organizing Committee</title>
		<meeting><address><addrLine>Mumbai, India, December</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1781" to="1796" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING 2012</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Non-classical lexical semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the HLT Workshop on Computational Lexical Semantics</title>
		<meeting>the HLT Workshop on Computational Lexical Semantics<address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="46" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Pado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dependency-based construction of semantic space models</title>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Do we still Need Gold Standards for Evaluation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Poibeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Messiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Language Resource and Evaluation Conference</title>
		<meeting>the Language Resource and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards pertinent evaluation methodologies for word-space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Language Resources and Evaluation</title>
		<meeting>the 5th International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A theory of term importance in automatic text analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="44" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A uniform approach to analogies, synonyms, antonyms, and associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="905" to="912" />
		</imprint>
	</monogr>
	<note>COLING &apos;08</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Automatic Lexico-Semantic Acquisition for Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Plas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>University of Groningen</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Co-occurrence retrieval: A flexible framework for lexical distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="439" to="475" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Measures and Applications of Lexical Distributional Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><forename type="middle">Elizabeth</forename><surname>Weeds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>University of Sussex</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bootstrapping distributional feature vector quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Zhitomirsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Geffet</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="435" to="461" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
