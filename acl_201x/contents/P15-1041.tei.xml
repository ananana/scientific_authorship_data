<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Adapt Credible Knowledge in Cross-lingual Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Kong Hong Kong Polytechnic University Shenzhen Research Institute</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Kong Hong Kong Polytechnic University Shenzhen Research Institute</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Lei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Kong Hong Kong Polytechnic University Shenzhen Research Institute</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xule</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxiang</forename><surname>He</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">The State Key Lab of Software Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Adapt Credible Knowledge in Cross-lingual Sentiment Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="419" to="429"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Cross-lingual sentiment analysis is a task of identifying sentiment polarities of texts in a low-resource language by using sentiment knowledge in a resource-abundant language. While most existing approaches are driven by transfer learning, their performance does not reach to a promising level due to the transferred errors. In this paper, we propose to integrate into knowledge transfer a knowledge validation model , which aims to prevent the negative influence from the wrong knowledge by distinguishing highly credible knowledge. Experiment results demonstrate the necessity and effectiveness of the model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the wide range of business value, sentiment analysis has drawn increasing attention in the past years. The extensive research and development efforts produce a variety of reliable sentiment resources for English, one of the most popular language in the world. These available rich resources become the treasure of knowledge to help conduct or enhance sentiment analysis in the other languages, which is a task known as cross-lingual sentiment analysis (CLSA). In the literature of CSLA, the language with abundant reliable resources is called the source language (e.g., English), while the low-resource language is referred to as the target language (e.g., Chinese). However, in this paper, the situation is a low resource language scenario, where the source language is English, and the target language is Chinese.</p><p>The main idea of existing CLSA researches is to first build up the connection between the source and target languages to overcome the language barrier, and then develop an appropriate knowl- edge transfer approach to leverage the annotated data from the source language to train a sentiment classification model in the target language, either supervised or semi-supervised. In particular, these approaches exploit and convert the knowledge learned from the source language to automatically generate and expand the pseudo-training data for the target language.</p><p>The machine translation (MT) service is one of the most common ways used to build the language connection <ref type="bibr" target="#b29">(Wan, 2008;</ref><ref type="bibr" target="#b0">Banea et al., 2008;</ref><ref type="bibr" target="#b30">Wan, 2009;</ref><ref type="bibr" target="#b32">Wei and Pal, 2010;</ref><ref type="bibr" target="#b8">Gui et al., 2014</ref>). Although it is claimed in <ref type="bibr" target="#b4">Duh et al. (2011)</ref> that the MT service is ripe for CLSA, the imperfect MT quality hinders existing MT- based CLSA approaches from the further advance. In our preliminary study, we find that even the Google translator 1 (i.e., one of the most widely used online MT service <ref type="bibr" target="#b25">(Shankland 2013)</ref>) may unavoidably changes the sentiment polarity of the translated text, as illustrated below, with a percentage of around 10%.</p><p>[Original English Text]: I am at home on bed rest and desperate for something good to read. <ref type="bibr">[Sentiment Label: Negative]</ref> [Translated Chinese Text]: ·3[¹K&gt;E Úý"ÀÜéÐw" {Meaning: I am in bed to rest at home and feel that desperate things are also good to read.} <ref type="bibr">[Sentiment Label: Positive]</ref> The noisy data generated by MT errors for sure will weaken the contribution of the transferred knowledge and even worse may create conflicting knowledge. While it is a critical step in CLSA to localize the sentiment knowledge learned from the source language in the target language, to the best of our knowledge, hardly any previous research has focused on knowledge validation to filter out the noisy knowledge having sentiment changes caused by wrong translations during knowledge transfer.</p><p>To reduce the noisy sentiment knowledge intro- duced into the target language, we are motivated to validate the knowledge transferred from the source language by checking its linguistic distributions and sentiment polarity consistency with the known knowledge in the target language. Different from previous co-training based approaches where two language views recommend knowledge to each other in the same manner, we consider the source language as the "supervisor" and the target language as the "learner". The "supervisor" boosts itself with its own accumulated labeled data (called knowledge) and meanwhile recommends its confident knowledge to the "learner". The "learner" tries to select trustworthy knowledge based on the recommendation to update and expand its training data. Adding a process to efficiently filter out noisy knowledge and retain the self-adaptive and interested new knowledge makes the subsequent boosting process more credible. This is why our approach can outperform state-of- the-art CLSA approaches.</p><p>The rest of this paper is organized as follows. Section 2 summarizes the related work. Section 3 explains the proposed model. Section 4 presents experimental results. Finally, Section 5 concludes the paper and suggests future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sentiment Analysis</head><p>Sentiment has been analyzed in different language granularity, e.g., entity, aspect, sentence and document. This paper focuses on sentiment analysis of online product reviews in the document level.</p><p>Existing approaches are generally categorized into lexicon-based and machine learning based approaches ( <ref type="bibr" target="#b16">Liu, 2012)</ref>. Lexicon-based approach- es highly depend on sentiment lexicons. Turney (2002) derives the overall phrase and document sentiment scores by averaging the sentiment scores provided in a lexicon over the words included. Similar idea is adopted in <ref type="bibr" target="#b12">(Hiroshi et al., 2004;</ref><ref type="bibr" target="#b13">Kennedy and Inkpen, 2006</ref>). Machine learning based approaches, on the other hand, apply classification models. The task-specific features are designed to train sentiment polarity classifiers. <ref type="bibr" target="#b20">Pang et al. (2002)</ref> compare the performance of NB, SVM and ME on movie reviews. SVM is found more effective. <ref type="bibr" target="#b6">Gamon (2004)</ref> shows that SVM with deep linguistic features can further improve the performance. A variety of other machine learning approaches are also proposed to sentiment classification ( <ref type="bibr" target="#b18">Mullen and Collier, 2004;</ref><ref type="bibr" target="#b22">Read, 2005;</ref><ref type="bibr" target="#b9">Hassan and Radev, 2010;</ref><ref type="bibr" target="#b26">Socher et al., 2013)</ref>.</p><p>Cross-domain sentiment classification (CDSC) shares certain common characteristics with cross- lingual sentiment classification (CLSC) ( <ref type="bibr" target="#b27">Tan et al., 2007;</ref><ref type="bibr" target="#b15">Li et al., 2009;</ref><ref type="bibr" target="#b19">Pan and Yang, 2010;</ref><ref type="bibr" target="#b10">He et al., 2011a;</ref><ref type="bibr" target="#b7">Glorot et al., 2011)</ref>. Notice that the gap between source domain and target domain is the main difference between CDSC and CLSC. CLSC copes with two different datasets in two different languages. This difference makes CLSC a new challenge, drawing specific attention to researcher recently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross-lingual Sentiment Analysis</head><p>There are two alternative solutions to cross-lingual sentiment analysis. One is ensemble learning that combines multiple classifiers. The other is transfer learning that develops strategies to adapt the knowledge from one language to the other. <ref type="bibr" target="#b29">Wan (2008)</ref> is among the pioneers to develop the ensemble learning solutions, where multiple classifiers learned from different training datasets including those in original languages and trans- lated languages are combined by voting. Most researches, on the other hand, explore transfer learning and focus on knowledge adaptation. For example, Wan (2009) applies a supervised co- training framework to iteratively adapt knowledge learned from the two languages by transferring translated texts to each other. Other similar work includes <ref type="bibr" target="#b32">(Wei and Pal, 2010)</ref> and <ref type="bibr" target="#b12">(He, 2011b)</ref>. All these approaches rely on MT to build language connection.</p><p>Meanwhile, the unlabeled parallel data is also employed to fill the gap between two languages. To solve the feature coverage problem with the EM algorithm, <ref type="bibr" target="#b17">Meng et al. (2012)</ref> leverage the unlabeled parallel data to learn unseen sentiment words. Similarly, <ref type="bibr" target="#b21">Popat et al. (2013)</ref> use the unlabeled parallel data to cluster features in order to reduce the data sparsity problem. <ref type="bibr" target="#b17">Meng et al. (2012)</ref> and <ref type="bibr" target="#b21">Popat et al. (2013)</ref> also use the unlabeled parallel data to reduce the negative influence of the noisy and incorrect sentiment labels introduced by machine translation and knowledge transfer. However, the parallel data is also a scarce resource. Some existing transfer learning based CLSA methods have attempted to address the noisy knowledge problem caused by wrong labels by checking label consistency. For example, to filter out the unconfident labels in Chinese, the supervised learning method proposed by <ref type="bibr" target="#b33">(Xu et al., 2011</ref>) runs boosting in Chinese by checking consistency between the labels manually annotat- ed in English and predicted by Chinese classifiers on translated Chinese. The work in <ref type="bibr" target="#b8">(Gui et al., 2014</ref>) follows the same line although it considers knowledge transferring between two languages. On the contrary, the main focus of our work is to filter out the noisy knowledge having sentiment changes by wrong translations. Actually, both label consistency checking and linguistic distribu- tion checking are important. Any one alone cannot work well. In fact, both of them are considered as the knowledge validation in our work, though the later is our focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Credible Boosting Model</head><p>In this paper, we propose a knowledge validation approach to improve the effectiveness of knowl- edge transfer without directly using extra parallel data. Our target is to filter out the noisy senti- ment labels introduced by MT and the incorrect sentiment labels generated by imperfect classifier in the source language. Here, the knowledge is referred to as a collection of distributed document presentations with sentiment labels that have been verified to be robust in sentiment classification ( <ref type="bibr" target="#b14">Le and Mikolov, 2014)</ref>. A novel credible boosting model, namely CredBoost is proposed to apply transfer-supervised learning with an added self- validation mechanism to guarantee the knowledge transferred highly credible and self-adaptive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Description</head><p>In a standard cross-lingual sentiment analysis setting, the training data includes labeled English reviews</p><formula xml:id="formula_0">L EN = {(x len i , y i )} M i=1 and unlabeled Chinese reviews U CN = {x ucn j } N j=1</formula><p>, where x k i (k = l en or u cn ) represents review i and y i ∈ {−1, 1} is the sentiment label of review x l i . The test data is Chinese reviews</p><formula xml:id="formula_1">T CN = {x tcn s } S s=1 . We now introduce the unlabeled data into credBoost's setting. L EN is divided into two disjoint parts L T EN and L B EN , where L T EN for basic training and L B EN for self-boosting. We translate L EN into Chinese to obtain extra labeled Chinese pseudo-reviews L T rCN = {(x l cnT r i , y i )} M i=1 and U CN into English to obtain extra unlabeled English pseudo-reviews U T rEN = {x l enT r j } N j=1</formula><p>. Thereby, we obtain a pair of pseudo-parallel data</p><formula xml:id="formula_2">(U CN , U T rEN ).</formula><p>The task is to use L EN and U CN to train a Chinese classifier to predict sentiment polarity for the test data T CN . It is a standard transfer learning problem. We consider two language views, i.e., source language view D S and target language view D τ . D S boosts itself with the labeled English data and recommend translated knowledge to D τ , while D t selects self-adaptive ones to boost itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Framework of CredBoost</head><p>The CredBoost model involves two synchronously boosting views for two languages respectively. During training, one view acts as a "supervisor" that recommends and passes the knowledge to the other view. The same knowledge is also added into its own view for boosting by automatically updating the weights of the labeled data. The other view acts as a "learner" that receives the recommended knowledge and selects the best- suited new knowledge to learn.</p><p>As mentioned before, the knowledge trans- ferred through MT is not reliable. The source language view may also make wrong predictions and thus transfer the wrong knowledge to the target language even the translations are correct. Whether or not the "learner" can benefit from its "supervisor" and how much it benefits highly depends on the credibility and adaptiveness of the recommended knowledge accepted by the "learner". Knowledge validation is necessary to ensure the quality of learning. The objective of knowledge validation is to identify the new and acquired knowledge from recommendations. Both language views are iteratively trained until learning converges or reaches the iteration upper bound.</p><p>In the source language view, at iteration (t), the CredBoost model first uses L T (t) EN to train a basic classifier C T rEN respectively, by Formula (1) :</p><formula xml:id="formula_3">O (t) EN = {(x LB i , ˆ y LB i )} men i =1 T R (t) EN = {(x U T r i , ˆ y U T r i )} nen i=1</formula><p>( <ref type="formula">1)</ref> where</p><formula xml:id="formula_4">O (t)</formula><p>EN denotes the candidates to be added into the training data, and T R (t)</p><p>EN the knowledge to be recommended to the target language view. We use the source knowledge validation function</p><formula xml:id="formula_5">V S (O (t) EN ) to identify the acquired knowledge K (t)</formula><p>Ac learned in the previous learning process and the new knowledge K (t) N w fresh to the current knowledge system from O (t) EN . The importance of each training instance is updated according to the performance of prediction by Formula <ref type="formula" target="#formula_6">(2)</ref> :</p><formula xml:id="formula_6">ω Ac i =    e (t) · ν (t) i · c (t) i ifˆyifˆ ifˆy Ac i = y Ac i ν (t) i · c (t) i otherwise; ω N w j = e (t) · log (1 + √ e · c (t) j ) ifˆyifˆ ifˆy Ac j = y Ac j log (1 + √ e · c (t) j ) otherwise.<label>(2)</label></formula><p>where c</p><formula xml:id="formula_7">(t)</formula><p>j is the confidence of an instance given by C (t)</p><formula xml:id="formula_8">EN , thus log (1 + √ e · c (t)</formula><p>j ) &gt; 1 is to enhance the weight of new knowledge because of the higher significance contributing to the later learning. ν EN and y Ac i is the manually annotated label. For the incorrectly predicted instance, the weight is boosted inversely to the performance of the current classifier. The instance identified as the new knowledge which contributes more to performance improvement is given a reward parameter to enhance its significant in the next training iteration. Data sets update by Formula (3). The training starts with iteration (1), the training data is initially set as L</p><formula xml:id="formula_9">T (1) EN = L T EN . L T (t+1) EN = L T (t) EN ∪ K (t) Ac ∪ K (t) N w L B(t+1) EN = L B(t) EN − (K (t) Ac ∪ K (t) N w )<label>(3)</label></formula><p>In the target language view, at iteration (t), the CredBoost model receives the recommended knowledge T R T rEN . The weight of an instance is updated by Formula (4), and the parameter setting is similar to that in the source language view. The confidence c (t) i is directly transferred from D s . We reward the validated knowledge to raise their significance in the training data considering they are originally Chinese.</p><formula xml:id="formula_10">ω Ac i = c (t) i · log(1 + √ e · v (t) i ) ω N w j = e log (1+ √ e·c (t) j ) = 1 + √ e · c (t) j<label>(4)</label></formula><p>We update the data setting by Formula (5). The training data is initially set as U</p><formula xml:id="formula_11">T (1) CN = U T CN . The CredBoost model is illustrated in Algorithm 1. L (t+1) T rCN = L (t) T rCN ∪ K (t) Ac ∪ K (t) N w U (t+1) CN = U (t) CN − (K (t) Ac ∪ K (t) N w ) U (t+1) T rEN = U (t) T rEN − (K (t) * Ac ∪ K (t) * N w ) (5) Algorithm 1 CredBoost Model Input:</formula><note type="other">English labeled data L T EN and L B EN , translated English unlabeled data UT rEN , translated Chinese data LT rCN and unlabeled Chinese data UCN ; Initialize: Weights W (1) EN = {1} M for L T EN and W (1)</note><formula xml:id="formula_12">T rCN = {1} M for LT rCN ; For t = 1, · · · , T : 1. Use L T (t) EN to learn English classifier C EN (t) ; 2. Use C (t) EN to predict L B(t) EN and U (t)</formula><p>T rEN sample top m and top n instances from L </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Knowledge Validation</head><p>Knowledge is familiarity, awareness or under- standing of someone or something, such as facts, information or skills, which is acquired through experience or education by perceiving, discovering or learning 2 . It can be implicit or explicit.</p><p>In machine learning, natural language knowl- edge is a continuously improving hypothesis that consists of both semantic and significant domain characters. While language is the expression of semantic, semantic is the carrier of sentiment. Using another word, two texts with more smaller semantic distance have higher probability to share the same sentiment polarity. <ref type="bibr" target="#b3">Choi and Cardie (2008)</ref> assert that the sentiment polarity of natural language can be better inferred by compositional semantics. They also suggest that incorporating compositional semantics into learning can im- prove the performance of sentiment classifiers. <ref type="bibr" target="#b24">Saif et al. (2012)</ref> </p><note type="other">also demonstrate that the addition of extra semantic features can further improve performance.</note><p>In order to filter out noisy and incorrect senti- ment labels, we propose a knowledge validation approach to reduce these noisy data that hinder the improvement of learning performance. Knowl- edge validation is a way to identify the acquired knowledge implied in current knowledge system and also the new knowledge fresh to current knowledge system. The knowledge can be repre- sented in the semantic space. ( <ref type="bibr" target="#b14">Le and Mikolov, 2014</ref>) project documents into a low-dimension semantic space with a deep learning approach, known as document-to-vector (Doc2Vec 3 ). Con- sidering that Dov2Vec has been verified to be efficient in many NLP tasks including sentiment analysis, we follow previous research to represent knowledge embedded in product reviews with the vectors generated by Doc2Vec.</p><p>Suppose distributed representations (i.e., low- dimensional vectors) of the all reviews including</p><formula xml:id="formula_13">{L T EN , L B EN , U T rEN } and {L T rCN , U CN } are {V(L T EN ), V(L B EN ), V(U T rEN )} and {V(L T rCN ), V(U CN )} respectively. At iteration (t), V(L T (t)</formula><p>EN ) is the current knowledge system of the English view and V(L (t)</p><p>T rCN ) is that of the Chinese. The knowledge validation runs separately in the source and target views.</p><p>In the target language view, at iteration (t), suppose the prediction confidence of the candidate</p><formula xml:id="formula_14">(x U i , ˆ y U i ) ∈ O (t) CN is c (t) i .</formula><p>We define the adaptiveness score as the average distance of top ζ + semantic distances between the instance x LB i and the positive cluster of L </p><formula xml:id="formula_15">− = ζ + · L (t) + L (t) − semantic distances</formula><p>between x U i and the negative cluster, denoted as 3 Doc2Vec is one of the models implemented in the free python library Gensim which can be freely downloaded at: https://pypi.python.org/pypi/gensim.</p><formula xml:id="formula_16">L (t)− T rCN , where L (t) + and L (t)</formula><p>− are the numbers of the elements in L (t)+ T rCN and L (t)− T rCN respectively. The validation parameters are defined by Formula (6), ω r is the weight of training instance V(r), ν </p><formula xml:id="formula_17">D(V(x LB i ), V(r)) = V(x LB i ) T · V(r) V(x LB i ) · V(r) ⇒        ν (t)+ i = 1 ζ + r∈L (t)+ EN ωr D(V(x LB i ), V(r)) ν (t)− i = 1 ζ (t) − r ∈L (t)− EN ω r D(V(x LB i ), V(r )) ⇒ ∆(ν (t) i ) = ν (t)+ i − ν (t)− i ⇒ δ (t) i = 1 e 1+∆(ν (t) i ) ⇒ V label * = 1 if δ (t) i &gt; 0.5, −1 if δ (t) i ≤ 0.5. ⇒ ν (t) i = ν (t)+ i if V label * = 1, ν (t)− i if V label * = −1. (6)</formula><p>where D(V(x LB i ), V(r)) is the Cosine distance between the distributed representations of the two reviews. ν </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We evaluate the proposed CredBoost model on an open cross-lingual sentiment analysis task in NLP&amp;CC 2013 <ref type="bibr">4</ref> . The data set provided is a Algorithm 2 Knowledge Validation Vτ (Dτ ) Input: Labeled Chinese training data L (t)</p><p>T rCN , weights of labeled data W (t) CN and semantics vectors of all English data for iteration (t):</p><formula xml:id="formula_18">{V(L (t) T rCN ), V(U (t) CN )}; Initialize: K (1) Ac = φ, K (1) N w = φ; For x U i in O (t) CN : 1. Use L (t)</formula><p>T rCN to train a classifier C </p><formula xml:id="formula_19">= V label * : Then K (t) N w ← K (t) N w + x U i ; Else: IfˆyIfˆ Ifˆy LB i = V label * = y CN i : Then K (t) Ac ← K (t) Ac + x U i ; End For. Output: K (t) N w , K (t)</formula><p>Ac .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Knowledge Validation VS (DS )</head><p>Input: Weights of labeled data W <ref type="formula">(1)</ref> EN and semantics vectors of all English data for iteration (t):</p><formula xml:id="formula_20">{V(L T (t) EN ), V(L B(t) EN ), V(U (t) T rEN )}; Initialize: K (1) Ac = φ, K (1) N w = φ; For x LB i in O (t) EN : 1. Get validated label V label</formula><p>, positive and negative average distances ν</p><formula xml:id="formula_21">(t)+ i , ν (t)− i of x LB i by fomula (6); 2. If ν (t)+ i &lt; ψ and ν (t)− i &lt; ψ: IfˆyIfˆ Ifˆy LB i = V label : Then K (t) N w ← K (t) N w + x LB i ; Else: IfˆyIfˆ Ifˆy LB i = V label : Then K (t) Ac ← K (t) Ac + x LB i ; End For. Output: K (t) N w , K (t) Ac .</formula><p>collection of bilingual Amazon product reviews in Books, DVD and Music domains. It contains 4,000 labeled English reviews, 4,000 Chinese test reviews, and 17,814, 47,071, 29,677 unlabeled Chinese reviews in three different domains. We randomly select 2,000 unlabeled Chinese reviews in each domain to train classifiers. Besides, the pseudo-data sets described in CredBoost model are translated with Google translator. The data set is summarized in <ref type="table">Table 1</ref>.</p><p>To better illustrate the significance of knowl- edge validation during knowledge transfer, we compare the proposed method with the following baseline methods:</p><p>Lexicon-based (LB): Chinese and then utilized together with a small number of Chinese turning words, negations and intensifiers to predict the sentiment polarities of the Chinese test reviews. Basic SVM (BSVM-CN): The labeled English reviews are translated into Chinese, which are then used as the pseudo-training data to train a Chinese SVM classifier.</p><p>Primarily boost transfer learning (BTL-1): The labeled English reviews are used to train the English classifier, which is applied to label the English translations of the unlabeled Chinese reviews. These labeled Chinese reviews obtained via MT together with the Chinese translations of the labeled English reviews are then used as the pseudo-training data to train a Chinese sentiment classifier.</p><p>Best result in NLP&amp;CC 2013 (BR2013): This is the best result reported in NLP&amp;CC 2013. Unfortunately, the specification of the method is not available.</p><p>Self-boost (SB-CN) in Chinese: The labeled English reviews are translated into Chinese, which are used as the pseudo-training data to train a basic Chinese classifier. This classifier is iteratively refined by choosing the most confidently predicted English reviews to add into the Chinese training data until a predefined iteration number reaches. It can be also considered as a self-adaptive boosting approach.</p><p>Iteratively boost transfer learning (BTL-2): This is an enhanced transfer learning method shar- ing the same learning framework with CredBoost but it ignores knowledge validation. It iteratively transfers the knowledge from English to Chinese. The learning in both languages iteratively boosts themselves separately. The transfer size is 16, comparable to that in CredBoost.</p><p>Basic co-training (CoTr): The co-training method proposed in <ref type="bibr" target="#b30">(Wan, 2009</ref>) is implemented. It is bidirectional transfer learning. In each iteration, 10 positive and 10 negative reviews are transferred from one language to the other.</p><p>Doc2vec feature CredBoost (dCredB): This method is similar to CredBoost except that document-to-vector is used to generate features when training basic classifiers. The vectors are obtained from both original and translated reviews. The dimension of doc2vec is 300, while the other parameters are set as default.</p><p>The baseline methods described above are categorized into three classes: the first four which are preliminary methods, the middle three which are several state-of-the-art models being comparable to our proposed model, and the last one which is a comparison to suggest that the knowledge representation is not the answer to the performance improvement. For all the methods excluding LB and BR2013, we use support vector machines (SVMs) as basic classifiers. We use the Liblinear package <ref type="bibr" target="#b5">(Fan et al., 2008</ref>) with the linear kernel 5 . All methods use Unigram+Bigram features to train the basic classifiers, except for dCredB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Result</head><p>In this work, there are two main parameters that may significantly influence the performance of our proposed model. They are the new knowledge validation boundary ψ and the validation scale ζ + in the training data. We set the values of parameters with the grid search strategy. We first fix initial ζ + = 14 to search the best new knowledge validation boundary ψ from an empirical value set {0.30, 0.35, 0.40, 0.45, 0.50}. We then fix the best ψ = 0.40 to check the suitable validation scale ζ + from the initial value set {6, 8, 9, 10, 11, 12, 14, 16} in which values are comparable with the knowledge transfer scale of CoTr in the training data.</p><p>Besides, the recommendation size m for English is set to 20 and the recommendation size n for Chinese is set to 40. The final settings are listed in <ref type="table" target="#tab_3">Table 2</ref>. The performance is evaluated in terms of accuracy (Ac) defined by Formula (7).</p><formula xml:id="formula_22">Ac(f ) = p f P f , Avg Ac = 1 3 · f ∈F Ac(f ) (7)</formula><p>where p f is the number of correct predictions and P f is the total number of the test data; F ∈ {Books, DV D, M usic} is the domain set. <ref type="bibr">5</ref> The parameter setting used in this paper is '-s 7'.   The performances are reported in <ref type="table" target="#tab_4">Tables 3 and  4</ref>. As shown, CredBoost outperforms all the other comparison methods. The first four baselines have poor performances compared to others. This suggests that the CLSA problem cannot be well solved by directly learning from the labeled translated data without any knowledge adaption or knowledge validation. SB-CN, BTL-2 and CoTr employ iterative boosting to adapt knowledge from the source English to the target Chinese with- out validating the transferred knowledge. They inevitably mis-recommend the massive noisy data into Chinese. CredBoost, in contrast, introduces knowledge validation into transfer learning with iterative boosting. It better adapts knowledge from English to Chinese and thus ensures the credibility of the accepted knowledge. Its best result justifies our assumption.</p><p>Specifically, SB-CN leverages both the Chinese training data translated from the labeled English data and the unlabeled Chinese data used for boosting. The boosting in Chinese iteratively selects the trustworthy data with the labels as- signed by the Chinese classifier. Our proposed method, however, exploits two different languages simultaneously with an additional boosting step, i.e., it transfers knowledge from English to Chinese during boosting. We then use knowledge validation model to validate the unlabeled Chinese data whose labels are assigned by the English    <ref type="table" target="#tab_4">Tables 3 and 4</ref>, the better performance of our proposed method compared with that of the self-boosting method further suggests the effectiveness of our proposed knowledge validation model. <ref type="figure" target="#fig_8">Figure 1</ref> illustrates the continuous changes of performances vs. the corresponding growth sizes of the training data sets for SB-CN, BTL-2, CoTr, and CredBoost. According to our common sense, noisy data have negative influence on performance improvement. Compared to the other three methods, CredBoost accepts less number of training instances during learning while it achieves more improvement. This verifies the ability of CredBoost that can filter out the noisy data recommended by the English sentiment classifier.</p><p>In <ref type="figure" target="#fig_8">Figure 1</ref>(a), the curves of BTL-2 and CoTr suggest that directly transferring the knowledge recommended from English imports many noisy data into Chinese. It is also obvious that the performance curve of CredBoost implies a stable improvement trend while the other three decrease after certain iterations because of the accumulated negative influence from the noisy data. <ref type="figure" target="#fig_8">Figure  1</ref>(b) shows CredBoost accepts decreased training instances after certain iterations because the number of "high-quality" instances decrease when learning proceeds. This finding suggests that knowledge validation would rather abandon "less- credible" knowledge with higher probability than easily accept it. Knowledge validation in the proposed model guarantees highly-credible learn- ing when transferring knowledge from English to Chinese. The results also show that CredBoost has great potential to achieve better performance approaching to supervised approaches if more unlabeled Chinese data are available. Another interesting finding is also observed.  The similar performance curves of CoTr is also reported in ( <ref type="bibr" target="#b8">Gui et al., 2014</ref>).</p><p>Although document-to-vector represents content semantic well, it cannot determine the sentiment polarity of text well, even when the document- to-vectors that are used to train basic classifiers are learned on the mixture of the translated and original reviews. The superior performance of CredBoost to dCredB suggests that the semantic representation is effective to identify highly- credible acquired knowledge and new knowledge but it alone may not be sufficient enough to model the sentiment information.</p><p>We also conduct some other experiments to study the sensitivity of the new knowledge valida- tion boundary ψ and the validation scale ζ + in the training data. The experimental results show that the performances with different parameter settings fluctuate around the best result reported in <ref type="table" target="#tab_4">Tables  3 and 4</ref> in a small range. Our model is basically quite stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a semi-supervised learn- ing model, called CredBoost, to address cross- lingual (English vs Chinese) sentiment analysis without direct labeled Chinese data nor direct parallel data. We propose to introduce knowledge validation during transfer learning to reduce the noisy data caused by machine translation errors or inevitable mistakes made by the source language sentiment classifier. The experimental result demonstrates the effectiveness of the proposed model. In the future, we will explore more suitable knowledge representations and knowledge valida- tion in the CredBoost framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>T rEN . Top m and top n instances are sampled from L B(t) EN and U (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>i</head><label></label><figDesc>(&lt; 1) is the adaptiveness score given by the source knowledge validation function V S (O (t) EN ). (t) (&gt; 1) is the error rate of C (t) EN , thus e (t) &gt; 1 is to reward the wrongly predicted data in the next iteration. ˆ y Ac i is the label given by C (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>EN and projects it to O (t) CN from the unlabeled Chinese data U (t) CN with the pseudo- parallel data (U (t) CN , U (t) T rEN ). O CN (t) is validat- ed by the target knowledge validation function V τ (O (t) CN ) to identify the acquired knowledge K (t) Ac and the new knowledge K (t) N w . K (t) Ac and K (t) N w are projected to K (t) * Ac and K (t) * N w from the unlabeled English pseudo-data U (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>is the adaptiveness score, and V label * ∈ {1, −1} is the validated label which denotes the knowledge belonging to the positive cluster L (t)+ T rCN or the negative cluster L (t)− T rCN . The validation process is illustrated in Algorithm 2, where the acquired knowledge is k (t) Ac , and the new knowledge is k (t) N w .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>are the weighted averages of the semantic distances. δ (t) i is the Sigmoid function which computes the probability that the data is distributed in the positive cluster L (t)+ T rCN . In the source language view, at iteration (t), let's suppose the prediction confidence of candidate (x LB i , ˆ y LB i ) ∈ O (t) EN to be c (t) i . The definitions of validation parameters are similar to those in the target language view. The validation process is illustrated in Algorithm 3. The validation is looser, because the training data and candidates are both in English. This differs from it in the target view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performances vs. Growth Sizes for SB-CN, CoTr, BTL-2, and CredBoost in three domains. The similar performance curves of CoTr is also reported in (Gui et al., 2014).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>The standard English MPQA sentiment lexicons are translated into resource, you can available at: http://tcci.ccf.org. cn/conference/2013/index.html.</figDesc><table>Domain 
English 
Chinese 
L 
U 
L 
U 

Books 
Train 4,000 
-
-
2,000 
Test 
-
-
4,000 
-

DVD 
Train 4,000 
-
-
2,000 
Test 
-
-
4,000 
-

Music 
Train 4,000 
-
-
2,000 
Test 
-
-
4,000 
-

Table 1: Experimental data sets. All data sets 
are balanced, L represents labeled data and U 
represents unlabeled data. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 : Parameter settings of three domains in this paper.</head><label>2</label><figDesc></figDesc><table>Approaches 
Domain 
Avg Ac 
Books 
DVD 
Music 
LB 
0.7770 0.7832 0.7595 
0.7709 
BSVM-CN 0.7940 0.7995 0.7778 
0.7904 
BTL-1 
0.8010 0.8058 0.7605 
0.7891 
BR2013 
0.7850 0.7773 0.7513 
0.7712 
SB-CN 
0.8400 0.8428 0.8012 
0.8280 
BTL-2 
0.8105 0.8265 0.7980 
0.8117 
CoTr 
0.8025 0.8508 0.7812 
0.8115 
dCredB 
0.6485 0.6753 0.6700 
0.6646 
CredBoost 
0.8465 0.8518 0.8093 
0.8359 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Macro performance of all approaches 
in three domains. All values are accuracies and 
Avg-Ac represents the average accuracy in three 
domains. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>425 Model (Books)</head><label></label><figDesc></figDesc><table>Positive 
Negative 
Ac 
P 
R 
F1 
P 
R 
F1 
LB 
0.7368 0.8400 0.7850 0.8140 0.7000 0.7527 0.7700 
BSVM-CN 
0.8249 0.7465 0.7837 0.7685 0.8415 0.8033 0.7940 
BTL-1 
0.8537 0.7265 0.7850 0.7620 0.8755 0.8148 0.8010 
BR2013 
-
-
-
-
-
-
0.7850 
SB-CN 
0.8716 0.7975 0.8329 0.8134 0.8825 0.8465 0.8400 
BTL-2 
0.7105 0.8881 0.7894 0.9105 0.7588 0.8278 0.8105 
CoTr 
0.8339 0.7555 0.7928 0.7765 0.8495 0.8114 0.8025 
dCredB 
0.5310 0.6941 0.6017 0.7660 0.6202 0.6854 0.6485 
CredBoost 
0.8225 0.8640 0.8427 0.8705 0.8306 0.8501 0.8465 

Model (DVD) 
Positive 
Negative 
Ac 
P 
R 
F1 
P 
R 
F1 
LB 
0.7648 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F 
measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro 
performance is unavailable. 

classifier. It is reasonable that a Chinese classifier 
performs better on Chinese text than an English 
classifier performs on the translated English text 
due to the different language distributions and MT 
errors. However, as shown in </table></figure>

			<note place="foot" n="1"> http://translate.google.com</note>

			<note place="foot" n="2"> Definition from Oxford Dictionary of English, available at: http://oxforddictionaries.com/view/ entry/m_en_us126.</note>

			<note place="foot" n="4"> NLP&amp;CC is an annual conference of Chinese information technology professional committee organized by Chinese computer Federation (CCF). It mainly focuses on the study and application novelty of natural language processing and Chinese computation. CLSA task is the task 3 of NLP&amp;CC 2013. For more details and open</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank all the anonymous reviewers for their detailed and insightful comments on this paper. The work described in this paper was supported by the Research Grants Council of Hong Kong project (PolyU 5202/12E and PolyU 152094/14E) and the grants from the National Natural Sci-ence Foundation of China (61272291, 61472290, 61472291 and 61303115).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multilingual Subjectivity Analysis Using Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samer</forename><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natual Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natual Language Processing<address><addrLine>Honolulu</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonjung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samer</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Claire</title>
		<imprint>
			<biblScope unit="volume">427</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CPNCORE: A Text Semantic Similarity System Infused with Opinion Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference and the SHared Task in *SEM 2013</title>
		<meeting>the Main Conference and the SHared Task in *SEM 2013<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06-13" />
			<biblScope unit="page" from="221" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Honolulu</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="792" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Is Machine Translation Ripe for Crosslingual Sentiment Classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: shortpapers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: shortpapers<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06-19" />
			<biblScope unit="page" from="429" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A Library for Large Linear Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Ksieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sentiment Classification on Customer Feedback Data: Noisy Data, Large Feature Vectors and the Role of Linguistic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micheal</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computational Linguistics</title>
		<meeting>the 20th International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>CH</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="841" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain Adaptation for Large-scale Sentiment Classification: A Deep Learning Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning<address><addrLine>Bellevue, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cross-lingual Opinion Analysis via Negative Transfer Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-23" />
			<biblScope unit="page" from="860" to="865" />
		</imprint>
	</monogr>
	<note>short paper</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">2010. Identifying Text Polarity Using Random Walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting on Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harith</forename><surname>Alani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatically Extracting Polarity-bearing Topics for Cross Domain Sentiment Classification</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Huamn Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Huamn Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="123" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Latent Sentiment Model for WeaklySupervised Cross-Lingual Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He ; Dublin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ireland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hiroshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tetsuya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watanabe Hideo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33th European Conference on Information Retrieval(ECIR 2011)</title>
		<meeting>the 33th European Conference on Information Retrieval(ECIR 2011)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="494" to="500" />
		</imprint>
	</monogr>
	<note>Proceedings of the 20th International Conference on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sentiment Classification of Movie and Product Reviews Using Contextual Valence Shifters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="110" to="125" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowledge Transformation for CrossDomain Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 32th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="716" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sentiment Analysis and Opinion Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-05" />
			<publisher>Morgan &amp; Claypool Publisher</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-Lingual Mixture Model for Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinfan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sentiment analysis using support vector machines with diverse inoformation sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Mullen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="412" to="418" />
		</imprint>
	</monogr>
	<note>poster paper</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ieee</forename><surname>Fellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Survey on Transfer Learning. In Journal of IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Thumps Up? Sentiment Classification using Machine Learning Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Haves and the Have-Nots: Leverage Unlabeled Corpora for Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A R</forename><surname>Balamurali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="4" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using Emotions to reduce Dependency in Machine Learning Techniques for Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Read</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43th</title>
		<meeting>the 43th</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<title level="m">Annual Meeting on Association for Computational Linguistics Student Research Workshop</title>
		<imprint>
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic Sentiment Analysis of Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harith</forename><surname>Alani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Semantics Web Conference ISWC 2012</title>
		<meeting>the 11th International Semantics Web Conference ISWC 2012<address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Google Translate now serves 200 millon people daily</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Shankland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CNET. CBS Interactive Inc</title>
		<imprint>
			<date type="published" when="2013-05-18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recursive Deep Models for Semantics Computationality Over a Sentiment Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiristopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Novel Scheme for Domain-transfer Problem in the context of Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songbo</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaowei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM 2007</title>
		<meeting><address><addrLine>Lisboa, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-11-06" />
		</imprint>
	</monogr>
	<note>Huifeng Tang and Xueqi Cheng</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Thumps Up or Thumps Down? Semantic Orientation Applied to Unsupervised Classification of Reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using Bilingual Knowledge and Ensemble Technics for Unsupervised Chinese Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natual Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natual Language Processing<address><addrLine>Honolulu</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="553" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Co-Training for Cross-Lingual Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th</title>
		<meeting>the 47th</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<title level="m">Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP</title>
		<meeting><address><addrLine>Suntec, Singapore, 2-7</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross Lingual Adaptation: An Experiment on Sentiment Classifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48 Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48 Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
	<note>short paper</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Instance Level Transfer Learning for Cross Lingual Opinion Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011</title>
		<meeting>the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06-24" />
			<biblScope unit="page" from="182" to="188" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
