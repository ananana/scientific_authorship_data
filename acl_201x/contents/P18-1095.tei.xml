<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Scaling for Sparse Detection in Information Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">State Key Laboratory of Computer Science Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">State Key Laboratory of Computer Science Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">State Key Laboratory of Computer Science Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">State Key Laboratory of Computer Science Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Scaling for Sparse Detection in Information Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1033" to="1043"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1033</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper focuses on detection tasks in information extraction, where positive instances are sparsely distributed and models are usually evaluated using F-measure on positive classes. These characteristics often result in deficient performance of neural network based detection models. In this paper, we propose adaptive scaling, an algorithm which can handle the positive sparsity problem and directly optimize over F-measure via dynamic cost-sensitive learning. To this end, we borrow the idea of marginal utility from economics and propose a theoretical framework for instance importance measuring without introducing any additional hyper-parameters. Experiments show that our algorithm leads to a more effective and stable training of neural network based detection models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Detection problems, aiming to identify occur- rences of specific kinds of information (e.g., events, relations, or entities) in documents, are fundamental and widespread in information ex- traction (IE). For instance, an event detec- tion ( <ref type="bibr">Walker et al., 2006</ref>) system may want to detect triggers for "Attack" events, such as "shot" in sentence "He was shot". In relation detec- tion ( <ref type="bibr" target="#b17">Hendrickx et al., 2009)</ref>, we may want to identify all instances of a specific relation, such as "Jane joined Google" for "Employment" relation.</p><p>Recently, a number of researches have em- ployed neural network models to solve detection problems, and have achieved significant improve- ment in many tasks, such as event detection <ref type="bibr" target="#b3">(Chen et al., 2015;</ref><ref type="bibr" target="#b34">Nguyen and Grishman, 2015)</ref>  <ref type="table">Table 1</ref>: Comparison between standard classifica- tion tasks and detection problems. detection ( <ref type="bibr">Zeng et al., 2014;</ref><ref type="bibr" target="#b37">Santos et al., 2015)</ref> and named entity recognition ( <ref type="bibr" target="#b19">Huang et al., 2015;</ref><ref type="bibr" target="#b5">Chiu and Nichols, 2015;</ref><ref type="bibr" target="#b25">Lample et al., 2016)</ref>. These methods usually regard detection problems as standard classification tasks, with several posi- tive classes for targets to detect and one negative class for irrelevant (background) instances. For example, an event detection model will identify event triggers in sentence "He was shot" by classi- fying word "shot" into positive class "Attack", and classifying all other words into the negative class "NIL". To optimize classifiers, cross-entropy loss function is commonly used in this paradigm.</p><p>However, different from standard classification tasks, detection tasks have unique class inequality characteristic, which stems from both data dis- tribution and applied evaluation metric. <ref type="table">Table 1</ref> shows their differences. First, positive instances are commonly sparsely distributed in detection tasks. For example, in event detection, less than 2% of words are a trigger of an event in RichERE dataset ( <ref type="bibr">Song et al., 2015</ref>). Furthermore, detection tasks are commonly evaluated using F-measure on positive classes, rather than accuracy or F-measure on all classes. Therefore positive and negative classes play different roles in the evaluation: the performance is evaluated by only considering how well we can detect positive instances, while cor- rect predictions of negative instances are ignored.</p><p>Due to the class inequality characteristic, re- ported results indicate that simply applying stan-dard classification paradigm to detection tasks will result in deficient performance <ref type="bibr" target="#b0">(Anand et al., 1993;</ref><ref type="bibr" target="#b2">Carvajal et al., 2004;</ref><ref type="bibr">Lin et al., 2017)</ref>. This is because minimizing cross-entropy loss function corresponds to maximize the accuracy of neural networks on all training instances, rather than F- measure on positive classes. Furthermore, due to the positive sparsity problem, training procedure will easily achieve a high accuracy on negative class, but is difficult to converge on positive class- es and often leads to a low recall rate. Although simple sampling heuristics can alleviate this prob- lem to some extent, they either suffer from losing inner class information or over-fitting positive in- stances ( <ref type="bibr" target="#b16">He and Garcia, 2009;</ref><ref type="bibr" target="#b13">Fernández-Navarro et al., 2011)</ref>, which often result in instability during the training procedure.</p><p>Some previous approaches <ref type="bibr" target="#b23">(Joachims, 2005;</ref><ref type="bibr" target="#b20">Jansche, 2005</ref><ref type="bibr" target="#b21">Jansche, , 2007</ref><ref type="bibr" target="#b6">Dembczynski et al., 2011;</ref><ref type="bibr" target="#b4">Chinta et al., 2013;</ref><ref type="bibr" target="#b31">Narasimhan et al., 2014;</ref><ref type="bibr" target="#b32">Natarajan et al., 2016</ref>) tried to solve this problem by directly optimizing F-measure. <ref type="bibr" target="#b36">Parambath et al. (2014)</ref> proved that it is sufficient to solve F-measure optimization problem via cost-sensitive learning, where class-specific cost factors are ap- plied to indicate the importance of different class- es to F-measure. However, optimal factors are not known a priori so ε-search needs to be applied, which is too time consuming for the optimization of neural networks.</p><p>To solve the class inequality problem for sparse detection model optimization, this paper proposes a theoretical framework to quantify the importance of positive/negative instances during training. We borrow the idea of marginal utility from Eco- nomics <ref type="bibr">(Stigler, 1950)</ref>, and regard the evaluation metric (i.e., F-measure commonly) as the utility to optimize. Based on the above idea, the importance of an instance is measured using the marginal utility of correctly predicting it. For standard classification tasks evaluated using accuracy, our framework proves that correct predictions of pos- itive and negative instances will have equal and unchanged marginal utility, i.e., all instances are with the same importance. For detection problems evaluated using F-measure, our framework proves that the utility of correctly predicting one more positive instance (marginal positive utility) and that of correctly predicting one more negative instance (marginal negative utility) are different and dynamically changed during model training.</p><p>That is, the importance of instances of each class is not only determined by their data distribution, but also affected by how well the current model can converge on different classes.</p><p>Based on the above framework, we propose adaptive scaling, a dynamic cost-sensitive learn- ing algorithm which adaptively scales costs of instances of different classes with above quantified importance during the training procedure, and thus can make the optimization criteria consistent with the evaluation metric. Furthermore, a batch- wise version of our adaptive scaling algorithm is proposed to make it directly applicable as a plug-in of conventional neural network training algorithms. Compared with previous methods, adaptive scaling is designed based on marginal utility framework and doesn't introduce any ad- ditional hyper-parameter, and therefore is more efficient and stable to transfer among datasets and models.</p><p>Generally, the main contributions of this paper are:</p><p>• We propose a marginal utility based frame- work for detection model optimization, which can dynamically quantify instance im- portance to different evaluation metrics.</p><p>• Based on the above framework, we present adaptive scaling, a plug-in algorithm which can effectively resolve the class inequality problem in neural detection model optimiza- tion via dynamic cost-sensitive learning.</p><p>We conducted experimental studies 1 on event detection, a typical sparse detection task in IE. We thoroughly compared various methods for adapt- ing classical neural network models into detection problems. Experiment results show that our adap- tive scaling algorithm not only achieves a better performance, but also is more stable and more adaptive for training neural networks on various models and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Relation between Accuracy Metric and Cross- Entropy Loss. Recent neural network method- s usually regard detection problems as standard classification tasks, with several positive classes to detect, and one negative class for other irrelevant instances. Formally, given P positive training instances P = {(x i , y i ) P i=1 }, and N negative instances N = {(x i , y i ) N i=1 } (due to positive sparsity, P N ), the training of neural net- work classifiers usually involves in minimizing the softmax cross-entropy loss function regarding to model parameters θ:</p><formula xml:id="formula_0">LCE(θ) = − 1 P + N (x i ,y i )∈P N log p(yi|xi; θ) (1)</formula><p>and if P, N → ∞, we have</p><formula xml:id="formula_1">lim P,N →∞ LCE(θ) = −E[log p(y|x; θ)] = − log(Accuracy)<label>(2)</label></formula><p>which reveals that minimizing cross-entropy loss corresponds to maximize the expected accuracy of the classifier on training data. Divergence between F-Measure and Cross- Entropy Loss. However, detection tasks are most- ly evaluated using F-measure computed on posi- tive classes, which makes it unsuitable to optimize classifiers using cross-entropy loss. For instance, due to the positive sparsity, simply classifying all instances into negative class will achieve a high accuracy but zero F-measure.</p><p>To show where this divergence comes from, let c 1 , c 2 , ..., c k−1 denote k −1 positive classes and c k is the negative class, we define T P = k−1 i=1 T P i , where T P i is the population of correctly predicted instances of positive class c i . T N denotes the number of correctly predicted negative instances. P E represents positive-positive error, where an instance is classified into one positive class c i but its golden label is another positive class c j . Then we have following metrics 2 :</p><formula xml:id="formula_2">Accuracy = T P + T N P + N (3) Precision = T P N − T N + P E + T P (4) Recall = T P P (5) F β = (1 + β 2 ) Precision · Recall β 2 · Precision + Recall = (1 + β 2 ) T P β 2 P + N − T N + P E + T P (6)</formula><p>where β in F β is a factor indicating the metric attaches β times as much importance to recall as precision. We can easily see that for accuracy metric, correct predictions of positive and negative instances are equally regarded (i.e., T P and T N are symmetric), which is consistent with cross- entropy loss function. However, when measuring using F-measure, this condition is no longer hold- ing. The importance varies from different classes (i.e., T P and T N are asymmetric). Therefore, to make the training procedure consistent with F-measure, it is critical to take this importance difference into consideration. F-measure Optimization via Cost-sensitive Learning. <ref type="bibr" target="#b36">Parambath et al. (2014)</ref> have shown that F-measure can be optimized via cost-sensitive learning, where a cost (importance) is set for each class for adjusting their impact on model learning. However, most previous studies set such costs manually ( <ref type="bibr" target="#b0">Anand et al., 1993;</ref><ref type="bibr" target="#b7">Domingos, 1999;</ref><ref type="bibr" target="#b24">Krawczyk et al., 2014</ref>) or search them on large scale dataset <ref type="bibr" target="#b30">(Nan et al., 2012;</ref><ref type="bibr" target="#b36">Parambath et al., 2014</ref>), whose best settings are not transferable and very time-consuming to find for neural network models. This motivates us to develop a theoretical framework for measuring such importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adaptive Scaling for Sparse Detection</head><p>This section describes how to effectively opti- mize neural network detection models via dy- namic cost-sensitive learning. Specifically, we first propose a marginal utility based theoretical framework for measuring the importance of pos- itive/negative instances. Then we present our adaptive scaling algorithm, which can leverage the importance of each class for effective and robust training of neural network detection models. Fi- nally, a batch-wise version of our algorithm is proposed to make it can be applied as a plug-in of batch-based neural network training algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Marginal Utility based Importance Measuring</head><p>Conventional methods commonly deal with the class inequality problem in sparse detection by deemphasizing the importance of negative in- stances during training. This raises two questions: 1) How to quantify the importance of instances of each class? As mentioned by <ref type="bibr" target="#b36">Parambath et al. (2014)</ref>, that importance is related to the conver- gence ability of models, which means that this problem cannot be solved by only considering the distribution of training data. 2) Is the im-portance of positive/negative instances remaining unchanged during the entire training process? If not, how it changes according to the convergence of the model? To this end, we borrow the idea of marginal utility from economics, which means the change of utility from consuming one more unit of prod- uct. In detection tasks, we regard its evaluation metric (F-measure) as the utility function. The increment of utility from correctly predicting one more positive instance (marginal positive utility) can be regarded as the relative importance of posi- tive classes, and that from correctly predicting one more negative instance (marginal negative utility) is look upon as the relative importance of the neg- ative class. If marginal positive utility overweighs marginal negative utility, positive instances should be considered more important during optimization because it can lead to more improvement on the evaluation metric. In contrast, if marginal negative utility is higher, training procedure should incline to negative instances since it is more effective for optimizing the evaluation metric.</p><p>Formally, we derive marginal positive u- tility M U (T P ) and marginal negative utility M U (T N ) by computing the partial derivative of the evaluation metric with respect to T P and T N respectively. For instance, the marginal positive utility M U acc (T P ) and the marginal negative util- ity M U acc (T N ) regarding to accuracy metric are:</p><formula xml:id="formula_3">M Uacc(T P ) = ∂(Accuracy) ∂(T P ) = 1 P + N (7) M Uacc(T N ) = ∂(Accuracy) ∂(T N ) = 1 P + N<label>(8)</label></formula><p>We can see that M U acc (T P ) and M U acc (T N ) are equal and constant regardless of the values of T P and T N . This indicates that, to optimize accuracy, we can simply treat positive and negative instances equally during the training phase, and this is what we exactly do when optimizing cross-entropy loss in Equation 1. For detection problems evaluated using F-measure, we can obtain the marginal util- ities from Equation 6 as:</p><formula xml:id="formula_4">M UF β (T P ) = (1 + β 2 )(β 2 P + N − T N + P E) (β 2 P + N − T N + P E + T P ) 2 (9) M UF β (T N ) = (1 + β 2 ) · T P (β 2 P + N − T N + P E + T P ) 2 (10)</formula><p>This result is different from that of accuracy metric. First, M U F β (T P ) and M U F β (T N ) is no longer equal, indicating that the importance of positive/negative instances to F-measure are different. Besides, it is notable that M U F β (T P ) and M U F β (T N ) are dynamically changed during the training phase and are highly related to how well current model can fit positive instances and negative instances, i.e., T P and T N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adaptive Scaling Algorithm</head><p>In this section, we describe how to incorporate the above importance measures into the training procedure of neural networks, so that it can dy- namically adjust weights of positive and negative instances regarding to F-measure.</p><p>Specifically, given the current model of neural networks parameterized by θ, let w β (θ) denote the relative importance of negative instances to positive instances for F β -measure. Then w β (θ) can be computed as the ratio of marginal negative utility M U F β (T N (θ)) to the marginal positive utility M U F β (T P (θ)), where T P (θ) and T N (θ) are T P and T N on training data with respect to θ-parameterized model:</p><formula xml:id="formula_5">w β (θ) = M UF β (T N (θ)) M UF β (T P (θ)) = T P (θ) β 2 P + N − T N (θ) + P E<label>(11)</label></formula><p>Then at each iteration of the model optimization (i.e., each step of gradient descending), we want the model to take next update step proportional to the gradient of the w β -scaled cross-entropy loss function L AS (θ) at the current point:</p><formula xml:id="formula_6">LAS(θ) = − (x i ,y i )∈P log p(yi|xi; θ) − (x i ,y i )∈N w β (θ) · log p(yi|xi; θ)<label>(12)</label></formula><p>Consequently, based on the contributions that cor- rectly predicting one more instances of each class bringing to F-measure, the training procedure dy- namically adjusts its attention between positive and negative instances. Thus our adaptive scaling algorithm can take the class inequality characteris- tic of detection problems into consideration with- out introducing any additional hyper-parameter 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Properties and Relations to Previous Empirical Conclusions</head><p>In this section, we investigate the properties of our adaptive scaling algorithm. By investigating the change of scaling coefficient w β (θ) during train- ing, we find that our method has a tight relation to previous empirical conclusions on solving the class inequality problem. Property 1. The relative importance of pos- itive/negative instances is related to the ratio of the instance number of each class, as well as how well current model can fit each class. It is easy to derive that if we fix the accuracies of each classes, w β (θ) will be smaller if the ratio of the size of negative instances to that of the positive instances (i.e., N P ) increases. This indi- cates that the training procedure should pay more attention to positive instances if the empirical dis- tribution inclines more severely towards negative class, which is identical to conventional practice that we should deemphasize more on negative instances if the positive sparsity problem is more severe <ref type="bibr" target="#b22">(Japkowicz and Stephen, 2002</ref>). Besides, w β (θ) highly depends on T P and T N , which is identical to previous conclusion that the best cost factors are related to the convergence ability of models ( <ref type="bibr" target="#b36">Parambath et al., 2014)</ref>.</p><p>Property 2. For micro-averaged F-measure, all positive instances are equally weighted regardless of the sample size of its class. Let M U (T P i ) be the marginal utility of positive class c i , we have:</p><formula xml:id="formula_7">M UF β (T Pi) = ∂(F β ) ∂(T P ) · ∂(T P ) ∂(T Pi) = M UF β (T P )<label>(13)</label></formula><p>This corresponds to the applied micro-averaged F-measure, in which all positive instances are equally considered regardless of the sample size of its class. Thus correctly predicting one more positive instance of any class will result in the same increment of micro-averaged F-measure. Property 3. The importance of negative in- stances increases with the rise of accuracy on positive classes. This is a straightforward conse- quence because if the model has higher accuracy on positive instances then it should shift more of its attention to negative ones. Besides, if the accuracy of positive class is close to zero, F-measure will also be close to zero no matter how high the accuracy on negative class is, i.e., correctly predicting negative instances can result in little F-measure increment. Therefore negative instances are inconsequential when the accuracy on positive class is low. And with the increment of positive accuracy, the importance of negative class also increases. Property 4. The importance of negative in- stances increased with the rise of accuracy on the negative class. This can make the training proce- dure incline to hard negative instances, which is similar to Focal Loss ( <ref type="bibr">Lin et al., 2017)</ref>. During model convergence, easy negative instances can be correctly classified at the very beginning of training and its loss (negative log probability) will reduce very quickly. This is analogical to removing easy negative instances out of the training procedure and the hard negative instances remaining become more balanced proportional to positive instances. Therefore the importance w β of remaining hard negative instances are increased to make the model fit them better.</p><p>Property 5. The importance of negative in- stances increases when more attention is paid to precision than recall. We can see that w β decreas- es with the rise of β, which indicates we focus more on recall than precision. This is identical to practice in sampling heuristics that models should attach more attention to negative instances and sub-sample more of them if evaluation metrics incline more to precision than recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Batch-wise Adaptive Scaling</head><p>In large-scale machine learning, batch-wise gradi- ent based algorithm is more popular and efficient for neural network training. This section presents a batch-wise version of our adaptive scaling algo- rithm, which uses batch-based estimatorˆwestimatorˆ estimatorˆw β (θ) to replace w β (θ) in Equation 12.</p><p>First, because the main challenge of detec- tion tasks is to identify positive instances from background ones, rather than distinguish between positive classes, we ignore the positive-positive error P E in our experiments. In fact, we found that compared with P and N − T N , P E is much smaller and has very limited impact on the final result. Besides, for T P and T N , we approximate them using their expectation on the current batch, which can produce a robust estimation even when the batch size is not large enough. Specifically, let</p><formula xml:id="formula_8">P B = {(x i , y i ) P B i=1 } denotes P B positive instances and N B = {(x i , y i ) N B i=1</formula><p>} is N B negative instances in the batch, we estimate T P (θ) and T N (θ) as:</p><formula xml:id="formula_9">T P B (θ) = (x i ,y i )∈P B p(yi|xi; θ)<label>(14)</label></formula><p>T N B (θ) =</p><formula xml:id="formula_10">(x i ,y i )∈N B p(yi|xi; θ)<label>(15)</label></formula><p>Then we can compute the estimatorˆwestimatorˆ estimatorˆw β (θ) for w β (θ) as:</p><formula xml:id="formula_11">ˆ w β (θ) = T P B (θ) β 2 P B + N B − T N B (θ)<label>(16)</label></formula><p>wherê w β (θ) is computed using only the instances in a batch, which makes it can be directly applied as a plug-in of conventional batch-based neural network optimization algorithm where the loss of negative instances in batch are scaled byˆwbyˆ byˆw β (θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Preparation</head><p>To assess the effectiveness of our method, we conducted experiments on event detection, which is a typical detection task in IE. We used the offi- cial evaluation datasets of TAC KBP 2017 Event Nugget Detection Evaluation (LDC2017E55) as test sets, which contains 167 English documents and 167 Chinese documents annotated with Rich ERE annotation standard. For English, we used previously annotated <ref type="bibr">RichERE datasets, including LDC2015E29, LDC2015E68, LDC2016E31 and TAC KBP 2015</ref> Evaluation datasets in LDC2017E02 as the training set. For Chi- nese, the training set includes LDC2015E105, LDC2015E112, LDC2015E78 and the Chinese part of LDC2017E02. For both Chinese and English, we sampled 20 documents from the e- valuation dataset of 2016 year as the development set. Finally, there are 866/20/167 documents in English train/development/test set and 506/20/167 documents in Chinese train/development/test set respectively. We used Stanford CoreNLP toolk- it ( <ref type="bibr" target="#b28">Manning et al., 2014</ref>) for sentence splitting and word segmentation in Chinese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>To verify the effectiveness of our adaptive s- caling algorithm, we conducted experiments on two state-of-the-art neural network event detec- tion models. The first one is Dynamic Multi- pooling Convolutional Neural network (DMCNN) proposed by <ref type="bibr" target="#b3">Chen et al. (2015)</ref>, a one-layer CNN model with a dynamic multi-pooling operation over convolutional feature maps. The second one is BiLSTM used by  and <ref type="bibr">Yang and Mitchell (2017)</ref>, where a bidirectional LSTM layer is firstly applied to the input sentence and then word-wise classification is directly conducted on the output of the BiLSTM layer of each word.</p><p>We compared our method with following base- lines upon above-mentioned two models:</p><p>1) Vanilla models (Vanilla), which used the original cross-entropy loss function without any additional treatment for class inequality problem.</p><p>2) Under-sampling (Sampling), which sam- ples only part of negative instances as the training data. This is the most widely used solution in event detection <ref type="figure" target="#fig_0">(Chen et al., 2015)</ref>.</p><p>3) Static scaling (Scaling), which scales loss of negative instances with a constant. This is a simple but effective cost-sensitive learning method. 4) Focal Loss (Focal) ( <ref type="bibr">Lin et al., 2017)</ref>, which scales loss of an instance with a factor proportional to the probability of incorrectly predicting it. This method proves to be effective in some detection problems such as Object Detection.</p><p>5) Softmax-Margin Loss (CLUZH) <ref type="bibr" target="#b27">(Makarov and Clematide, 2017)</ref>, which sets additional costs for false-negative error and positive-positive error. This method was used in the 5-model ensembling CLUZH system in TAC KBP 2017 Evaluation. Besides, it also introduced several strong hand- craft features, which makes it achieve the best performance on Chinese and very competitive performance on English in the evaluation.</p><p>We evaluated all systems with micro-F1 metric computed using the official evaluation toolkit <ref type="bibr">4</ref> . We reported the average performance of 10 runs (Mean) of each system on the official type classi- fication task. <ref type="bibr">5</ref> We also reported the variance (Var) of the performance to evaluate the stabilities of d- ifferent methods. As TAC KBP2017 allowed each team to submit 3 different runs, to make our results comparable with evaluation results, we selected 3 best runs of each system on the development set and reported the best test set performance among them, which is referred as Best3 in this paper. We applied grid search ( <ref type="bibr" target="#b18">Hsu et al., 2003</ref>) to find best hyper-parameters for all methods. <ref type="table" target="#tab_2">Table 2</ref> shows the overall results on both English and Chinese. From this table, we can see that:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Overall results</head><p>1) The class inequality problem is crucial for sparse detection tasks and requires special consideration. Compared with vanilla models, all  other methods trying to tackle this problem have shown significant improvements on both models and both languages, especially on Chinese dataset where the positive sparsity problem is more se- vere ( <ref type="bibr" target="#b27">Makarov and Clematide, 2017)</ref>.</p><p>2) It is critical to take the different roles of classes into consideration for F-measure opti- mization. Even down-weighting the loss assigned to well-classified examples can alleviate the posi- tive sparsity problem by deemphasizing easy neg- ative instances during optimization, Focal Loss cannot achieve competitive performance because it does not distinguish between different classes.</p><p>3) Marginal Utility based framework pro- vides a solid foundation for measuring instance importance, thus makes our adaptive scal- ing algorithm steadily outperform all heuristic baselines. No matter on mean or Best3 met- ric, adaptive scaling steadily outperforms other baselines on both BiLSTM and DMCNN model. Furthermore, we can see that simple models with adaptive scaling outperform the state-of-the-art CLUZH system on Chinese (which has more se- vere positive sparsity problem) and achieve com- parable results with it on English. Please note that CLUZH is an ensemble of five models and uses extra hand-crafted features. This verified the effectiveness of our adaptive scaling algorithm. 4) Our adaptive scaling algorithm doesn't need additional hyper-parameters and the im- portance of instances is dynamically estimated. This leads to a more stable and transferable solution for detection model optimization. First, we can see that adaptive scaling has the lowest variance among all methods, which means that it is more stable than other methods. Besides, adaptive scaling doesn't introduce any additional hyper-parameters. In contrast, in experiment we found that the best hyper-parameters for under- sampling (the ratio of sampled negative instances to positive instances) and static scaling (the pri- or cost for negative instances) remarkably varied from models and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Stability Analysis</head><p>This section investigated the stability of different methods. <ref type="table" target="#tab_2">Table 2</ref> have shown that adaptive scaling has a much smaller variance than other baselines. To investigate its reason, <ref type="figure" target="#fig_0">Figure 1</ref> shows the box plots of adaptive scaling and other heuristic meth- ods on both models and both languages.</p><p>We can see that interquartile ranges (i.e., the difference between 75th and 25th percentiles of data) of the performances of adaptive scaling are smaller than other methods. In all groups of exper- iments, the performances of our adaptive scaling algorithm are with a smaller fluctuation. This demonstrates the stability of adaptive scaling al- gorithm. Furthermore, we found that conventional methods are more instable on Chinese dataset where the data distribution is more skewed. We believe that this is because: 1) Under-Sampling might undermine the inner sub-concept structure of negative class by simply dropping negative instances, and its performance depends on the quality of sampled data, which can result in the instability.</p><p>2) Static scaling sets the importance of negative instances statically in the entire training proce- dure. However, as shown in Section 3, the rel- ative importance between different classes is dy- namically changed during the training procedure, which makes static scaling incapable of achieving stable performance in different phases of training.</p><p>3) Adaptive scaling achieves more stable perfor- mance during the entire training procedure. First, it doesn't drop any instances, so it can maintain the inner structure of negative class without any information loss. Besides, our algorithm can dy- namically adjust the scaling factor during training, therefore can automatically shift attention between positive and negative classes according to the convergence state of the model. <ref type="figure" target="#fig_1">Figure 2</ref> shows the change of Precision, Recall and F1 measures regarding to different β. We can see that when β increases, the precision decreased and the recall increased by contrast. This is identical to the nature of F β where β represents the relative importance of precision and recall. Furthermore, adaptive scaling with β = 1 achieved the best performance on F 1 measure. This further demon- strates that w β derived from our marginal utility framework is a good and adaptive estimator for the relative importance of the negative class to positive classes of F β measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Adaptability on Different β</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>This paper proposes adaptive scaling algorithm for sparse detection problem. Related work to this paper mainly includes: Classification on Imbalanced Data. Conven- tional approaches addressed data imbalance from either data-level or algorithm-level. Data-level approaches resample the training data to maintain the balance between different classes <ref type="bibr" target="#b22">(Japkowicz and Stephen, 2002;</ref><ref type="bibr" target="#b8">Drummond et al., 2003)</ref>. Further improvements on this direction involve how to better sampling data with minimum in- formation loss <ref type="bibr" target="#b2">(Carvajal et al., 2004;</ref><ref type="bibr" target="#b11">Estabrooks et al., 2004;</ref><ref type="bibr" target="#b15">Han et al., 2005;</ref><ref type="bibr" target="#b13">Fernández-Navarro et al., 2011</ref>). Algorithm-level approaches attempt to choose an appropriate inductive bias on models or algorithms to make them more suitable on data imbalance condition, including instance weight- ing <ref type="bibr">(Ting, 2002;</ref><ref type="bibr">Lin et al., 2017)</ref>, cost-sensitive learning <ref type="bibr" target="#b0">(Anand et al., 1993;</ref><ref type="bibr" target="#b7">Domingos, 1999;</ref><ref type="bibr">Sun et al., 2007;</ref><ref type="bibr" target="#b24">Krawczyk et al., 2014</ref>) and active learning approaches ( <ref type="bibr">Ertekin et al., 2007a,b;</ref><ref type="bibr" target="#b39">Zhu and Hovy, 2007)</ref>. F-Measure Optimization. Previous research on F-measure optimization mainly fell into t- wo paradigms <ref type="bibr" target="#b30">(Nan et al., 2012</ref>): 1) Decision- theoretic approaches (DTA), which first estimate a probability model and find the optimal predictions according to that model <ref type="bibr" target="#b23">(Joachims, 2005;</ref><ref type="bibr" target="#b20">Jansche, 2005</ref><ref type="bibr" target="#b21">Jansche, , 2007</ref><ref type="bibr" target="#b6">Dembczynski et al., 2011;</ref><ref type="bibr">BusaFekete et al., 2015;</ref><ref type="bibr" target="#b32">Natarajan et al., 2016</ref>). The main drawback of these methods is that they need to estimate the joint probability with exponentially many combinations, thus make them hard to use in practice; 2) Empirical utility maximization (EUM) approaches, which adapt approximate methods to find a best classifier in hypothesises ( <ref type="bibr" target="#b29">Musicant et al., 2003;</ref><ref type="bibr" target="#b4">Chinta et al., 2013;</ref><ref type="bibr" target="#b36">Parambath et al., 2014;</ref><ref type="bibr" target="#b31">Narasimhan et al., 2014</ref>). However, EUM methods depend on thresholds or costs that are not known a priori so time-consuming searching on large development set is required. Our adaptive scaling algorithm is partially inspired by EUM approaches, but is based on the marginal utility framework, which doesn't introduce any addition- al hyper-parameter or searching procedure. Neural Network based Event Detection. Event detection is a typical task of detection problem- s. Recently neural network based methods have achieved significant progress in Event Detection. <ref type="bibr">CNNs (Chen et al., 2015;</ref><ref type="bibr" target="#b34">Nguyen and Grishman, 2015)</ref> and Bi-LSTMs ( <ref type="bibr" target="#b38">Zeng et al., 2016;</ref><ref type="bibr">Yang and Mitchell, 2017</ref>) are two effective and widely used models. Some improvements have been made by jointly predicting triggers and argu- ments (  or introducing more complicated architectures to capture larger scale of contexts <ref type="bibr" target="#b14">Ghaeini et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper proposes adaptive scaling algorithm for detection tasks, which can deal with its positive sparsity problem and directly optimize F-measure by adaptively scaling the influence of negative instances in loss function. Based on the marginal utility theory framework, our method leads to more effective, stable and transferable optimiza- tion of neural networks without introducing ad- ditional hyper-parameters. Experiments on event detection verified the effectiveness and stability of our adaptive scaling algorithm.</p><p>The divergence between loss functions and e- valuation metrics is common in NLP and machine learning. In the future we want to apply our marginal utility based framework to other metrics, such as Mean Average Precision (MAP).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Box plots of three different methods. * indicates outliers not shown in the figure exist.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Change of Precision, Recall and F1 regarding to β using adaptive scaling on DMCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>, relation</head><label>relation</label><figDesc></figDesc><table>Classification 
Detection 
Target 
Instances 

All instances 
Sparse positive 
instances 
Evaluation Accuracy or F-measure 
on all classes 

F-measure on only 
positive classes 
Typical 
Tasks 

Text Classification, 
Sentiment 
Classification 

Event Detection, 
Relation Detection 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Experiment results on TAC KBP 2017 
evaluation datasets. * indicates the best (ensem-
bling) results reported in the original paper. "A-
Scaling" is batch-wise adaptive scaling algorithm. 

</table></figure>

			<note place="foot" n="1"> Our source code is openly available at github.com/ sanmusunrise/AdaScaling.</note>

			<note place="foot" n="2"> This paper considers micro-averaged metrics. But our conclusions can be easily extended to macro-averaged metrics by scaling above-mentioned coefficients with sample sizes of each class.</note>

			<note place="foot" n="3"> Note that β is set according to the applied F β evaluation metric and therefore is not a hyper-parameter.</note>

			<note place="foot" n="4"> github.com/hunterhector/EvmEval/ tarball/master 5 Realis classification, another task in the evaluation, can be regarded as a standard classification task without background class, so we didn&apos;t include it here.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We sincerely thank the reviewers for their valuable comments. Moreover, this work is supported by the National Natural Science Foundation of China under Grants no. 61433015, 61572477 and 61772505, and the Young Elite Scientists Sponsorship Program no. YESS20160177.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved algorithm for neural network classification of imbalanced training sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangachari</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kishan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chilukuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ranka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="962" to="969" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Krzysztof Dembczynski, and Eyke Hüllermeier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Róbert</forename><surname>Busa-Fekete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balázs</forename><surname>Szörényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
	<note>Online f-measure optimization</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural network method for failure detection with skewed class distribution. Insight-Non-Destructive Testing and Condition Monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Carvajal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chacón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acuna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="399" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multi-pooling convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimizing f-measure with non-convex loss and sparse linear classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Punya Murthy Chinta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirish</forename><surname>Balamurugan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shevade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2013 International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Neural Networks (IJCNN)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08308</idno>
		<title level="m">Named entity recognition with bidirectional lstm-cnns</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An exact algorithm for f-measure maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krzysztof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Dembczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Waegeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1404" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Metacost: A general method for making classifiers cost-sensitive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">C4. 5, class imbalance, and cost sensitivity: why undersampling beats over-sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert C Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on learning from imbalanced datasets II</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning on the border: active learning in imbalanced data classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Seyda Ertekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixteenth ACM conference on Conference on information and knowledge management</title>
		<meeting>the sixteenth ACM conference on Conference on information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Active learning for class imbalance problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Seyda Ertekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lee</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 30th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="823" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A multiple resampling method for learning from imbalanced data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Estabrooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeho</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Japkowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="36" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A languageindependent neural network for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A dynamic over-sampling procedure based on sensitivity for multi-class problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Fernández-Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">César</forename><surname>Hervás-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">Antonio</forename><surname>Gutiérrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1821" to="1833" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Event nugget detection with forward-backward recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Ghaeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiaoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Borderline-smote: a new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Huan</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="878" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edwardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A practical guide to support vector classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Wei</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maximum expected f-measure training of logistic regression models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jansche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="692" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A maximum expected utility framework for binary sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jansche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="736" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The class imbalance problem: A systematic study. Intelligent data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaju</forename><surname>Stephen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="429" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A support vector method for multivariate performance measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cost-sensitive decision tree ensembles for effective imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartosz</forename><surname>Krawczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michał</forename><surname>Wo´zniakwo´zniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Schaefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="554" to="562" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Dollár. 2017. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">UZH at TAC KBP 2017: Event nugget detection via joint learning with softmax-margin objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Makarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Clematide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TAC 2017</title>
		<meeting>TAC 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optimizing f-measure with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>David R Musicant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysel</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ozgur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS conference</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="356" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Optimizing f-measure: A tale of two approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Kian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><forename type="middle">Leong</forename><surname>Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chieu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.4625</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the statistical consistency of plugin classifiers for non-decomposable performance measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harikrishna</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivani</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1493" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimal classification with multivariate losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oluwasanmi</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint event extraction via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Thien Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Event detection and domain adaptation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2015</title>
		<meeting>ACL 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling skip-grams for event detection with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2016</title>
		<meeting>EMNLP 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Optimizing f-measures by cost-sensitive classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Shameem Puthiya Parambath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grandvalet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2123" to="2131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno>arX- iv:1504.06580</idno>
		<title level="m">Classifying relations by ranking with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A convolution bilstm neural network model for chinese event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NLPCC-ICCPOL</title>
		<meeting>NLPCC-ICCPOL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Active learning for word sense disambiguation with methods for addressing the class imbalance problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>EMNLPCoNLL</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
