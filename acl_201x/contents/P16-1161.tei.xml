<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Target-Side Context for Discriminative Models in Statistical Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Tamchyna</surname></persName>
							<email>{tamchyna,bojar}@ufal.mff.cuni.cz fraser@cis.uni-muenchen.de junczys@amu.edu.pl</email>
							<affiliation key="aff0">
								<orgName type="institution">LMU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Charles University in Prague</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">LMU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Charles University in Prague</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Adam Mickiewicz University in Pozna´nPozna´n</orgName>
								<address>
									<settlement>Pozna´nPozna´n</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Target-Side Context for Discriminative Models in Statistical Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1704" to="1714"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target-context model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Discriminative lexicons address some of the core challenges of phrase-based MT (PBMT) when translating to morphologically rich languages, such as Czech, namely sense disambiguation and morphological coherence. The first issue is se- mantic: given a source word or phrase, which of its possible meanings (i.e., which stem or lemma) should we choose? Previous work has shown that this can be addressed using a discriminative lex- icon. The second issue has to do with morphol- ogy (and syntax): given that we selected the cor- rect meaning, which of its inflected surface forms is appropriate? In this work, we integrate such a model directly into the SMT decoder. This enables our classifier to extract features not only from the full source sentence but also from a limited target- side context. This allows the model to not only help with semantics but also to improve morpho- logical and syntactic coherence.</p><p>For sense disambiguation, source context is the main source of information, as has been shown in previous work <ref type="bibr" target="#b24">(Vickrey et al., 2005</ref>), <ref type="bibr" target="#b3">(Carpuat and Wu, 2007)</ref>, <ref type="bibr" target="#b7">(Gimpel and Smith, 2008)</ref> inter alia. Consider the first set of examples in <ref type="figure">Figure 1</ref>, pro- duced by a strong baseline PBMT system. The English word "shooting" has multiple senses when translated into Czech: it may either be the act of firing a weapon or making a film. When the cue word "film" is close, the phrase-based model is able to use it in one phrase with the ambiguous "shooting", disambiguating correctly the transla- tion. When we add a single word in between, the model fails to capture the relationship and the most frequent sense is selected instead. Wider source context information is required for correct disam- biguation.</p><p>While word/phrase senses can usually be in- ferred from the source sentence, the correct se- lection of surface forms requires also information from the target. Note that we can obtain some information from the source. For example, an English subject is often translated into a Czech subject; in which case the Czech word should be in nominative case. But there are many deci- sions that happen during decoding which deter- mine morphological and syntactic properties of words -verbs can have translations which differ in valency frames, they may be translated in either active or passive voice (in which case subject and object would be switched), nouns may have dif- ferent possible translations which differ in gender, etc.</p><p>The correct selection of surface forms plays a crucial role in preserving meaning in morpho- logically rich languages because it is morphol- ogy rather than word order that expresses rela- tions between words. (Word order tends to be Input PBMT Output shooting of the film .</p><p>natáčení filmu . shooting camera of film . shooting of the expensive film . střelby na drah´ydrah´y film .</p><p>shootings gun at expensive film . the man saw a cat .</p><p>muž uviděl kočku . man saw cat acc . the man saw a black cat .</p><p>muž spatřilspatřilˇspatřilčernou kočku . man saw black acc cat acc . the man saw a yellowish cat .</p><p>muž spatřil nažloutlá kočka . man saw yellowish nom cat nom . <ref type="figure">Figure 1</ref>: Examples of problems of PBMT: lexical selection and morphological coherence. Each trans- lation has a corresponding gloss in italics.</p><p>relatively free and driven more by semantic con- straints rather than syntactic constraints.) The language model is only partially able to capture this phenomenon. It has a limited scope and perhaps more seriously, it suffers from data sparsity. The units captured by both the phrase ta- ble and the LM are mere sequences of words. In order to estimate their probability, we need to ob- serve them in the training data (many times, if the estimates should be reliable). However, the num- ber of possible n-grams grows exponentially as we increase n, leading to unrealistic requirements on training data sizes. This implies that the current models can (and often do) miss relationships be- tween words even within their theoretical scope.</p><p>The second set of sentences in <ref type="figure">Figure 1</ref> demon- strates the problem of data sparsity for morpho- logical coherence. While the phrase-based sys- tem can correctly transfer the morphological case of "cat" and even "black cat", the less usual "yellowish cat" is mistranslated into nominative case, even though the correct phrase "yellowish ||| nažloutlou" exists in the phrase table. A model with a suitable representation of two preceding words could easily infer the correct case in this example.</p><p>Our contributions are the following:</p><p>• We show that the addition of a feature-rich discriminative model significantly improves translation quality even for large data sizes and that target-side context information con- sistently further increases this improvement.</p><p>• We provide an analysis of the outputs which confirms that source-context features indeed help with semantic disambiguation (as is well known). Importantly, we also show that our novel use of target context improves morpho- logical and syntactic coherence.</p><p>• In addition to extensive experimentation on translation from English to Czech, we also evaluate English to German, English to Pol- ish and English to Romanian tasks, with im- provements on translation quality in all tasks, showing that our work is broadly applicable.</p><p>• We describe several optimizations which al- low target-side features to be used efficiently in the context of phrase-based decoding.</p><p>• Our implementation is freely available in the widely used open-source MT toolkit Moses, enabling other researchers to explore dis- criminative modelling with target context in MT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Discriminative Model with Target-Side Context</head><p>Several different ways of using feature-rich mod- els in MT have been proposed, see Section 6. We describe our approach in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Definition</head><p>Let f be the source sentence and e its translation. We denote source-side phrases (given a particular phrasal segmentation) ( ¯ f 1 , . . . , ¯ f m ) and the indi- vidual words (f 1 , . . . , f n ). We use a similar nota- tion for target-side words/phrases.</p><p>For simplicity, let e prev , e prev−1 denote the words preceding the current target phrase. As- suming target context size of two, we model the following probability distribution:</p><formula xml:id="formula_0">P (e|f ) ∝ ( ¯ e i , ¯ f i )∈(e,f ) P ( ¯ e i | ¯ f i , f, e prev , e prev−1 )</formula><p>(1) The probability of a translation is the product of phrasal translation probabilities which are condi- tioned on the source phrase, the full source sen- tence and several previous target words.</p><p>Let GEN( ¯ f i ) be the set of possible translations of the source phrase ¯ f i according to the phrase table. We also define a "feature vector" function fv( ¯ e i , ¯ f i , f, e prev , e prev−1 ) which outputs a vector of features given the phrase pair and its context information. We also have a vector of feature weights w estimated from the training data. Then our model defines the phrasal translation probabil- ity simply as follows:</p><formula xml:id="formula_1">P ( ¯ e i | ¯ f i , f, e prev , e prev−1 ) = exp(w · fv( ¯ e i , ¯ f i , f, e prev , e prev−1 )) ¯ e ∈GEN( ¯ f i ) exp(w · fv( ¯ e , ¯ f i , f, e prev , e prev−1 ))<label>(2)</label></formula><p>This definition implies that we have to locally normalize the classifier outputs so that they sum to one.</p><p>In PBMT, translations are usually scored by a log-linear model. Our classifier produces a single score (the conditional phrasal probability) which we add to the standard log-linear model as an addi- tional feature. The MT system therefore does not have direct access to the classifier features, only to the final score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Global Model</head><p>We use the Vowpal Wabbit (VW) classifier 1 in this work.  already integrated VW into Moses. We started from their implemen- tation in order to carry out our work. Classifier features are divided into two "namespaces":</p><p>• S. Features that do not depend on the current phrasal translation (i.e., source-and target- context features).</p><p>• T. Features of the current phrasal translation.</p><p>We make heavy use of feature processing avail- able in VW, namely quadratic feature expansions 1 http://hunch.net/ ˜ vw/ and label-dependent features. When generating features for a particular set of translations, we first create the shared features (in the namespace S). These only depend on (source and target) context and are therefore constant for all possible transla- tions of a given phrase. (Note that target-side con- text naturally depends on the current partial trans- lation. However, when we process the possible translations for a single source phrase, the target context is constant.)</p><p>Then for each translation, we extract its features and store them in the namespace T . Note that we do not provide a label (or class) to VW -it is up to these translation features to describe the target phrase. (And this is what is referred to as "label- dependent features" in VW.)</p><p>Finally, we add the Cartesian product between the two namespaces to the feature set: every shared feature is combined with every translation feature.</p><p>This setting allows us to train only a single, global model with powerful feature sharing. For example, thanks to the label-dependent format, we can decompose both the source phrase and the tar- get phrase into words and have features such as s cat t kočka which capture phrase-internal word translations. Predictions for rare phrase pairs are then more robust thanks to the rich statistics collected for these word-level feature pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Extraction of Training Examples</head><p>Discriminative models in MT are typically trained by creating one training instance per extracted phrase from the entire training data. The target side of the extracted phrase is a positive label, and all other phrases observed aligned to the extracted phrase (anywhere in the training data) are the neg- ative labels.</p><p>We train our model in a similar fashion: for each sentence in the parallel training data, we look at all possible phrasal segmentations. Then for each source span, we create a training example. We ob- tain the set of possible translations GEN( ¯ f ) from the phrase table. Because we do not have actual classes, each translation is defined by its label- dependent features and we associate a loss with it: 0 loss for the correct translation and 1 for all others.</p><p>Because we train both our model and the stan- dard phrase table on the same dataset, we use leaving-one-out in the classifier training to avoid Feature Type</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Configurations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Czech</head><p>German Polish, Romanian Source Indicator f, l, l+t, t f, l, l+t, t l, t Source Internal f, f+a, f+p, l, l+t, t, a+p f, f+a, f+p, l, l+t, t, a+p l, l+a, l+p, t, a+p Source Context f <ref type="bibr">(-3,3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>), l (-3,3), t (-5,5) f (-3,3), l (-3,3), t (-5,5) l (-3,3), t (-5,5) Target Context f (2), l (2), t (2), l+t (2) f (2), l (2), t (2), l+t (2) l (2), t (2)</head><p>Bilingual Context - l+t/l+t (2) l+t/l+t (2) Target Indicator f, l, t f, l, t l, t Target Internal f, l, l+t, t f, l, l+t, t l, t <ref type="table">Table 1</ref>: List of used feature templates. Letter abbreviations refer to word factors: f (form), l (lemma), t (morphological tag), a (analytical function), p (lemma of dependency parent). Numbers in parentheses indicate context size.</p><p>over-fitting. We look at phrase counts and co- occurrence counts in the training data, we subtract one from the number of occurrences for the cur- rent source phrase, target phrase and the phrase pair. If the count goes to zero, we skip the train- ing example. Without this technique, the classifier might learn to simply trust very long phrase pairs which were extracted from the same training sen- tence.</p><p>For target-side context features, we simply use the true (gold) target context. This leads to train- ing which is similar to language model estima- tion; this model is somewhat similar to the neural joint model for MT <ref type="bibr" target="#b6">(Devlin et al., 2014</ref>), but in our case implemented using a linear (maximum- entropy-like) model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training</head><p>We use Vowpal Wabbit in the --csoaa ldf mc setting which reduces our multi-class problem to one-against-all binary classification. We use the logistic loss as our objective. We experimented with various settings of L2 regularization but were not able to get an improvement over not using reg- ularization at all. We train each model with 10 iterations over the data.</p><p>We evaluate all of our models on a held-out set. We use the same dataset as for MT system tuning because it closely matches the domain of our test set. We evaluate model accuracy after each pass over the training data to detect over-fitting and we select the model with the highest held-out accu- racy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Feature Set</head><p>Our feature set requires some linguistic process- ing of the data. We use the factored MT setting <ref type="bibr" target="#b9">(Koehn and Hoang, 2007)</ref> and we represent each type of information as an individual factor. On the source side, we use the word surface form, its lemma, morphological tag, analytical function (such as Subj for subjects) and the lemma of the parent node in the dependency parse tree. On the target side, we only use word lemmas and mor- phological tags. <ref type="table">Table 1</ref> lists our feature sets for each language pair. We implemented indicator features for both the source and target side; these are simply con- catenations of the words in the current phrase into a single feature. Internal features describe words within the current phrase. Context features are extracted either from a window of a fixed size around the current phrase (on the source side) or from a limited left-hand side context (on the tar- get side). Bilingual context features are concatena- tions of target-side context words and their source- side counterparts (according to word alignment); these features are similar to bilingual tokens in bilingual <ref type="bibr">LMs (Niehues et al., 2011</ref>). Each of our feature types can be configured to look at any in- dividual factors or their combinations.</p><p>The features in <ref type="table">Table 1</ref> are divided into three sets. The first set contains label-independent (=shared) features which only depend on the source sentence. The second set contains shared features which depend on target-side context; these can only be used when VW is applied dur- ing decoding. We use target context size two in all our experiments. <ref type="bibr">2</ref> Finally, the third set con- tains label-dependent features which describe the currently predicted phrasal translation.</p><p>Going back to the examples from <ref type="figure">Figure 1</ref>, our model can disambiguate the translation of "shoot- ing" based on the source-context features (either the full form or lemma). For the morphologi- cal disambiguation of the translation of "yellow- ish cat", the model has access to the morpholog- ical tags of the preceding target words which can disambiguate the correct morphological case.</p><p>We used slightly different subsets of the full fea- ture set for different languages. In particular, we left out surface form features and/or bilingual fea- tures in some settings because they decreased per- formance, presumably due to over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Efficient Implementation</head><p>Originally, we assumed that using target-side con- text features in decoding would be too expen- sive, considering that we would have to query our model roughly as often as the language model. In preliminary experiments, we therefore focused on n-best list re-ranking. We obtained small gains but all of our results were substantially worse than with the integrated model, so we omit them from the paper.</p><p>We find that decoding with a feature-rich target- context model is in fact feasible. In this section, we describe optimizations at different stages of our pipeline which make training and inference with our model practical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Extraction</head><p>We implemented the code for feature extraction only once; identical code is used at training time and in decoding. At training time, the generated features are written into a file whereas at test time, they are fed directly into the classifier via its li- brary interface.</p><p>This design decision not only ensures consis- tency in feature representation but also makes the process of feature extraction efficient. In training, we are easily able to use multi-threading (already implemented in Moses) and because the process- ing of training data is a trivially parallel task, we can also use distributed computation and run sep- arate instances of (multi-threaded) Moses on sev- eral machines. This enables us to easily produce training files from millions of parallel sentences within a short time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Training</head><p>VW is a very fast classifier by itself, however for very large data, its training can be further sped up by using parallelization. We take advantage of its implementation of the AllReduce scheme which we utilize in a grid engine environment. We shuf- fle and shard the data and then assign each shard to a worker job. With AllReduce, there is a master job which synchronizes the learned weight vector with all workers. We have compared this approach with the standard single-threaded, single-process training and found that we obtain identical model accuracy. We usually use around 10-20 training jobs.</p><p>This way, we can process our large training files quickly and train the full model (using multi- ple passes over the data) within hours; effectively, neither feature extraction nor model training be- come a significant bottleneck in the full MT sys- tem training pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoding</head><p>In phrase-based decoding, translation is generated from left to right. At each step, a partial transla- tion (initially empty) is extended by translating a previously uncovered part of the source sentence. There are typically many ways to translate each source span, which we refer to as translation op- tions. The decoding process gradually extends the generated partial translations until the whole source sentence is covered; the final translation is then the full translation hypothesis with the high- est model score. Various pruning strategies are ap- plied to make decoding tractable.</p><p>Evaluating a feature-rich classifier during de- coding is a computationally expensive operation. Because the features in our model depend on target-side context, the feature function which computes the classifier score cannot evaluate the translation options in isolation (independently of the partial translation). Instead, similarly to a lan- guage model, it needs to look at previously gener- ated words. This also entails maintaining a state which captures the required context information.</p><p>A naive integration of the classifier would sim- ply generate all source-context features, all target- context features and all features describing the translation option each time a partial hypothesis is evaluated. This is a computationally very expen- sive approach.</p><p>We instead propose several technical solutions which make decoding reasonably fast. Decoding a single sentence with the naive approach takes 13.7 seconds on average. With our optimization, this average time is reduced to 2.9 seconds, i.e. almost by 80 per cent. The baseline system produces a translation in 0.8 seconds on average. Separation of source-context and target- context evaluation. Because we have a linear model, the final score is simply the dot product be- tween a weight vector and a (sparse) feature vec- tor. It is therefore trivial to separate it into two components: one that only contains features which depend on the source context and the other with target context features. We can pre-compute the source-context part of the score before decoding (once we have all translation options for the given sentence). We cache these partial scores and when the translation option is evaluated, we add the par- tial score of the target-context features to arrive at the final classifier score.</p><p>Caching of feature hashes. VW uses feature hashing internally and it is possible to obtain the hash of any feature that we use. When we en- counter a previously unseen target context (=state) during decoding, we store the hashes of extracted features in a cache. Therefore for each context, we only run the expensive feature extraction once. Similarly, we pre-compute feature hash vectors for all translation options.</p><p>Caching of final results. Our classifier locally normalizes the scores so that the probabilities of translations for a given span sum to one. This cannot be done without evaluating all translation options for the span at the same time. Therefore, when we get a translation option to be scored, we fetch all translation options for the given source span and evaluate all of them. We then normalize the scores and add them to a cache of final results. When the other translation options come up, their scores are simply fetched from the cache. This can also further save computation when we get into a previously seen state (from the point of view of our classifier) and we evaluate the same set of transla- tion options in that state; we will simply find the result in cache in such cases.</p><p>When we combine all of these optimizations, we arrive at the query algorithm shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>We run the main set of experiments on English to Czech translation. To verify that our method is function EVALUATE(t, s) span = t.getSourceSpan() if not resultCache.has(span, s) then scores = () if not stateCache.has(s) then stateCache[s] = CtxFeatures(s) end if for all t ← span.tOpts() do srcScore = srcScoreCache <ref type="bibr">[t ]</ref> c.addFeatures(stateCache <ref type="bibr">[s]</ref>) c.addFeatures(translationCache <ref type="bibr">[t ]</ref>) tgtScore = c.predict() scores[t ] = srcScore + tgtScore end for normalize(scores) resultCache <ref type="bibr">[span, s]</ref> = scores end if return resultCache <ref type="bibr">[span, s]</ref>[t] end function applicable to other language pairs, we also present experiments in English to German, Polish, and Ro- manian.</p><p>In all experiments, we use Treex (Popel andŽabokrtsk´y andˇandŽabokrtsk´andŽabokrtsk´y, 2010) to lemmatize and tag the source data and also to obtain dependency parses of all English sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">English-Czech Translation</head><p>As parallel training data, we use (subsets of) the CzEng 1.0 corpus ( <ref type="bibr">Bojar et al., 2012)</ref>. For tuning, we use the WMT13 test set ( <ref type="bibr" target="#b1">Bojar et al., 2013)</ref> and we evaluate the systems on the WMT14 test set ( <ref type="bibr" target="#b2">Bojar et al., 2014</ref>). We lemmatize and tag the Czech data using <ref type="bibr">Morphodita (Straková et al., 2014</ref>).</p><p>Our baseline system is a standard phrase-based Moses setup. The phrase table in both cases is fac- tored and outputs also lemmas and morphological tags. We train a 5-gram LM on the target side of parallel data.</p><p>We evaluate three settings in our experiments:</p><p>• baseline -vanilla phrase-based system,</p><p>• +source -our classifier with source-context features only,</p><p>• +target -our classifier with both source- context and target-context features.</p><p>For each of these settings, we vary the size of the training data for our classifier, the phrase ta- ble and the LM. We experiment with three dif- ferent sizes: small (200 thousand sentence pairs), medium (5 million sentence pairs), and full (the whole CzEng corpus, over 14.8 million sentence pairs).</p><p>For each setting, we run system weight opti- mization (tuning) using minimum error rate train- ing <ref type="bibr" target="#b13">(Och, 2003)</ref> five times and report the aver- age BLEU score. We use <ref type="bibr">MultEval (Clark et al., 2011</ref>) to compare the systems and to determine whether the differences in results are statistically significant. We always compare the baseline with +source and +source with +target. <ref type="table">Table 2</ref> shows the obtained results. Statisti- cally significant differences (α=0.01) are marked in bold. The source-context model does not help in the small data setting but brings a substantial im- provement of 0.7-0.8 BLEU points for the medium and full data settings, which is an encouraging re- sult.</p><p>Target-side context information allows our model to push the translation quality further: even for the small data setting, it brings a substantial improvement of 0.5 BLEU points and the gain re- mains significant as the data size increases. Even in the full data setting, target-side features improve the score by roughly 0.2 BLEU points.</p><p>Our results demonstrate that feature-rich mod- els scale to large data size both in terms of techni- cal feasibility and of translation quality improve- ments. Target side information seems consistently beneficial, adding further 0.2-0.5 BLEU points on top of the source-context model.  <ref type="table">Table 2</ref>: BLEU scores obtained on the WMT14 test set. We report the performance of the baseline, the source-context model and the full model.</p><p>Intrinsic Evaluation. For completeness, we report intrinsic evaluation results. We evaluate the classifier on a held-out set (WMT13 test set) by extracting all phrase pairs from the test in- put aligned with the test reference (similarly as we would in training) and scoring each phrase pair (along with other possible translations of the source phrase) with our classifier. An instance is classified correctly if the true translation obtains the highest score by our model. A baseline which always chooses the most frequent phrasal trans- lation obtains accuracy of 51.5. For the source- context model, the held-out accuracy was 66.3, while the target context model achieved accuracy of 74.8. Note that this high difference is some- what misleading because in this setting, the target- context model has access to the true target context (i.e., it is cheating).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Additional Language Pairs</head><p>We experiment with translation from English into German, Polish, and Romanian.</p><p>Our English-German system is trained on the data available for the WMT14 translation task: Europarl ( <ref type="bibr" target="#b10">Koehn, 2005</ref>) and the Common Crawl corpus, 3 roughly 4.3 million sentence pairs alto- gether. We tune the system on the WMT13 test set and we test on the WMT14 set. We use Tree- Tagger <ref type="bibr" target="#b17">(Schmid, 1994)</ref> to lemmatize and tag the German data.</p><p>English-Polish has not been included in WMT shared tasks so far, but was present as a language pair for several IWSLT editions which concentrate on TED talk translation. Full test sets are only available for 2010, 2011, and 2012. The refer- ences for 2013 and 2014 were not made public. We use the development set and test set from 2010 as development data for parameter tuning. The remaining two test sets <ref type="bibr">(2011,</ref><ref type="bibr">2012)</ref> are our test data. We train on the concatenation of Europarl and WIT 3 (Cettolo et al., 2012), ca. 750 thousand sentence pairs. The Polish half has been tagged using WCRFT <ref type="bibr" target="#b16">(Radziszewski, 2013)</ref> which pro- duces full morphological tags compatible with the NKJP tagset <ref type="bibr" target="#b15">(Przepiórkowski, 2009)</ref>.</p><p>English-Romanian was added in WMT16. We train our system using the available parallel data -Europarl and SETIMES2 <ref type="bibr" target="#b22">(Tiedemann, 2009)</ref>, roughly 600 thousand sentence pairs. We tune the English-Romanian system on the official develop- ment set and we test on the WMT16 test set. We use the online tagger by <ref type="bibr" target="#b23">Tufis et al. (2008)</ref> to pre- process the data. <ref type="table" target="#tab_2">Table 3</ref> shows the obtained results. Similarly to English-Czech experiments, BLEU scores are av-input: the most intensive mining took place there from 1953 to 1962 . baseline: nejvíce intenzivní těžba došlo tam z <ref type="bibr">roku 1953 , aby 1962 .</ref> the most intensive mining nom there occurred there from 1953 , in order to 1962 . +source: nejvíce intenzivní těžby místo tam z roku 1953 do roku 1962 . the most intensive mining gen place there from year 1953 until year 1962 . +target: nejvíce intenzivní těžba probíhala od roku 1953 do roku 1962 . the most intensive mining nom occurred from year 1953 until year 1962 . language de pl <ref type="formula" target="#formula_1">(2011)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>We manually analyze the outputs of English- Czech systems. <ref type="figure" target="#fig_1">Figure 3</ref> shows an example sen- tence from the WMT14 test set translated by all the system variants. The baseline system makes an error in verb valency; the Czech verb "došlo" could be used but this verb already has an (im- plicit) subject and the translation of "mining" ("těžba") would have to be in a different case and at a different position in the sentence. The second error is more interesting, however: the baseline system fails to correctly identify the word sense of the particle "to" and translates it in the sense of purpose, as in "in order to". The source-context model takes the context (span of years) into con- sideration and correctly disambiguates the trans- lation of "to", choosing the temporal meaning. It still fails to translate the main verb correctly, though. Only the full model with target-context information is able to also correctly translate the verb and inflect its arguments according to their roles in the valency frame. The translation pro- duced by this final system in this case is almost flawless.</p><p>In order to verify that the automatically mea- sured results correspond to visible improvements in translation quality, we carried out two annota- tion experiments. We took a random sample of 104 sentences from the test set and blindly ranked two competing translations (the selection of sen- tences was identical for both experiments). In the first experiment, we compared the baseline sys- tem with +source. In the other experiment, we compared the baseline with +target. The instruc- tions for annotation were simply to compare over- all translation quality; we did not ask the annota- tor to look for any specific phenomena. In terms of automatic measures, our selection has similar characteristics as the full test set: BLEU scores obtained on our sample are 15.08, 16.22 and 16.53 for the baseline, +source and +target respectively.</p><p>In the first case, the annotator marked 52 trans- lations as equal in quality, 26 translations pro- duced by +source were marked as better and in the remaining 26 cases, the baseline won the rank- ing. Even though there is a difference in BLEU, human annotation does not confirm this measure- ment, ranking both systems equally.</p><p>In the second experiment, 52 translations were again marked as equal. In 34 cases, +target pro- duced a better translation while in 18 cases, the baseline output won. The difference between the baseline and +target suggests that the target- context model may provide information which is useful for translation quality as perceived by hu- mans.</p><p>Our overall impression from looking at the sys- tem outputs was that both the source-context and target-context model tend to fix many morpho- syntactic errors. Interestingly, we do not observe as many improvements in the word/phrase sense disambiguation, though the source context does help semantics in some sentences. The target- context model tends to preserve the overall agree- ment and coherence better than the system with a source-context model only. We list several such examples in <ref type="figure" target="#fig_2">Figure 4</ref>. Each of them is fully cor-input: destruction of the equipment means that Syria can no longer produce new chemical weapons . +source: zničením zařízení znamená , ˇ ze S´yrieS´yrie již nemůže vytvářet nové chemické zbraně . destruction ofinstr equipment means , that Syria already cannot produce new chemical weapons . +target: zničení zařízení znamená , ˇ ze S´yrieS´yrie již nemůže vytvářet nové chemické zbraně . destruction ofnom equipment means , that Syria already cannot produce new chemical weapons . input: nothing like that existed , and despite that we knew far more about each other . +source: nic takového neexistovalo , a přesto jsme věděli daleko víc o jeden na druhého . nothing like that existed , and despite that we knew far more about onenom on other . +target: nic takového neexistovalo , a přesto jsme věděli daleko víc o sobě navzájem . nothing like that existed , and despite that we knew far more about each other . input: the authors have been inspired by their neighbours . +source: autoři byli inspirováni sv´ychsv´ych sousedůsoused˚sousedů . the authors have been inspired theirgen neighboursgen . +target: autoři byli inspirováni sv´ymisv´ymi sousedy .</p><p>the authors have been inspired theirinstr neighboursinstr . rected by the target-context model, producing an accurate translation of the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Discriminative models in MT have been proposed before. <ref type="bibr" target="#b3">Carpuat and Wu (2007)</ref> trained a maxi- mum entropy classifier for each source phrase type which used source context information to disam- biguate its translations. The models did not cap- ture target-side information and they were inde- pendent; no parameters were shared between clas- sifiers for different phrases. They used a strong feature set originally developed for word sense disambiguation. <ref type="bibr" target="#b7">Gimpel and Smith (2008)</ref> also used wider source-context information but did not train a classifier; instead, the features were in- cluded directly in the log-linear model of the de- coder. <ref type="bibr" target="#b11">Mauser et al. (2009)</ref> introduced the "dis- criminative word lexicon" and trained a binary classifier for each target word, using as features only the bag of words (from the whole source sen- tence). Training sentences where the target word occurred were used as positive examples, other sentences served as negative examples. <ref type="bibr" target="#b8">Jeong et al. (2010)</ref> proposed a discriminative lexicon with a rich feature set tailored to translation into mor- phologically rich languages; unlike our work, their model only used source-context features. Subotin (2011) included target-side context in- formation in a maximum-entropy model for the prediction of morphology. The work was done within the paradigm of hierarchical PBMT and as- sumes that cube pruning is used in decoding. Their algorithm was tailored to the specific problem of passing non-local information about morphologi- cal agreement required by individual rules (such as explicit rules enforcing subject-verb agreement). Our algorithm only assumes that hypotheses are constructed left to right and provides a general way for including target context information in the classifier, regardless of the type of features. Our implementation is freely available and can be fur- ther extended by other researchers in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We presented a discriminative model for MT which uses both source and target context infor- mation. We have shown that such a model can be used directly during decoding in a relatively efficient way. We have shown that this model consistently significantly improves the quality of English-Czech translation over a strong baseline with large training data. We have validated the ef- fectiveness of our model on several additional lan- guage pairs. We have provided an analysis show- ing concrete examples of improved lexical selec- tion and morphological coherence. Our work is available in the main branch of Moses for use by other researchers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Algorithm for obtaining classifier predictions during decoding. The variable t stands for the current translation, s is the current state and c is an instance of the classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example sentence from the test set. Each translation has a corresponding gloss in italics. Errors are marked in bold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example sentences from the test set showing improvements in morphological coherence. Each translation has a corresponding gloss in italics. Errors are marked in bold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>BLEU scores of the baseline and of the 
full model for English to German, Polish, and Ro-
manian. 

eraged over 5 independent optimization runs. Our 
system outperforms the baseline by 0.5-0.7 BLEU 
points in all cases, showing that the method is ap-
plicable to other languages with rich morphology. 

</table></figure>

			<note place="foot" n="2"> In preliminary experiments we found that using a single word was less effective and larger context did not bring improvements, possibly because of over-fitting.</note>

			<note place="foot" n="3"> http://commoncrawl.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreements no. 644402 (HimL) and 645452 (QT21), from the European Research Council (ERC) under grant agreement no. 640550, and from the SVV project num-ber 260 333. This work has been using lan-guage resources stored and distributed by the LIN-DAT/CLARIN project of the Ministry of Edu-cation, Youth and Sports of the Czech Republic (project LM2015071).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Jiří Maršík, Michal Novák, Martin Popel, and Aleš Tamchyna. 2012. The Joy of Parallelism with CzEng 1.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Zdeněkzdeněkˇzdeněkžabokrtsk´zdeněkžabokrtsk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Galuščáková</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Majliš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mareček</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<publisher>ELRA</publisher>
			<biblScope unit="page" from="3921" to="3928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation<address><addrLine>Sofia, Bulgaria, August</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="44" />
		</imprint>
	</monogr>
	<note>Findings of the 2013 Workshop on Statistical Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Findings of the 2014 workshop on statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Tamchyna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="12" to="58" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving statistical machine translation using word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wit 3 : Web inventory of transcribed and translated talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Girardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16 th Conference of the European Association for Machine Translation (EAMT)</title>
		<meeting>the 16 th Conference of the European Association for Machine Translation (EAMT)<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-05" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014-06-22" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rich Source-Side Context for Statistical Machine Translation. Columbus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<pubPlace>Ohio</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A discriminative lexicon model for complex morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Ninth Conference of the Association for Machine Translation in the Americas. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Factored translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="868" to="876" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Europarl: A Parallel Corpus for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Proceedings: the tenth Machine Translation Summit</title>
		<meeting><address><addrLine>Phuket, Thailand. AAMT, AAMT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasa</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="210" to="218" />
			<pubPlace>Suntec, Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wider Context by Using Bilingual Language Models in Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teresa</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="198" to="206" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Minimum Error Rate Training in Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Sapporo, Japan. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TectoMT: Modular NLP Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zdeněkzdeněkˇzdeněkžabokrtsk´zdeněkžabokrtsk´y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hrafn Loftsson, Eirikur Rögnvaldsson, and Sigrun Helgadottir</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6233</biblScope>
			<biblScope unit="page" from="293" to="304" />
		</imprint>
	</monogr>
	<note>Iceland Centre for Language Technology (ICLT</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A comparison of two morphosyntactic tagsets of Polish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Przepiórkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Representing Semantics in Digital Lexicography: Proceedings of MONDILEX Fourth Open Workshop</title>
		<editor>Violetta Koseska-Toszewa, Ludmila Dimitrova, and Roman Roszko</editor>
		<meeting><address><addrLine>Warsaw</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="138" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A tiered CRF tagger for Polish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Radziszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Tools for Building a Scientific Information Platform</title>
		<editor>Robert Bembenik, Lukasz Skonieczny, Henryk Rybinski, Marzena Kryszkiewicz, and Marek Niezgodka</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">467</biblScope>
			<biblScope unit="page" from="215" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probabilistic part-of-speech tagging using decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on New Methods in Language Processing</title>
		<meeting><address><addrLine>Manchester, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="44" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Open-Source Tools for Morphology, Lemmatization, POS Tagging and Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Straková</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An exponential translation model for target language morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Subotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference</title>
		<meeting><address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06-24" />
			<biblScope unit="page" from="230" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Integrating a discriminative classifier into phrase-based and hierarchical decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Tamchyna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabienne</forename><surname>Braune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="29" to="41" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Hal Daumé III, and Chris Quirk</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">News from OPUS-A collection of multilingual parallel corpora with tools and interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Natural Language Processing</title>
		<editor>N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov</editor>
		<meeting><address><addrLine>Borovets, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="237" to="248" />
		</imprint>
	</monogr>
	<note>John Benjamins, Amsterdam/Philadelphia</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Racai&apos;s linguistic web services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Tufis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Ion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Ceausu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Stefanescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation</title>
		<meeting>the International Conference on Language Resources and Evaluation<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-05-26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Word-Sense Disambiguation for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vickrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Biewald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teyssier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting><address><addrLine>Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10" />
		</imprint>
	</monogr>
	<note>Vancouver</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
