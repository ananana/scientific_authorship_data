<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transforming trees into hedges and parsing with &quot;hedgebank&quot; grammars</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Yarmohammadi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dunlop</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Oregon Health &amp; Science University</orgName>
								<address>
									<settlement>Portland</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Oregon • Google, Inc</orgName>
								<address>
									<settlement>New York</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transforming trees into hedges and parsing with &quot;hedgebank&quot; grammars</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="797" to="802"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Finite-state chunking and tagging methods are very fast for annotating non-hierarchical syntactic information, and are often applied in applications that do not require full syntactic analyses. Scenarios such as incremental machine translation may benefit from some degree of hierarchical syntactic analysis without requiring fully connected parses. We introduce hedge parsing as an approach to recovering constituents of length up to some maximum span L. This approach improves efficiency by bounding constituent size, and allows for efficient segmentation strategies prior to parsing. Unlike shallow parsing methods, hedge parsing yields internal hierarchical structure of phrases within its span bound. We present the approach and some initial experiments on different inference strategies.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Parsing full hierarchical syntactic structures is costly, and some NLP applications that could ben- efit from parses instead substitute shallow prox- ies such as NP chunks. Models to derive such non-hierarchical annotations are finite-state, so in- ference is very fast. Still, these partial annota- tions omit all but the most basic syntactic segmen- tation, ignoring the abundant local structure that could be of utility even in the absence of fully con- nected structures. For example, in incremental (si- multaneous) machine translation ( <ref type="bibr" target="#b12">Yarmohammadi et al., 2013</ref>), sub-sentential segments are trans- lated independently and sequentially, hence the fully-connected syntactic structure is not generally available. Even so, locally-connected source lan- guage parse structures can inform both segmen- tation and translation of each segment in such a translation scenario.</p><p>One way to provide local hierarchical syntactic structures without fully connected trees is to fo- cus on providing full hierarchical annotations for structures within a local window, ignoring global constituents outside that window. We follow the XML community in naming structures of this type hedges (not to be confused with the rhetorical de- vice of the same name), due to the fact that they are like smaller versions of trees which occur in se- quences. Such structures may be of utility to var- ious structured inference tasks, as well as within a full parsing pipeline, to quickly constrain subse- quent inference, much as finite-state models such as supertagging <ref type="bibr" target="#b0">(Bangalore and Joshi, 1999</ref>) or chart cell constraints <ref type="bibr" target="#b8">(Roark and Hollingshead, 2008;</ref><ref type="bibr" target="#b9">Roark et al., 2012</ref>) are used.</p><p>In this paper, we consider the problem of hedge parsing, i.e., discovering every constituent of length up to some span L. Similar constraints have been used in dependency parsing <ref type="bibr" target="#b5">(Eisner and Smith, 2005;</ref><ref type="bibr" target="#b4">Dreyer et al., 2006</ref>), where the use of hard constraints on the distance between heads and dependents is known as vine parsing. It is also reminiscent of so-called Semi-Markov models ( <ref type="bibr" target="#b11">Sarawagi and Cohen, 2004</ref>), which allow finite-state models to reason about segments rather than just tags by imposing segment length limits. In the XML community, trees and hedges are used for models of XML document instances and for the contents of elements <ref type="bibr" target="#b2">(Brüggemann-Klein and Wood, 2004</ref>). As far as we know, this paper is the first to consider this sort of partial parsing ap- proach for natural language.</p><p>We pursue this topic via tree transformation, whereby non-root non-terminals labeling con- stituents of span &gt; L in the tree are recursively elided and their children promoted to attach to their parent. In such a way, hedges are sequen- tially connected to the top-most non-terminal in the tree, as demonstrated in <ref type="figure" target="#fig_0">Figure 1</ref>. After apply- ing such a transform to a treebank, we can induce grammars and modify parsing to search as needed to recover just these constituents.</p><p>In this paper, we propose several methods to parse hedge constituents and examine their accu- racy/efficiency tradeoffs. This is compared with a baseline of parsing with a typically induced context-free grammar and transforming the result via the hedge transform, which provides a ceiling on accuracy and a floor on efficiency. We inves- tigate pre-segmenting the sentences with a finite- state model prior to hedge parsing, and achieve large speedups relative to hedge parsing the whole string, though at a loss in accuracy due to cas- cading segmentation errors. In all cases, we find it crucial that our "hedgebank" grammars be re- trained to match the conditions during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In this section, we present the details of our ap- proach. First, we present the simple tree transform from a full treebank parse tree to a (root attached) sequence of hedges. Next, we discuss modifica- tions to inference and the resulting computational complexity gains. Finally, we discuss segmenting to further reduce computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hedge Tree Transform</head><p>The hedge tree transform converts the original parse tree into a hedge parse tree. In the resulting hedge parse tree, every child of the top-most node spans at most L words. To transform an original tree to a hedge tree, we remove every non-terminal with span larger than L and attach its children to its parent. We label span length on each node by re- cursively summing the span lengths of each node's children, with terminal items by definition having span 1. A second top-down pass evaluates each node before evaluating its children, and removes nodes spanning &gt; L words. For example, the span of the non-root S, SBAR, ADJP, and VP nodes in <ref type="figure" target="#fig_0">Figure 1</ref>(a) have spans between 10 and 13, hence are removed in the tree in <ref type="figure" target="#fig_0">Figure 1</ref>(b). If we apply this transform to an entire tree- bank, we can use the transformed trees to induce a PCFG for parsing. <ref type="figure" target="#fig_2">Figure 2</ref> plots the percentage of constituents from the original WSJ Penn tree- bank (sections 2-21) retained in the transformed version, as we vary the maximum span length pa- rameter L. Over half of constituents have span 3 or less (which includes frequent base noun phrases); L = 7 covers approximately three quarters of the original constituents, and L = 15 over 90%. Most experiments in this paper will focus on L = 7, which is short enough to provide a large speedup yet still cover a large fraction of constituents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hedge Parsing</head><p>As stated earlier, our brute-force baseline ap- proach is to parse the sentence using a full context- free grammar (CFG) and then hedge-transform the result. This method should yield a ceiling on Maximum span size (L)</p><p>Percentage of constituents retained cess to compared with grammars trained on trans- formed trees; but it will be slower to parse. We aim to dramatically improve efficiency upon this baseline while losing as little accuracy as possible.</p><p>Since we limit the span of non-terminal la- bels, we can constrain the search performed by the parser, greatly reduce the CYK processing time. In essence, we perform no work in chart cells span- ning more than L words, except for the cells along the periphery of the chart, which are just used to connect the hedges to the root. Consider the flat tree in <ref type="figure" target="#fig_0">Figure 1</ref>. For use by a CYK parsing al- gorithm, trees are binarized prior to grammar in- duction, resulting in special non-terminals created by binarization. Other than the symbol at the root of the tree, the only constituents with span length greater than L in the binarized tree will be labeled with these special binarization non-terminals. Fur- ther, if the binarization systematically groups the leftmost or the rightmo non-terminals (the mo constituents with span begin at the first word at the last word (rightm straining the number o work. Complexity of parsi</p><formula xml:id="formula_0">O(n 3 |G|)</formula><p>where n is th the grammar size cons ity of parsing with a he duced to O((nL 2 + n 2 case, consider that ther or less, and each has a which accounts for th there are O(n) remain possible midpoints, wh term. Note, however, cells may be less, sin terminals is reduced.</p><p>It is possible to pars PCFG using this sort ing that only considers and speedups are achie non-optimal, since the bining hedges into flat tree. This results in d formance by tens of p standard parsing. Inst a the chart is constrain learn a grammar from bank, matched to the m the parser, which we c A hedgebank grammar and it can be used wi gorithm, i.e., these ar equivalent models. H ley grammar learner (s hedgebank grammars treebank grammars, a speedup via the gramm A unique property pared to constituents i that they are sequenti node. This property sentence into segment plete hedges, and pa dently (and simultaneo entire sentence. In sec proach to hedge segm segments with a gramm</p><p>Maximum span size (L) hedge-parsing accuracy, as it has access to rich contextual information (as compared to grammars trained on transformed trees). Naturally, inference will be slow; we aim to improve efficiency upon this baseline while minimizing accuracy loss.</p><p>Since we limit the span of non-terminal la- bels, we can constrain the search performed by the parser, greatly reduce the CYK processing time. In essence, we perform no work in chart cells span- ning more than L words, except for the cells along the periphery of the chart, which are just used to connect the hedges to the root. Consider the flat tree in <ref type="figure" target="#fig_0">Figure 1(b)</ref>. For use by a CYK parsing al- gorithm, trees are binarized prior to grammar in- duction, resulting in special non-terminals created by binarization. Other than the symbol at the root of the tree, the only constituents with span length greater than L in the binarized tree will be labeled with these special binarization non-terminals. Fur- ther, if the binarization systematically groups the leftmost or the rightmost children under these new non-terminals (the most common strategy), then constituents with span greater than L will either begin at the first word (leftmost grouping) or end at the last word (rightmost), further constraining the number of cells in the chart requiring work.</p><p>Complexity of parsing with a full CYK parser is O(n 3 |G|) where n is the length of input and |G| is the grammar size constant. In contrast, complex- ity of parsing with a hedge constrained CYK is re- duced to O((nL 2 + n 2 )|G|). To see that this is the case, consider that there are O(nL) cells of span L or less, and each has a maximum of L midpoints, which accounts for the first term. Beyond these, there are O(n) remaining active cells with O(n) possible midpoints, which accounts for the second term. Note also that these latter cells (spanning &gt; L words) may be less expensive, as the set of possible non-terminals is reduced to only those in- troduced by binarization.</p><p>It is possible to parse with a standardly induced PCFG using this sort of hedge constrained pars- ing that only considers a subset of the chart cells, and speedups are achieved, however this is clearly non-optimal, since the model is ill-suited to com- bining hedges into flat structures at the root of the tree. Space constraints preclude inclusion of tri- als with this method, but the net result is a se- vere degradation in accuracy (tens of points of F- measure) versus standard parsing. Thus, we train a grammar in a matched condition, which we call it a hedgebank grammar. A hedgebank gram- mar is a fully functional PCFG which is learned from a hedge transformed treebank. A hedgebank grammar can be used with any standard parsing algorithm, i.e., these are not generally finite-state equivalent models. However, using the Berke- ley grammar learner (see <ref type="table" target="#tab_2">§3)</ref>, we find that hedge- bank grammars are typically smaller than tree- bank grammars, reducing the grammar constant and contributing to faster inference. A unique property of hedge constituents com- pared to constituents in the original parse trees is that they are sequentially connected to the top- most node. This property enables us to chunk the sentence into segments that correspond to com- plete hedges, and parse the segments indepen- dently (and simultaneously) instead of parsing the entire sentence. In section 2.3, we present our ap- proach to hedge segmentation.</p><p>In all scenarios where the chart is constrained to search for hedges, we learn a hedgebank gram- mar, which is matched to the maximum length al- lowed by the parser. In the pre-segmentation sce- nario, we first decompose the hedge transformed treebank into its hedge segments and then learn a hedgebank grammar from the new corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hedge Segmentation</head><p>In this section we present our segmentation model which takes the input sentence and chunks it into appropriate segments for hedge parsing. We treat this as a binary classification task which decides if a word can begin a new hedge. We use hedge segmentation as a finite-state pre-processing step for hedge context-free parsing.</p><p>Our task is to learn which words can begin (B) a hedge constituent. Given a set of labeled pairs (S, H) where S is a sentence of n words w 1 . . . w n and H is its hedge parse tree, word w b belongs to B if there is a hedge constituent span- ning w b . . . w e for some e ≥ b and w b belongs to ¯ B otherwise. To predict the hedge boundaries more accurately, we grouped consecutive unary or POS-tag hedges together under a new non-terminal la- beled G. Unlabeled segmentation tags for the words in the example sentence in <ref type="figure" target="#fig_0">Figure 1(b)</ref> are:</p><formula xml:id="formula_1">"Analysts/B are/ ¯ B concerned/ ¯ B that/ ¯ B much/B of/ ¯ B the/ ¯ B high-yield/ ¯ B market/ ¯ B will/B remain/ ¯ B treacherous/ ¯ B for/ ¯ B investors/ ¯ B ./B"</formula><p>In addition to the simple unlabeled segmentation with B and ¯ B tags, we try a labeled segmenta- tion with B C and ¯ B C tags where C is hedge con- stituent type. We restrict the types to the most im- portant types -following the 11 chunk types an- notated in the CoNLL-2000 chunking task <ref type="bibr" target="#b10">(Sang and Buchholz, 2000</ref>) -by replacing all other types with a new type OUT. Thus, "Analysts" is labeled B G ; "much", B NP ; "will", B VP and so on.</p><p>To automatically predict the class of each word position, we train a multi-class classifier from la- beled training data using a discriminative linear model, learning the model parameters with the av- eraged perceptron algorithm <ref type="bibr" target="#b3">(Collins, 2002</ref>). We follow <ref type="bibr" target="#b9">Roark et al. (2012)</ref> in the features they used to label words as beginning or ending constituents. The segmenter extracts features from word and POS-tag input sequences and hedge-boundary tag output sequences. The feature set includes tri- grams of surrounding words, trigrams of surround- ing POS tags, and hedge-boundary tags of the pre- vious words. An additional orthographical fea- ture set is used to tag rare 1 and unknown words. This feature set includes prefixes and suffixes of the words (up to 4 characters), and presence of a hyphen, digit, or an upper-case character. Re- ported results are for a Markov order-2 segmenter, which includes features with the output classes of the previous two words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>We ran all experiments on the WSJ Penn Tree- bank corpus <ref type="bibr" target="#b6">(Marcus et al., 1999</ref>) using section 2-21 for training, section 24 for development, and section 23 for testing. We performed exhaustive CYK parsing using the BUBS parser 2 (Bodenstab et al., 2011) with Berkeley SM6 latent-variable grammars <ref type="bibr" target="#b7">(Petrov and Klein, 2007</ref>) learned by the Berkeley grammar trainer with default settings. We compute accuracy from the 1-best Viterbi tree extracted from the chart using the standard EVALB script. Accuracy results are reported as precision, recall and F1-score, the harmonic mean between the two. In all trials, we evaluate accuracy with respect to the hedge transformed reference <ref type="bibr">1</ref> Rare words occur less than 5 times in the training data. <ref type="bibr">2</ref>  treebank, i.e., we are not penalizing the parser for not discovering constituents longer than the max- imum length. Segmentation accuracy is reported as an F1-score of unlabeled segment bracketing. We ran timing tests on an Intel 2.66GHz proces- sor with 3MB of cache and 2GB of memory. Note that segmentation time is negligible compared to the parsing time, hence is omitted in reported time. Efficiency results are reported as number of words parsed per second (w/s). <ref type="table">Table 1</ref> presents hedge parsing accuracy on the development set for the full parsing baseline, where the output of regular PCFG parsing is trans- formed to hedges and evaluated, versus parsing with a hedgebank grammar, with no segmenta- tion of the strings. We find an order of magnitude speedup of parsing, but at the cost of 3 percent F- measure absolute. Note that most of that loss is in recall, indicating that hedges predicted in that condition are nearly as reliable as in full parsing. <ref type="table" target="#tab_1">Table 2</ref> shows the results on the development set when segmenting prior to hedge parsing. The first row shows the result with no segmentation, the same as the last row in <ref type="table">Table 1</ref> for ease of ref- erence. The next row shows behavior with per- fect segmentation. The final two rows show per- formance with automatic segmentation, using a model that includes either unlabeled or labeled segmentation tags, as described in the last section. Segmentation accuracy is better for the model with labels, although overall that accuracy is rather low. We achieve nearly another order of magnitude speedup over hedge parsing without segmentation, but again at the cost of nearly 5 percent F1. <ref type="table" target="#tab_2">Table 3</ref> presents results of our best configura- tions on the eval set, section 23. The results show the same patterns as on the development set. Fi- nally, <ref type="figure">Figure 3</ref> shows the speed of inference, la-   beled precision and labeled recall of annotating hedge constituents on the test set as a function of the maximum span parameter L, versus the baseline parser. Keep in mind that the number of reference constituents increases as L increases, hence both precision and recall can decrease as the parameter grows. Segmentation achieves large speedups for smaller L values, but the accuracy degradation is consistent, pointing to the need for improved segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>We proposed a novel partial parsing approach for applications that require a fast syntactic analysis of the input beyond shallow bracketing. The span- limit parameter allows tuning the annotation of in- ternal structure as appropriate for the application domain, trading off annotation complexity against inference time. These properties make hedge pars- ing potentially very useful for incremental text or speech processing, such as streaming text analysis or simultaneous translation.</p><p>One interesting characteristic of these anno- tations is that they allow for string segmenta- tion prior to inference, provided that the segment boundaries do not cross any hedge boundaries. We found that baseline segmentation models did pro- vide a significant speedup in parsing, but that cas- cading errors remain a problem.</p><p>There are many directions of future work to pursue here. First, the current results are all for exhaustive CYK parsing, and we plan to per- form a detailed investigation of the performance of hedgebank parsing with prioritization and prun- ing methods of the sort available in BUBS <ref type="bibr" target="#b1">(Bodenstab et al., 2011</ref>). Further, this sort of annota- tion seems well suited to incremental parsing with beam search, which has been shown to achieve high accuracies even for fully connected parsing ( <ref type="bibr" target="#b13">Zhang and Clark, 2011</ref>). Improvements to the transform (e.g., grouping items not in hedges un- der non-terminals) and to the segmentation model (e.g., increasing precision at the expense of recall) could improve accuracy without greatly reducing efficiency. Finally, we intend to perform an ex- trinsic evaluation of this parsing in an on-line task such as simultaneous translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: a) Full parse tree, b) Hedge parse tree with maximum constituent span of 7 (L = 7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Percentage of constituents retained at various span length parameters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Hedge parsing a) efficiency, and b) accuracy on test data, section 23, for L = 3 20. Maximum span size (L) Maximum span size (L) a) b) Figure 3: Hedge parsing a) efficiency, and b) accuracy on test data, section 23, for L = 3-20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Hedge segmentation and parsing results on section 

24 for L = 7. 

Segmen-
Seg 
Hedge Parsing Acc/Eff 
tation 
F1 
P 
R 
F1 
w/s 
None 
n/a 87.6 84.4 86.0 25.7 
Oracle 
100 91.3 88.9 90.1 188.6 
Unlabeled 80.6 77.2 75.3 76.2 159.1 
Labeled 
83.8 83.1 79.5 81.3 195.8 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Hedge segmentation and parsing results on test data, section 23, for L = 7.</head><label>3</label><figDesc></figDesc><table>Words per second 

Full  Parsing 

Hedge  No  Seg 

Hedge  With  Seg 

0 
5 
10 
15 
20 

0 

200 

400 

600 

800 

Maximum  span  size  (L) 

Words  parsed  per  second 

Words per second 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by NSF grant #IIS-0964102. Any opinions, findings, conclu-sions or recommendations expressed in this pub-lication are those of the authors and do not neces-sarily reflect the views of the NSF.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Supertagging: An approach to almost parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="265" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beam-width prediction for efficient context-free parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Bodenstab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dunlop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting ACL: HLT</title>
		<meeting>the 49th Annual Meeting ACL: HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="440" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Balanced context-free grammars, hedge grammars and pushdown caterpillar automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Brüggemann-Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derick</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extreme Markup Languages</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vine parsing and minimum risk reranking for speed and precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the Tenth Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="201" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Parsing with soft and hard constraints on dependency length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Parsing Technology (IWPT)</title>
		<meeting>the Ninth International Workshop on Parsing Technology (IWPT)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="30" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Taylor</surname></persName>
		</author>
		<title level="m">Treebank3. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning and inference for hierarchically split PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd national conference on Artificial intelligence</title>
		<meeting>the 22nd national conference on Artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1663" to="1666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Classifying chart cells for quadratic complexity contextfree inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristy</forename><surname>Hollingshead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="745" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finite-state chart constraints for reduced complexity context-free parsing pipelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristy</forename><surname>Hollingshead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Bodenstab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="719" to="753" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2000 shared task: Chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="127" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SemiMarkov conditional random fields for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incremental segmentation and decoding strategies for simultaneous translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Yarmohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Rangarajan</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sankaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Joint Conference on Natural Language Processing (IJCNLP)</title>
		<meeting>the 6th International Joint Conference on Natural Language Processing (IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1032" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Syntactic processing using the generalized perceptron and beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="151" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
