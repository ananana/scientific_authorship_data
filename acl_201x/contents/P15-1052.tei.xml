<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Strategic Reasoning Model for Generating Alternative Answers</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">Scott</forename><surname>Stevens</surname></persName>
							<email>stevens@zas.gwz-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for General Linguistics</orgName>
								<orgName type="department" key="dep2">Center for General Linguistics</orgName>
								<orgName type="institution" key="instit1">Ruhr University Bochum</orgName>
								<orgName type="institution" key="instit2">Ruhr University Bochum</orgName>
								<address>
									<settlement>Berlin, Berlin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Benz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for General Linguistics</orgName>
								<orgName type="department" key="dep2">Center for General Linguistics</orgName>
								<orgName type="institution" key="instit1">Ruhr University Bochum</orgName>
								<orgName type="institution" key="instit2">Ruhr University Bochum</orgName>
								<address>
									<settlement>Berlin, Berlin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Reu√üe</surname></persName>
							<email>sebastian.reusse@rub.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for General Linguistics</orgName>
								<orgName type="department" key="dep2">Center for General Linguistics</orgName>
								<orgName type="institution" key="instit1">Ruhr University Bochum</orgName>
								<orgName type="institution" key="instit2">Ruhr University Bochum</orgName>
								<address>
									<settlement>Berlin, Berlin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Klabunde</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for General Linguistics</orgName>
								<orgName type="department" key="dep2">Center for General Linguistics</orgName>
								<orgName type="institution" key="instit1">Ruhr University Bochum</orgName>
								<orgName type="institution" key="instit2">Ruhr University Bochum</orgName>
								<address>
									<settlement>Berlin, Berlin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Strategic Reasoning Model for Generating Alternative Answers</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="534" to="542"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We characterize a class of indirect answers to yes/no questions, alternative answers , where information is given that is not directly asked about, but which might nonetheless address the underlying motivation for the question. We develop a model rooted in game theory that generates these answers via strategic reasoning about possible unobserved domain-level user requirements. We implement the model within an interactive question answering system simulating real estate dialogue. The system learns a prior probability distribution over possible user requirements by analyzing training dialogues, which it uses to make strategic decisions about answer selection. The system generates pragmatically natural and inter-pretable answers which make for more efficient interactions compared to a baseline.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In natural language dialogue, questions are often answered indirectly. This is particularly apparent for yes/no questions, where a wide range of re- sponses beyond literal "yes" and "no" answers is available. Sometimes indirect answers serve to an- ticipate the next step of the hearer's plan, as in (1) <ref type="bibr" target="#b0">(Allen and Perrault, 1980)</ref>, where the literal an- swer is entailed by the supplied answer, and some- times indirect answers leave it to the hearer to in- fer the literal answer from common contextual as- sumptions, as in (2) <ref type="bibr" target="#b4">(de Marneffe et al., 2009</ref>).</p><p>(1) Q: Has the train to Windsor left yet? A: It's leaving soon from gate 7.</p><p>(2) Q: Is Sue at work? A: She's sick with the flu.</p><p>But other times there is no semantic link between the question and the supplied answer. Rather, the answer must be interpreted in light of the task- specific goals of the interlocutors. Consider <ref type="formula">(3)</ref> in a context where a customer is posing questions to a real estate agent with the aim of renting an apartment.</p><p>(3) Q: Does the apartment have a garden? A: Well, it has a large balcony.</p><p>Whether there is a balcony has no logical bear- ing on whether there is a garden. Intuitively, the realtor is inferring that the customer's question might have been motivated by a more general re- quirement (perhaps the customer wants a place to grow flowers) and supplying an alternative at- tribute to satisfy that requirement. In this case the answerer must reason about which attributes of an apartment might satisfy a customer who would ask about a garden. Note that multiple motivating requirements are possible (perhaps the customer just wants to relax outside), such that the answerer might just as easily have said, "It has a large bal- cony, and there is a park close by." In either case, the hearer can infer from the lack of a direct an- swer that the apartment must not have a garden, because if it did, to say so would have been more obviously helpful.</p><p>This paper focuses on this class of answers, which we call alternative answers. We character- ize these as indirect answers to yes/no questions that offer attributes of an object under discussion which might satisfy an unobserved domain-level requirement of the questioner. We conceive of a requirement as a set of satisfying conditions, such that a particular domain-related need would be met by any one member of the set. For example, in the context of (3) we can encode a possible customer requirement of a place to grow flowers in an apart- ment, FLOWERS = {GARDEN, BALCONY}, such that either GARDEN or BALCONY would suffice to satisfy the requirement.</p><p>In order to generate alternative answers auto- matically, we must first solve two problems: (i) how does one learn and represent a space of likely user requirements?, and (ii) how does one use such a space to select indirect answers? To do this in a natural, pragmatically interpretable way, we must not only derive answers like in (3), but cru- cially, also rule out infelicitous responses like the following, where a logically possible alternative leads to incoherence due to the low probability of an appropriate requirement like {GARDEN, <ref type="bibr">BASEMENT}.</ref> (In other words, wanting a garden has little effect on the probability of wanting a base- ment.) (4) Q: Does the apartment have a garden? A: #Well, it has a large basement.</p><p>To solve these problems, we propose an approach rooted in decision-theoretic and game-theoretic analyses of indirectness in natural language <ref type="bibr" target="#b14">(van Rooij, 2003;</ref><ref type="bibr" target="#b2">Benz and van Rooij, 2007;</ref><ref type="bibr" target="#b3">Benz et al., 2011;</ref><ref type="bibr" target="#b12">Stevens et al., 2014</ref>) whereby a system uses strategic reasoning to derive an optimal re- sponse to a yes/no question given certain domain assumptions. The model operates by assuming that both the questioner and the answerer are ratio- nal, i.e. that both participants want to further their own goals, and will behave so as to maximize the probability of success at doing so. One appeal of the strategic approach is its rela- tive simplicity: the model utilizes a learned prob- ability distribution over possible domain-level re- quirements of the questioner and applies simple probabilistic reasoning to feed content selection during online answer generation. Unlike plan in- ference approaches, we do not need to represent any complex taxonomies of stimulus conditions <ref type="bibr" target="#b6">(Green and Carberry, 1994)</ref> or coherence relations <ref type="bibr" target="#b7">(Green and Carberry, 1999;</ref><ref type="bibr" target="#b1">Asher and Lascarides, 2003)</ref>.</p><p>By implementing the strategic reasoning model within a simple interactive question answering system ( <ref type="bibr" target="#b9">Konstantinova and Orasan, 2012)</ref>, simu- lating real estate dialogues with exchanges like in (3), we are able to evaluate the current approach quantitatively in terms of dialogue efficiency, per- ceived coherence of the supplied answers, and ability of users to draw natural pragmatic infer- ences. We conclude that strategic reasoning pro- vides a promising framework for developing an- swer generation methods by starting with princi- pled theoretical analyses of human dialogue.</p><p>The following section presents the model, in- cluding a concrete content selection algorithm used for producing answers to questions, and then walks through a simple illustrative example. Sec- tion 3 describes our implementation, addresses the problem of learning requirement probabilities, and presents the results of our evaluation, providing quantitative support for our approach. Section 4 concludes with a general summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>We derive our model beginning with a simple description of the discourse situation. In our case, this is an exchange of questions and answers where a user poses questions to be answered by an expert who has access to a database of in- formation that the user wants. The expert has no advance knowledge of the database, and thus must look up information as needed. Each user question is motivated by a requirement, conceived of as a (possibly singleton) set of database at- tributes (restricted for current purposes to boolean attributes), any one of which satisfies a user need (e.g. {GARDEN, BALCONY} in the previous sec- tion). Only the user has direct access to her own requirements, and only the expert can query the database to inform the user whether her require- ments can be satisfied. For current purposes we assume that each question and answer in the di- alogue pertains to a specific object o from the database which is designated as the object un- der discussion. This way we can represent an- swers and question denotations with attributes, like GARDEN, where the queried/supplied attribute is assumed to predicate over o. In these terms, the expert can either ASSERT an attribute (if it holds of o) or DENY an attribute (if it does not hold of o) in response to a user query. Now we describe the goals of the interlocutors. The user wants her requirements to be satisfied, and will not accept an object until she is sure this is the case. If it is clear that an object cannot sat- isfy one or more requirements, the user will ask to discuss a different object from the database. We can thus characterize the set of possible user re- sponses as follows: the user may ACCEPT the ob- ject as one that meets all requirements, the user may REJECT the object and ask to see something else, or the user may FOLLOW UP, continuing to pose questions about the current object. The user's goal, then, is ultimately to accept an object that in fact satisfies her requirements, and to reject any object that does not.</p><p>The expert's goal is to help the user find an op- timal object as efficiently as possible. Given this goal, the expert does better to provide alternative attributes (like BALCONY for GARDEN in <ref type="bibr">(3)</ref>) in place of simple "no" answers only when those at- tributes are relevant to the user's underlying re- quirements. To use some economic terminology, we can define the benefit (B) of looking up a po- tential alternative attribute a in the database as a binary function indicating whether a is relevant to (i.e. a member of) the user requirement œÅ which motivated the user's question. For example, in (3), if the user's question is motivated by requirement {GARDEN, BALCONY}, then the benefit of looking up whether there is a balcony is 1, because if that attribute turns out to hold of o, then the customer's requirement is satisfied. If, on the other hand, the questioner has requirement {GARDEN}, then the benefit of looking up BALCONY is 0, because this attribute cannot satisfy this requirement.</p><p>B(a|œÅ) = 1 if a ‚àà œÅ and 0 otherwise <ref type="formula">(1)</ref> Regardless of benefit, the expert incurs a cost by looking up information. To fully specify what cost means in this context, first assume a small, fixed effort cost associated with looking up an attribute. Further assume a larger cost incurred when the user has to ask a follow-up question to find out whether a requirement is satisfied. What really matters are not the raw cost amounts, which may be very small, but rather the relative cost of look- ing up an attribute compared to that of receiving a follow-up. We can represent the ratio of look-up cost to follow-up cost as a constant Œ∫, which en- codes the reluctance of the expert to look up new information. Intuitively, if Œ∫ is close to 1 (i.e. if follow-ups are not much more costly than simple look-ups), the expert should give mostly literal an- swers, and if Œ∫ is close to 0, (i.e. if relative follow- up cost is very high), the expert should look up all potentially beneficial attributes. With this, let the utility (U ) of looking up a be the benefit of looking up a minus the relative cost.</p><formula xml:id="formula_0">U (a|œÅ) = B(a|œÅ) ‚àí Œ∫ (2)</formula><p>The expert is utility-maximizing under game- theoretic assumptions, and (assuming a baseline utility of zero for doing nothing) should aim to look up attributes for which U is positive, i.e. for which benefit outweighs cost. But the expert has a problem: œÅ, on which U depends, is known only to the user. Therefore, the best the expert can do is to reason probabilistically, based on the user's ques- tion, to maximize expected utility, or the weighted average of U (a|œÅ) for all possible values of œÅ. The expected utility of looking up an attribute a can be written as the expected benefit of a-the weighted average of B(a|œÅ) for all œÅ-minus the relative cost. Let REQS be the set of all possible user re- quirements and let q be the user's question.</p><formula xml:id="formula_1">EU (a|q, REQS) = EB(a|q, REQS) ‚àí Œ∫ (3) EB(a|q, REQS) = œÅ‚ààREQS P (œÅ|q) √ó B(a|œÅ) (4)</formula><p>The probability of a user requirement P (œÅ|q) is calculated via Bayes' rule, assuming that users will choose their questions randomly from the set of questions whose denotations are in their re- quirement set. This yields the following.</p><formula xml:id="formula_2">P (œÅ|q) = P (q|œÅ) √ó P (œÅ) œÅ ‚ààREQS P (q|œÅ ) √ó P (œÅ )<label>(5)</label></formula><p>P (q|œÅ) = 1 |œÅ| if q ‚àà œÅ and 0 otherwise <ref type="formula">(6)</ref> The prior probability of a user requirement, P (œÅ), is given as input to the model. We will see in the next section that it is possible to learn a prior prob- ability distribution from training dialogues. We have now fully characterized the expected benefit (EB) of looking up an attribute in the database. As per Eq.3, the expert should only bother looking up an attribute if EB is greater than the relative cost Œ∫, since that is when EU is positive. The final step is to give the expert a sensible way to iteratively look up attributes to potentially produce multiple alternatives. To this end, we first point out that if an alternative has been found which satisfies a certain requirement, then it no longer adds any benefit to consider that requirement when selecting further alternatives. For example, in the context of example <ref type="formula">(3)</ref>, when the realtor queries the database to find the apart- ment has a balcony, she no longer needs to con- sider the probability of a requirement {BALCONY, GARDEN} when considering additional attributes, since that is already satisfied. Given this consid- eration, the order in which database attributes are looked up can make a difference to the outcome. So, we need a consistent and principled criterion for determining the order in which to look up at- tributes. The most efficient method is to start with the attribute with the highest possible EB value and then iteratively move down to the next best at- tribute until EB is less than or equal to cost.</p><p>Note that the attribute that was asked about will always have an EB value of 1. Consider again the QA exchange in <ref type="bibr">(3)</ref>. Recall that the expert as- sumes that the user's query is relevant to an un- derlying requirement œÅ. This means that œÅ must contain the attribute GARDEN. Therefore, by defi- nition, supplying GARDEN will always yield posi- tive benefit. We can use this fact to explain how al- ternative answers are interpreted by the user. The user knows that the most beneficial attribute to look up (in terms of EB) is the one asked about. If that attribute is not included in the answer, the user is safe to assume that it does not hold of the object under discussion. By reasoning about the expert's reasoning, the user can derive the implicature that the literal answer to her question is "no". In fact, this is what licenses the expert to leave the nega- tion of the garden attribute out of the answer: the expert knows that the user knows that the expert would have included it if it were true. This type of "I know that you know" reasoning is characteristic of game-theoretic analysis. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Algorithm and example</head><p>Our algorithm for generating alternative answers (Algorithm 1), which simulates strategic reason- ing by the expert in our dialogue situation, is couched in a simple information state update (ISU) framework <ref type="bibr" target="#b10">(Larsson and Traum, 2000;</ref><ref type="bibr" target="#b13">Traum and Larsson, 2003)</ref>, whereby the answerer keeps track of the current object under discussion (o) as well as a history of attributes looked up for o (HIST o ). The output of the algorithm takes the form of a dialogue move, either an assertion (or set of assertions) or denial that an attribute holds of o. These dialogue moves can then be translated into natural language with simple sentence templates. The answerer uses HIST o to make sure redundant alternatives aren't given across QA exchanges. If</p><formula xml:id="formula_3">Requirement set P (œÅ) P (q|œÅ) P (œÅ|q) œÅ G ={GARDEN} 0.5 1 0.67 œÅ F ={GARDEN, BALCONY} 0.25 0.5 0.17 œÅ P ={GARDEN, PARK}</formula><p>0.2 0.5 0.13 œÅ S ={GARDEN, BASEMENT} 0.05 0.5 0.03 <ref type="table">Table 1</ref>: A toy example of a customer requirement space with probabilities for q = 'Does the apart- ment have a garden?' all possible answers are redundant, the answerer falls back on a direct yes/no response.</p><p>To illustrate how the algorithm works, consider a simple toy example. <ref type="table">Table 1</ref> gives a hypothetical space of possible requirements along with a dis- tribution of priors, likelihoods and Bayesian pos- teriors. We imagine that a customer might want a garden (œÅ G ), or more generally a place to grow flowers (œÅ F ), a place for their child to play outside (œÅ P ), or, in rare cases, either a garden or a base- ment to use as storage space (œÅ S ). The rather odd nature of œÅ S is reflected in its low prior. Consider a variant of <ref type="formula">(3)</ref>  To start, let REQS contain the requirements in <ref type="table">Table 1</ref>, and let Œ∫ = 0.1. The algorithm derives the answer as follows. First, the algorithm looks up whether GARDEN holds of o. It does not hold, so GARDEN is not added to the answer; it is only added to the history of looked up attributes. a = GARDEN; EB(GARDEN) = 1; HISTo = {GARDEN} Then, the system finds the next best attribute, BAL- CONY, which does hold of o, appends it to the an- swer as well as the history, and removes the rele- vant requirement from consideration. The attribute BASEMENT is next in line. However, its EB value is below the threshold of 0.1 due</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 An algorithm for generating alternative answers</head><p>Input: A set of attributes Œ¶, an object under discussion o, a database DBo of attributes which hold of o, a history HISTo of attributes that have been looked up in the database, a set of possible user requirements REQS, a prior probability distribution over REQS, a user-supplied question q with denotation q and a relative cost threshold Œ∫ ‚àà (0, 1) Initialize: ANSWER = {}; LOOKUP = TRUE 1: while LOOKUP do 2: Œ¶ = (Œ¶ \ HISTo) ‚à™ {q} Only consider alternatives once per object per dialogue. 3: a = arg max œÜ‚ààŒ¶ EB(œÜ|q, REQS) Find the best candidate answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>if EB(a|q,</p><note type="other">REQS) &gt; Œ∫ then Check whether expected benefit outweighs cost. 5: HISTo = HISTo ‚à™ {a} Log which attribute has been looked up. 6: if a ‚àà DBo then 7: ANSWER = ANSWER ‚à™ {a} Add to answer if attribute holds. 8: REQS = REQS \ {œÅ ‚àà REQS | œÅ ‚à© ANSWER = ‚àÖ} Don't consider requirements that are already satisfied. 9: end if 10: else 11: LOOKUP = FALSE Stop querying the database when there are no promising candidates left. 12: end if 13: end while 14: if ANSWER = ‚àÖ then ASSERT(ANSWER), 15: else DENY(q) 16: end if</note><p>to its low prior probability, and thus the iteration stops there, and BASEMENT is never looked up. a = BASEMENT; EB(BASEMENT) = 0.03; EB &lt; Œ∫; exit loop 3 Implementation and evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>A simple interactive question answering sys- tem was built using a modified version of the PyTrindiKit toolkit 2 with a database back end im- plemented using an adapted version of PyKE, a Horn logic theorem prover. <ref type="bibr">3</ref> The system was set up to emulate the behavior of a real estate agent answering customers' yes/no questions about a range of attributes pertaining to individual apart- ments. A set of 12 attributes was chosen for the current evaluation experiment. The system gen- erates answers by first selecting a discourse move (i.e. assertion or denial of an attribute) and then translating the move into natural language with simple sentence templates like, "It has a(n) X" or "There is a(n) X nearby". When answers are in- direct (i.e. not asserting or denying the attribute asked about), the system begins its reply with the discourse connective "well" as in example (3). <ref type="bibr">4</ref> 2 https://code.google.com/p/ py-trindikit 3 http://pyke.sourceforge.net/ 4 Early feedback indicated that alternative answers were more natural when preceded by such a discourse connective. To assess this effect, we ran a separate evaluation experiment with an earlier version of the system that produced alterna- tive answers without "well". Dialogue lengths and coherence scores were not very different from what is reported in this Subjects interacted with our system by means of an online text-based interface accessible remotely through a web browser. At the outset of the exper- iment, subjects were told to behave as if they were finding an apartment for a hypothetical friend, and given a list of requirements for that friend. The task required them to identify which from among a sequence of presented apartments would satisfy the given set of requirements. One out of four lists, each containing three requirements (one of which was a singleton), was assigned to subjects at random. The requirements were constructed by the researchers to be plausible desiderata for users looking for a place to rent or buy (e.g. connection to public transit, which could be satisfied either by a nearby bus stop, or by a nearby train station).</p><p>The apartments presented by the system were individually generated for each experiment such that there was an apartment satisfying one attribute for each possible combination of the three require- ments issued to subjects, plus two additional apart- ments that each satisfied two of the conditions (2 3 + 2 = 10 apartments overall). Attributes out- side a subject's requirement sets were added at random to assess the effect of "unhelpful" alter- native answers.</p><p>Subject interacted with one of two answer gen- eration models: a literal model, which only pro- duced direct yes/no answers, and the strategic section; however, in contrast with the current evaluation, we found a large effect of model type (a 69% decrease for strate- gic vs. literal) on whether the subjects successfully completed the task (z=-2.19, p=0.03). This is consistent with the early feedback. model as outlined above. Crucially, in both con- ditions, the sequence in which objects were pre- sented was fixed so that the last apartment of- fered would be the sole object satisfying all of the desired criteria. Also, we set the strategic model's Œ∫ parameter high enough ( 1 /7) that only single-attribute answers were ever given. These two properties of the task, taken together, allow us to obtain an apples-to-apples comparison of the models with respect to average dialogue length. If subjects failed to accept the optimal solution, the interaction was terminated. After completing in- teraction with our system, subjects were asked to complete a short survey designed to get at the per- ceived coherence of the system's answers. Sub- jects were asked to rate, on a seven-point Likert scale, the relevance of the system's answers to the questions asked, overall helpfulness, the extent to which questions seemed to be left open, and the extent to which the system seemed evasive.</p><p>We predict that the strategic system will im- prove overall efficiency of dialogue over that of the literal system by (i) offering helpful alternatives to satisfy the customer's needs, and (ii) allowing cus- tomers to infer implicit "no" answers from alterna- tive answers, leading to rejections of sub-optimal apartments. If, contrary to our hypothesis, sub- jects fail to draw inferences/implicatures from al- ternative answers, then we expect unhelpful alter- natives (i.e. alternatives not in the user's require- ment set) to prompt repeated questions and/or fail- ures to complete the task.</p><p>With respect to the questionnaire items, the lit- eral system is predicted to be judged maximally coherent, since only straightforward yes/no an- swers are offered. The question is whether the pragmatic system also allows for coherent dia- logue. If subjects judge alternative answers to be incoherent, then we expect any difference in aver- age Likert scale ratings between strategic and lit- eral system to reflect the proportion of alternative answers that are given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning prior probabilities</head><p>Before presenting our results, we explain how prior probabilities can be learned within this framework. One of the assumptions of the strate- gic reasoning model is that users ask questions that are motivated by specific requirements. Moreover, we should assume that users employ a reason- able questioning strategy for finding out whether S: An apartment in the north of town might suit you. I have an additional offer for you there. U: Does the apartment have a garden? S: The apartment does not have a garden. U: Does the apartment have a balcony? S: The apartment does not have a balcony. U: I'd like to see something else <ref type="figure">Figure 1</ref>: An example of the negation-rejection se- quence GARDEN, BALCONY requirements hold, which is tailored to the sys- tem they are interacting with. For example, if a user interacts with a system that only produces lit- eral yes/no answers, the user should take all an- swers at face value, not drawing any pragmatic inferences. In such a scenario, we expect the user's questioning strategy to be roughly as fol- lows: for a 1 , a 2 , ¬∑ ¬∑ ¬∑ , a n in requirement œÅ, ask about a 1 , then if a 1 is asserted, accept (or move on to the next requirement if there are multiple re- quirements), and if not, ask about a 2 ; if a 2 is as- serted, accept, and if not, ask about a 3 , and so on, until a n is asked about. If a n is denied, then reject the object under discussion. If you need a place to grow flowers, ask if there is a balcony or gar- den, then, if the answer is no, ask about the other attribute. If no "yes" answers are given, reject.</p><p>Such a strategy predicts that potential user re- quirements should be able to be gleaned from dia- logues with a literal system by analyzing negation- rejection sequences (NRSs). A negation-rejection sequence is a maximal observed sequence of ques- tions which all receive "no" answers, without any intervening "yes" answers or any other interven- ing dialogue moves, such that at the end of that sequence of questions, the user chooses to reject the current object under discussion. Such a se- quence is illustrated in <ref type="figure">Fig.1</ref>. By hypothesis, the NRS GARDEN, BALCONY indicates a possible user requirement {GARDEN, BALCONY}.</p><p>By considering NRSs, the system can learn from training data a reasonable prior probability distribution over possible customer requirements. This obviates the need to pre-supply the system with complex world knowledge. If customer re- quirements can in principle be learned, then the strategic approach could be expanded to dialogue situations where the distribution of user require- ments could not sensibly be pre-supplied. While the system in its current form is not guaranteed to scale up in this way, its success here provides us with a promising proof of concept.</p><p>Using the dialogues with the literal system as training data, we were able to gather frequen- cies of observed negation-rejection sequences. By transforming the sequences into unordered sets and then normalizing the frequencies of those sets, we obtained a prior probability distribution over possible customer requirements. In the training di- alogues, subjects were given the same lists of re- quirements as was given for the evaluation of the strategic model. If successful, the system should use the yes/no dialogue data to learn high prob- abilities for requirements which customers actu- ally had, and low probabilities for any others, al- lowing us to evaluate the system without giving it any prior clues as to which customer requirements were assigned.</p><p>Because we know in advance which require- ments the subjects wanted to fulfill, we have a gold standard against which we can compare the question-alternative answer pairs that different variants of the model are able to produce. For ex- ample, we know that if a subject asked whether the apartment had a balcony and received an an- swer about a nearby caf√©, that answer could not have been beneficial, since no one was assigned the requirement {CAF√â, BALCONY}. <ref type="table">Table 2</ref> compares three variant models: (i) the system we use in our evaluation, which sets prior probabilities proportional to NRS frequency, (ii) a system with flat priors, where probability is zero if NRS frequency is zero, but where all observed NRSs are taken to correspond to equiprobable re- quirements, and finally (iii) a baseline which does not utilize an EB threshold, but rather simply ran- domly selects alternatives which were observed at least once in an NRS with the queried attribute. These models are compared by the maximum ben- efit of their possible outputs using best-case val- ues for Œ∫. We see that there is a good match be- tween the answers given by the strategic model with learned priors and the actual requirements that users were told to fulfill.</p><p>Though it remains to be seen whether this would scale up to more complex requirement spaces, this result suggests that NRSs can in fact be indicative of disjunctive requirement sets, and can indeed be useful in learning what possible alternatives might be. For purposes of our evaluation, we will see that the method was successful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Precision Recall F1 Frequency-based 1 0.92 0.96 Flat 0.88 0.92 0.90 Baseline 0.23 1 0.37 <ref type="table">Table 2</ref>: Comparison of best-case output with respect to potential benefit of alternative answer types to subjects. Precision = hits / hits+misses, and Recall = hits / possible hits. A "hit" is a QA pair which is a possible output of the model, such that A could be a beneficial answer to a customer asking Q, and a "miss" is such a QA pair such that A is irrelevant to Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation results</head><p>We obtained data from a total of 115 subjects via Amazon Mechanical Turk; 65 subjects interacted with the literal comparison model, and 50 sub- jects interacted with the strategic model. We ex- cluded a total of 13 outliers across both condi- tions who asked too few or too many questions (1.5 interquartile ranges below the 1st or above the 3rd quartile). These subjects either quit the task early or simply asked all available questions even for apartments that were obviously not a good fit for their requirements. Two subjects were excluded for not filling out the post-experiment questionnaire. This left 100 subjects (59 literal/41 strategic), of which 86 (49/37) successfully com- pleted the task, accepting the object which met all assigned requirements. There was no statisti- cally significant difference between the literal and strategic models with respect to task success. We first compare the literal and strategic models with regard to dialogue length, looking only at the subjects who successfully completed the task. Due to the highly structured nature of the experiment it was always the case that a successful dialogue consisted of 10 apartment proposals, some num- ber of QA pairs, where each question was given a single answer, 9 rejections and, finally, one accep- tance. This allows us to use the number of ques- tions asked as a proxy for dialogue length. <ref type="figure" target="#fig_1">Fig- ure 2</ref> shows the comparison. The strategic model yields 27.4 questions on average, more than four fewer than the literal model's 31.6. Standard sta- tistical tests show the effect to be highly signif- icant, with a one-way ANOVA yielding F=16.2, p = 0.0001, and a mixed effects regression model with a random slope for item (the items in this case being the set of requirements assigned to the sub- 540 We now ask whether the observed effect is due only to the presence of helpful alternatives which preclude the need for follow-up questions, or whether the ability of users to draw pragmatic inferences from unhelpful alternatives (i.e. alterna- tives that don't actually satisfy the user's require- ment) also contributes to dialogue efficiency. <ref type="figure">Fig- ure 3</ref>, taken from a real dialogue with our system, illustrates such an inference. The subject specifi- cally wants a caf√© nearby, and infers from the al- ternative answer that this requirement cannot be satisfied, and therefore rejects. The subject could have asked the question again to get a direct an- swer, which would have had a negative effect on dialogue efficiency, but this did not happen. We want to know if subjects' aggregate behavior re- flects this example.</p><p>First, take the null hypothesis to be that subjects do not reliably draw such negative implicatures. In that case we would expect a certain proportion of questions to be repeated. Subjects are allowed to ask questions multiple times, and alternatives are never presented twice, such that repeating ques- tions will ultimately lead to a direct yes/no answer. We do see some instances of this behavior in the  proportion repeated questions dialogues. If this is indicative of an overall diffi- culty in drawing pragmatic inferences from an on- line dialogue system, then we expect the number of such repetitions to reflect the number of unhelp- ful alternatives that are offered. Instead, we find that when we plot a linear regression of repeated questions vs. unhelpful alternatives, we get a flat line with no observable correlation <ref type="figure" target="#fig_3">(Fig.4)</ref>. More- over, we also find no effect of unhelpful alterna- tives on whether the task was successfully com- pleted. This suggests that the correct inferences are being drawn, as in <ref type="figure">Fig.3</ref>.</p><formula xml:id="formula_4">0.0 0.1 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.2 Unhelpful alternative answers / Total answers</head><p>We now look at the perceived coherence of the dialogues as assessed by our post-experiment questionnaire. We obtain a composite coher- ence score from all coherence-related items on the seven point Likert scale by summing all per-item scores for each subject and normalizing them to a unit interval, where 1 signifies the upper bound of perceived coherence. Although there is a differ- ence in mean coherence score between the strate- gic and literal models, with the strategic model ex- hibiting 88% perceived coherence and the literal model 93%, the difference is not statistically sig- nificant. Moreover, we can rule out the possibility that the strategic model is judged to be coherent only when the number of alternative answers is low. To rule this out, we calculate the expected coherence score under the null hypothesis that co- herence is directly proportional to the proportion of literal answers. Taking the literal model's av- erage score of 0.93 as a ceiling, we multiply this by the proportion of literal answers to obtain a null hypothesis expected score of about 0.75 for the strategic model. This null hypothesis is dis- confirmed (F=12.5, t=30.6, p&lt;0.01). The strate- gic model is judged, by the criteria assessed by our post-experiment questionnaire, to be pragmat- ically coherent independently of the rate of indi- rect answers given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have characterized the class of alternative an- swers to yes/no questions and proposed a content selection model for generating these answers in dialogue. The model is based on strategic rea- soning about unobserved user requirements, and is based on work in game-theoretic pragmatics <ref type="bibr" target="#b2">(Benz and van Rooij, 2007;</ref><ref type="bibr" target="#b12">Stevens et al., 2014</ref>). The model was implemented as an answer selec- tion algorithm within an interactive question an- swering system in a real estate domain. We have presented an evaluation of this system against a baseline which produces only literal answers. The results show that the strategic reasoning approach leads to efficient dialogues, allows pragmatic in- ferences to be drawn, and does not dramatically reduce the overall perceived coherence or natural- ness of the produced answers. Although the strate- gic model requires a form of world knowledge- knowledge of possible user requirements and their probabilities-we have shown that there is a sim- ple method, the analysis of negation-rejection se- quences in yes/no QA exchanges, that can be used to learn this knowledge with positive results. Fur- ther research is required to address issues of scala- bility and generalizability, but the current model represents a promising step in the direction of pragmatically competent dialogue systems with solid basis in formal pragmatic theory.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>a</head><label></label><figDesc>= BALCONY; EB(BALCONY) = 0.17; HISTo = {GARDEN, BALCONY}; ANSWER = {BALCONY}; REQS = {œÅG, œÅP , œÅS} The attribute PARK is similarly added. a = PARK; EB(PARK) = 0.13; HISTo = {GARDEN, BALCONY, PARK}; ANSWER = {BALCONY, PARK}; REQS = {œÅG, œÅS}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Avg. number of QA pairs by model S: How about an apartment in the east of the city? I have an offer for you there. U: Does the apartment have a caf√© nearby? S: Well, there is a restaurant nearby. U: I'd like to see something else Figure 3: A QA exchange from a dialogue where the user was instructed to find an apartment with a caf√© nearby</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Repeated questions / Total questionsFigure 4 :</head><label>4</label><figDesc>Figure 4: Proportion unhelpful alternatives vs. proportion repeated questions</figDesc></figure>

			<note place="foot" n="1"> It can be shown that the answer selection algorithm presented in this section, combined with a simple user interpretation model, constitutes a perfect Bayesian equilibrium (Harsanyi, 1968; Fudenberg and Tirole, 1991) in a signaling game (Lewis, 1969) with private hearer types which formally describes this kind of dialogue.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the Deutsche Forschungsgemeinschaft (DFG) (Grant nrs. BE 4348/3-1 and KL 1109/6-1, project 'Pragmatic Requirements for Answer Generation in a Sales Dialogue'), and by the Bundesministerium f√ºr Bildung und Forschung (BMBF) (Grant nr. 01UG0711).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing intention in utterances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">F</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Raymond</forename><surname>Perrault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="143" to="178" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Logics of Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Asher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lascarides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in Natural Language Processing</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Optimal assertions, and what they implicate. a uniform game theoretic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Benz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Van Rooij</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Topoi</publisher>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="63" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A decision-theoretic approach to finding optimal responses to over-constrained queries in a conceptual search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Benz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuria</forename><surname>Bertomeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Strekalova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Workshop on the Semantics and Pragmatics of Dialogue</title>
		<meeting>the 15th Workshop on the Semantics and Pragmatics of Dialogue</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="37" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Not a simple yes or no</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the SIGDIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="136" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Perfect Bayesian equilibrium and sequential equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Fudenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Tirole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Theory</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="260" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating indirect answers to yes-no questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Carberry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Natural Language Generation</title>
		<meeting>the Seventh International Workshop on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interpreting and generating indirect answers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Carberry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="389" to="435" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Games of incomplete information played by &apos;Bayesian&apos; players, part II</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Harsanyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="320" to="334" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Konstantinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantin</forename><surname>Orasan</surname></persName>
		</author>
		<title level="m">Interactive question answering. Emerging Applications of Natural Language Processing: Concepts and New Research</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="149" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Information state and dialogue management in the TRINDI dialogue move engine toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Staffan</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Traum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3&amp;4</biblScope>
			<biblScope unit="page" from="323" to="340" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Convention: A Philosophical Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Indirect answers as potential solutions to decision problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">Scott</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Benz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Reu√üe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronja</forename><surname>Laarmann-Quante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Klabunde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Workshop on the Semantics and Pragmatics of Dialogue</title>
		<meeting>the 18th Workshop on the Semantics and Pragmatics of Dialogue</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="145" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The information state approach to dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Staffan</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Current and new directions in discourse and dialogue</title>
		<editor>Jan van Kuppevelt and Ronnie W. Smith</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="325" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Questioning to resolve decision problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert Van Rooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistics and Philosophy</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="727" to="763" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
