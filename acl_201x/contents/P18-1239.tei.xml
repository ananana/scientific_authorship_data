<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Translations via Images with a Massively Multilingual Image Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 2566</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hewitt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer and Information Science Department</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer and Information Science Department</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Callahan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer and Information Science Department</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reno</forename><surname>Kriz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer and Information Science Department</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derry</forename><surname>Wijaya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer and Information Science Department</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer and Information Science Department</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Translations via Images with a Massively Multilingual Image Dataset</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2566" to="2576"/>
							<date type="published">July 15-20, 2018. 2018. 2566</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We conduct the most comprehensive study to date into translating words via images. To facilitate research on the task, we introduce a large-scale multilingual corpus of images, each labeled with the word it represents. Past datasets have been limited to only a few high-resource languages and unrealistically easy translation settings. In contrast, we have collected by far the largest available dataset for this task, with images for approximately 10,000 words in each of 100 languages. We run experiments on a dozen high resource languages and 20 low resources languages , demonstrating the effect of word concreteness and part-of-speech on translation quality. To improve image-based translation, we introduce a novel method of predicting word concreteness from images , which improves on a previous state-of-the-art unsupervised technique. This allows us to predict when image-based translation may be effective, enabling consistent improvements to a state-of-the-art text-based word translation system. Our code and the Massively Multilingual Image Dataset (MMID) are available at http: //multilingual-images.org/.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning the translations of words is important for machine translation and other tasks in natu- ral language processing. Typically this learning is done using sentence-aligned bilingual parallel texts. However, for many languages, there are not * These authors contributed equally; listed alphabetically. sufficiently large parallel texts to effectively learn translations. In this paper, we explore the question of whether it is possible to learn translations with images. We systematically explore an idea origi- nally proposed by Bergsma and Van Durme (2011): translations can be identified via images associated with words in different languages that have a high degree of visual similarity. This is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Most previous image datasets compiled for the task of learning translations were limited to the translation of nouns in a few high-resource lan- guages. In this work, we present a new large-scale dataset that contains images for 100 languages, and is not restricted by part-of-speech. We collected im- ages using Google Image Search for up to 10,000 words in each of 100 foreign languages, and their English translations. For each word, we collected up to 100 images and the text on images' corre- sponding web pages.</p><p>We conduct a broad range of experiments to eval- uate the utility of image features across a number of factors:</p><p>• We evaluate on 12 high-resource and 20 low- resource languages.</p><p>• We evaluate translation quality stratified by part-of-speech, finding that nouns and adjec- tives are translated with much higher accuracy than adverbs and verbs.</p><p>• We present a novel method for predicting word concreteness from image features that better correlates with human perception than existing methods. We show that choosing con- crete subsets of words to translate results in higher accuracy.</p><p>• We augment a state-of-the-art text-based word translation system with image feature scores and find consistent improvements to the text- only system, ranging from 3.12% absolute top-1 accuracy improvement at 10% recall to 1.30% absolute improvement at 100% recall.</p><p>A further contribution of this paper is our dataset, which is the largest of its kind and should be a stan- dard for future work in learning translations from images. The dataset may facilitate research into multilingual, multimodal models, and translation of low-resource languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The task of learning translations without sentence- aligned bilingual parallel texts is often called bilin- gual lexicon induction <ref type="bibr" target="#b23">(Rapp, 1999;</ref><ref type="bibr" target="#b4">Fung and Yee, 1998</ref>). Most work in bilingual lexicon induction has focused on text-based methods. Some re- searchers have used similar spellings across related languages to find potential translations ( <ref type="bibr" target="#b16">Koehn and Knight, 2002;</ref><ref type="bibr" target="#b7">Haghighi et al., 2008)</ref>. Oth- ers have exploited temporal similarity of word frequencies to induce translation pairs ( <ref type="bibr" target="#b25">Schafer and Yarowsky, 2002;</ref><ref type="bibr" target="#b15">Klementiev and Roth, 2006</ref>). Irvine and Callison-Burch (2017) provide a sys- tematic study of different text-based features used for bilingual lexicon induction. Recent work has focused on building joint distributional word em- bedding spaces for multiple languages, leveraging a range of levels of language supervision from bilin- gual dictionaries to comparable texts <ref type="bibr" target="#b26">(Vuli´cVuli´c and Korhonen, 2016;</ref><ref type="bibr" target="#b28">Wijaya et al., 2017</ref>). The most closely related work to ours is research into bilingual lexicon induction using image simi- larity by Bergsma and Van Durme (2011) and <ref type="bibr" target="#b12">Kiela et al. (2015)</ref>. Their work differs from ours in that they focused more narrowly on the translation of nouns for a limited number of high resource lan- guages. Bergsma and Van Durme (2011) compiled datasets for Dutch, English, French, German, Ital- ian, and Spanish by downloading 20 images for up to 500 concrete nouns in each of the foreign languages, and 20,000 English words.</p><p>Another dataset was generated by <ref type="bibr" target="#b27">Vulic and Moens (2013)</ref> who collected images for 1,000 words in Spanish, Italian, and Dutch, along with the English translations for each. Their dataset also consists of only nouns, but includes abstract nouns. Our corpus will allow researchers to explore im- age similarity for bilingual lexicon induction on a much wider range of languages and parts of speech, which is especially desirable given the potential utility of the method for improving translation be- tween languages with little parallel text.</p><p>The ability of images to usefully represent a word is strongly dependent on how concrete or abstract the word is. The terms abstractness and concreteness are used in the psycholinguistics and cognitive psychology literature. Concrete words directly reference a sense experience ( <ref type="bibr" target="#b20">Paivio et al., 1968)</ref>, while abstract words can denote ideas, emo- tions, feelings, qualities or other abstract or intan- gible concepts. Concreteness ratings are closely correlated with imagery ratings, defined as the ease with which a word arouses a mental image <ref type="bibr" target="#b6">(Gilhooly and Logie, 1980;</ref><ref type="bibr" target="#b2">Friendly et al., 1982)</ref>. Intuitively, concrete words are easier to represent visually, so a measure of a word's concreteness ought to be able to predict the effectiveness of us- ing images to translate the word. <ref type="bibr" target="#b11">Kiela et al. (2014)</ref> defines an unsupervised method called image dispersion that approximates a word's concreteness by taking the average pair- wise cosine distance of a set of image representa- tions of the word. <ref type="bibr" target="#b12">Kiela et al. (2015)</ref> show that image dispersion helps predict the usefulness of image representations for translation. In this paper, we introduce novel supervised approaches for pre- dicting word concreteness from image and textual features. We make use of a dataset created by <ref type="bibr">Brysbaert et al. (2014)</ref> containing human evaluations of concreteness for 39,954 English words.</p><p>Concurrently with our work, <ref type="bibr" target="#b8">Hartmann and Søgaard (2017)</ref> released an unpublished arXiv draft challenging the efficacy of using images for transla- tion. Their work presents several difficulties of us- ing image features for translation, difficulties which our methods address. They find that image features are only useful in translating simple nouns. While we did indeed find that nouns perform better than other parts of speech, we do not find that images are only effective in translating simple words. Instead, we show a gradual degradation in performance as words become more abstract. Their dataset is re- stricted to six high-resource languages and a small vocabulary of 557 English words. In contrast, we present results for over 260,000 English words and 32 foreign languages.</p><p>Recent research in the NLP and computer vision communities has been enabled by large collections of images associated with words or longer texts. Object recognition has seen dramatic gains in part due to the ImageNet database <ref type="bibr">(Deng et al., 2009)</ref>, which contains 500-1000 images associated with 80,000 synsets in WordNet. <ref type="bibr" target="#b1">Ferraro et al. (2015)</ref> surveys existing corpora that are used in vision and language research. Other NLP+Vision tasks that have been enabled by the availability of large datasets include caption generation for images, ac- tion recognition in videos, visual question answer- ing, and others.</p><p>Most existing work on multilingual NLP+Vision relies on having a corpus of images manually an- notated with captions in several languages, as in the Multi30K dataset <ref type="bibr" target="#b0">(Elliott et al., 2016)</ref>. Sev- eral works have proposed using image features to improve sentence level translations or to translate image captions <ref type="bibr" target="#b5">(Gella et al., 2017;</ref><ref type="bibr" target="#b9">Hitschler and Riezler, 2016;</ref><ref type="bibr" target="#b18">Miyazaki and Shimizu, 2016)</ref>. <ref type="bibr" target="#b3">Funaki and Nakayama (2015)</ref> show that automatically scraped data from websites in English and Japanese can be used to effectively perform zero-shot learn- ing for the task of cross-lingual document retrieval. Since collecting multilingual annotations is diffi- cult at a large-scale or for low-resource languages, our approach relies only on data scraped automati- cally from the web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Corpus Construction</head><p>We present a new dataset for image-based word translation that is more expansive than any previous ones, encompassing all parts-of-speech, the gamut of abstract to concrete, and both low-and high- resource languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dictionaries</head><p>We collect images for words in 100 bilingual dic- tionaries created by <ref type="bibr" target="#b21">Pavlick et al. (2014)</ref>. They selected the 10,000 most frequent words on Wikipedia pages in the foreign language, and then collected their translations into English via crowd- sourcing. We will denote these dictionaries as CROWDTRANS. The superset of English trans- lations for all foreign words consists of 263,102 translations. The English portion of their data tends to be much noisier than the foreign portion due to its crowdsourced nature (e.g. misspellings, or defi- nition included with translations.)</p><p>We computed part-of-speech for entries in each dictionary. We found that while nouns are the most common, other parts-of-speech are reasonably rep- resented (Section 5.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method</head><p>For each English and foreign word, we query Google Image Search to collect 100 images as- sociated with the word. A potential criticism of our use of Google Image Search is that it may be using a bilingual dictionary to translate queries into En- glish (or other high resource languages) and return- ing images associated with the translated queries <ref type="bibr" target="#b13">(Kilgarriff, 2007)</ref>. We take steps (Section 3.3) to filter out images that did not appear on pages writ- ten in the language that we are gathering images for. After assembling the collection of images asso- ciated with words, we construct low-dimensional vector representations of the images using convolu- tional neural networks (CNNs). We also save the text from each web page that an image appeared on. Further detail on our corpus construction pipeline can be found in Section 2 of the supplemental ma- terials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Filtering by Web Page Language</head><p>We used the following heuristic to filter images: if text could be extracted from an image's web page, and the expected language was in the top-3 most likely languages output by the CLD2 1 language de- tection system then we kept the image; otherwise it was discarded. This does not filter all images from webpages with English text; instead it acknowl- edges the presence of English in the multilingual web and keeps images from pages with some target- language presence. An average of approximately 42% of images for each foreign language remained after the language-filtering step.  <ref type="table">Table 1</ref>: The proportion of images determined to be good representations of their corresponding word. In columns 2-5, we bucket the results by the word's ground-truth concreteness, while column 6 shows the results over all words. The last row shows the number of words in each bucket of concreteness, and the number of words overall for each language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Manual Evaluation of Images</head><p>By using a dataset scraped from the web, we ex- pect some fraction of the images for each word to be incorrectly labeled. To confirm the overall quality of our dataset, we asked human evaluators on Amazon Mechanical Turk to label a subset of the images returned by queries in four languages: our target language, English; a representative high- resource language, French; and two low-resource languages, Indonesian and Uzbek. In total, we col- lected 36,050 judgments of whether the images re- turned by Google Image Search were a good match for the keyword. Details on the experimental setup can be found in Section 1 of the Supplemental Ma- terials. <ref type="table">Table 1</ref> shows the fraction of images that were judged to be good representations of the search word. It also demonstrates that as the concreteness of a word increases, the proportion of good images associated with that word increases as well. We further discuss the role of concreteness in Section 6.1. Overall, 85% of the English images, 72% of French, 66% of Indonesian, and 60% of Uzbek were judged to be good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Finding Translations Using Images</head><p>Can images help us learn translations for low- resource languages? In this section we replicate prior work in high-resource languages, and then evaluate on a wide array of low-resource languages.</p><p>Although we scraped images and text for 100 languages, we have selected a representative set of 32 for evaluation. <ref type="bibr" target="#b12">Kiela et al. (2015)</ref> established that CNN features are superior to the SIFT plus color histogram features used by Bergsma and Van Durme (2011), and so we restrict all analysis to the former.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Translation Prediction with AVGMAX</head><p>To learn the English translation of each foreign word, we rank the English words as candidate trans- lations based on their visual similarity with the for- eign words. We take the cosine similarity score for each image i f associated the foreign word w f with each of image i e for the English word w e , and then compute the average maximum similarity as</p><formula xml:id="formula_0">AVGMAX(w f , w e ) = 1 |w f | i f ∈w f max ie∈we (cosine(i f , i e ))</formula><p>Each image is represented by a 4096-dimensional vector from the fully connected 7th (FC7) layer of a CNN trained on ImageNet ( <ref type="bibr" target="#b17">Krizhevsky et al., 2012)</ref>. AvgMax is the best-performing method described by Bergsma and Van Durme (2011) on images created with SIFT and color histogram fea- tures. It was later validated on CNN features by <ref type="bibr" target="#b12">Kiela et al. (2015)</ref>.</p><p>The number of candidate English words is the number of entries in the bilingual dictionary after filtering out dictionary entries where the English word and foreign word are identical. In order to compare with <ref type="bibr" target="#b12">Kiela et al. (2015)</ref>, we evaluate the models' rankings using Mean Reciprocal Rank (MRR), top-1, top-5 and top-20 accuracy. We prefer the more interpretable top-k accuracy in our subsequent experiments. We choose to follow <ref type="bibr" target="#b28">Wijaya et al. (2017)</ref> in standardizing to k = 10, and we report top-1 accuracy only when it is particularly informative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Replication of Prior Work</head><p>We evaluate on the five languages-Dutch, French, German, Italian, and Spanish-which have been the focus of prior work. <ref type="table">Table 2</ref> shows the results re- ported by <ref type="bibr" target="#b12">Kiela et al. (2015)</ref> on the BERGSMA500 dataset, along with results using our image crawl method (Section 3.2) on BERGSMA500's vocabu- lary.</p><p>On all five languages, our dataset performs bet- ter than that of <ref type="bibr" target="#b12">Kiela et al. (2015)</ref>. We attribute this to improvements in image search since they collected images. We additionally note that in the BERGSMA500 vocabularies, approximately 11% of the translation pairs are string-identical, like film ↔ film. In all subsequent experiments, we remove trivial translation pairs like this.</p><p>We also evaluate the identical model on our full data set, which contains 8,500 words, covering all parts of speech and the full range of concreteness ratings. The top-1 accuracy of the model is 23% on our more realistic and challenging data set, versus 68% on the easier concrete nouns set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">High-and Low-resource Languages</head><p>To determine whether image-based translation is effective for low resource languages, we sample 12 high-resource languages (HIGHRES), and 20 low- resource languages (LOWRES). <ref type="table">Table 3</ref> reports the top-10 accuracy across all 32 languages.</p><p>For each language, we predict a translation for each foreign word in the language's CROWD- TRANS dictionary. This comes to approximately 7,000 to 10,000 foreign words per language. We find that high-resource languages' image features are more predictive of translation than those of low-resource languages. Top-10 accuracy is 29% averaged across high-resource languages, but only 16% for low-resource languages. This may be due to the quality of image search in each language, and the number of websites in each language in- dexed by Google, as suggested by <ref type="table">Table 1</ref>. The difficulty of the translation task is dependent on the size of the English vocabulary used, as distinguish- ing between 5, 000 English candidates as in Slovak is not as difficult as distinguishing between 10, 000 words as in Tamil.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Large Target Vocabulary</head><p>How does increasing the number of candidate trans- lations affect accuracy? Prior work used an English vocabulary of 500 or 1,000 words, where the cor- rect English translation is guaranteed to appear. This is unrealistic for many tasks such as machine translation, where the target language vocabulary is likely to be large. To evaluate a more realistic scenario, we take the union of the English vocab- ulary of every dictionary in CROWDTRANS, and run the same translation experiments as before. We call this large common vocabulary LARGEENG.</p><p>Confirming our intuition, experiments with LARGEENG give significantly lower top-10 accu- racies across parts of speech, but still provide dis- criminative power. We find .181 average top-10 accuracy using LARGEENG, whereas on the same languages, average accuracy on the CROWDTRANS vocabularies was .260. The full results for these experiments are reported in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation by Part-of-speech</head><p>Can images be used to translate words other than nouns? This section presents our methods for de-  <ref type="table">Table 3</ref>: Top-10 accuracy on 12 high-resource languages and 20 low-resource languages. The parts of speech Noun, Adjective, Adverb, and Verb are referred to as NN, JJ, RB, VB, respectively. The "all" column reports accuracy on the entire dictionary. The "#" column reports the size of the English vocabulary used for each experiment.  <ref type="table">Table 4</ref>: Top-10 accuracy on the expanded English dictionary task. For each experiment, 263,102 English words were used as candidate translations for each foreign word. The SMALL average is given for reference, averaging the results from <ref type="table">Table 3</ref> across the same 10 languages.</p><p>termining part-of-speech for foreign words even in low-resource languages, and presents our image- based translation results across part-of-speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Assigning POS Labels</head><p>To show the performance of our translation method for each particular POS, we first assign a POS tag to each foreign word. Since we evaluate on high- and low-resource languages, many of which do not have POS taggers, we POS tag English words, and transfer the tag to their translations. We scraped the text on the web pages associated with the images of each English word, and collected the sentences that contained each query (English) word. We chose to tag words in sentential context, rather than simply collecting parts of speech from a dictionary, be- cause many words have multiple senses, often with different parts of speech. We assign universal POS tags (Petrov et al., 2012) using spaCy 2 , giving each word its majority tag. We gathered part-of-speech tags for 42% of the English words in our translations. Of the remaining untagged English entries, 40% were multi-word ex- pressions, and 18% were not found in the text of the web pages that we scraped.</p><p>When transferring POS tags to foreign words, we only considered foreign words where every En- glish translation had the same POS. Across all 32 languages, on average, we found that, after filtering, 65% of foreign words were nouns, 14% were verbs, 14% were adjectives, 3% were adverbs, and 3% were other (i.e. they were labeled a different POS).</p><p>2 https://spacy.io </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Accuracy by Part-of-speech</head><p>As we see in the results in <ref type="table">Table 3</ref>, the highest trans- lation performance is obtained for nouns, which confirms the observation by <ref type="bibr" target="#b8">Hartmann and Søgaard (2017)</ref>. However, we see considerable signal in translating adjectives as well, with top-10 accura- cies roughly half that of nouns. This trend extends to low-resource languages. We also see that trans- lation quality is relatively poor for adverbs and verbs. There is higher variation in our performance on adverbs across languages, because there were relatively few adverbs (3% of all words.) From these results, it is clear that one can achieve higher accuracy by choosing to translate only nouns and adjectives.</p><p>Analysis by part-of-speech only indirectly addresses the question of when translation with images is useful. For example, <ref type="figure" target="#fig_1">Figure 2</ref> shows that nouns like concept translate incorrectly because of a lack of consistent visual representation. However, verbs like walk may have concrete visual representation. Thus, one might perform better overall at translation on concrete words, regardless of part-of-speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation by Concreteness</head><p>Can we effectively predict the concreteness of words in a variety of languages? If so, can these predictions be used to determine when translation via images is helpful? In this section, we answer both of these questions in the affirmative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Predicting Word Concreteness</head><p>Previous work has used image dispersion as a mea- sure of word concreteness ( <ref type="bibr" target="#b11">Kiela et al., 2014</ref>). We introduce a novel supervised method for predicting word concreteness that more strongly correlates with human judgements of concreteness.</p><p>To train our model, we took <ref type="bibr">Brysbaert et al. (2014)</ref>'s dataset, which provides human judgments for about 40k words, each with a 1-5 abstractness- to-concreteness score, and scraped 100 images from English Google Image Search for each word. We then trained a two-layer perceptron with one hidden layer of 32 units, to predict word concrete- ness. The inputs to the network were the element- wise mean and standard deviation (concatenated into a 8094-dimensional vector)of the CNN fea- tures for each of the images corresponding to a word. To better assess this image-only approach, we also experimented with using the distributional word embeddings of <ref type="bibr" target="#b24">Salle et al. (2016)</ref> as input. We used these 300-dimensional vectors either seper- ately or concatentated with the image-based fea- tures. Our final network was trained with a cross- entropy loss, although an L2 loss performed nearly as well. We randomly selected 39,000 words as our training set. Results on the remaining held-out validation set are visualized in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Although the concatenated image and word em- bedding features performed the best, we do not expect to have high-quality word embeddings for words in low-resource languages. Therefore, for the evaluation in Section 6.2, we used the image- embeddings-only model to predict concreteness for every English and foreign word in our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Accuracy by Predicted Concreteness</head><p>It has already been shown that the images of more abstract words provide a weaker signal for trans- lation ( <ref type="bibr" target="#b12">Kiela et al., 2015</ref>). Using our method for predicting concreteness, we determine which im- ages sets are most concrete, and thereby estimate the likelihood that we will obtain a high quality translation. <ref type="figure">Figure 4</ref> shows the reduction in translation ac- curacy as increasingly abstract words are included in the set. The concreteness model can be used to establish recall thresholds. For the 25% of foreign words we predict to be most concrete, (25% re- call,) AVGMAX achieves top-10 accuracy of 47.0% for high-resource languages and 32.8% for low- resource languages. At a 50% most-concrete recall treshold, top-10 translation accuracies are 25.0% and 37.8% for low-and high-resource languages re- spectively, compared to 18.6% and 29.3% at 100% recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Translation with Images and Text</head><p>Translation via image features performs worse than state-of-the-art distributional similarity-based methods. For example, <ref type="bibr" target="#b28">Wijaya et al. (2017)</ref> demon- strate top-10 accuracies in range of above 85% on the VULIC1000 a 1,000-word dataset, whereas with only image features, <ref type="bibr" target="#b12">Kiela et al. (2015)</ref> report top-10 accuracies below 60%. However, there may be utility in combining the two methods, as it is likely that visual and textual distributional repre- sentations are contributing different information, and fail in different cases.</p><p>We test this intuition by combining image scores with the current state-of-the-art system of <ref type="bibr" target="#b28">Wijaya et al. (2017)</ref>, which uses Bayesian Personalized Ranking (BPR). In their arXiv draft, <ref type="bibr" target="#b8">Hartmann and Søgaard (2017)</ref> presented a negative result when di- rectly combining image representations with distri- butional representations into a single system. Here, we present a positive result by formalizing the prob- lem as a reranking task. Our intuition is that we hope to guide BPR, clearly the stronger system, with aid from image features and a predicted con- creteness value, instead of joining them as equals and potentially washing out the stronger signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Reranking Model</head><p>For each foreign word w f and each English word w e , we have multiple scores for the pair p f,e = (w f , w e ), used to rank w e against all other w e ∈ E, where E is the English dictionary used in the ex- periment. Specifically, we have TXT(p f,e ) and IM- AGE(p f,e ) for all pairs. For each foreign word, we also have the concreteness score, CNC(w f ), pre- dicted from its image set by the method described in Section 6.1.</p><p>We use a small bilingual dictionary, taking all pairs p f,e and labeling them {±1}, with 1 denoting the words are translations. We construct training data out of the dictionary, treating each labeled pair as an independent observation. We then train a 2-layer perceptron (MLP), with 1 hidden layer of 4 units, to predict translations from the individual scores, minimizing the squared loss. 3</p><formula xml:id="formula_1">MLP(p f,e ) = MLP [TXT(p f,e ); IMAGE(p f,e ); CNC(w f )] = {±1}</formula><p>3 We use DyNet ( <ref type="bibr">Neubig et al., 2017</ref>) for constructing and training our network with the Adam optimization method <ref type="bibr" target="#b14">(Kingma and Ba, 2014</ref>  Once the model is trained, we fix each foreign word w f , and score all pairs (w f , w e ) for all e ∈ E, using the learned model MLP(p f,e ). Using these scores, we sort E for each w f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Evaluation</head><p>We evaluate our text-based and image-based com- bination method by translating Bosnian, Dutch, French, Indonesian, Italian, and Spanish into En- glish. For each language, we split our bilingual dictionary (of 8,673 entries, on average) into 2,000 entries for a testing set, 20% for training the text- based BPR system, 35% for training the reranking MLP, and the rest for a development set. We filtered out multi-word phrases, and translations where w f and w e are string identical.</p><p>We compare three models: TXT is <ref type="bibr" target="#b28">Wijaya et al. (2017)</ref>'s text-based state-of-the-art model. TXT+IMG is our MLP-learned combination of the two features. TXT+IMG+CNC uses our pre- dicted concreteness of the foreign word as well.</p><p>We evaluate all models on varying percents of test- ing data sorted by predicted concreteness, as in Section 6.2. As shown in <ref type="figure">Figure 5</ref>, both image- augmented methods beat TXT across concreteness thresholds on the top-1 accuracy metric.</p><p>Results across the 6 languages are reported in <ref type="table" target="#tab_5">Table 5</ref>. Confirming our intuition, images are use- ful at high concreteness, improving the SOA text- based method 3.21% at 10% recall. At 100% recall our method with images still improves the SOA by 1.3%. For example, the text-only system translates the Bosnian word košarkaški incorrectly as foot- ball, whereas the image+text system produces the correct basketball.</p><p>Further, gains are more pronounced for low- resource languages than for high-resource lan- guages. Concreteness scores are useful for high- resource languages, for example Spanish, where TXT+IMG falls below TXT alone on more ab- stract words, but TXT+IMG+CNC remains an im- provement. Finally, we note that the text-only sys- tem also performs better on concrete words than abstract words, indicating a general trend of ease in translating concrete words regardless of method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Summary</head><p>We have introduced a large-scale multilingual im- age resource, and used it to conduct the most com- prehensive study to date on using images to learn translations. Our Massively Multilingual Image Dataset will serve as a standard for future work in image-based translation due to its size and general- ity, covering 100 languages, hundreds of thousands <ref type="figure">Figure 5</ref>: Reranking top-1 and top-10 accuracies of our image+text combination sytems compared to the text-only Bayesian Personalized Ranking system. The X-axis shows percent of foreign words evaluated on, sorted by decreasing predicted concreteness.  of words, and a broad range of parts of speech. Us- ing this corpus, we demonstrated the substantial utility in supervised prediction of word concrete- ness when using image features, improving over the unsupervised state-of-the-art and finding that image-based translation is much more accurate for concrete words. Because of the text we collected with our corpus, we were also able to collect part- of-speech information and demonstrate that im- age features are useful in translating adjectives and nouns. Finally, we demonstrate a promising path forward, showing that incorporating images can im- prove a state-of-the-art text-based word translation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>% words evaluated</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Dataset and Code</head><p>The MMID will be distributed both in raw form and for a subset of languages in mem- ory compact featurized versions from http: //multilingual-images.org along with code we used in our experiments. Additional de- tails are given in our Supplemental Materials doc- ument, which also describes our manual image annotation setup, and gives numerous illustrative examples of our system's predictions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our dataset and approach allow translations to be discovered by comparing the images associated with foreign and English words. Shown here are five images for the Indonesian word kucing, a word with high predicted concreteness, along with its top 4 ranked translations using CNN features.</figDesc><graphic url="image-1.png" coords="1,307.28,222.54,218.26,141.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Shown here are five images for the abstract Indonesian word konsep, along with its top 4 ranked translations using CNN features. The actual translation, concept, was ranked 3,465.</figDesc><graphic url="image-2.png" coords="6,307.28,62.81,218.27,149.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Plots visualizing the distribution of concreteness predictions on the validation set for our three trained models and for image dispersion. Spearman correlation coefficients are shown. For the model trained only on images, the three worst failure cases are annotated. False positives tend to occur when one concrete meaning of an abstract word dominates the search results (i.e. many photos of "satisfyingly" show food). False negatives often stem from related proper nouns or an overabundance of clipart, as is the case for "fishhook."</figDesc><graphic url="image-6.png" coords="8,65.99,62.62,135.30,135.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Top-1 accuracy results across high-resource (Dutch, 
French, Italian, Spanish) and low-resource (Bosnian, Indone-
sian) languages. Words evaluated on are again sorted by con-
creteness for the sake of analysis. The best result on each % 
of test data is bolded. 

</table></figure>

			<note place="foot" n="1"> https://github.com/CLD2Owners/cld2</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We gratefully acknowledge Amazon for its sup-port of this research through the Amazon Research Awards program and through AWS Research Cred-its.</p><p>This material is based in part on research spon-sored by DARPA under grant number HR0011-15-C-0115 (the LORELEI program). The U.S. Gov-ernment is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA and the U.S. Government. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multi30k: Multilingual englishgerman image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno>abs/1605.00459</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey of current datasets for vision and language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Hao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="207" to="213" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Toronto word pool: Norms for imagery, concreteness, orthographic variables, and grammatical usage for 1,080 words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Friendly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><forename type="middle">E</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">C</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods &amp; Instrumentation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="375" to="399" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagemediated learning for zero-shot cross-lingual document retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruka</forename><surname>Funaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Nakayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="585" to="590" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An IR approach for translating new words from nonparallel, comparable texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Lo Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international Conference on Computational Linguistics</title>
		<meeting>the 17th international Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="414" to="420" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image pivoting for learning multilingual multimodal representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spandana</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2839" to="2845" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Age-ofacquisition, imagery, concreteness, familiarity, and ambiguity measures for 1,944 words. Behavior Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Gilhooly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Logie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods &amp; Instrumentation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="427" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="771" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Limitations of cross-lingual learning from image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mareike</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<idno>abs/1709.05914</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multimodal pivots for image caption translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<idno>abs/1601.03916</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A comprehensive analysis of bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="273" to="310" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving multi-modal representations using image dispersion: Why less is sometimes more</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="835" to="841" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual bilingual lexicon induction with transferred convnet features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="148" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Googleology is bad science. Computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="147" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weakly supervised named entity transliteration and discovery from multilingual comparable corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning a translation lexicon from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition</title>
		<meeting>the ACL-02 workshop on Unsupervised lexical acquisition</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Crosslingual image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuyuki</forename><surname>Shimizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1780" to="1790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomi</forename><surname>Saphra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
		<title level="m">Swabha Swayamdipta, and Pengcheng Yin. 2017. Dynet: The dynamic neural network toolkit</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Concreteness, imagery, and meaningfulness values for 925 nouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Paivio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Madigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="207" to="213" />
			<date type="published" when="1968" />
			<publisher>American Psychological Association</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The language demographics of Amazon Mechanical Turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kachaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="79" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012)</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC-2012)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">Identifier</biblScope>
			<biblScope unit="page" from="12" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic identification of word translations from unrelated English and German corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</title>
		<meeting>the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="519" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Matrix factorization using window sampling and negative sampling for improved word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Salle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Idiart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Villavicencio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="419" to="424" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Inducing translation lexicons via diverse similarity measures and bridge languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 6th conference on Natural language learning</title>
		<meeting>the 6th conference on Natural language learning</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the role of seed lexicons in learning bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="247" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Crosslingual semantic similarity of words as the similarity of their semantic word responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT 2013)</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT 2013)</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="106" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning translations via matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Derry Tanti Wijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1453" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
