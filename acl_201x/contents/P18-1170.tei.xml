<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AMR Dependency Parsing with a Typed Semantic Algebra</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Groschwitz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Macquarie University</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Lindemann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meaghan</forename><surname>Fowlie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
							<email>mark.johnson@mq.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">Macquarie University</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AMR Dependency Parsing with a Typed Semantic Algebra</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1831" to="1841"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1831</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a semantic parser for Abstract Meaning Representations which learns to parse strings into tree representations of the compositional structure of an AMR graph. This allows us to use standard neural techniques for supertagging and dependency tree parsing, constrained by a linguistically principled type system. We present two approximative decoding algorithms, which achieve state-of-the-art accuracy and out-perform strong baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past few years, Abstract Meaning Repre- sentations (AMRs, <ref type="bibr" target="#b1">Banarescu et al. (2013)</ref>) have become a popular target representation for seman- tic parsing. AMRs are graphs which describe the predicate-argument structure of a sentence. Be- cause they are graphs and not trees, they can cap- ture reentrant semantic relations, such as those in- duced by control verbs and coordination. How- ever, it is technically much more challenging to parse a string into a graph than into a tree. For instance, grammar-based approaches ( <ref type="bibr" target="#b22">Peng et al., 2015;</ref><ref type="bibr" target="#b0">Artzi et al., 2015)</ref> require the induction of a grammar from the training corpus, which is hard because graphs can be decomposed into smaller pieces in far more ways than trees. Neural sequence-to-sequence models, which do very well on string-to-tree parsing ( <ref type="bibr" target="#b29">Vinyals et al., 2014)</ref>, can be applied to AMRs but face the challenge that graphs cannot easily be represented as sequences <ref type="bibr">(van Noord and Bos, 2017a,b)</ref>.</p><p>In this paper, we tackle this challenge by making the compositional structure of the AMR explicit. As in our previous work, <ref type="bibr" target="#b11">Groschwitz et al. (2017)</ref>, we view an AMR as consisting of atomic graphs representing the meanings of the individual words, which were combined compositionally using lin- guistically motivated operations for combining a head with its arguments and modifiers. We repre- sent this structure as terms over the AM algebra as defined in <ref type="bibr" target="#b11">Groschwitz et al. (2017)</ref>. This previous work had no parser; here we show that the terms of the AM algebra can be viewed as dependency trees over the string, and we train a dependency parser to map strings into such trees, which we then evaluate into AMRs in a postprocessing step. The dependency parser relies on type information, which encodes the semantic valencies of the atomic graphs, to guide its decisions.</p><p>More specifically, we combine a neural supertag- ger for identifying the elementary graphs for the individual words with a neural dependency model along the lines of <ref type="bibr" target="#b12">Kiperwasser and Goldberg (2016)</ref> for identifying the operations of the algebra. One key challenge is that the resulting term of the AM algebra must be semantically well-typed. This makes the decoding problem NP-complete. We present two approximation algorithms: one which takes the unlabeled dependency tree as given, and one which assumes that all dependencies are pro- jective. We evaluate on two data sets, achieving state-of-the-art results on one and near state-of-the- art results on the other (Smatch f-scores of 71.0 and 70.2 respectively). Our approach clearly out- performs strong but non-compositional baselines.</p><p>Plan of the paper. After reviewing related work in Section 2, we explain the AM algebra in Sec- tion 3 and extend it to a dependency view in Sec- tion 4. We explain model training in Section 5 and decoding in Section 6. Section 7 evaluates a number of variants of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recently, AMR parsing has generated considerable research activity, due to the availability of large-scale annotated data ( <ref type="bibr" target="#b1">Banarescu et al., 2013</ref>) and two successful SemEval Challenges <ref type="bibr" target="#b19">(May, 2016;</ref><ref type="bibr" target="#b20">May and Priyadarshi, 2017)</ref>.</p><p>Methods from dependency parsing have been shown to be very successful for AMR parsing. For instance, the JAMR parser ( <ref type="bibr" target="#b9">Flanigan et al., 2014</ref><ref type="bibr" target="#b8">Flanigan et al., , 2016</ref>) distinguishes concept identification (assigning graph fragments to words) from rela- tion identification (adding graph edges which con- nect these fragments), and solves the former with a supertagging-style method and the latter with a graph-based dependency parser. <ref type="bibr" target="#b10">Foland and Martin (2017)</ref> use a variant of this method based on an intricate neural model, yielding state-of-the-art re- sults. We go beyond these approaches by explicitly modeling the compositional structure of the AMR, which allows the dependency parser to combine AMRs for the words using a small set of universal operations, guided by the types of these AMRs.</p><p>Other recent methods directly implement a de- pendency parser for AMRs, e.g. the transition- based model of <ref type="bibr" target="#b5">Damonte et al. (2017)</ref>, or postpro- cess the output of a dependency parser by adding missing edges ( <ref type="bibr" target="#b6">Du et al., 2014;</ref><ref type="bibr" target="#b30">Wang et al., 2015)</ref>. In contrast to these, our model makes no strong assumptions on the dependency parsing algorithm that is used; here we choose that of <ref type="bibr" target="#b12">Kiperwasser and Goldberg (2016)</ref>.</p><p>The commitment of our parser to derive AMRs compositionally mirrors that of grammar-based AMR parsers ( <ref type="bibr" target="#b0">Artzi et al., 2015;</ref><ref type="bibr" target="#b22">Peng et al., 2015</ref>). In particular, there are parallels between the types we use in the AM algebra and CCG categories (see Section 3 for details). As a neural system, our parser struggles less with coverage issues than these, and avoids the complex grammar induction process these models require.</p><p>More generally, our use of semantic types to re- strict our parser is reminiscent of <ref type="bibr" target="#b14">Kwiatkowski et al. (2010)</ref>, <ref type="bibr" target="#b13">Krishnamurthy et al. (2017)</ref> and <ref type="bibr" target="#b31">Zhang et al. (2017)</ref>, and the idea of deriving semantic represen- tations from dependency trees is also present in <ref type="bibr" target="#b25">Reddy et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The AM algebra</head><p>A core idea of this paper is to parse a string into a graph by instead parsing a string into a dependency- style tree representation of the graph's composi- tional structure, represented as terms of the Apply- Modify (AM) algebra ( <ref type="bibr" target="#b11">Groschwitz et al., 2017)</ref>.  <ref type="figure">Figure 1</ref>: Elementary as-graphs G want , G writer , G sleep , and G sound for the words "want", "writer", "sleep", and "soundly" respectively.</p><p>graphs, or as-graphs: directed graphs with node and edge labels in which certain nodes have been designated as sources <ref type="bibr" target="#b4">(Courcelle and Engelfriet, 2012)</ref> and annotated with type information. Some examples of as-graphs are shown in <ref type="figure">Fig. 1</ref>. Each as-graph has exactly one root, indicated by the bold outline. The sources are indicated by red labels; for instance, G want has an S-source and an O-source. The annotations, written in square brackets behind the red source names, will be explained below. We use these sources to mark open argument slots; for example, G sleep in <ref type="figure">Fig. 1</ref> represents an intransitive verb, missing its subject, which will be added at the S-source.</p><p>The AM algebra can combine as-graphs with each other using two linguistically motivated op- erations: apply and modify. Apply (APP) adds an argument to a predicate. For example, we can add a subject -the graph G writer in <ref type="figure">Fig. 1</ref> -to the graph G VP in <ref type="figure">Fig. 2d</ref> using APP S , yielding the complete AMR in <ref type="figure">Fig. 2b</ref>. Linguistically, this is like filling the subject (S) slot of the predicate wants to sleep soundly with the argument the writer. In general, for a source a, APP a (G P , G A ), combines the as- graph G P representing a predicate, or head, with the as-graph G A , which represents an argument. It does this by plugging the root node of G A into the a-source u of G P -that is, the node u of G P marked with source a. The root of the resulting as-graph G is the root of G P , and we remove the a marking on u, since that slot is now filled.</p><p>The modify operation (MOD) adds a modifier to a graph. For example, we can combine two ele- mentary graphs from <ref type="figure">Fig. 1</ref> with MOD m (G sleep , G sound ), yielding the graph in <ref type="figure">Fig. 2c</ref>. The M- source of the modifier G soundly attaches to the root of G sleep . The root of the result is the same as the root of G sleep in the same sense that a verb phrase with an adverb modifier is still a verb phrase. In general, MOD a (G H , G M ), combines a head G H with a modifier G M . It plugs the root of G H into the a-source u of G M . Although this may add in- coming edges to the root of G H , that node is still the root of the resulting graph G. We remove the a marking from G M .</p><p>In both APP and MOD, if there is any other source b which is present in both graphs, the nodes marked with b are unified with each other. For ex- ample, when G want is O-applied to t 1 in <ref type="figure">Fig. 2d</ref>, the S-sources of the graphs for "want" and "sleep soundly" are unified into a single node, creating a reentrancy. This falls out of the definition of merge for s-graphs which formally underlies both operations (see <ref type="bibr" target="#b4">(Courcelle and Engelfriet, 2012)</ref>).</p><p>Finally, the AM algebra uses types to restrict its operations. Here we define the type of an as-graph as the set of its sources with their annotations 1 ; thus for example, in <ref type="figure">Fig. 1</ref>, the graph for "writer" has the empty type [ ], G sleep has type <ref type="bibr">[S]</ref>, and G want has type [S, <ref type="bibr">O[S]</ref>]. Each source in an as-graph specifies with its annotation the type of the as-graph which is plugged into it via APP. In other words, for a source a, we may only a-apply G P with G A if the annotation of the a-source in G P matches the type of G A . For example, the O-source of G wants <ref type="figure">(Fig. 1)</ref> requires that we plug in an as-graph of type <ref type="bibr">[S]</ref>; observe that this means that the reentrancy in <ref type="figure">Fig. 2b</ref> is lexically specified by the control verb "want". All other source nodes in <ref type="figure">Fig. 1</ref> have no annotation, indicating a type requirement of [ ].</p><p>Linguistically, modification is optional; we there- fore want the modified graph to be derivationally just like the unmodified graph, in that exactly the same operations can apply to it. In a typed algebra, this means MOD should not change the type of the head. MOD a therefore requires that the modifier G M have no sources not already present in the head G H , except a, which will be deleted anyway.</p><p>As in any algebra, we can build terms from con- stants (denoting elementary as-graphs) by recur- sively combining them with the operations of the AM algebra. By evaluating the operations bottom- up, we obtain an as-graph as the value of such a term; see <ref type="figure">Fig. 2</ref> for an example. However, as discussed above, an operation in the term may be undefined due to a type mismatch. We call an AM- term well-typed if all its operations are defined. Ev- ery well-typed AM-term evaluates to an as-graph. Since the applicability of an AM operation depends only on the types, we also write τ = f (τ 1 , τ 2 ) if as-graphs of type τ 1 and τ 2 can be combined with the operation f and the result has type τ .</p><p>Relationship to CCG. There is close relation- ship between the types of the AM algebra and the categories of CCG. A type <ref type="bibr">[S, O]</ref> specifies that the as-graph needs to be applied to two arguments to be semantically complete, similar a CCG category such as S\NP/NP, where a string needs to be ap- plied to two NP arguments to be syntactically com- plete. However, AM types govern the combination of graphs, while CCG categories control the com- bination of strings. This relieves AM types of the need to talk about word order; there are no "for- ward" or "backward" slashes in AM types, and a smaller set of operations. Also, the AM algebra spells out raising and control phenomena more ex- plicitly in the types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Indexed AM terms</head><p>In this paper, we connect AM terms to the input string w for which we want to produce a graph. We do this in an indexed AM term, exemplified in <ref type="figure">Fig. 3a</ref>. We assume that every elementary as-graph G at a leaf represents the meaning of an individual word token w i in w, and write G[i] to annotate the leaf G with the index i of this token. This induces a connection between the nodes of the AMR and the tokens of the string, in that the label of each node was contributed by the elementary as-graph of exactly one token.</p><p>We define the head index of a subtree t to be the index of the token which contributed the root of the as-graph to which t evaluates. For a leaf with annotation i, the head index is i; for an APP or MOD node, the head index is the head index of the left child, i.e. of the head argument. We annotate each APP and MOD operation with the head index of the left and right subtree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">AM dependency trees</head><p>We can represent indexed AM terms more com- pactly as AM dependency trees, as shown in <ref type="figure">Fig. 3b</ref>. The nodes of such a dependency tree are the tokens of w. We draw an edge with label f from i to k if there is a node with label f <ref type="bibr">[i, k]</ref> in the indexed AM term. For example, the tree in 3b has an edge labeled MOD m from 5 (G sleep ) to 6 (G soundly ) be- cause there is a node in the term in 3a labeled MOD m <ref type="bibr">[5,</ref><ref type="bibr">6]</ref>. The same AM dependency tree may represent multiple indexed AM terms, because the order of apply and modify operations is not spec- ified in the dependency tree. However, it can be shown that all well-typed AM terms that map to</p><formula xml:id="formula_0">APP s G want APP o MOD m G sleep G soundly G writer want A R G 0 A R G 1 person write A R G 0 sleep ARG0 sound m a n n e r sleep s A R G 0 sound m a n n e r (a) (b) (c) (d) want A R G 0 A R G 1 sleep ARG0</formula><p>sound m a n n e r s <ref type="figure">Figure 2</ref>: (a) An AM-term with its value (b), along with the values for its subexpressions (c) </p><formula xml:id="formula_1">t 1 = MOD m (G sleep , G sound ) and (d) t 2 = APP o (G want , t 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3: G want</head><p>Figure 3: (a) An indexed AM term and (b) an AM dependency tree, linking the term in <ref type="figure">Fig. 2</ref>;a to the sentence "The writer wants to sleep soundly".</p><p>the same AM dependency tree evaluate to the same as-graph. We define a well-typed AM dependency tree as one that represents a well-typed AM term.</p><p>Because not all words in the sentence contribute to the AMR, we include a mechanism for ignoring words in the input. As a special case, we allow the constant ⊥, which represents a dummy as-graph (of type ⊥) which we use as the semantic value of words without a semantic value in the AMR. We furthermore allow the edge label IGNORE in an AM dependency tree, where IGNORE(τ 1 , τ 2 ) = τ 1 if τ 2 = ⊥ and is undefined otherwise; in particular, an AM dependency tree with IGNORE edges is only well-typed if all IGNORE edges point into ⊥ nodes. We keep all other operations f (τ 1 , τ 2 ) as is, i.e. they are undefined if either τ 1 or τ 2 is ⊥, and never yield ⊥ as a result. When reconstructing an AM term from the AM dependency tree, we skip IGNORE edges, such that the subtree below them will not contribute to the overall AMR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Converting AMRs to AM terms</head><p>In order to train a model that parses sentences into AM dependency trees, we need to convert an AMR corpus -in which sentences are annotated with AMRs -into a treebank of AM dependency trees. We do this in three steps: first, we break each AMR up into elementary graphs and identify their roots; second, we assign sources and annotations to make elementary as-graphs out of them; and third, com- bine them into indexed AM terms.</p><p>For the first step, an aligner uses hand-written heuristics to identify the string token to which each node in the AMR corresponds (see Section C in the Supplementary Materials for details). We proceed in a similar fashion as the JAMR aligner ( <ref type="bibr" target="#b9">Flanigan et al., 2014</ref>), i.e. by starting from high-confidence token-node pairs and then extending them until the whole AMR is covered. Unlike the JAMR aligner, our heuristics ensure that exactly one node in each elementary graph is marked as the root, i.e. as the node where other graphs can attach their edges through APP and MOD. When an edge connects nodes of two different elementary graphs, we use the "blob decomposition" algorithm of <ref type="bibr" target="#b11">Groschwitz et al. (2017)</ref> to decide to which elementary graph it belongs. For the example AMR in <ref type="figure">Fig. 2b</ref>, we would obtain the graphs in <ref type="figure">Fig. 1</ref> (without source annotations). Note that ARG edges belong with the nodes at which they start, whereas the "manner" edge in G soundly goes with its target.</p><p>In the second step we assign source names and annotations to the unlabeled nodes of each elemen- tary graph. Note that the annotations are crucial to our system's ability to generate graphs with reentrancies. We mostly follow the algorithm of <ref type="bibr" target="#b11">Groschwitz et al. (2017)</ref>, which determines neces- sary annotations based on the structure of the given graph. The algorithm chooses each source name de- pending on the incoming edge label. For instance, the two leaves of G want can have the source labels S and O because they have incoming edges labeled ARG0 and ARG1. However, the Groschwitz algo- rithm is not deterministic: It allows object promo- tion (the sources for an ARG3 edge may be O3, O2, or O), unaccusative subjects (promoting the mini- mal object to S if the elementary graph contains an ARGi-edge (i &gt; 0) but no ARG0-edge <ref type="bibr" target="#b24">(Perlmutter, 1978)</ref>), and passive alternation (swapping O and S). To make our as-graphs more consistent, we prefer constants that promote objects as far as possible, use unaccusative subjects, and no passive alterna- tion, but still allow constants that do not satisfy these conditions if necessary. This increased our Smatch score significantly.</p><p>Finally, we choose an arbitrary AM dependency tree that combines the chosen elementary as-graphs into the annotated AMR; in practice, the differ- ences between the trees seem to be negligible. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training</head><p>We can now model the AMR parsing task as the problem of computing the best well-typed AM de- pendency tree t for a given sentence w. Because t is well-typed, it can be decoded into an (indexed) AM term and thence evaluated to an as-graph.</p><p>We describe t in terms of the elementary as- graphs G[i] it uses for each token i and of its edges f <ref type="bibr">[i, k]</ref>. We assume a node-factored, edge-factored model for the score ω(t) of t:</p><formula xml:id="formula_2">ω(t) = 1≤i≤n ω(G[i]) + f [i,k]∈E ω(f [i, k]), (1)</formula><p>where the edge weight further decomposes into the</p><formula xml:id="formula_3">sum ω(f [i, k]) = ω(i → k) + ω(f | i → k) of a score ω(i → k)</formula><p>for the presence of an edge from i to k and a score ω(f | i → k) for this edge having label f . Our aim is to compute the well-typed t with the highest score.</p><p>We present three models for ω: one for the graph scores and two for the edge scores. All of these are based on a two-layer bidirectional LSTM, which reads inputs x = (x 1 , . . . , x n ) token by token, con- catenating the hidden states of the forward and the backward LSTMs in each layer. On the sec- ond layer, we thus obtain vector representations v i = BiLSTM(x, i) for the individual input tokens (see <ref type="figure">Fig. 4</ref>). Our models differ in the inputs x and the way they predict scores from the v i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Supertagging for elementary as-graphs</head><p>We construe the prediction of the as-graphs G <ref type="bibr">[i]</ref> for each input position i as a supertagging task ( <ref type="bibr" target="#b15">Lewis et al., 2016)</ref>. The supertagger reads inputs x i = (w i , p i , c i ), where w i is the word token, p i its POS tag, and c i is a character-based LSTM encod- ing of w i . We use pretrained GloVe embeddings ( <ref type="bibr" target="#b23">Pennington et al., 2014</ref>) concatenated with learned embeddings for w i , and learned embeddings for p i .</p><p>To predict the score for each elementary as-graph out of a set of K options, we add a K-dimensional output layer as follows:</p><formula xml:id="formula_4">ω(G[i]) = log softmax(W · v i + b)</formula><p>2 Indeed, we conjecture that for a fixed set of constants and a fixed AMR, there is only one dependency tree.</p><formula xml:id="formula_5">x 1 v 1 x 2 v 2 x n v n ... ... ω(G[1]) ω(G[2]) ω(G[n]) ω(2 → n) ω(f | 2 → n) v ⊥ ω(⊥ → 1)</formula><p>Figure 4: Architecture of the neural taggers. and train the neural network using a cross-entropy loss function. This maximizes the likelihood of the elementary as-graphs in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Kiperwasser &amp; Goldberg edge model</head><p>Predicting the edge scores amounts to a dependency parsing problem. We chose the dependency parser of <ref type="bibr" target="#b12">Kiperwasser and Goldberg (2016)</ref>, henceforth K&amp;G, to learn them, because of its accuracy and its fit with our overall architecture. The K&amp;G parser scores the potential edge from i to k and its label from the concatenations of v i and v k :</p><formula xml:id="formula_6">MLP θ (v) = W 2 · tanh(W 1 · v + b 1 ) + b 2 ω(i → k) = MLP E (v i • v k ) ω(f | i → k) = MLP LBL (v i • v k )</formula><p>We use inputs x i = (w i , p i , τ i ) including the type τ i of the supertag G[i] at position i, using trained embeddings for all three. At evaluation time, we use the best scoring supertag according to the model of Section 5.1. At training time, we sample from q, where q(τ i ) = (1 − δ) + δ · p(τ i |p i , p i−1 ), q(τ ) = δ · p(τ |p i , p i−1 ) for any τ = τ i and δ is a hyperparameter controlling the bias towards the aligned supertag. We train the model using K&amp;G's original DyNet implementation. Their algorithm uses a hinge loss function, which maximizes the score difference between the gold dependency tree and the best predicted dependency tree, and there- fore requires parsing each training instance in each iteration. Because the AM dependency trees are highly non-projective, we replaced the projective parser used in the off-the-shelf implementation by the Chu-Liu-Edmonds algorithm implemented in the TurboParser ( <ref type="bibr" target="#b18">Martins et al., 2010)</ref>, improving the LAS on the development set by 30 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Local edge model</head><p>We also trained a local edge score model, which uses a cross-entropy rather than a hinge loss and therefore avoids the repeated parsing at training time. Instead, we follow the intuition that every node in a dependency tree has at most one incom- ing edge, and train the model to score the correct incoming edge as high as possible. This model takes inputs x i = (w i , p i ).</p><p>We define the edge and edge label scores as in Section 5.2, with tanh replaced by ReLU. We fur- ther add a learned parameter v ⊥ for the "LSTM em- bedding" of a nonexistent node, obtaining scores ω(⊥ → k) for k having no incoming edge.</p><p>To train ω(i → k), we collect all scores for edges ending at the same node k into a vector ω(• → k). We then minimize the cross-entropy loss for the gold edge into k under softmax(ω(• → k)), maximizing the likelihood of the gold edges. To train the labels ω(f | i → k), we simply mini- mize the cross-entropy loss of the actual edge labels f of the edges which are present in the gold AM dependency trees.</p><p>The PyTorch code for this and the supertag- ger are available at bitbucket.org/tclup/ amr-dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Decoding</head><p>Given learned estimates for the graph and edge scores, we now tackle the challenge of comput- ing the best well-typed dependency tree t for the input string w, under the score model (equation (1)). The requirement that t must be well-typed is crucial to ensure that it can be evaluated to an AMR graph, but as we show in the Supplementary Materials (Section A), makes the decoding prob- lem NP-complete. Thus, an exact algorithm is not practical. In this section, we develop two differ- ent approximation algorithms for AM dependency parsing: one which assumes the (unlabeled) de- pendency tree structure as known, and one which assumes that the AM dependency tree is projective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Projective decoder</head><p>The projective decoder assumes that the AM de- pendency tree is projective, i.e. has no crossing dependency edges. Because of this assumption, it can recursively combine adjacent substrings using dynamic programming. The algorithm is shown in <ref type="figure" target="#fig_0">Fig. 5</ref> as a parsing schema ( <ref type="bibr" target="#b26">Shieber et al., 1995)</ref>, which derives items of the form ( <ref type="bibr">[i, k]</ref>, r, τ ) with scores s. An item represents a well-typed deriva- tion of the substring from i to k with head index r, and which evaluates to an as-graph of type τ .</p><p>The parsing schema consists of three types of rules. First, the Init rule generates an item for each graph fragment G[i] that the supertagger predicted for the token w i , along with the score and type of that graph fragment. Second, given items for adjacent substrings <ref type="bibr">[i, j]</ref> and <ref type="bibr">[j, k]</ref>, the Arc rules apply an operation f to combine the indexed AM terms for the two substrings, with Arc-R making the left-hand substring the head and the right-hand substring the argument or modifier, and Arc-L the other way around. We ensure that the result is well-typed by requiring that the types can be com- bined with f . Finally, the Skip rules allow us to extend a substring such that it covers tokens which do not correspond to a graph fragment (i.e., their AM term is ⊥), introducing IGNORE edges. After all possible items have been derived, we extract the best well-typed tree from the item of the form ( <ref type="bibr">[1, n]</ref>, r, τ ) with the highest score, where τ = [ ].</p><formula xml:id="formula_7">s = ω(G[i]) G = ⊥ ([i, i + 1], i, τ (G)) : s Init ([i, k], r, τ ) : s s = ω(⊥[k]) ([i, k + 1], r, τ ) : s + s Skip-R ([i, k], r, τ ) : s s = ω(⊥[i − 1]) ([i − 1, k], r, τ ) : s + s Skip-L ([i, j], r1, τ1) : s1 ([j, k], r2, τ2) : s2 τ = f (τ1, τ2) defined s = ω(f [r1, r2]) Arc-R [f ] ([i, k], r1, τ ) : s1 + s2 + s ([i, j], r1, τ1) : s1 ([j, k], r2, τ2) : s2 τ = f (τ2, τ1) defined s = ω(f [r2, r1]) Arc-L [f ] ([i, k], r2, τ ) : s1 + s2 + s</formula><p>Because we keep track of the head indices, the projective decoder is a bilexical parsing algorithm, and shares a parsing complexity of O(n 5 ) with other bilexical algorithms such as the Collins parser. It could be improved to a complexity of O(n 4 ) using the algorithm of Eisner and Satta (1999).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Fixed-tree decoder</head><p>The fixed-tree decoder computes the best unlabeled dependency tree t r for w, using the edge scores ω(i → k), and then computes the best AM depen- dency tree for w whose unlabeled version is t r . The Chu-Liu-Edmonds algorithm produces a forest of dependency trees, which we want to combine into t r . We choose the tree whose root r has the highest score for being the root of the AM dependency tree and make the roots of all others children of r.</p><p>At this point, the shape of t r is fixed. We choose</p><formula xml:id="formula_8">s = ω(G[i]) (i, ∅, τ (G)) : s Init (i, C1, τ1) : s1 (k, Ch(k), τ2) : s2 k ∈ Ch(i)\C1 τ = f (τ1, τ2) defined s = ω(f [i, k])</formula><p>Edge[f ] (i, C1 ∪ {k}, τ ) : s1 + s2 + s <ref type="figure">Figure 6</ref>: Rules for the fixed-tree decoder.</p><p>supertags for the nodes and edge labels for the edges by traversing t r bottom-up, computing types for the subtrees as we go along. Formally, we apply the parsing schema in <ref type="figure">Fig. 6</ref>. It uses items of the form (i, C, τ ) : s, where 1 ≤ i ≤ n is a node of t r , C is the set of children of i for which we have already chosen edge labels, and τ is a type. We write Ch(i) for the set of children of i in t r .</p><p>The Init rule generates an item for each graph that the supertagger can assign to each token i in w, ensuring that every token is also assigned ⊥ as a possible supertag. The Edge rule labels an edge from a parent node i in t r to one of its children k, whose children already have edge labels. As above, this rule ensures that a well-typed AM dependency tree is generated by locally checking the types. In particular, if all types τ 2 that can be derived for k are incompatible with τ 1 , we fall back to an item for k with τ 2 = ⊥ (which always exists), along with an IGNORE edge from i to k.</p><p>The complexity of this algorithm is O(n · 2 d · d), where d is the maximal arity of the nodes in t r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>We evaluate our models on the LDC2015E86 and LDC2017T10 3 datasets <ref type="bibr">(henceforth "2015" and "2017")</ref>. Technical details and hyperparameters of our implementation can be found in Sections B to D of the Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Training data</head><p>The original LDC datasets pair strings with AMRs. We convert each AMR in the training and devel- opment set into an AM dependency tree, using the procedure of Section 4.2. About 10% of the training instances cannot be split into elementary as-graphs by our aligner; we removed these from the training data. Of the remaining AM dependency trees, 37% are non-projective.</p><p>Furthermore, the AM algebra is designed to han- dle short-range reentrancies, modeling grammati-Each elementary as-graph generated by the pro- cedure of Section 4.2 has a unique node whose label corresponds most closely to the aligned word (e.g. the "want" node in G want and the "write" node in G writer ). We replace these node labels with LEX in preprocessing, reducing the number of different elementary as-graphs from 28730 to 2370. We fac- tor the supertagger model of Section 5.1 such that the unlexicalized version of G[i] and the label for LEX are predicted separately. At evaluation, we re-lexicalize all LEX nodes in the predicted AMR. For words that were frequent in the training data (at least 10 times), we take the supertagger's prediction for the label. For rarer words, we use simple heuristics, explained in the Supplementary Materials (Section D). For names, we just look up name nodes with their children and wiki entries observed for the name string in the training data, and for unseen names use the literal tokens as the name, and no wiki entry. Similarly, we collect the type for each encountered name (e.g. "person" for "Agatha Christie"), and correct it in the output if the tagger made a different prediction. We recover dates and numbers straightforwardly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Supertagger accuracy</head><p>All of our models rely on the supertagger to predict elementary as-graphs; they differ only in the edge scores. We evaluated the accuracy of the supertag- ger on the converted development set (in which each token has a supertag) of the 2015 data set, and achieved an accuracy of 73%. The correct supertag is within the supertagger's 4 best predictions for 90% of the tokens, and within the 10 best for 95%.</p><p>Interestingly, supertags that introduce grammat- ical reentrancies are predicted quite reliably, al- though they are relatively rare in the training data. The elementary as-graph for subject control verbs (see G want in <ref type="figure">Fig. 1</ref>) accounts for only 0.8% of supertags in the training data, yet 58% of its oc- currences in the development data are predicted correctly (84% in 4-best). The supertag for VP co- ordination (with type [OP1[S], OP2[S]]) makes up for 0.4% of the training data, but 74% of its oc- currences are recognized correctly (92% in 4-best). Thus the prediction of informative types for indi- vidual words is feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Comparison to Baselines</head><p>Type-unaware fixed-tree baseline. The fixed-tree decoder is built to ensure well-typedness of the pre- dicted AM dependency trees. To investigate to what extent this is required, we consider a baseline which just adds the individually highest-scoring supertags and edge labels to the unlabeled depen- dency tree t u , ignoring types. This leads to AM dependency trees which are not well-typed for 75% of the sentences (we fall back to the largest well- typed subtree in these cases). Thus, an off-the- shelf dependency parser can reliably predict the tree structure of the AM dependency tree, but cor- rect supertag and edge label assignment requires a decoder which takes the types into account.</p><p>JAMR-style baseline. Our elementary as- graphs differ from the elementary graphs used in JAMR-style algorithms in that they contain explicit source nodes, which restrict the way in which they can be combined with other as-graphs. We investi- gate the impact of this choice by implementing a strong JAMR-style baseline. We adapt the AMR-to- dependency conversion of Section 4.2 by removing all unlabeled nodes with source names from the Model 2015 2017 Ours local edge + projective decoder 70.2±0.3 71.0±0.5 local edge + fixed-tree decoder 69.4±0.6 70.2±0.5 K&amp;G edge + projective decoder 68.6±0.7 69.4±0.4 K&amp;G edge + fixed-tree decoder 69.6±0.4 69.9±0.2 Baselines fixed-tree (type-unaware)</p><p>26.0±0.6 27.9±0.6 JAMR-style 66.1 66.2 Previous work CAMR ( <ref type="bibr" target="#b30">Wang et al., 2015)</ref> 66.5 - JAMR ( <ref type="bibr" target="#b8">Flanigan et al., 2016)</ref> 67 - <ref type="bibr" target="#b5">Damonte et al. (2017)</ref> 64 - van <ref type="bibr" target="#b28">Noord and Bos (2017b)</ref> 68.5 71.0 <ref type="bibr" target="#b10">Foland and Martin (2017)</ref> 70.7 - Buys and Blunsom (2017) - 61.9  <ref type="figure">Fig. 1</ref> now only consists of a single "want" node. We then aim to directly predict AMR edges be- tween these graphs, using a variant of the local edge scoring model of Section 5.3 which learns scores for each edge in isolation. (The assumption for the original local model, that each node has only one incoming edge, does not apply here.) When parsing a string, we choose the highest- scoring supertag for each word; there are only 628 different supertags in this setting, and 1-best su- pertagging accuracy is high at 88%. We then follow the JAMR parsing algorithm by predicting all edges whose score is over a threshold (we found -0.02 to be optimal) and then adding edges until the graph is connected. Because we do not predict which node is the root of the AMR, we evaluated this model as if it always predicted the root correctly, overestimating its score slightly. <ref type="table" target="#tab_2">Table 1</ref> shows the Smatch scores ) of our models, compared to a selection of previously published results. Our results are av- erages over 4 runs with 95% confidence intervals (JAMR-style baselines are single runs). On the 2015 dataset, our best models (local + projective, K&amp;G + fixed-tree) outperform all previous work, with the exception of the <ref type="bibr" target="#b10">Foland and Martin (2017)</ref> model; on the 2017 set we match state of the art re- sults (though note that van Noord and Bos (2017b) use 100k additional sentences of silver data). The fixed-tree decoder seems to work well with either edge model, but performance of the projective <ref type="bibr">de</ref>  Figure 7: A named entity labeled dependency tree in the fixed-tree decoder, scores for bad edges -which are never used when computing the hinge loss -are not trained accu- rately. Thus such edges may be erroneously used by the projective decoder. As expected, the type-unaware baseline has low recall, due to its inability to produce well-typed trees. The fact that our models outperform the JAMR-style baseline so clearly is an indication that they indeed gain some of their accuracy from the type information in the elementary as-graphs, confirming our hypothesis that an explicit model of the compositional structure of the AMR can help the parser learn an accurate model. <ref type="table" target="#tab_4">Table 2</ref> analyzes the performance of our two best systems (PD = projective, FTD = fixed-tree) in more detail, using the categories of <ref type="bibr" target="#b5">Damonte et al. (2017)</ref>, and compares them to Wang's, Flani- gan's, and Damonte's AMR parsers on the 2015 set and , and van Noord and Bos (2017b) for the 2017 dataset. <ref type="bibr" target="#b10">(Foland and Martin (2017)</ref> did not publish such results.) The good scores we achieve on reentrancy identification, despite removing a large amount of reentrant edges from the training data, indicates that our elementary as-graphs suc- cessfully encode phenomena such as control and coordination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Results</head><p>The projective decoder is given 4, and the fixed- tree decoder 6, supertags for each token. We trained the supertagging and edge scoring models of Sec- tion 5 separately; joint training did not help. Not sampling the supertag types τ i during training of the K&amp;G model, removing them from the input, and removing the character-based LSTM encod- ings c i from the input of the supertagger, all re- duced our models' accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Differences between the parsers</head><p>Although the Smatch scores for our two best mod- els are close, they sometimes struggle with different sentences. The fixed-tree parser is at the mercy of the fixed tree; the projective parser cannot produce non-projective AM dependency trees. It is remark- able that the projective parser does so well, given the prevalence of non-projective trees in the train- ing data. Looking at its analyses, we find that it frequently manages to find a projective tree which yields an (almost) correct AMR, by choosing su- pertags with unusual types, and by using modify rather than apply (or vice versa).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We presented an AMR parser which applies meth- ods from supertagging and dependency parsing to map a string into a well-typed AM term, which it then evaluates into an AMR. The AM term repre- sents the compositional semantic structure of the AMR explicitly, allowing us to use standard tree- based parsing techniques.</p><p>The projective parser currently computes the complete parse chart. In future work, we will speed it up through the use of pruning techniques. We will also look into more principled methods for splitting the AMRs into elementary as-graphs to replace our hand-crafted heuristics. In particular, advanced methods for alignments, as in <ref type="bibr" target="#b16">Lyu and Titov (2018)</ref>, seem promising. Overcoming the need for heuristics also seems to be a crucial in- gredient for applying our method to other semantic representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Rules for the projective decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>2015 &amp; 2017 test set Smatch scores 

elementary graphs. For instance, the graph G want 
in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>- coder drops with the K&amp;G edge scores. It may be that, while the hinge loss used in the K&amp;G edge scoring model is useful to finding the correct un</figDesc><table>-49 
46 
SRL 
60 
60 
56 
63 
61 
66 
64 
62 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Details for the LDC2015E86 and LDC2017T10 test sets 

Agatha_Christie 
name 

person 

n a m e 

w 
ik 
i 

Agatha 
Christie 

o p 1 
o p 2 

</table></figure>

			<note place="foot" n="1"> See (Groschwitz et al., 2017) for a more formally complete definition.</note>

			<note place="foot" n="3"> https://catalog.ldc.upenn.edu/ LDC2017T10, identical to LDC2016E25. cal phenomena such as control and coordination, as in the derivation in Fig. 2. It cannot easily handle the long-range reentrancies in AMRs which are caused by coreference, a non-compositional phenomenon. 4 We remove such reentrancies from our training data (about 60% of the roughly 20,000 reentrant edges). Despite this, our model performs well on reentrant edges (see Table 2). 7.2 Pre-and postprocessing We use simple pre-and postprocessing steps to handle rare words and some AMR-specific patterns. In AMRs, named entities follow a pattern shown in Fig. 7. Here the named entity is of type &quot;person&quot;, has a name edge to a &quot;name&quot; node whose children spell out the tokens of &quot;Agatha Christie&quot;, and a link to a wiki entry. Before training, we replace each &quot;name&quot; node, its children, and the corresponding span in the sentence with a special NAME token, and we completely remove wiki edges. In this example, this leaves us with only a &quot;person&quot; and a NAME node. Further, we replace numbers and some date patterns with NUMBER and DATE tokens. On the training data this is straightforward, since names and dates are explicitly annotated in the AMR. At evaluation time, we detect dates and numbers with regular expressions, and names with Stanford CoreNLP (Manning et al., 2014). We also use Stanford CoreNLP for our POS tags.</note>

			<note place="foot" n="4"> As Damonte et al. (2017) comment: &quot;A valid criticism of AMR is that these two reentrancies are of a completely different type, and should not be collapsed together.&quot;</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Broad-coverage CCG Semantic Parsing with AMR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation for Sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Oxford at SemEval2017 task 9: Neural AMR parsing with pointeraugmented attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="914" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Smatch: an evaluation metric for semantic feature structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Graph Structure and Monadic Second-Order Logic, a Language Theoretic Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Courcelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Engelfriet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An incremental parser for abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Long Papers. Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Peking: Profiling syntactic tree parsing techniques for semantic graph parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>SemEval</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient parsing for bilexical context-free grammars and head automaton grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th ACL</title>
		<meeting>the 37th ACL</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CMU at SemEval-2016 task 8: Graph-based AMR parsing with infinite ramp loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
		<meeting>the 10th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>SemEval-2016</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A discriminative graph-based parser for the abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation Parsing using LSTM Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Foland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A constrained graph algebra for semantic parsing with amrs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Groschwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meaghan</forename><surname>Fowlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Computational Semantics (IWCS)</title>
		<meeting>the 12th International Conference on Computational Semantics (IWCS)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<title level="m">Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="313" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural semantic parsing with type constraints for semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1516" to="1526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inducing probabilistic CCG grammars from logical form with higherorder unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 conference on empirical methods in natural language processing</title>
		<meeting>the 2010 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1223" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LSTM CCG Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Amr parsing as graph prediction with latent alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunchuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Conference of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 56th Annual Conference of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Turbo parsers: Dependency parsing by approximate variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M Q</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Mário</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 8: Meaning representation parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). Association for Computational Linguistics</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Abstract meaning representation parsing and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Priyadarshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017). Association for Computational Linguistics</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A novel neural network model for joint POS tagging and graph-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.05952</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A synchronous hyperedge replacement grammar based approach for amr parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Conference on Computational Language Learning</title>
		<meeting>the 19th Conference on Computational Language Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Impersonal passives and the unaccusative hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perlmutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">annual meeting of the Berkeley Linguistics Society</title>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="157" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mark Steedman, and Mirella Lapata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D17-1009" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="89" to="101" />
		</imprint>
	</monogr>
	<note>Universal semantic parsing</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Principles and implementation of deductive parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Schabes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Logic Programming</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="3" to="36" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dealing with co-reference in neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Semantic Deep Learning</title>
		<meeting>the 2nd Workshop on Semantic Deep Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural semantic parsing by character-based translation: Experiments with abstract meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics in the Netherlands Journal</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>CoRR abs/1412.7449</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Transition-based Algorithm for AMR Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Macro grammars and holistic triggering for efficient semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D17-1125" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1214" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
