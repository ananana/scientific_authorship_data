<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dependency Recurrent Neural Language Models for Sentence Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
							<email>piotr.mirowski@computer.org</email>
							<affiliation key="aff0">
								<orgName type="institution">University College London a</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College London a</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
							<email>vlachos@cs.ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University College London a</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dependency Recurrent Neural Language Models for Sentence Completion</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="511" to="517"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent work on language modelling has shifted focus from count-based models to neural models. In these works, the words in each sentence are always considered in a left-to-right order. In this paper we show how we can improve the performance of the recurrent neural network (RNN) language model by incorporating the syntactic dependencies of a sentence, which have the effect of bringing relevant contexts closer to the word being predicted. We evaluate our approach on the Microsoft Research Sentence Completion Challenge and show that the dependency RNN proposed improves over the RNN by about 10 points in accuracy. Furthermore, we achieve results comparable with the state-of-the-art models on this task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language Models (LM) are commonly used to score a sequence of tokens according to its prob- ability of occurring in natural language. They are an essential building block in a variety of applica- tions such as machine translation, speech recogni- tion and grammatical error correction. The stan- dard way of evaluating a language model has been to calculate its perplexity on a large corpus. How- ever, this evaluation assumes the output of the lan- guage model to be probabilistic and it has been observed that perplexity does not always correlate with the downstream task performance.</p><p>For these reasons, Zweig and Burges (2012) proposed the Sentence Completion Challenge, in which the task is to pick the correct word to com- plete a sentence out of five candidates. Perfor- mance is evaluated by accuracy (how many sen- tences were completed correctly), thus both prob- abilistic and non-probabilistic models (e.g. <ref type="bibr" target="#b23">Roark et al. (2007)</ref>) can be compared. Recent approaches for this task include both neural and count-based language models ( <ref type="bibr" target="#b5">Gubbins and Vlachos, 2013;</ref><ref type="bibr" target="#b18">Mnih and Kavukcuoglu, 2013;</ref><ref type="bibr" target="#b14">Mikolov et al., 2013</ref>).</p><p>Most neural language models consider the to- kens in a sentence in the order they appear, and the hidden state representation of the network is typically reset at the beginning of each sen- tence. In this work we propose a novel neu- ral language model that learns a recurrent neu- ral network (RNN) ( <ref type="bibr" target="#b12">Mikolov et al., 2010</ref>) on top of the syntactic dependency parse of a sen- tence. Syntactic dependencies bring relevant con- texts closer to the word being predicted, thus en- hancing performance as shown by <ref type="bibr" target="#b5">Gubbins and Vlachos (2013)</ref> for count-based language models. Our Dependency RNN model is published simul- taneously with another model, introduced in <ref type="bibr" target="#b24">Tai et al. (2015)</ref>, who extend the Long-Short Term Mem- ory (LSTM) architecture to tree-structured net- work topologies and evaluate it at sentence-level sentiment classification and semantic relatedness tasks, but not as a language model. Adapting the RNN to use the syntactic depen- dency structure required to reset and run the net- work on all the paths in the dependency parse tree of a given sentence, while maintaining a count of how often each token appears in those paths. Fur- thermore, we explain how we can incorporate the dependency labels as features.</p><p>Our results show that the dependency RNN lan- guage model proposed outperforms the RNN pro- posed by <ref type="bibr" target="#b13">Mikolov et al. (2011)</ref> by about 10 points in accuracy. Furthermore, it improves upon the count-based dependency language model of <ref type="bibr" target="#b5">Gubbins and Vlachos (2013)</ref>, while achieving slightly worse than the recent state-of-the-art results by <ref type="bibr" target="#b18">Mnih and Kavukcuoglu (2013)</ref>. Finally, we make the code and preprocessed data available to facili- tate comparisons with future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dependency Recurrent Neural Network</head><p>Count-based language models operate by assign- ing probabilities to sentences by factorizing their likelihood into n-grams. Neural language mod- els further embed each word w(t) into a low- dimensional vector representation (denoted by s(t)) 1 . These word representations are learned as the language model is trained ( <ref type="bibr" target="#b0">Bengio et al., 2003)</ref> and enable to define a word in relation to other words in a metric space.</p><p>Recurrent Neural Network <ref type="bibr" target="#b12">Mikolov et al. (2010)</ref> suggested the use of Recurrent Neural Net- works (RNN) to model long-range dependencies between words as they are not restricted to a fixed context length, like the feedforward neural net- work ( <ref type="bibr" target="#b0">Bengio et al., 2003</ref>). The hidden representa- tion s(t) for the word in position t of the sentence in the RNN follows a first order auto-regressive dynamic (Eq. 1), where W is the matrix connect- ing the hidden representation of the previous word s(t − 1) to the current one, w(t) is the one-hot in- dex of the current word (in a vocabulary of size N words) and U is the matrix containing the embed- dings for all the words in the vocabulary:</p><formula xml:id="formula_0">s(t) = f (Ws(t − 1) + Uw(t))<label>(1)</label></formula><p>The nonlinearity f is typically the logistic sigmoid function f (x) = 1 1+exp(−x) . At each time step, the RNN generates the word probability vector y(t) for the next word w(t + 1), using the output word embedding matrix V and the softmax nonlinearity</p><formula xml:id="formula_1">g(x i ) = exp(x i ) i exp(x i ) : y(t) = g (Vs(t))<label>(2)</label></formula><p>RNN with Maximum Entropy Model Mikolov et al. (2011) combined RNNs with a maximum en- tropy model, essentially adding a matrix that di- rectly connects the input words' n-gram context w(t − n + 1, . . . , t) to the output word proba- bilities. In practice, because of the large vocab- ulary size N , designing such a matrix is computa- tionally prohibitive. Instead, a hash-based imple- mentation is used, where the word context is fed through a hash function h that computes the in- dex h(w(t − n + 1, . . . , t)) of the context words in a one-dimensional array d of size D (typically, D = 10 9 ). Array d is trained in the same way as the rest of the RNN model and contributes to the output word probabilities:</p><formula xml:id="formula_2">y(t) = g Vs(t) + d h(w(t−n+1,...,t))<label>(3)</label></formula><p>As we show in our experiments, this additional matrix is crucial to a good performance on word completion tasks.</p><p>Training RNNs RNNs are trained using maxi- mum likelihood through gradient-based optimiza- tion, such as Stochastic Gradient Descent (SGD) with an annealed learning rate λ. The Back- Propagation Through Time (BPTT) variant of SGD enables to sum-up gradients from consecu- tive time steps before updating the parameters of the RNN and to handle the long-range temporal dependencies in the hidden s and output y se- quences. The loss function is the cross-entropy between the generated word distribution y(t) and the target one-hot word distribution w(t + 1), and involves the log-likelihood terms log y w(t+1) (t).</p><p>For speed-up, the estimation of the output word probabilities is done using hierarchical softmax outputs, i.e., class-based factorization <ref type="bibr" target="#b11">(Mikolov and Zweig, 2012)</ref>. Each word w i is assigned to a class c i and the corresponding log-likelihood is effectively log y w i (t) = log y c i (t) + log y w j (t), where j is the index of word w i among words belonging to class c i . In our experiments, we binned the words found in our training corpus into 250 classes according to frequency, roughly corre- sponding to the square root of the vocabulary size.</p><p>Dependency RNN RNNs are designed to pro- cess sequential data by iteratively presenting them with word w(t) and generating next word's proba- bility distribution y(t) at each time step. They can be reset at the beginning of a sentence by setting all the values of hidden vector s(t) to zero.</p><p>Dependency parsing <ref type="bibr" target="#b20">(Nivre, 2005</ref>) generates, for each sentence (which we note {w(t)} T t=0 ), a parse tree with a single root, many leaves and an unique path (also called unroll) from the root to each leaf, as illustrated on <ref type="figure" target="#fig_0">Figure 1</ref>. We now note {w i } i the set of word tokens appearing in the parse tree of a sentence. The order in the notation de- rives from the breadth-first traversal of that tree (i.e., the root word is noted w 0 ). Each of the un- rolls can be seen as a different sequence of words 512 ROOT I saw the ship with very strong binoculars {w i }, starting from the single root w 0 , that are vis- ited when one takes a specific path on the parse tree. We propose a simple transformation to the RNN algorithm so that it can process dependency parse trees. The RNN is reset and independently run on each such unroll. As detailed in the next paragraph, when evaluating the log-probability of the sentence, a word token w i can appear in mul- tiple unrolls but its log-likelihood is counted only once. During training, and to avoid over-training the network on word tokens that appear in more than one unroll (words near the root appear in more unrolls than those nearer the leaves), each word token w i is given a weight discount α i = 1 n i , based on the number n i of unrolls the token ap- pears in. Since the RNN is optimized using SGD and updated at every time-step, the contribution of word token w i can be discounted by multiplying the learning rate by the discount factor: α i λ.</p><p>Sentence Probability in Dependency RNN Given a word w i , let us define the ancestor se- quence A(w i ) to be the subsequence of words, taken as a subset from {w k } i−1 k=0 and describing the path from the root node w 0 to the parent of w i . For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, the ancestors A(very) of word token very are saw, binoculars and strong. Assuming that each word w i is con- ditionally independent of the words outside of its ancestor sequence, given its ancestor sequence A(w i ), <ref type="bibr" target="#b5">Gubbins and Vlachos (2013)</ref> showed that the probability of a sentence (i.e., the probability of a lexicalized tree S T given an unlexicalized tree T ) could be written as:</p><formula xml:id="formula_3">P [S T |T ] = |S| i=1 P [w i |A(w i )]<label>(4)</label></formula><p>This means that the conditional likelihood of a word given its ancestors needs to be counted only once in the calculation of the sentence likelihood, even though each word can appear in multiple un- rolls. When modeling a sentence using an RNN, the state s j that is used to generate the distribution of words w i (where j is the parent of i in the tree), represents the vector embedding of the history of the ancestor words A(w i ). Therefore, we count the term P [w i |s j ] only once when computing the likelihood of the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Labelled Dependency RNN</head><p>The model presented so far does not use dependency labels.</p><p>For this purpose we adapted the context-dependent RNN <ref type="bibr" target="#b11">(Mikolov and Zweig, 2012</ref>) to handle them as additional M - dimensional label input features f (t). These fea- tures require a matrix F that connects label fea- tures to word vectors, thus yielding a new dynam- ical model (Eq. 5) in the RNN, and a matrix G that connects label features to output word proba- bilities. The full model becomes as follows:</p><formula xml:id="formula_4">s(t) = f (Ws(t − 1) + Uw(t) + Ff (t))(5) y(t) = g Vs(t) + Gf (t) + d h(w t t−n+1 )<label>(6)</label></formula><p>On our training dataset, the dependency parsing model found M = 44 distinct labels (e.g., nsubj, det or prep). At each time step t, the context word w(t) is associated a single dependency label f (t) (a one-hot vector of dimension M ).</p><p>Let G(w) be the sequence of grammatical rela- tions (dependency tree labels) between successive elements of (A(w), w). The factorization of the sentence likelihood from Eq. 4 becomes:</p><formula xml:id="formula_5">P [S T |T ] = |S| i=1 P [w i |A(w i ), G(w i )]<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation and Dataset</head><p>We modified the Feature-Augmented RNN toolkit 2 and adapted it to handle tree-structured data. Specifically, and instead of being run se- quentially on the entire training corpus, the RNN is run on all the word tokens in all unrolls of all the sentences in all the books of the corpus. The RNN is reset at the beginning of each unroll of a sentence. When calculating the log-probability of a sentence, the contribution of each word token is counted only once (and stored in a hash-table specific for that sentence). Once all the unrolls of a sentence are processed, the log-probability of the sentence is the sum of the per-token log- probabilities in that hash-table. We also further enhanced the RNN library by replacing some large matrix multiplication routines by calls to the CBLAS library, thus yielding a two-to three-fold speed-up in the test and training time. <ref type="bibr">3</ref> The training corpus consists of 522 19th cen- tury novels from Project Gutenberg ( . All processing (sentence-splitting, PoS tagging, syntactic parsing) was performed us- ing the Stanford CoreNLP toolkit ( <ref type="bibr" target="#b8">Manning et al., 2014</ref>). The test set contains 1040 sentences to be completed. Each sentence consists of one ground truth and 4 impostor sentences where a specific word has been replaced with a syntactically cor- rect but semantically incorrect impostor word. De- pendency trees are generated for each sentence candidate. We split that set into two, using the first 520 sentences in the validation (development) set and the latter 520 sentences in the test set. Dur- ing training, we start annealing the learning rate λ with decay factor 0.66 as soon as the classification error on the validation set starts to increase. <ref type="table">Table 1</ref> shows the accuracy (validation and test sets) obtained using a simple RNN with 50, 100, 200 and 300-dimensional hidden word represen- tation and 250 frequency-based word classes (vo- cabulary size N = 72846 words appearing at least 5 times in the training corpus). One notices that adding the direct word context to target word con- nections (using the additional matrix described in section 2), enables to jump from a poor perfor- mance of about 30% accuracy to about 40% test accuracy, essentially matching the 39% accuracy reported for Good-Turing n-gram language mod- els in . Modelling 4-grams yields even better results, closer to the 45% accu- racy reported for RNNs in ( ). <ref type="bibr">4</ref> As <ref type="table">Table 2</ref> shows, dependency RNNs (de- pRNN) enable about 10 point word accuracy im- provement over sequential RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The best accuracy achieved by the depRNN on the combined development and test sets used to re- port results in previous work was 53.5%. The best reported results in the MSR sentence completion challenge have been achieved by Log-BiLinear Models (LBLs) <ref type="bibr" target="#b17">(Mnih and Hinton, 2007</ref>), a vari- 3 Our code and our preprocessed datasets are avail- able from: https://github.com/piotrmirowski/ DependencyTreeRnn <ref type="bibr">4</ref> The paper did not provide details on the maximum en- tropy features or on class-based hierarchical softmax).  <ref type="table">Table 2</ref>: Accuracy of (un-)labeled dependency RNN <ref type="figure">(depRNN and ldepRNN respectively)</ref>.</p><p>ant of neural language models with 54.7% to 55.5% accuracy <ref type="bibr" target="#b19">(Mnih and Teh, 2012;</ref><ref type="bibr" target="#b18">Mnih and Kavukcuoglu, 2013)</ref>. We conjecture that their su- perior performance might stem from the fact that LBLs, just like n-grams, take into account the or- der of the words in the context and can thus model higher-order Markovian dynamics than the simple first-order autoregressive dynamics in RNNs. The depRNN proposed ignores the left-to-right word order, thus it is likely that a combination of these approaches will result in even higher accuracies. Gubbins and Vlachos (2013) developed a count- based dependency language model achieving 50% accuracy. Finally, <ref type="bibr" target="#b14">Mikolov et al. (2013)</ref> report that they achieved 55.4% accuracy with an ensemble of RNNs, without giving any other details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Related work <ref type="bibr" target="#b15">Mirowski et al. (2010)</ref> incorpo- rated syntactic information into neural language models using PoS tags as additional input to LBLs but obtained only a small reduction of the word error rate in a speech recognition task. Similarly, <ref type="bibr" target="#b1">Bian et al. (2014)</ref> enriched the Continuous Bag-of-Words (CBOW) model of <ref type="bibr" target="#b14">Mikolov et al. (2013)</ref> by incorporating morphology, PoS tags and en- tity categories into 600-dimensional word embed- dings trained on the Gutenberg dataset, increas- ing sentence completion accuracy from 41% to 44%. Other work on incorporating syntax into lan- guage modeling include <ref type="bibr" target="#b3">Chelba et al. (1997)</ref> and <ref type="bibr" target="#b22">Pauls and Klein (2012)</ref>, however none of these ap- proaches considered neural language models, only count-based ones. <ref type="bibr" target="#b7">Levy and Goldberg (2014)</ref> and <ref type="bibr" target="#b25">Zhao et al. (2014)</ref> proposed to train neural word embeddings using skip-grams and CBOWs on de- pendency parse trees, but did not extend their ap- proach to actual language models such as LBL and RNN and did not evaluate the word embeddings on word completion tasks.</p><p>Note that we assume that the dependency tree is supplied prior to running the RNN which limits the scope of the Dependency RNN to the scoring of complete sentences, not to next word prediction (unless a dependency tree parse for the sentence to be generated is provided). Nevertheless, it is common in speech recognition and machine trans- lation to use a conventional decoder to produce an N-best list of the most likely candidate sentences and then re-score them with the language model. ( <ref type="bibr" target="#b3">Chelba et al., 1997;</ref><ref type="bibr">Klein, 2011) Tai et al. (2015)</ref> propose a similar approach to ours, learning Long Short-Term Memory (LSTM) <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b4">Graves, 2012</ref>) RNNs on dependency parse tree network topologies. Their architectures is not designed to predict next-word probability distributions, as in a language model, but to classify the input words (sentiment analysis task) or to measure the sim- ilarity in hidden representations (semantic relat- edness task). Their relative improvement in per- formance (tree LSTMs vs standard LSTMs) on these two tasks is smaller than ours, probably be- cause the LSTMs are better than RNNs at storing long-term dependencies and thus do not benefit form the word ordering from dependency trees as much as RNNs. In a similar vein to ours, Miceli- Barone and Attardi (2015) simply propose to en- hance RNN-based machine translation by permut- ing the order of the words in the source sentence to match the order of the words in the target sentence, using a source-side dependency parsing.  reported that RNNs achieve lower perplexity than n-grams but do not always <ref type="figure">Figure 2</ref>: Perplexity vs. accuracy of RNNs outperform them on word completion tasks. As illustrated in <ref type="figure">Fig. 2</ref>, the validation set perplex- ity (comprising all 5 choices for each sentence) of the RNN keeps decreasing monotonically (once we start annealing the learning rate), whereas the validation accuracy rapidly reaches a plateau and oscillates. Our observation confirms that, once an RNN went through a few training epochs, change in perplexity is no longer a good predictor of change in word accuracy. We presume that the log-likelihood of word distribution is not a train- ing objective crafted for precision@1, and that further perplexity reduction happens in the middle and tail of the word distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations of RNNs for word completion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper we proposed a novel language model, dependency RNN, which incorporates syntactic dependencies into the RNN formulation. We eval- uated its performance on the MSR sentence com- pletion task and showed that it improves over RNN by 10 points in accuracy, while achieving re- sults comparable with the state-of-the-art. Further work will include extending the dependency tree language modeling to Long Short-Term Memory RNNs to handle longer syntactic dependencies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example dependency tree</figDesc></figure>

			<note place="foot" n="1"> In our notation, we make a distinction between the word token w(t) at position t in the sentence and its one-hot vector representation w(t). We note wi the i-th word token on a breadth-first traversal of a dependency parse tree.</note>

			<note place="foot" n="2"> http://research.microsoft.com/en-us/projects/rnn/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank our anonymous reviewers for their valuable feedback. PM also thanks Geoffrey Zweig, Daniel Voinea, Francesco Nidito and Da-vide di Gennaro for sharing the original Feature-Augmented RNN toolkit on the Microsoft Re-search website and for insights about that code, as well as Bhaskar Mitra, Milad Shokouhi and An-driy Mnih for enlighting discussions about word embedding and sentence completion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge-powered deep learning for word embedding</title>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<biblScope unit="volume">8724</biblScope>
			<biblScope unit="page" from="132" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structure and performance of a dependency language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Engle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><surname>Mangu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Printz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Ristad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurospeech</title>
		<meeting>Eurospeech</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="2775" to="2778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Supervised Sequence Labelling with Recurrent Neural Networks. Studies in Computational Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dependency language models for sentence completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gubbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">17351780</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd</title>
		<meeting>52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<imprint>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Non-projective dependency-based prereordering with recurrent neural network for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli-Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Attardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 53rd Annual Meeting of the Association for Computational Linguistics and The 7th International Joint Conference of the Asian Federation of Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech Language Technologies (SLT)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Makuhari, Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Strategies for training large scale neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011-01" />
			<biblScope unit="page" from="196" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature-rich continuous language models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhrid</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="241" to="246" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">641648</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML-12)</title>
		<meeting>the 29th International Conference on Machine Learning (ICML-12)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1751" to="1758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dependency grammar and dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MSI report</title>
		<imprint>
			<biblScope unit="volume">5133</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster and Smaller N-Gram Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="258" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale syntactic language modeling with treelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="959" to="968" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discriminative n-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Saraclar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="373" to="392" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 53rd Annual Meeting of the Association for Computational Linguistics and The 7th International Joint Conference of the Asian Federation of Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning word embeddings from dependency relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Asian Language Processing</title>
		<meeting>Asian Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>IALP</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A challenge set for advancing language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</title>
		<meeting>the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Computational approaches to sentence completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainur</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yessenalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
