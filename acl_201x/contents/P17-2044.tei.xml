<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Japanese Sentence Compression with a Large Training Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Hasegawa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<country>Japan, Japan, Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Japanese Sentence Compression with a Large Training Dataset</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="281" to="286"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2044</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In English, high-quality sentence compression models by deleting words have been trained on automatically created large training datasets. We work on Japanese sentence compression by a similar approach. To create a large Japanese training dataset, a method of creating En-glish training dataset is modified based on the characteristics of the Japanese language. The created dataset is used to train Japanese sentence compression models based on the recurrent neural network.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentence compression is the task of shorten- ing a sentence while preserving its important information and grammaticality. Robust sen- tence compression systems are useful by them- selves and also as a module in an extractive sum- marization system <ref type="bibr" target="#b0">(Berg-Kirkpatrick et al., 2011;</ref><ref type="bibr" target="#b11">Thadani and McKeown, 2013)</ref>. In this paper, we work on Japanese sentence compression by delet- ing words. One advantage of compression by deleting words as opposed to abstractive compres- sion lies in the small search space. Another one is that the compressed sentence is more likely to be free from incorrect information not mentioned in the source sentence.</p><p>There are many sentence compression mod- els for Japanese ( <ref type="bibr" target="#b5">Harashima and Kurohashi, 2012;</ref><ref type="bibr" target="#b6">Hirao et al., 2009;</ref><ref type="bibr" target="#b7">Hori and Furui, 2004</ref>) and for English <ref type="bibr" target="#b9">(Knight and Marcu, 2000;</ref><ref type="bibr" target="#b12">Turner and Charniak, 2005;</ref><ref type="bibr" target="#b1">Clarke and Lapata, 2006</ref>).</p><p>In recent years, a high-quality En- glish sentence compression model by deleting words was trained on a large training dataset ( <ref type="bibr" target="#b3">Filippova and Altun, 2013;</ref><ref type="bibr" target="#b2">Filippova et al., 2015)</ref>. While it is impractical to create a large training dataset by hand, one can be created auto- matically from news articles ( <ref type="bibr" target="#b3">Filippova and Altun, 2013</ref>). The procedure is as follows (where S, H, and C respectively denote the first sentence of an article, the headline, and the created compressed sentence of S). Firstly, to restrict the training data to grammatical and informative sentences, only news articles satisfying certain conditions are used. Then, nouns, verbs, adjectives, and adverbs (i.e., content words) shared by S and H are identified by matching word lemmas, and a rooted dependency subtree that contains all the shared content words is regarded as C.</p><p>However, their method is designed for English, and cannot be applied to Japanese as it is. Thus, in this study, their method is modified based on the following three characteristics of the Japanese language: (a) Abbreviation of nouns and nominal- ization of verbs frequently occur in Japanese. (b) Words that are not verbs can also be the root node especially in headlines. (c) Subjects and objects that can be easily estimated from the context are often omitted.</p><p>The created training dataset is used to train three models. The first model is the original Filippova et al.'s model, an encoder-decoder model with a long short-term memory (LSTM), which we extend in this paper to make the other two models that can control the output length ( <ref type="bibr" target="#b8">Kikuchi et al., 2016)</ref>, because controlling the output length makes a compressed sentence more informative under the desired length.   imum rooted subtree. We modified their method based on the characteristics of the Japanese lan- guage as follows. To explain our method, a de- pendency tree of S and a sequence of bunsetsu chunks of H in Japanese are shown in Figures 1 and 2. Note that nodes of dependency trees in Japanese are bunsetsu chunks each consisting of content words followed by function words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Creating training dataset for Japanese</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Identification of shared content words</head><p>Content words shared by S and H are identified by matching lemmas and pronominal anaphora reso- lution in Filippova et al.'s method. Abbreviation of nouns and nominalization of verbs frequently oc- cur in Japanese (characteristic (a) in Introduction), and it is difficult to identify these transformations simply by matching lemmas. Thus, after the iden- tification by matching lemmas, two identification methods (described below) using character-level information are applied. Note that pronominal anaphora resolution is not used, because pronouns are often omitted in Japanese. Abbreviation of nouns: There are two types of abbreviations of nouns in Japanese. One is the abbreviation of a proper noun, which shortens the form by deleting characters (e.g., the pair with "+" in <ref type="figure" target="#fig_1">Figures 1 and 2</ref>). The other is the abbreviation of consecutive nouns, which deletes nouns that be- have as adjectives (e.g., the pair with "-"). To deal with such cases, if the character sequence of the noun sequence in a chunk in H is identical to a subsequence of characters composing the noun se- quence in a chunk in S, the noun sequences in H and S are regarded as shared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nominalization of verbs:</head><p>Many verbs in Japanese have corresponding nouns with similar meanings (e.g., the pair with # in <ref type="figure" target="#fig_1">Figures 1 and 2</ref>). Such pairs often share the same Chinese character, kanji. Kanji is an ideogram and is more informative than the other types of Japanese letters. Thus, if a noun in H and a verb in S 2 share one kanji, the noun and the verb are regarded as shared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transformation of a dependency tree</head><p>Some edges in dependency trees cannot be cut without changing the meaning or losing the gram- maticality of the sentence. In Filippova et al.'s method, the nodes linked by such an edge are merged into a single node before extraction of the subtree. The method is adapted to Japanese as fol- lows. If the function word of a chunk is one of the specific particles 3 , which often make obliga- tory cases, the chunk and its modifiee are merged into a single node. In <ref type="figure" target="#fig_1">Figure 1</ref>, the chunks at the start and end of a bold arrow are merged into a single node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Extraction of the minimum rooted subtree</head><p>In Filippova et al.'s method, the minimum rooted subtree that contains all the shared content words is extracted from the transformed tree. We modify their method to take into account the characteris- tics (a) and (b). Deleting the global root: In English, only verbs can be the root node of a subtree. However, in Japanese, words with other parts-of-speech can also be the root node in headlines (characteristics (b)). Therefore, the global root, which is the root node of S, and the chunks including a word that can be located at the end of a sentence 4 , are the candidates for the root node of a subtree. Then, if the root node is not the global root, words suc- ceeding the word that can be located at the end are removed from the root node. In <ref type="figure" target="#fig_1">Figure 1</ref>, among the two words with "*" that can be located at the end, the latter is extracted as the root, and the suc- ceeding word is removed from the chunk. Reflecting abbreviated forms: Abbreviation of nouns frequently occurs in Japanese (characteris- tic (a)). Thus, in C, original forms are replaced by their abbreviated forms obtained as explained in Section 2.1 (e.g., the pair with "-" in <ref type="figure" target="#fig_1">Figures  1 and 2)</ref>. However, we do not allow the head of a chunk to be deleted to keep the grammaticality. We also restrict here ourselves to word-level dele- tion and do not allow character-level deletion, be- cause our purpose is to construct a training dataset for compression by word deletion. In the example of <ref type="figure" target="#fig_1">Figure 1</ref>, chunks in bold squares are extracted as C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Conditions imposed on news articles</head><p>Filippova et al.'s method imposed eight conditions on news articles to restrict the training data to grammatical and informative sentences. In our method, these conditions are modified to adapt to Japanese. Firstly, the condition "S should include content words of H in the same order" is removed, because word order in Japanese is relatively free. Secondly, the condition "S should include all con- tent words of H" is relaxed to "the ratio of shared content words to content words in H is larger than threshold θ" because in Japanese, subjects and ob- jects that can be easily estimated from the context are often omitted (characteristics (c)). In addition, two conditions "H should have a verb" and "H must not begin with a verb" are removed, leaving four conditions 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sentence compression with LSTM</head><p>Three models are used for sentence compres- sion. Each model predicts a label sequence, where the label for each word is either "retain" or "delete" ( <ref type="bibr" target="#b2">Filippova et al., 2015</ref>). The first is an encoder-decoder model with LSTM (lstm) ( <ref type="bibr" target="#b10">Sutskever et al., 2014)</ref>. Given an input sequence x = (x 1 , x 2 , . . . , x n ), y t in label sequence y = (y 1 , y 2 , . . . , y n ) is computed as follows:</p><formula xml:id="formula_0">s 0 = encoder(x) (s t , m t ) = decoder(s t−1 , m t−1 , e 1 (x t )⊕e l (y t−1 )) y t = sof tmax(W * s t + b),</formula><p>5 "H is not a question", "both H and S are not less than four words.", "S is more than 1.5 times longer than H", and "S is more than 1.5 times longer than C". threshold θ 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.0">size(10k) 202 189 172 154 134 106 81 59 35 27 vocab(k) 105 102 98 94 90 81 72 62 48 42</head><p>ROUGE-2 54.6 54.4 54.0 54.3 55.3 54.0 55.1 54.4 53.0 53.3 <ref type="table">Table 1</ref>: Created training data with each θ. ROUGE-2 is the score of compressed sentence generated by the model trained on each training data to target.</p><p>where ⊕ indicates concatenation, s t and m t are re- spectively a hidden state and a memory cell at time t, and e 1 (word ) is embedding of word. If label is "retain", e l (label) is <ref type="figure" target="#fig_1">(1; 0)</ref>; otherwise, (0; 1). m 0 is a zero vector.</p><p>As the second and the third models, we extend the first model to control the output length ( <ref type="bibr" target="#b8">Kikuchi et al., 2016</ref>). The second model, lstm+leninit, initializes the memory cell of the de- coder as follows: m 0 = tarlen * b len where tarlen is the desired output length, and b len is a train- able parameter. The third model, lstm+lenemb, uses the embedding of the potential desired length e 2 (length) as an additional input. In this case, e 1 (x t ) ⊕ e l (y t−1 ) ⊕ e 2 (l t ) is used as the input of the decoder where l t is the potential desired length at time t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The created training datasets were used to train three models for sentence compression. To see the effect of the modified subtree extraction method (Section 2.3), two training datasets were tested: rooted and multi-root+. rooted includes only dele- tion of the leafs in a dependency tree. In contrast, multi-root+ includes deleting the global root and reflecting abbreviated forms besides it. Setting: Training datasets were created from seven million, 35-years' worth of news articles from the Mainichi, Nikkei, and Yomiuri news- papers, from which duplicate sentences and sen- tences in test data were filtered out. Gold-standard data were composed of the first sentences of the 1,000 news articles from Mainichi, each of which has 5 compressed sentences separately created by five human annotators. 100 sentences of the gold-standard data were used as development data, while the other sentences were used as test data. The three models were trained on datasets created with each value of threshold θ 6 , which is the pa- rameter used in the condition (introduced in Sec-</p><formula xml:id="formula_1">dataset model R-1 ROUGE-2 R-L CR R(R-2)</formula><p>P F prop-w-dpnd 73.8 56.5 51.5 53.7 32.7 60.5 tree-base 68.6 52.6 50.4 51.4 34.4 59.1 rooted lstm 66.3 51.5 59.3 54.6 36.1 51.8 rooted lstm+leninit 70.8 55.2 57.1 55.9 36.0 56.8 rooted lstm+lenemb 71.0 55.2 56.7 55.7 35.9 57.8 multi-root+ lstm 60.8 50.6 60.5 54.1 33.7 47.9 multi-root+ lstm+leninit 71.7 56.0 57.8 56.7 36.3 57.8 multi-root+ lstm+lenemb 72.6 56.2 56.4 56.1 35.8 59.4 <ref type="table" target="#tab_0">Table 2</ref>: Automatic evaluation. R-1, R-2, and R-L are ROUGE-1, ROUGE-2, and ROUGE-L score. R, P and F are recall, precision and F-measure. tion 2.4). The properties of the dataset in relation to θ are shown in <ref type="table">Table 1</ref>. θ is tuned for ROUGE- 2 score on the development data. In <ref type="table">Table 1</ref>, we show the tendency of the ROUGE-2 scores when lstm+leninit was trained on created rooted with each θ. From <ref type="table">Table 1</ref>, we chose 0.5 as θ. All models are three stacked LSTM layers <ref type="bibr">7</ref> . Words with frequency lower than five are regarded as un- known words. ADAM 8 was used as the optimiza- tion method. The desired length was set to the bytes of a compressed sentence randomly chosen from the five human-generated sentences. In the test step, beam-search was used (beam size: 20) and candidates exceeding the desired length were truncated. tree-base and prop-w-dpnd: Existing methods for Japanese sentence compression are not based on the training on a large dataset. Therefore, the proposed method is compared with two methods, tree-base, (Filippova and Strube, 2008) and prop- w-dpnd (Harashima and Kurohashi, 2012), which are not based on supervised learning. tree-base is implemented as an integer linear programming problem that finds a subtree of a dependency tree.</p><p>prop-w-dpnd is also implemented as an integer lin- ear programming problem, but modified based on characteristics of the Japanese language. prop-w- dpnd allows the deletion inside the chunks. The models trained on a large training dataset achieved higher F-measure than the unsupervised models. Moreover, F-measure of lstm+leninit and lstm+lenemb, which were trained on either multi- root+ or rooted, is significantly better than prop- w-dpnd (p &lt; 0.001). lstm achieved lower R-1 and R-2 than the other models trained on a large train- ing dataset, probably because it tends to gener- ate too short sentences, as indicated by low CR. It is also noteworthy that CRs of lstm+leninit and lstm+lenemb are mostly closer to the aver- age CR of the test data than lstm. Furthermore, lstm+leninit and lstm+lenemb trained on multi- root+ instead of rooted worked better in terms of F-measure. We consider it is because vari- ous types of deletion make the compression model more flexible, as indicated by closer CR of the model trained on multi-root+ instead of rooted to the average CR of the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Automatic evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Human evaluation</head><p>The difference between lstm+leninit and lstm+lenemb trained on multi-root+ was in- vestigated first. With lstm+leninit, 2 out of 100 sentences, chosen randomly, ended with a word that cannot be located at the end of a sentence. In contrast, with lstm+lenemb, 24 sentences ended with such words and therefore are ungrammatical, although lenemb has shown to be effective in ab- stractive sentence summarization ( <ref type="bibr" target="#b8">Kikuchi et al., 2016</ref>). This result suggests that lstm+lenemb is excessively affected by the desired length because lenemb receives the potential desired length at each time of decoding. In fact, 21 out of the 24 sentences are as long as the desired length. Then, lstm+leninit trained on multi-root+ was evaluated by crowdsourcing in comparison with the gold-standard and tree-base. Each crowd-model read info gold-standard 4.22 3.76 lstm+leninit 3.99 3.09 prop-w-dpnd 3.55 2.81  <ref type="table">Table 4</ref>: Human relative evaluation sourcing worker reads each source sentence and a set of the compressed sentences, reordered ran- domly, and gives a score from 1 to 5 (where 5 is best) to each compressed sentence in terms of informativeness (info) and readability (read). As shown in <ref type="table" target="#tab_1">Table 3</ref>, in terms of both read and info, lstm+leninit archived higher scores than prop-w- dpnd, and the difference is significant (p &lt; 0.001). Next, lstm+leninit was compared with lstm (both are trained on multi-root+), and multi-root+ was compared with rooted (lstm+leninit was used as the model), by human relative evaluation. In this evaluation, each worker votes for one of lstm+leninit, lstm, or tie, and for one of multi- root+, rooted, or tie. 250 votes in total were re- ceived (50 sentences × 5 votes). The results are shown in <ref type="table">Table 4</ref>. lstm+leninit is better than lstm in terms of read, which is not directly related to the output length, as well as of info. It is also clear that multi-root+ achieves much higher info with a negligible reduction in read than rooted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Filippova et al.'s method of creating training data for English sentence compression was modified to create a training dataset for Japanese sentence compression. The effectiveness of the created Japanese training dataset was verified by auto- matic and human evaluation. Our method can re- fine the created dataset by giving more flexibil- ity in compression and achieved better informa- tiveness with negligible reduction in readability. Furthermore, it has been shown that controlling the output length improves the performance of the sentence compression models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Filippova et al.</head><label></label><figDesc>'s method of creating training data consists of the conditions imposed on news arti- cles and the following three steps: (1) identifica- tion of shared content words, (2) transformation of a dependency tree, and (3) extraction of the min-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dependency tree of S 1. Squares and arrows respectively indicate chunks and dependency relations. Bold arrows mean that chunks are merged into a single node. Bold squares are extracted as C.</figDesc><graphic url="image-1.png" coords="2,72.00,62.81,453.54,57.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A sequence of chunks of H 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 shows</head><label>2</label><figDesc></figDesc><table>the ROUGE score of each model. 
We used ROUGE-1, ROUGE-2, and ROUGE-
L (R-1, R-2, and R-L) for evaluation. In addi-
tion, we also show the precision and F-measure 
of ROUGE-2 because the output length of each 
model is not same. Compression ratio (CR) is 
the ratio of the bytes of the compressed sentence 
to the bytes of the source sentence. Average 

7 Dimension of word embedding and hidden layer:256, 
dropout:20% 
8 α:0.001, β1:0.9, β2:0.999, eps:1e-8, batchsize:180 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 3 : Human absolute evaluation</head><label>3</label><figDesc></figDesc><table>model 
read info 
lstm+leninit 78 108 
lstm 
47 
47 
tie 
125 95 

training data read info 
multi-root+ 
55 
85 
rooted 
57 
33 
tie 
138 132 

</table></figure>

			<note place="foot" n="1"> Words in red are regarded as shared through lemma matching, while words in blue (also underlined) are through other ways.</note>

			<note place="foot" n="2"> Candidates are restricted to verbs consisting of one kanji and some following hiragana. 3 &quot; (wo)&quot;, &quot; (ni)&quot;, &quot; (ga)&quot; and &quot; (ha)&quot; 4 An auxiliary verb, a noun, or a verb of which next word is not a noun, an auxiliary verb, or &quot; (no)&quot;</note>

			<note place="foot" n="6"> In Table 2, the thresholds for our models are 0.7, 0.5, 0.3, 0.6, 0.4, and 0.2, respectively, from the top.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by Google Re-search Award, JSPS KAKENHI Grant Number JP26280080, and JPMJPR1655.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Jointly learning to extract and compress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Models for sentence compression: A comparison across domains, training requirements and evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sentence compression by deletion with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Colmenares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Overcoming the lack of parallel data in sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1481" to="1491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dependency tree based sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Natural Language Generation Conference</title>
		<meeting>the Fifth International Natural Language Generation Conference</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Flexible Japanese sentence compression by relaxing unit constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Harashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
		<meeting>COLING 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A syntax-free approach to japanese sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="826" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speech summarization : An approach through word extraction and a method for evaluation (¡special section¿the 2002 ieice excellent paper award)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadaoki</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEICE transactions on information and systems pages</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="15" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Controlling output length in neural encoder-decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Statisticsbased summarization-step one: Sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence</title>
		<meeting>the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="703" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sentence compression with joint structural inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapil</forename><surname>Thadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supervised and unsupervised learning for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenine</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="290" to="297" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
