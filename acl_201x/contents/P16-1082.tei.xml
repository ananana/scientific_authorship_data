<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Concept Dependencies in a Scientific Corpus</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">USC Information Sciences Institute Marina del Rey</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhong</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">USC Information Sciences Institute Marina del Rey</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">USC Information Sciences Institute Marina del Rey</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">USC Information Sciences Institute Marina del Rey</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gully</forename><surname>Burns</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">USC Information Sciences Institute Marina del Rey</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Concept Dependencies in a Scientific Corpus</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="866" to="875"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Our goal is to generate reading lists for students that help them optimally learn technical material. Existing retrieval algorithms return items directly relevant to a query but do not return results to help users read about the concepts supporting their query. This is because the dependency structure of concepts that must be understood before reading material pertaining to a given query is never considered. Here we formulate an information-theoretic view of concept dependency and present methods to construct a &quot;concept graph&quot; automatically from a text corpus. We perform the first human evaluation of concept dependency edges (to be published as open data), and the results verify the feasibility of automatic approaches for inferring concepts and their dependency relations. This result can support search capabilities that may be tuned to help users learn a subject rather than retrieve documents based on a single query.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Corpora of technical documents, such as the ACL Anthology, are valuable for learners, but it can be difficult to find the most appropriate documents to read in order to learn about a concept. This problem is made more complicated by the need to trace the ideas back to those that need to be learned first (e.g., before you can learn about Markov logic networks, you should understand first-order logic and probability). That is, a crucial question when learning a new subject is "What do I need to know before I start reading about this?"</p><p>To answer this question, learners typically rely on the guidance of domain experts, who can devise pedagogically valuable reading lists that order doc- uments to progress from prerequisite to target con- cepts. Thus, it is desirable to have a model where each concept is linked to the prerequisite concepts it depends upon -a concept graph. A manually constructed concept graph excerpt related to au- tomatic speech recognition is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The dependency relation between two concepts is interpreted as whether understanding one concept would help a learner understand the other.</p><p>Representing a scientific corpus in this way can improve tasks such as curriculum plan- ning ( <ref type="bibr" target="#b24">Yang et al., 2015)</ref>, automatic reading list generation <ref type="bibr" target="#b9">(Jardine, 2014)</ref>, and improving educa- tion quality <ref type="bibr" target="#b15">(Rouly et al., 2015)</ref>. Motivated by the importance of representing the content of a scien- tific corpus as a concept graph, the challenge we address in this work is to automatically infer the concepts and their dependency relations.</p><p>Towards this end, we first instantiate each con- cept as a topic from statistical topic modeling ( <ref type="bibr" target="#b3">Blei et al., 2003)</ref>. To link concepts with directed depen-dency edges, we propose the use of information- theoretic measures, which we compare against baseline methods of computing word similarity, hierarchical clustering, and citation prediction. We then gather human annotations of concept graph nodes and edges learned from the ACL Anthology, which we use to evaluate these methods.</p><p>The main contributions of this paper are:</p><p>1 We introduce the concept graph representation for modeling the technical concepts in a corpus and their relations. 2 We present information-theoretic approaches to infer concept dependence relations. 3 We perform the first human annotation of con- cept dependence for a technical corpus. <ref type="bibr">4</ref> We release the human annotation data for use in future research.</p><p>In the following section, we contrast this prob- lem with previous work. We then describe the con- cept graph framework (Section 3) and present au- tomatic approaches for inferring concept graphs (Section 4). The details of human evaluation are presented in Section 5. We discuss some interest- ing open questions related to this work in Section 6 before concluding this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is a long history of work on identifying struc- ture in the contents of a text corpus. Our approach is to link documents to concepts and to model rela- tions among these concepts rather than to identify the specific claims <ref type="bibr" target="#b17">(Sch√§fer et al., 2011</ref>) or empiri- cal results <ref type="bibr" target="#b4">(Choi et al., 2016</ref>) in each document. In this section, we first provide an overview of differ- ent relations between concepts, followed by discus- sion of some representative methods for inferring them. We briefly discuss the differences between these relations and the concept dependency relation we are interested in.</p><p>Similarity Concepts are similar to the extent that they share content. <ref type="bibr" target="#b6">Grefenstette (1994)</ref> applied the Jaccard similarity measure to relate concepts to each other. <ref type="bibr" target="#b22">White and Jose (2004)</ref> empirically stud- ied 10 similarity metrics on a small sample of 10 pairs of topics, and the results suggested that correlation-based measures best match general sub- ject perceptions of search topic similarity.</p><p>Hierarchy Previous work on linking concepts has usually been concerned with forming subsump- tion hierarchies from text <ref type="bibr" target="#b23">(Woods, 1997;</ref><ref type="bibr" target="#b16">Sanderson and Croft, 1999;</ref><ref type="bibr" target="#b5">Cimiano et al., 2005</ref>) -e.g., Machine translation is part of Natural language processing -and more recent work does so for sta- tistical topic models. <ref type="bibr" target="#b10">Jonyer et al. (2002)</ref> applied graph-based hierarchical clustering to learn hierar- chies from both structured and unstructured data. <ref type="bibr" target="#b7">Ho et al. (2012)</ref> learn a topic taxonomy from the ACL Anthology and from Wikipedia with a method that scales linearly with the number of topics and the tree depth.</p><p>Other relations Every pair of concepts is statis- tically correlated with each other based on word co-occurrence ( <ref type="bibr" target="#b2">Blei and Lafferty, 2006</ref>) providing a simple baseline metric for comparison. For a topic modeling approach performed over document cita- tion links rather than over words or n-grams, <ref type="bibr" target="#b21">Wang et al. (2013)</ref> gave a topic A's dependence on an- other topic B as the probability of a document in A citing a document in B.</p><p>Our approach to studying concept dependence dif- fers from the relations derived from similarity, hi- erarchy, correlation and citation mentioned above, but intuitively they are related. We thus adapt one representative method for the similarity <ref type="bibr" target="#b6">(Grefenstette, 1994)</ref>, hierarchy <ref type="bibr" target="#b10">(Jonyer et al., 2002</ref>), and citation likelihood ( <ref type="bibr" target="#b21">Wang et al., 2013</ref>) relations as baselines for computing concept dependency rela- tions in Section 4.2.3.</p><p>Concept dependence is also related to curricu- lum planning. <ref type="bibr" target="#b24">Yang et al. (2015)</ref> and <ref type="bibr" target="#b20">Talukdar and Cohen (2012)</ref> studied prerequisite relationships be- tween course material documents based on external information from Wikipedia. They assumed that hyperlinks between Wikipedia pages and course material indicate a prerequisite relationship. With this assumption, <ref type="bibr" target="#b20">Talukdar and Cohen (2012)</ref> use crowdsourcing approaches to obtain a subset of the prerequisite structure and train a maximum entropy- based classifier to identify the prerequisite structure. <ref type="bibr" target="#b24">Yang et al. (2015)</ref> applied both classification and learning to rank approaches in order to classify or rank prerequisite structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Concept Graph Representation of a Text Corpus</head><p>We represent the scientific literature as a labeled graph, where nodes represent both documents and concepts -and, optionally, metadata (such as au- thor, title, conference, year) and features (such as words, or n-grams) -and labeled edges represent the relations between nodes. <ref type="figure" target="#fig_1">Figure 2</ref> shows an ex- ample schema for a concept graph representation for a scientific corpus.</p><p>Concepts are abstract and require a concrete rep- resentation. In this work, we use statistical topic modeling, where each topic -a multinomial distri- bution over a vocabulary of words -is taken as a single concept. Documents are linked to concepts by weighted edges, which can be derived from the topic model's document-topic composition distri- butions. Other approaches to identifying concepts are considered in Section 6.</p><p>Concepts exhibit various relations to other con- cepts, such as hierarchy, connecting more general and more specific concepts; similarity; and cor- relation. We model each concept as a node and concept-to-concept relations as directed, weighted, labeled edges. The label of an edge denotes the type of relation, such as "is similar to", "depends on", and "relates to", and the weights represent the strength of different relations.</p><p>In this work, we focus on concept dependency, which is the least studied of these relations and, intuitively, the most important for learners. We con- sider there to be a dependency relation between two concepts if understanding one concept would help you to understand the other. This notion forms the core of our human-annotated data set which demon- strates that this idea is meaningful and robust for expert annotators when asked to judge if there ex- ists a dependency relation between two concepts defined by LDA topics (see Section 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning the Concept Graph</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Identifying Concepts</head><p>The representation of concepts using topics is very general, and any effective topic modeling approach can be applied. These include probabilistic latent semantic indexing (PLSI) <ref type="bibr" target="#b8">(Hofmann, 1999)</ref>, latent Dirichlet allocation (LDA) ( <ref type="bibr" target="#b3">Blei et al., 2003)</ref>, and non-negative matrix factorization (NMF) ( <ref type="bibr" target="#b0">Arora et al., 2012</ref>). In our experiments, we use the open- source tool Mallet <ref type="bibr" target="#b11">(McCallum, 2002</ref>), which pro- vides a highly scalable implementation of LDA; see Section 5.1 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Discovering Concept Dependency Relations</head><p>Identifying concept dependency relations between topics is the key step for building a useful con- cept graph. These relations add semantic structure to the contents of the text corpus, and they facili- tate search and ordering in information retrieval. In this section, as a proof-of-concept, we propose two information-theoretic approaches to learn concept dependency relations: an approach based on cross entropy and another based on information flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Cross-entropy Approach</head><p>The intuition of the cross-entropy approach is sim- ple: Given concepts c i and c j , if most of the in- stances of c i can be explained by the occurrences of c j , but not vice versa, it is likely that c i depends on c j . For example, if c i is Markov logic networks (MLNs) and c j is Probability, we might say that ob- serving MLNs depends on seeing Probability since most of the times that we see MLNs, we also see Probability, but the opposite does not hold. Given concepts c i and c j , the cross-entropy ap- proach predicts that c i depends on c j if they satisfy these conditions: 1 The distribution of c i is better approximated by that of c j than the distribution of c j is approxi- mated by that of c i . 2 The co-occurrence frequency of instances of c i and c j is relatively higher than that of a non- dependency pair.</p><p>Therefore, to predict the concept dependency re- lation, we need to examine whether the distribution of c i could well approximate the distribution of c j and the joint distribution of c i and c j . For this, we use cross entropy and joint entropy:</p><p>Cross entropy measures the difference between two distributions. Specifically, the cross entropy for the distributions X and Y over a given set is defined as:</p><formula xml:id="formula_0">H(X;Y ) = H(X) + D KL (X||Y )<label>(1)</label></formula><p>where H(X) is the entropy of X, and D KL (X||Y ) is the Kullback-Leibler divergence of an estimated distribution Y from true distribution X. Therefore, H(X;Y ) examines how well the distribution of Y approximates that of X.</p><p>Joint entropy measures the information we ob- tained when we observe both X and Y . The joint Shannon entropy of two variables X and Y is de- fined as:</p><formula xml:id="formula_1">H(X,Y ) = ‚àë X ‚àë Y P(X,Y ) log 2 P(X,Y ) (2)</formula><p>where P(X,Y ) is the joint probability of these val- ues occurring together.</p><p>Based on the conditions listed above and these def- initions, we say that c i depends on c j if and only if they satisfy the following constraints:</p><formula xml:id="formula_2">H(c i ; c j ) &gt; H(c j ; c i ) H(c i , c j ) ‚â§ Œ∏<label>(3)</label></formula><p>with Œ∏ as a threshold value, which can be inter- preted as "the average joint entropy of any non- dependence concepts". The weight of the depen- dency is defined as:</p><formula xml:id="formula_3">D CE (c i , c j ) = H(c i ; c j )</formula><p>The cross-entropy method is general and can be applied to different distributions used to model concepts, such as distributions of relevant words, of relevant documents, or of the documents that are cited by relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Information-flow Approach</head><p>Now we consider predicting concept dependency relations from the perspective of navigating infor- mation. Imagine that we already have a perfect concept dependency graph. When we are at a con- cept node (e.g., reading a document about it), the navigation is more likely to continue to a concept it depends on than to other concepts that it doesn't depend on. To give a concrete example, if we are navigating from the concept Page rank, it is more likely for us to jump to Eigenvalue than to Lan- guage model. Therefore, if concept c i depends on concept c j , then c j generally receives more naviga- tion hits than c i and has higher "information flow".</p><p>Based on this intuition, we can predict con- cept dependency relations using information flow: Given concepts c i and c j , c i depends on c j if they satisfy these conditions: 1 The concept c i receives relatively lower naviga- tion hits than c j . 2 The number of navigation traces from concept c i to c j is much stronger than that to another non-dependent concept c k .</p><p>While we do not have data for human navigation between concepts, a natural way to simulate this is through information flow. As proposed by <ref type="bibr" target="#b14">Rosvall and Bergstrom (2008)</ref>, we use the probability flow of random walks on a network as a proxy for infor- mation flow in the real system. Given any observed graph G, the information score I(v) of a node v, is defined as its steady state visit frequency. The information flow I(u, v) from node u to node v, is consequently defined as the transition probability (or "exit probability") from u to v.</p><p>To this end, we construct a graph connecting concepts by their co-occurrences in documents, and we can use either Map Equation (Rosvall and <ref type="bibr">et al., 2014</ref>) to compute the information flow net- work and the information score for each concept node. The details are outlined as follows:</p><note type="other">Bergstrom, 2008) or Content Map Equation (Smith</note><p>1 Construct a concept graph G co based on co- occurrence observations. We define weighted, undirected edges within the concept graph based on the number of documents in which the con- cepts co-occur. Formally, given concepts c i and c j and a threshold 0 ‚â§ œÑ ‚â§ 1, the weighted edge is calculated as:</p><formula xml:id="formula_4">w co (c i , c j ) = ‚àë d p(c i |d)p(c j |d) if p(c|d) &gt; œÑ 0 otherwise (4)</formula><p>2 Given the graph G co , we compute the informa- tion score I(c) for each concept node c and infor- mation flow I(c i , c j ) between a pair of nodes c i and c j . For the details of calculating I(c) and I(c i , c j ), refer to Map Equation ( <ref type="bibr" target="#b14">Rosvall and Bergstrom, 2008)</ref> and Content Map Equa- tion ( <ref type="bibr" target="#b19">Smith et al., 2014</ref>). 3 Given two concepts c i and c j , we link c i to c j with a directed edge if I(c i ) &gt; I(c j ) with weight:</p><formula xml:id="formula_5">D IF (c i , c j ) = I(c i , c j )</formula><p>The information flow approach for inferring de- pendency can be further improved with a few true human navigation traces. As introduced earlier, the concept graph representation facilitates applica- tions such as reading list generation, and document retrieval. Those applications enable the collection of human navigation traces, which can provide a better approximation of dependency relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Baseline Approaches</head><p>Similarity Relations Intuitively, concepts that are more similar (e.g., Machine translation and Machine translation evaluation) are more likely to be connected by concept dependency relations than less similar concepts are. As a baseline, we compute the Jaccard similarity coefficient based on the top 20 words or n-grams in the concept's topic word distributions.</p><p>Hierarchical Relations Previous work has looked at learning hierarchies that connect broader topics (acting as equivalent proxies for concepts in our work) to more specific subtopics ( <ref type="bibr" target="#b5">Cimiano et al., 2005;</ref><ref type="bibr" target="#b16">Sanderson and Croft, 1999</ref>). We compare against a method for doing so to see how close iden- tifying hierarchical relations comes to our goal of identifying concept dependency relations. Specifi- cally, we perform agglomerative clustering over the topic-topic co-occurrence graph G co with weights defined in Eq. 4, in order to obtain the hierarchical representation for concepts.</p><p>Citation-based Given concepts c i and c j , if the documents that are highly related to c j are cited by most of the instances of c i , c i may depend on c j . <ref type="bibr" target="#b21">Wang et al. (2013)</ref> used this approach in the context of CitationLDA topic modeling, where topics are learned from citation links rather than text. We adapt this for regular LDA so that the concept c i depends on c j with weight</p><formula xml:id="formula_6">D Cite (c i , c j ) = ‚àë d 1 ‚ààD ‚àë d 2 ‚ààC d 1 T 1,i T 2, j<label>(5)</label></formula><p>where D is the set of all documents, C d are the documents cited by d, and T x,y is the distribution of documents d x composed of concepts c y . For this method, we return a score of 0 if the concepts do not co-occur in at least three documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation of Concept Graphs</head><p>There are two main approaches to evaluating a con- cept graph: We can directly evaluate the graph, using human judgments to measure the quality of the concepts and the reliability of the links between them. Alternatively, we can evaluate the applica- tion of a concept graph to a task, such as ordering documents for a reading list or recommending doc- uments to cite when writing a paper. Our motivation to build a concept graph from a technical corpus is to improve performance at the task of reading list generation. However, an applied evaluation makes it harder to judge the quality of the concept graph itself. Each document contains a combination of concepts, which have different ordering restrictions, and other factors also affect the quality of a reading list, such as the classification of document difficulty and type (e.g., survey, tutorial, or experimental results). As such, we focus on a direct human evaluation of our proposed methods for building a concept graph and leave the measure of applied performance to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Corpus and its Evaluation Concept Graphs</head><p>For this evaluation, the scientific corpus we use is the ACL Anthology. This consists of articles published in a variety of journals, conferences, and workshops related to computational linguis- tics. Specifically, we use a modified copy of the plain text distributed for the ACL Anthology Net- work (AAN), release 2013 ( <ref type="bibr">Radev et al., 2013)</ref>, which includes 23,261 documents from 1965 to 2013. The AAN includes plain text for documents, with OCR performed using PDFBox. We manually substituted OmniPage OCR output from the ACL Anthology Reference Corpus, version 1 ( <ref type="bibr" target="#b1">Bird et al., 2008</ref>) for documents where it was observed to be of higher quality. The text was processed to join words that were split across lines with hyphens. We manually removed documents that were not written in English or where text extraction failed, leaving 20,264 documents, though this filtering was not exhaustive.</p><p>The topic model we used was built using the Mallet <ref type="bibr" target="#b11">(McCallum, 2002</ref>) implementation of LDA. It is composed of bigrams, filtered of typical En- glish stop words before the generation of bigrams, so that, e.g., "word to word" yields the bigram "word word". We generated topic models consist- ing of between 20 and 400 topics and selected a 300-topic model based on manual inspection. Doc- uments were linked to concepts based on the docu- ment's LDA topic composition. The concept nodes for each topic were linked in concept dependency relations using each of the methods described in Section 4, producing five concept graphs to evalu- ate. We applied the general cross-entropy method to the distribution of top-k bigrams for each con- cept. For all methods, the results we report are for k = 20. Changing this value shifts the precision- recall trade-off, but in our experiments, the relative performance of the methods are generally consis- tent for different values of k.</p><p>Since it is impractical to manually annotate all pairs of concept nodes from a 300-node graph, we se- lected a subset of edges for evaluation. Intuitively, the evaluation set should satisfy the following sam- pling criteria: (1) The evaluation set should cover the top weighted edges for a precision evaluation. (2) The evaluation set should cover the bottom- weighted edges for a recall evaluation. <ref type="formula" target="#formula_2">(3)</ref> The evaluation set should provide low-biased sampling. With respect to these requirements, we generated an evaluation edge set as the union of the following three sets:</p><p>1 Top-20 edges for each approach (including base- line approaches) 2 A random shuffle selection from the union of  <ref type="table">Table 1</ref>: Inter-annotator agreement measured as Pearson correlation.</p><p>Relevant phrases:</p><p>machine translation, translation system, mt system, transfer rules, mt systems, lexical transfer, analysis transfer, translation process, transfer generation, transfer component, analysis synthesis, transfer phase, analysis generation, structural transfer, transfer approach, human translation, transfer grammar, analysis phase, translation systems, transfer process</p><p>Relevant documents:</p><p>‚Ä¢ the top-50 and bottom-50 edges in terms of the baseline word similarity. 1 3 A random shuffle section from the union of top- 100 edges in terms of the proposed approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Human Annotation</head><p>For annotation, we present pairs of topics followed by questions. Each topic is presented to a judge as a list of the most relevant bigrams in descending or- der of their topic-specific "collapsed" probabilities. These are presented in greyscale so that the most relevant items appear black, fading through grey to white as the strength of that item's association with the topic decreases. The evaluation interface also lists the documents that are most relevant to the topic, linked to the original PDFs. These doc- uments can be used to clarify the occurrence of unfamiliar terms, such as author names or common examples that may show up in the topic representa- tion. An example topic is shown in <ref type="figure" target="#fig_3">Figure 4</ref>. For each topic, judges were asked:  <ref type="table">Table 2</ref>: Precision, recall, and f-scores (with different thresholds for which edges are included) for the methods of predicting dependency relations between concepts described in Section 4.2.</p><p>If both topics are at least somewhat clear:</p><p>3 How related are these topics? 4 Would understanding Topic 1 help you to under- stand Topic 2? 5 Would understanding Topic 2 help you to under- stand Topic 1?</p><p>For each question, they could answer "I don't know" or select from an ordinal scale:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Not at all 2 Somewhat 3 Very much</head><p>The evaluation was completed by eight judges with varying levels of familiarity with the technical domain. Four judges are NLP researchers: Three PhD students working in the area and one of the au- thors. Four judges are familiar with NLP but have less experience with NLP research: two MS stu- dents, an AI PhD student, and one of the authors. The full evaluation was divided into 10 sets taking a total of around 6-8 hours per person to anno- tate. Their overall inter-annotator agreement and the agreement for each question type is given in Ta- ble 1. Agreement is higher when we consider only judgments from NLP researchers, but in all cases is moderate, indicating the difficulty of interpret- ing statistical topics as concepts and judging the strength (if any) of the concept dependency relation between them.</p><p>The topic coherence judgments that were col- lected served to make each human judge consider how well she understood each topic before judging their dependence. The topic relatedness questions provided an opportunity to indicate that if the an- notator recognized a relation between the topics without needing to say that their was a dependence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation of Automatic Methods</head><p>To measure the quality of the concept dependency edges in our graphs, we compute the average preci- sion for the strongest edges in each concept graph, up to three thresholds: the top 20 edges, the top 150, and all edges with strength &gt; 0. These precision scores are in <ref type="table">Table 2</ref> as well as the corresponding recall, and f 1 scores for the larger thresholds. De- spite the difference in inter-annotator agreement reported in <ref type="table">Table 1</ref>, the ordering of methods by precision is the same whether we consider only the judgments of NLP experts, non-NLP judges, or everyone, so we only report the average across all annotators.</p><p>When we examine the results of precision at 20 - the strongest edges predicted by each method -we see that the cross-entropy method performs best. For comparison, we report the accuracy of a base- line of random numbers between 0 and 1. While all methods have better than chance precision, the random baseline has higher recall since it predicts a dependency relation of non-zero strength for all pairs. As we consider edges predicted with lower confidence, the word similarity approach shows the highest precision. A limitation of the word similar- ity baseline is that it is symmetric while concept dependence relations can be asymmetric.</p><p>Annotators marked many pairs of concepts as being at least somewhat co-dependent. E.g., un- derstanding Speech recognition strongly helps you understand Natural language processing, but be- ing familiar with this broader topic also somewhat helps you understand the narrower one. The preci- sion scores we report count both annotations of con- cept dependence ("Somewhat" and "Very much") as positive predictions, but other evaluation met- rics might show a greater benefit for methods like D CE that can predict dependency with asymmetric strengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Another natural evaluation of an automatically gen- erated concept graph would be to compare it to a  human-generated gold standard, where an expert has created concept nodes at the optimal level of generality and linked these by her understanding of the conceptual dependencies among concepts in the domain. However, there are several difficulties with this approach: (1) It is quite labor-intensive to man- ually generate a concept graph; (2) we expect only moderate agreement between graphs produced by different experts, who have different ideas of what concepts are important and distinct and which con- cepts are important to understanding others; and (3) the concept graphs we learn from a collection of documents will differ significantly from those we imagine, without these differences necessarily being better or worse.</p><p>In this work, we assume that a topic model pro- vides a reasonable proxy for the concepts a person might identify in a technical corpus. However, topic modeling approaches are better at finding general areas of research than at identifying fine-grained concepts like those shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The concept graph formalism can be extended with the use of discrete entities, identified by a small set of names, e.g., (First-order logic, FOL). We have performed initial work on two approaches to extract entities:</p><p>1 We can use an external reference, Wikipedia, to help entity extraction. We count the occurrences of each article title in the scientific corpus, and we keep the high-frequency titles as entities. For example, in the ACL Anthology corpus, we ob- tain 56 thousand entities (page titles) that oc- curred at least once and 1,123 entities that occur at least 100 times. 2 We cannot assume that the important entities in every scientific or technical corpus will be well-represented on Wikipedia. In the absence of a suitable external reference source, we can use the open-source tool SKIMMR <ref type="bibr" target="#b12">(Nov√°ƒçek and Burns, 2014</ref>) or the method proposed by Jardine (2014) to extract important noun phrases to use as entities. The importance of a potential entity can be computed based on the occurrence frequency and the sentence-level co-occurrence frequency with other phrases.</p><p>Another limitation of using a topic model like LDA as a proxy for concepts is that the topics are static, while a corpus may span decades of research. Studying how latent models might evolve or "drift" over time within a textual corpus describing a tech- nical discipline is an important research question, and our approach could be extended to add or re- move topics in a central model over time.</p><p>Despite its limitations, a topic model is useful for automatically discovering concepts in a corpus even if the concept is not explicitly mentioned in a document (e.g., the words "axiom" or "predi-cate" might indicate discussion of logic) or has no canonical name. The concept graph representation allows for the introduction of additional or alter- native features for concepts, making it suitable for new methods of identifying and linking concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Problems such as reading list generation require a representation of the structure of the content of a scientific corpus. We have proposed the concept graph framework, which gives weighted links from documents to the concepts they discuss and links concepts to one another. The most important link in the graph is the concept dependency relation, which indicates that one concept helps a learner to understand another, e.g., Markov logic networks depends on Probability.</p><p>We have presented four approaches to predicting these relations. We propose information-theoretic measures based on cross entropy and on informa- tion flow. We also present baselines that compute the similarity of the word distributions associated with each concept, the likelihood of a citation con- necting the concepts, and a hierarchical clustering approach. While word similarity proves a strong baseline, the strongest edges predicted by the cross- entropy approach are more precise. We are releas- ing human annotations of concept nodes and pos- sible dependency edges learned from the ACL An- thology as well as implementations of the methods described in this paper to enable future research on modeling scientific corpora. <ref type="bibr">2</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A human-authored concept graph excerpt, showing possible concepts related to automatic speech recognition and their concept dependencies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Concept Graph Data Schema. Each node is a class and edges are named relations between classes (with associated attributes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A concept graph excerpt related to machine translation, where concepts are linked based on cross entropy. Concepts are represented by manually chosen names, and links to documents are omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example of the presentation of a topic for human evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A concept graph excerpt related to machine translation, where concepts are joined based on the judgments of human annotators. Concepts are represented by manually chosen names, and links to documents are omitted.</figDesc></figure>

			<note place="foot" n="1"> We observe that usually if the edge strength in terms of one of the information-theoretic methods is zero, the word similarity is zero as well, but if the word similarity is zero, the edge strength in terms of the proposed methods may be non-zero.</note>

			<note place="foot" n="2"> The code and data associated with this work are available at http://techknacq.isi.edu</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Yigal Arens, Emily Sheng, and Jon May for their valuable feedback on this work. This work was supported by the Intelligence Ad-vanced Research Projects Activity (IARPA) via the Air Force Research Laboratory. The U.S. Gov-ernment is authorized to reproduce and distribute reprints for Governmental purposes notwithstand-ing any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or en-dorsements, either expressed or implied, of IARPA, AFRL, or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning topic models-going beyond SVD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Symposium on Foundations of Computer Science</title>
		<meeting>the 53rd Annual Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The ACL Anthology Reference Corpus: A reference dataset for bibliographic research in computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Powley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Fan</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Language Resources and Evaluation</title>
		<meeting>the Sixth International Conference on Language Resources and Evaluation<address><addrLine>Marrakech, Morocco, May</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Correlated topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extracting structured scholarly information from the machine translation literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matic</forename><surname>Horvat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Language Resources and Evaluation. European Language Resources Association</title>
		<meeting>the 10th International Conference on Language Resources and Evaluation. European Language Resources Association</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning concept hierarchies from text corpora using formal concept analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Hotho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Staab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="305" to="344" />
			<date type="published" when="2005-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Explorations in Automatic Thesaurus Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Grefenstette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Norwell, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Document hierarchies from text and links</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qirong</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International World Wide Web Conference</title>
		<meeting>the International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2012-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Automatically generating reading lists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">G</forename><surname>Jardine</surname></persName>
		</author>
		<idno>UCAM-CL-TR- 848</idno>
		<imprint>
			<date type="published" when="2014-02" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge Computer Laboratory</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph-based hierarchical conceptual clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Istvan</forename><surname>Jonyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">B</forename><surname>Holder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="19" to="43" />
			<date type="published" when="2002-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">MALLET: A machine learning for language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://mallet.cs.umass.edu" />
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SKIMMR: Facilitating knowledge discovery in life sciences by machine-aided skim reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V√≠t</forename><surname>Nov√°ƒçek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apc</forename><surname>Gully</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">483</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Radev</surname></persName>
		</author>
		<title level="m">Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. 2013. The ACL Anthology Network Corpus. Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maps of random walks on complex networks reveal community structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rosvall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">T</forename><surname>Bergstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1118" to="1141" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What are we teaching?: Automated evaluation of CS curricula content using topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Michel Rouly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huzefa</forename><surname>Rangwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Johri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Annual International Conference on International Computing Education Research</title>
		<meeting>the Eleventh Annual International Conference on International Computing Education Research</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="189" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deriving concept hierarchies from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="206" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The ACL Anthology Searchbench</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Spurk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J√∂rg</forename><surname>Steffen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-HLT</title>
		<meeting>the ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<title level="m">System Demonstrations</title>
		<imprint>
			<biblScope unit="page" from="7" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allon</forename><forename type="middle">G</forename><surname>Percus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4332</idno>
		<title level="m">Partitioning networks with node attributes by compressing information flow</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Crowdsourced comprehension: Predicting prerequisite structure in Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Building Educational Applications Using NLP</title>
		<meeting>the Seventh Workshop on Building Educational Applications Using NLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="307" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding evolution of research themes: A probabilistic generative model for citations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1115" to="1138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A study of topic similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ryen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joemon</forename><forename type="middle">M</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and development in Information Retrieval</title>
		<meeting>the 27th Annual International ACM SIGIR Conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="520" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Conceptual indexing: A better way to organize knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">A</forename><surname>Woods</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Sun Microsystems, Inc</publisher>
			<pubPlace>Mountain View, CA, USA</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Concept graph learning from educational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eighth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="159" to="68" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
