<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compositional Vector Space Models for Knowledge Base Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003</postCode>
									<settlement>Amherst Amherst</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003</postCode>
									<settlement>Amherst Amherst</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003</postCode>
									<settlement>Amherst Amherst</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Compositional Vector Space Models for Knowledge Base Completion</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="156" to="166"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Knowledge base (KB) completion adds new facts to a KB by making inferences from existing facts, for example by inferring with high likelihood nationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop relational synonyms like this, or use as evidence a multi-hop re-lational path treated as an atomic feature, like bornIn(X,Z) → containedIn(Z,Y). This paper presents an approach that reasons about conjunctions of multi-hop relations non-atomically, composing the implications of a path using a recurrent neural network (RNN) that takes as inputs vector embeddings of the binary relation in the path. Not only does this allow us to generalize to paths unseen at training time, but also, with a single high-capacity RNN, to predict new relation types not seen when the compositional model was trained (zero-shot learning). We assemble a new dataset of over 52M relational triples, and show that our method improves over a traditional classifier by 11%, and a method leveraging pre-trained em-beddings by 7%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Constructing large knowledge bases (KBs) sup- ports downstream reasoning about resolved enti- ties and their relations, rather than the noisy tex- tual evidence surrounding their natural language mentions. For this reason KBs have been of in- creasing interest in both industry and academia <ref type="bibr" target="#b5">(Bollacker et al., 2008;</ref><ref type="bibr" target="#b41">Suchanek et al., 2007;</ref><ref type="bibr" target="#b8">Carlson et al., 2010)</ref>. Such KBs typically con- tain many millions of facts, most of them (en- tity1,relation,entity2) "triples" (also known as bi- nary relations) such as (Barack Obama, presi- dentOf, USA) and (Brad Pitt, marriedTo, Angelina Jolie).</p><p>However, even the largest KBs are woefully in- complete ( <ref type="bibr" target="#b26">Min et al., 2013)</ref>, missing many impor- tant facts, and therefore damaging their usefulness in downstream tasks. Ironically, these missing facts can frequently be inferred from other facts al- ready in the KB, thus representing a sort of incon- sistency that can be repaired by the application of an automated process. The addition of new triples by leveraging existing triples is typically known as KB completion.</p><p>Early work on this problem focused on learn- ing symbolic rules. For example, <ref type="bibr" target="#b35">Schoenmackers et al. (2010)</ref> learns Horn clauses predictive of new binary relations by exhausitively exploring re- lational paths of increasing length, and selecting those surpassing an accuracy threshold. (A "path" is a sequence of triples in which the second entity of each triple matches the first entity of the next triple.) <ref type="bibr" target="#b20">Lao et al. (2011)</ref> introduced the Path Rank- ing Algorithm (PRA), which greatly improves ef- ficiency and robustness by replacing exhaustive search with random walks, and using unique paths as features in a per-target-relation binary classifier. A typical predictive feature learned by PRA is that CountryOfHeadquarters(X, Y) is implied by Is- BasedIn(X,A) and StateLocatedIn(A, B) and Coun- tryLocatedIn(B, Y). Given IsBasedIn(Microsoft, Seattle), StateLocatedIn(Seattle, Washington) and CountryLocatedIn(Washington, USA), we can in- fer the fact CountryOfHeadquarters(Microsoft, USA) using the predictive feature. In later work, <ref type="bibr" target="#b21">Lao et al. (2012)</ref> greatly increase available raw material for paths by augmenting KB-schema rela- tions with relations defined by the text connecting mentions of entities in a large corpus (also known as OpenIE relations ( <ref type="bibr" target="#b2">Banko et al., 2007)</ref>).</p><p>However, these symbolic methods can produce many millions of distinct paths, each of which is categorically distinct, treated by PRA as a dis-tinct feature. (See <ref type="figure" target="#fig_1">Figure 1.</ref>) Even putting aside the OpenIE relations, this limits the applicability of these methods to modern KBs that have thou- sands of relation types, since the number of dis- tinct paths increases rapidly with the number of re- lation types. If textually-defined OpenIE relations are included, the problem is obviously far more severe.</p><p>Better generalization can be gained by operat- ing on embedded vector representations of rela- tions, in which vector similarity can be interpreted as semantic similarity. For example, <ref type="bibr" target="#b6">Bordes et al. (2013)</ref> learn low-dimensional vector representa- tions of entities and KB relations, such that vector differences between two entities should be close to the vectors associated with their relations. This approach can find relation synonyms, and thus per- form a kind of one-to-one, non-path-based relation prediction for KB completion. Similarly <ref type="bibr" target="#b30">Nickel et al. (2011) and</ref><ref type="bibr" target="#b38">Socher et al. (2013a)</ref> perform KB completion by learning embeddings of rela- tions, but based on matrices or tensors. Universal schema ( <ref type="bibr" target="#b34">Riedel et al., 2013</ref>) learns to perform rela- tion prediction cast as matrix completion (likewise using vector embeddings), but predicts textually- defined OpenIE relations as well as KB relations, and embeds entity-pairs in addition to individual entities. Like all of the above, it also reasons about individual relations, not the evidence of a connected path of relations. This paper proposes an approach combining the advantages of (a) reasoning about conjunctions of relations connected in a path, and (b) generaliza- tion through vector embeddings, and (c) reasoning non-atomically and compositionally about the el- ements of the path, for further generalization.</p><p>Our method uses recurrent neural networks (RNNs) <ref type="bibr" target="#b44">(Werbos, 1990)</ref> to compose the semantics of relations in an arbitrary-length path. At each path-step it consumes both the vector embedding of the next relation, and the vector representing the path-so-far, then outputs a composed vector (rep- resenting the extended path-so-far), which will be the input to the next step. After consuming a path, the RNN should output a vector in the semantic neighborhood of the relation between the first and last entity of the path. For example, after con- suming the relation vectors along the path Melinda Gates → Bill Gates → Microsoft → Seattle, our method produces a vector very close to the rela- tion livesIn.   Our compositional approach allow us at test time to make predictions from paths that were un- seen during training, because of the generaliza- tion provided by vector neighborhoods, and be- cause they are composed in non-atomic fashion. This allows our model to seamlessly perform in- ference on many millions of paths in the KB graph. In most of our experiments, we learn a separate RNN for predicting each relation type, but alterna- tively, by learning a single high-capacity composi- tion function for all relation types, our method can perform zero-shot learning-predicting new rela- tion types for which the composition function was never explicitly trained.</p><p>Related to our work, new versions of PRA ( <ref type="bibr" target="#b13">Gardner et al., 2013;</ref><ref type="bibr" target="#b14">Gardner et al., 2014</ref>) use pre-trained vector representations of relations to alleviate its feature explosion problem-but the core mechanism continues to be a classifier based on atomic-path features. In the 2013 work many paths are collapsed by clustering paths accord- ing to their relations' embeddings, and substitut- ing cluster ids for the original relation types. In the 2014 work unseen paths are mapped to nearby paths seen at training time, where nearness is mea- sured using the embeddings. Neither is able to per- form zero-shot learning since there must be a clas- sifer for each predicted relation type. Furthermore their pre-trained vectors do not have the opportu- nity to be tuned to the KB completion task because the two sub-tasks are completely disentangled.</p><p>An additional contribution of our work is a new large-scale data set of over 52 million triples, and its preprocessing for purposes of path-based KB completion (can be downloaded from http: //iesl.cs.umass.edu/downloads/ inferencerules/release.tar.gz). The dataset is build from the combination of Freebase ( <ref type="bibr" target="#b5">Bollacker et al., 2008</ref>) and Google's entity linking in ClueWeb ( <ref type="bibr" target="#b32">Orr et al., 2013</ref>  entity pairs, we use over 10k. All experimental comparisons below are performed on this new data set.</p><p>On this challenging large-scale dataset our com- positional method outperforms PRA ( <ref type="bibr" target="#b21">Lao et al., 2012)</ref>, and Cluster PRA ( <ref type="bibr" target="#b13">Gardner et al., 2013</ref>) by 11% and 7% respectively. A further contribution of our work is a new, surprisingly strong baseline method using classifiers of path bigram features, which beats PRA and Cluster PRA, and statisti- cally ties our compositional method. Our analysis shows that our method has substantially different strengths than the new baseline, and the combi- nation of the two yields a 15% improvement over <ref type="bibr" target="#b13">Gardner et al. (2013)</ref>. We also show that our zero- shot model is indeed capable of predicting new un- seen relation types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We give background on PRA which we use to ob- tain a set of paths connecting the entity pairs and the RNN model which we employ to model the composition function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Path Ranking Algorithm</head><p>Since it is impractical to exhaustively obtain the set of all paths connecting an entity pair in the large KB graph, we use PRA ( <ref type="bibr" target="#b20">Lao et al., 2011</ref>) to obtain a set of paths connecting the entity pairs. Given a training set of entity pairs for a relation, PRA heuristically finds a set of paths by perform- ing random walks from the source and target nodes keeping the most common paths. We use PRA to find millions of distinct paths per relation type. We do not use the random walk probabilities given by PRA since using it did not yield improvements in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Recurrent Neural Networks</head><p>Recurrent neural network (RNN) <ref type="bibr" target="#b44">(Werbos, 1990</ref>) is a neural network that constructs vector repre- sentation for sequences (of any length). For exam- ple, a RNN model can be used to construct vec- tor representations for phrases or sentences (of any length) in natural language by applying a compo- sition function <ref type="bibr" target="#b24">(Mikolov et al., 2010;</ref>). The vector representation of a phrase (w 1 , w 2 ) consisting of words w 1 and w 2 is given by f (W [v(w 1 ); v(w 2 )]) where v(w) ∈ R d is the vector representation of w, f is an element-wise non linearity function, [a; b] represents the concatenation two vectors a and b along with a bias term, and W ∈ R d×2 * d+1 is the composition matrix. This operation can be repeated to construct vector representations of longer phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Recurrent Neural Networks for KB Completion</head><p>This paper proposes a RNN model for KB comple- tion that reasons on the paths connecting an entity pair to predict missing relation types. The vec- tor representations of the paths (of any length) in the KB graph are computed by applying the com- position function recursively as shown in <ref type="figure" target="#fig_2">Figure  2</ref>. To compute the vector representations for the higher nodes in the tree, the composition function consumes the vector representation of the node's two children nodes and outputs a new vector of the same dimension. Predictions about missing rela- tion types are made by comparing the vector repre- sentation of the path with the vector representation of the relation using the sigmoid function. We represent each binary relation using a d- dimensional real valued vector. We model com- position using recurrent neural networks <ref type="bibr" target="#b44">(Werbos, 1990)</ref>. We learn a separate composition matrix for every relation that is predicted.</p><p>Let v r (δ) ∈ R d be the vector representation of relation δ and v p (π) ∈ R d be the vector represen- tation of path π. v p (π) denotes the relation vec- tor if path π is of length one. To predict relation δ = CountryOfHeadquarters, the vector represen- tation of the path π = IsBasedIn → StateLocate- dIn containing two relations IsBasedIn and State- LocatedIn is computed by <ref type="figure" target="#fig_2">(Figure 2)</ref>,</p><formula xml:id="formula_0">v p (π) = f (W δ [v r (IsBasedIn); v r (StateLocatedIn)])</formula><p>where f = sigmoid is the element-wise non- linearity function, W δ ∈ R d * 2d+1 is the compo- sition matrix for δ = CountryOfHeadquarters and [a; b] represents the concatenation of two vectors a ∈ R d , b ∈ R d along with a bias feature to get a new vector</p><formula xml:id="formula_1">[a; b] ∈ R 2d+1 .</formula><p>The vector representation of the path Π = Is- BasedIn → StateLocatedIn → CountryLocatedIn in <ref type="figure" target="#fig_2">Figure 2</ref> is computed similarly by,</p><formula xml:id="formula_2">v p (Π) = f (W δ [v p (π); v r ( CountryLocatedIn)])</formula><p>where v p (π) is the vector representation of path Is- BasedIn → StateLocatedIn. While computing the vector representation of a path we always traverse left to right, composing the relation vector in the right with the accumulated path vector in the left <ref type="bibr">1</ref> . This makes our model a recurrent neural network <ref type="bibr" target="#b44">(Werbos, 1990)</ref>.</p><p>Finally, we make a prediction regarding Coun- tryOfHeadquarters(Microsoft, USA) using the path Π = IsBasedIn → StateLocatedIn → Coun- tryLocatedIn by comparing the vector represen- tation of the path (v p (Π)) with the vector repre- sentation of the relation CountryOfHeadquarters (v r (CountryOfHeadquarters)) using the sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Training</head><p>We train the model with the existing facts in a KB using them as positive examples and nega- tive examples are obtained by treating the unob- served instances as negative examples <ref type="bibr" target="#b27">(Mintz et al., 2009;</ref><ref type="bibr" target="#b20">Lao et al., 2011;</ref><ref type="bibr" target="#b34">Riedel et al., 2013;</ref><ref type="bibr" target="#b6">Bordes et al., 2013)</ref>. Unlike in previous work that use <ref type="bibr">RNNs(Socher et al., 2011;</ref><ref type="bibr" target="#b19">Iyyer et al., 2014;</ref><ref type="bibr" target="#b18">Irsoy and Cardie, 2014</ref>), a challenge with using them for our task is that among the set of paths connect- ing an entity pair, we do not observe which of the path(s) is predictive of a relation. We select the path that is closest to the relation type to be pre- dicted in the vector space. This not only allows for faster training (compared to marginalization) but also gives improved performance. This tech- nique has been successfully used in models other than RNNs previously ( <ref type="bibr" target="#b29">Neelakantan et al., 2014</ref>). </p><note type="other">1 Training Algorithm of RNN model for rela- tion δ 1: Input:</note><formula xml:id="formula_3">Λ δ = Λ + δ ∪ Λ − δ , Φ δ</formula><p>, number of itera- tions T , mini-batch size B 2: Initialize v r , W δ randomly 3: for t = 1, 2, . . . , T do µ λ = arg max π∈Φ δ (γ) v p (π).v r (δ)</p><note type="other">7: Accumulate gradients to v r , W δ 8: using path µ λ . 9: b = b + 1 10: if b = B then 11: Gradient Update for v r , W δ 12: v r = 0, W δ = 0 and b = 0 13: end if 14: end for 15: if b &gt; 0 then 16: Gradient Update for v r , W δ 17: end if 18: end for 19: Output: v r , W δ</note><p>We assume that we are given a KB (for exam- ple, Freebase enriched with SVO triples) contain- ing a set of entity pairs Γ, set of relations ∆ and a set of observed facts Λ + where ∀λ = (γ, δ) ∈ Λ + (γ ∈ Γ, δ ∈ ∆) indicates a positive fact that entity pair γ is in relation δ. Let Φ δ (γ) denote the set of paths connecting entity pair γ given by PRA for predicting relation δ.</p><p>In our task, we only observe the set of paths connecting an entity pair but we do not observe which of the path(s) is predictive of the fact. We treat this as a latent variable (µ λ for the fact λ) and we assign µ λ the path whose vector represen- tation has maximum dot product with the vector representation of the relation to be predicted. For example, µ λ for the fact λ = (γ, δ) ∈ Λ + is given by,</p><formula xml:id="formula_4">µ λ = arg max π∈Φ δ (γ) v p (π).v r (δ)</formula><p>During training, we assign µ λ using the current parameter estimates. We use the same procedure to assign µ λ for unobserved facts that are used as negative examples during training.</p><p>We train a separate RNN model for predicting each relation and the parameters of the model for predicting relation δ ∈ ∆ are Θ = {v r (ω)∀ω ∈ ∆, W δ }. Given a training set consisting of posi-tive (Λ + δ ) and negative (Λ − δ ) instances 2 for relation δ, the parameters are trained to maximize the log likelihood of the training set with L-2 regulariza- tion.</p><formula xml:id="formula_5">Θ * = arg max Θ λ=(γ,δ)∈Λ + δ P (y λ = 1; Θ)+ λ=(γ,δ)∈Λ − δ P (y λ = 0; Θ) − ρΘ 2</formula><p>where y λ is a binary random variable which takes the value 1 if the fact λ is true and 0 otherwise, and the probability of a fact P (y λ = 1; Θ) is given by,</p><formula xml:id="formula_6">P (y λ = 1; Θ) = sigmoid(v p (µ λ ).v r (δ))</formula><p>where µ λ = arg max</p><formula xml:id="formula_7">π∈Φ δ (γ) v p (π).v r (δ)</formula><p>and P (y λ = 0; Θ) = 1 − P (y λ = 1; Θ). The relation vectors and the composition matrix are initialized randomly. We train the network us- ing backpropagation through structure <ref type="bibr" target="#b15">(Goller and Küchler, 1996</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Zero-shot KB Completion</head><p>The KB completion task involves predicting facts on thousands of relations types and it is highly de- sirable that a method can infer facts about relation types without directly training for them. Given the vector representation of the relations, we show that our model described in the previous section is ca- pable of predicting relational facts without explic- itly training for the target (or test) relation types (zero-shot learning).</p><p>In zero-shot or zero-data learning <ref type="bibr" target="#b22">(Larochelle et al., 2008;</ref><ref type="bibr" target="#b33">Palatucci et al., 2009)</ref>, some labels or classes are not available during training the model and only a description of those classes are given at prediction time. We make two modifications to the model described in the previous section, (1) learn a general composition matrix, and (2) fix re- lation vectors with pre-trained vectors, so that we can predict relations that are unseen during train- ing. This ability of the model to generalize to un- seen relations is beyond the capabilities of all pre- vious methods for KB inference ( <ref type="bibr" target="#b35">Schoenmackers et al., 2010;</ref><ref type="bibr" target="#b20">Lao et al., 2011;</ref><ref type="bibr" target="#b13">Gardner et al., 2013;</ref><ref type="bibr" target="#b14">Gardner et al., 2014</ref>).</p><p>We learn a general composition matrix for all relations instead of learning a separate composi- tion matrix for every relation to be predicted. So, for example, the vector representation of the path π = IsBasedIn → StateLocatedIn containing two relations IsBasedIn and StateLocatedIn is com- puted by <ref type="figure" target="#fig_2">(Figure 2)</ref>,</p><formula xml:id="formula_8">v p (π) = f (W [v r (IsBasedIn); v r (StateLocatedIn)])</formula><p>where W ∈ R d * 2d+1 is the general composition matrix.</p><p>We initialize the vector representations of the binary relations (v r ) using the representations learned in <ref type="bibr" target="#b34">Riedel et al. (2013)</ref> and do not update them during training. The relation vectors are not updated because at prediction time we would be predicting relation types which are never seen dur- ing training and hence their vectors would never get updated. We learn only the general composi- tion matrix in this model. We train a single model for a set of relation types by replacing the sigmoid function with a softmax function while computing probabilities and the parameters of the composi- tion matrix are learned using the available train- ing data containing instances of few relations. The other aspects of the model remain unchanged.</p><p>To predict facts whose relation types are unseen during training, we compute the vector represen- tation of the path using the general composition matrix and compute the probability of the fact us- ing the pre-trained relation vector. For example, using the vector representation of the path Π = Is- BasedIn → StateLocatedIn → CountryLocatedIn in <ref type="figure" target="#fig_2">Figure 2</ref>, we can predict any relation irrespec- tive of whether they are seen at training by com- paring it with the pre-trained relation vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>The hyperparameters of all the models were tuned on the same held-out development data. All the neural network models are trained for 150 itera- tions using 50 dimensional relation vectors, and we set the L2-regularizer and learning rate to 0.0001 and 0.1 respectively. We halved the learn- ing rate after every 60 iterations and use mini- batches of size 20. The neural networks and the classifiers were optimized using AdaGrad <ref type="bibr" target="#b11">(Duchi et al., 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>We ran experiments on Freebase ( <ref type="bibr" target="#b5">Bollacker et al., 2008</ref>) enriched with information from ClueWeb. We use the publicly available entity links to Free- base in the ClueWeb dataset <ref type="bibr" target="#b32">(Orr et al., 2013</ref>). Hence, we create nodes only for Freebase enti- ties in our KB graph. We remove facts containing /type/object/type as they do not give useful pre- dictive information for our task. We get triples from ClueWeb by considering sentences that con- tain two entities linked to Freebase. We extract the phrase between the two entities and treat them as the relation types. For phrases that are of length greater than four we keep only the first and last two words. This helps us to avoid the time con- suming step of dependency parsing the sentence to get the relation type. These triples are similar to facts obtained by OpenIE ( <ref type="bibr" target="#b2">Banko et al., 2007)</ref>. To reduce noise, we select relation types that occur at least 50 times. We evaluate on 46 relation types in Freebase that have the most number of instances. The methods are evaluated on a subset of facts in Freebase that were hidden during training. <ref type="table">Table  1</ref> shows important statistics of our dataset. <ref type="table" target="#tab_3">Table 2</ref> shows predictive paths for 4 relations learned by the RNN model. The high quality of unseen paths is indicative of the fact that the RNN model is able to generalize to paths that are never seen during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Predictive Paths</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>Using our dataset, we compare the performance of the following methods: PRA Classifier is the method in <ref type="bibr" target="#b21">Lao et al. (2012)</ref> which trains a logistic regression classifier by cre- ating a feature for every path type. Cluster PRA Classifier is the method in <ref type="bibr" target="#b13">Gardner et al. (2013)</ref> which replaces relation types from ClueWeb triples with their cluster membership in the KB graph before the path finding step. Af- ter this step, their method proceeds in exactly the same manner as <ref type="bibr" target="#b21">Lao et al. (2012)</ref> training a logis- tic regression classifier by creating a feature for every path type. We use pre-trained relation vec- tors from <ref type="bibr" target="#b34">Riedel et al. (2013)</ref> and use k-means clustering to cluster the relation types to 25 clus- ters as done in <ref type="bibr" target="#b13">Gardner et al. (2013)</ref>. Composition-Add uses a simple element-wise ad- dition followed by sigmoid non-linearity as the composition function similar to <ref type="bibr" target="#b46">Yang et al. (2014)</ref>. RNN-random is the supervised RNN model de- scribed in section 3 with the relation vectors ini- tialized randomly. RNN is the supervised RNN model described in section 3 with the relation vectors initialized using the method in <ref type="bibr" target="#b34">Riedel et al. (2013)</ref>. PRA Classifier-b is our simple extension to the method in <ref type="bibr" target="#b21">Lao et al. (2012)</ref> which additionally uses bigrams in the path as features. We add a special start and stop symbol to the path before computing the bigram features. Cluster PRA Classifier-b is our simple extension to the method in <ref type="bibr" target="#b13">Gardner et al. (2013)</ref> which ad- ditionally uses bigram features computed as previ- ously described. RNN + PRA Classifier combines the predictions of RNN and PRA Classifier. We combine the pre- dictions by assigning the score of a fact as the sum of their rank in the two models after sorting them in ascending order. RNN + PRA Classifier-b combines the predictions of RNN and PRA Classifier-b using the technique described previously. <ref type="table">Table 3</ref> shows the results of our experiments. The method described in <ref type="bibr" target="#b14">Gardner et al. (2014)</ref> is not included in the table since the publicly avail- able implementation does not scale to our large dataset. First, we show that it is better to train the models using all the path types instead of using only the top 1, 000 path types as done in previous work ( <ref type="bibr" target="#b13">Gardner et al., 2013;</ref><ref type="bibr" target="#b14">Gardner et al., 2014</ref>). We can see that the RNN model performs signif- icantly better than the baseline methods of <ref type="bibr" target="#b21">Lao et al. (2012)</ref> and <ref type="bibr" target="#b13">Gardner et al. (2013)</ref>. The perfor- mance of the RNN model is not affected by initial- ization since using random vectors and pre-trained vectors results in similar performance.</p><p>A surprising result is the impressive perfor- mance of our simple extension to the classifier approach. After the addition of bigram features, the naive PRA method is as effective as the Clus-  <ref type="table">Table 3</ref>: Results comparing different methods on 46 types. All the methods perform better when trained using all the paths than training using the top 1, 000 paths. When training with all the paths, RNN performs significantly (p &lt; 0.005) better than PRA Classifier and Cluster PRA Classifier. The small difference in performance between RNN and both PRA Classifier-b and Cluster PRA Classifier-b is not statistically significant. The best results are obtained by combining the predictions of RNN with PRA Classifier-b which performs significantly (p &lt; 10 −5 ) better than both PRA Classifier-b and Cluster PRA Classifier-b.</p><p>ter PRA method. The small difference in perfor- mance between RNN and both PRA Classifier-b and Cluster PRA Classifier-b is not statistically significant. We conjecture that our method has substantially different strengths than the new base- line. While the classifier with bigram features has an ability to accurately memorize important local structure, the RNN model generalizes better to un-train with top 1000 paths train with all paths Method MAP MAP RNN 43.82 50.10 zero-shot 19. <ref type="bibr">28</ref> 20.61 Random 7.59 <ref type="table">Table 4</ref>: Results comparing the zero-shot model with supervised RNN and a random baseline on 10 types. RNN is the fully supervised model de- scribed in section 3 while zero-shot is the model described in section 4. The zero-shot model with- out explicitly training for the target relation types achieves impressive results by performing signifi- cantly (p &lt; 0.05) better than a random baseline. seen paths that are very different from the paths seen is training. Empirically, combining the pre- dictions of RNN and PRA Classifier-b achieves a statistically significant gain over PRA Classifier-b. <ref type="table">Table 4</ref> shows the results of the zero-shot model described in section 4 compared with the fully su- pervised RNN model (section 3) and a baseline that produces a random ordering of the test facts. We evaluate on randomly selected 10 (out of 46) relation types, hence for the fully supervised ver- sion we train 10 RNNs, one for each relation type. For evaluating the zero-shot model, we randomly split the relations into two sets of equal size and train a zero-shot model on one set and test on the other set. So, in this case we have two RNNs making predictions on relation types that they have never seen during training. As expected, the fully supervised RNN outperforms the zero-shot model by a large margin but the zero-shot model with- out using any direct supervision clearly performs much better than a random baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Zero-shot</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Discussion</head><p>To investigate whether the performance of the RNNs were affected by multiple local optima is- sues, we combined the predictions of five different RNNs trained using all the paths. Apart from RNN and RNN-random, we trained three more RNNs with different random initialization and the perfor- mance of the three RNNs individually are 57.09, 57.11 and 56.91. The performance of the ensem- ble is 59.16 and their performance stopped im- proving after using three RNNs. So, this indicates that even though multiple local optima affects the performance, it is likely not the only issue since the performance of the ensemble is still less than the performance of RNN + PRA Classifier-b.</p><p>We suspect the RNN model does not capture some of the important local structure as well as the classifier using bigram features. To overcome this drawback, in future work, we plan to explore compositional models that have a longer memory <ref type="bibr" target="#b17">(Hochreiter and Schmidhuber, 1997;</ref>). We also plan to in- clude vector representations for the entities and develop models that address the issue of polysemy in verb phrases ( <ref type="bibr" target="#b9">Cheng et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>KB Completion includes methods such as <ref type="bibr" target="#b23">Lin and Pantel (2001)</ref>, <ref type="bibr" target="#b47">Yates and</ref><ref type="bibr" target="#b4">Berant et al. (2011)</ref> that learn inference rules of length one. <ref type="bibr" target="#b35">Schoenmackers et al. (2010)</ref> learn general inference rules by considering the set of all paths in the KB and selecting paths that sat- isfy a certain precision threshold. Their method does not scale well to modern KBs and also de- pends on carefully tuned thresholds. <ref type="bibr" target="#b20">Lao et al. (2011)</ref> train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while <ref type="bibr" target="#b13">Gardner et al. (2013)</ref> and <ref type="bibr" target="#b14">Gardner et al. (2014)</ref> extend it by using pre-trained re- lation vectors to overcome feature sparsity. Re- cently, <ref type="bibr" target="#b46">Yang et al. (2014)</ref> learn inference rules us- ing simple element-wise addition or multiplication as the composition function. Compositional Vector Space Models have been developed to represent phrases and sentences in natural language as vectors <ref type="bibr" target="#b28">(Mitchell and Lapata, 2008;</ref><ref type="bibr" target="#b3">Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b48">Yessenalina and Cardie, 2011</ref>). Neural networks have been successfully used to learn vector representa- tions of phrases using the vector representations of the words in that phrase. Recurrent neural net- works have been used for many tasks such as lan- guage modeling <ref type="bibr" target="#b24">(Mikolov et al., 2010)</ref>, machine translation ( ) and parsing ( ). Recursive neural networks, a more general version of the recurrent neural net- works have been used for many tasks like pars- ing <ref type="bibr" target="#b36">(Socher et al., 2011</ref>), sentiment classification ( <ref type="bibr" target="#b37">Socher et al., 2012;</ref><ref type="bibr" target="#b40">Socher et al., 2013c;</ref><ref type="bibr" target="#b18">Irsoy and Cardie, 2014</ref>), question answering ( <ref type="bibr" target="#b19">Iyyer et al., 2014</ref>) and natural language logical semantics <ref type="bibr" target="#b7">(Bowman et al., 2014</ref>). Our overall approach is similar to RNNs with attention ( <ref type="bibr" target="#b16">Graves, 2013</ref>) since we select a path among the set of paths connecting the entity pair to make the final prediction. Zero-shot or zero-data learning was introduced in <ref type="bibr" target="#b22">Larochelle et al. (2008)</ref> for character recogni- tion and drug discovery. <ref type="bibr" target="#b33">Palatucci et al. (2009)</ref> perform zero-shot learning for neural decoding while there has been plenty of work in this direc- tion for image recognition <ref type="bibr" target="#b39">(Socher et al., 2013b;</ref><ref type="bibr" target="#b12">Frome et al., 2013;</ref><ref type="bibr" target="#b31">Norouzi et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We develop a compositional vector space model for knowledge base completion using recurrent neural networks.</p><p>In our challeng- ing large-scale dataset available at http: //iesl.cs.umass.edu/downloads/ inferencerules/release.tar.gz, our method outperforms two baseline methods and performs competitively with a modified stronger baseline. The best results are obtained by combining the predictions of our model with the predictions of the modified baseline which achieves a 15% improvement over <ref type="bibr" target="#b13">Gardner et al. (2013)</ref>. We also show that our model has the ability to perform zero-shot inference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Semantically similar paths connecting entity pair (Microsoft, USA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Vector Representations of the paths are computed by applying the composition function recursively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Predictive paths, according to the RNN model, for 4 target relations. Two examples of seen and unseen paths are shown for each target relation.</figDesc><table>Inverse relations are marked by −1 , i.e, r(x, y) =⇒ 
</table></figure>

			<note place="foot" n="1"> we did not get significant improvements when we tried more sophisticated ordering schemes for computing the path representations.</note>

			<note place="foot" n="2"> we sub-sample a portion of the set of all unobserved instances.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Matt Gardner for releasing the PRA code, and for answering numerous question about the code and data. We also thank the Stanford NLP group for releasing the neural networks code. This work was supported in part by the Center for Intelligent Information Retrieval, in part by DARPA under agreement number FA8750-13-2-0020, in part by an award from Google, and in part by NSF grant #CNS-0958392. The U.S. Gov-ernment is authorized to reproduce and distribute reprints for Governmental purposes notwithstand-ing any copyright notation thereon. Any opinions, findings and conclusions or recommendations ex-pressed in this material are those of the authors and do not necessarily reflect those of the sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">birth/ (person &quot;x&quot; born in place &quot;y&quot;) Seen paths: &quot;was,born,in&quot; → /location/mailing address/citytown −1 → /location/mailing address/state province region</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>/location/location/contains −1 Unseen paths: &quot;born,in&quot; → /location/location/contains → &quot;near&quot; −1 &quot;was,born,in&quot; → commonly,known,as −1 Relation: /geography/river/cities/ (river &quot;x&quot; flows through or borders &quot;y&quot;) Seen paths: &quot;at&quot; → /location/location/contains −1 &quot;meets,the&quot; → /transportation/bridge/body of water spanned −1 → /location/location/contains −1 → &quot;in&quot; Unseen paths</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>References Dzmitry Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ArXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Open information extraction from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Global learning of typed entailment rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcíadurán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recursive neural networks for learning logical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Estevam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hruschka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Investigating the role of prior disambiguation in deep-learning compositional models of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Semantics workshop NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Devise: A deep visualsemantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving learning and inference in a large knowledge-base using latent syntactic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Incorporating vector space similarity in random walk inference over knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Küchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>In ArXiv</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A neural network for factoid question answering over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Claudino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in a large scale knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reading the web with learned syntactic-semantic inference rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Zero-data learning of new tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">National Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dirt-discovery of inference rules from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning longer memory in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In CoRR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with an incomplete knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="777" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics and International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Rion Snow, and Dan Jurafsky</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">11 billion clues in 800 million documents: A web research corpus annotated with freebase concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
		</author>
		<ptr target="http://googleresearch.blogspot.com/2013/07/11-billion-clues-in-800-million.html" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning first-order horn clauses from web text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schoenmackers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff Chiung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning (ICML)</title>
		<meeting>the 26th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Yago: A core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web</title>
		<meeting>the 16th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Nonlinear latent factorization by embedding multiple user interests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Yee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Recommender Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised resolution of objects and relations on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Compositional matrix-space models for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainur</forename><surname>Yessenalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
