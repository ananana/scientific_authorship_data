<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Academic Paper Rating Based on Modularized Hierarchical Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Deep Learning Lab</orgName>
								<orgName type="institution" key="instit1">Beijing Institute of Big Data Research</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Deep Learning Lab</orgName>
								<orgName type="institution" key="instit1">Beijing Institute of Big Data Research</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Academic Paper Rating Based on Modularized Hierarchical Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="496" to="502"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>As more and more academic papers are being submitted to conferences and journals , evaluating all these papers by professionals is time-consuming and can cause inequality due to the personal factors of the reviewers. In this paper, in order to assist professionals in evaluating academic papers, we propose a novel task: automatic academic paper rating (AAPR), which automatically determine whether to accept academic papers. We build a new dataset for this task and propose a novel modularized hierarchical convolu-tional neural network to achieve automatic academic paper rating. Evaluation results show that the proposed model outperforms the baselines by a large margin. The dataset and code are available at https: //github.com/lancopku/AAPR</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Every year there are thousands of academic pa- pers submitted to conferences and journals. Rating all these papers can be exhausting, and sometimes rating scores can be affected by the personal fac- tors of the reviewers, leading to inequality prob- lem. Therefore, there is a great need for rating academic papers automatically. In this paper, we explore how to automatically rate the academic papers based on their L A T E X source file and meta information, which we call the task of automatic academic paper rating (AAPR).</p><p>A task that is similar to the AAPR is automatic essay scoring (AES). AES has been studied for a long time. Project Essay Grade <ref type="bibr" target="#b15">(Page, 1967</ref><ref type="bibr" target="#b16">(Page, , 1968</ref>) is one of the earliest attempts to solve the AES task by predicting the score using linear regression over expert crafted textual features. Much of the fol- lowing work applied similar methods by using var- ious classifiers with more sophisticated features including grammar, vocabulary and style <ref type="bibr" target="#b19">(Rudner and Liang, 2002;</ref><ref type="bibr" target="#b1">Attali and Burstein, 2004</ref>). These traditional methods can work almost as well as human raters. However, they all demand a large amount of feature engineering, which requires a lot of expertise.</p><p>Recent studies turn to use deep neural networks, claiming that deep learning models can relieve the system from heavy feature engineering. <ref type="bibr" target="#b0">Alikaniotis et al. (2016)</ref> proposed to use long short term memory network <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997</ref>) with a linear regression output layer to pre- dict the score. They added a score prediction loss to the original C&amp;W embedding <ref type="bibr" target="#b4">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b5">Collobert et al., 2011)</ref>, so that the word embeddings are related to the quality of the essay. <ref type="bibr" target="#b22">Taghipour and Ng (2016)</ref> also applied re- current neural networks to process the essay, ex- cept that they put a convolutional layer ahead of the recurrent layer to extract local features. <ref type="bibr" target="#b6">Dong and Zhang (2016)</ref> proposed to apply a two-layer convolutional neural network (CNN) to model the essay. The first layer is responsible for encoding the sentence and the second layer is to encode the whole essay.  further proposed to add attention mechanism to the pooling layer to automatically decide which part is more important in determining the quality of the essay.</p><p>Although there has been a lot of work dealing with AES task, researchers have not attempted the AAPR task. Different from the essay in language capability tests, academic papers are much longer with much more information, and the overall qual- ity is affected by a variety of factors besides the writing. Therefore, we propose a model that con- siders the overall information of one academic pa- per, including the title, authors, abstract and the main content of the L A T E X source file of the paper.</p><p>Our main contributions are listed as follows:</p><p>• We propose the task of automatically rating academic papers and build a new dataset for this task.</p><p>• We propose a modularized hierarchical con- volutional neural network model that consid- ers the overall information of the source pa- per. Experimental results show that the pro- posed method outperforms the baselines by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head><p>A source paper usually consists of several mod- ules, such as abstract 1 , title and so on. There is also a hierarchical structure from word-level to sentence-level in each module. The structure in- formation is likely to be helpful to make more ac- curate predictions. Besides, the model can be im- proved by considering the difference in contribu- tions of various parts of the source paper. Based on this observation, we propose a modularized hierar- chical CNN. An overview of our model is shown in <ref type="figure" target="#fig_1">Figure 1</ref>. We assume that a source paper has l modules, with m words and the filter size is h (de- tailed explanations can be referred to Section 2.1 and Section 2.2). l, m and h are set to be 3, 3, 2, respectively in the <ref type="figure" target="#fig_1">Figure 1</ref> for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Modularized Hierarchical CNN</head><p>Given a complete source paper r, represented by a sequence of tokens, we first divide it into sev- eral modules (r 1 , r 2 , · · · , r l ) based on the gen- eral structure of the source paper (abstract, title, authors, introduction, related work, methods and conclusion). For each module, the one-hot rep- resentation of the i-th word w i is embedded to a dense vector x i through an embedding matrix. For the following modules (abstract, introduction, related work, methods, conclusion), we use the attention-based CNN (illustrated in Section 2.2) in word-level to get the representation s i of the i-th sentence. Another attention-based CNN layer is applied to encode the sentence-level representa- tions into the representation m i of the i-th mod- ule.</p><p>There is only one sentence in the title of the source paper, so it is reasonable to get the module- level representation of title only using attention- based CNN in word-level. Besides, the weighted 1 Italicized words represent modules of the source paper.  average method is applied to obtain the module- level representation of authors by Equation <ref type="formula">(1)</ref> be- cause the authors are independent of each other.</p><formula xml:id="formula_0">m authors = A i=1 γ i a i (1)</formula><p>where γ = (γ 1 , . . . , γ A ) T is the weight parame- ter. a i is the embedding vector of the i-th author in the source paper, which is randomly initialized and can be learned at the training stage. A is the maximum length of the author sequence.</p><p>Representations m 1 , m 2 , · · · , m l of all mod- ules are aggregated to form the paper-level repre- sentation d of the source paper with an attentive pooling layer. A sof tmax layer is used to take d as input and predict the probability of being ac- cepted. At the training stage, the cross entropy loss function is optimized as objective function, which is widely used in various classification tasks.</p><formula xml:id="formula_1">ˆ y = sof tmax(W d d + b d ) (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Details of Attention-Based CNN</head><p>Attention-based CNN consists of a convolution layer and an attentive pooling layer. The convolu- tion layer is used to capture local features and at- tentive pooling layer can automatically decide the relative weights of words, sentences, and modules. Convolution layer: A sequence of vectors of length m is represented as the row con- catenation of m k-dimensional vectors:</p><formula xml:id="formula_2">X = [x 1 ; x 2 ; · · · ; x m ]. A filter W x ∈ R h×k convolves</formula><p>with the window vectors at each position to gener- ate a feature map c ∈ R m−h+1 . Each element c j of the feature map is calculated as follows:</p><formula xml:id="formula_3">c j = f (W x • [x j : x j+h−1 ] + b x )<label>(3)</label></formula><p>where • is element-wise multiplication, b x ∈ R is a bias term, and f is a non-linear activation func- tion. Here we choose f to be ReLU ( <ref type="bibr" target="#b14">Nair and Hinton, 2010)</ref>. n different filters can be used to extract multiple feature maps c 1 , c 2 , · · · , c n . We get new feature representations C ∈ R (m−h+1)×n as the column concatenation of feature maps</p><formula xml:id="formula_4">C = [c 1 , c 2 , · · · , c n ].</formula><p>The i-th row c (i) of C is the new feature representation generated at position i. Attentive pooling layer: Given a sequence c <ref type="bibr">(1)</ref> , c <ref type="bibr">(2)</ref> , · · · , c (q) , which are q n-dimensional vectors, the attentive pooling is applied to aggre- gate the representations of the sequence by mea- suring the contribution of each vector to form the high-level representation s of the whole sequence. Formally, we have</p><formula xml:id="formula_5">z i = tanh(W c c (i) + b c )<label>(4)</label></formula><formula xml:id="formula_6">α i = z T i u w k exp(z T k u w )<label>(5)</label></formula><formula xml:id="formula_7">s = i α i z i<label>(6)</label></formula><p>where W c and b c are weight matrix and bias vec- tor, respectively. u w is a randomly initialized vec- tor, which can be learned at the training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we evaluate our model on the dataset we build for this task. We first introduce the dataset, evaluation metric, and experimental details. Then, we compare our model with base- lines. Finally, we provide the analysis and the dis- cussion of experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>Arxiv Academic Paper Dataset: As there is no existing dataset that can be used directly, we cre- ate a dataset by collecting data on academic pa- pers in the field of artificial intelligence from the website 2 . The dataset consists of 19,218 academic papers. The information of each source paper con- sists of the venue which marks whether the paper is accepted, and the source L A T E X file. We divide the dataset into training, validation, and test parts. The details are shown in <ref type="table">Table 1.</ref> 2 https://arxiv.org/ <ref type="table">Table 1</ref>: Statistical information of Arxiv aca- demic paper dataset. Positive and Negative de- note whether the source paper is accepted.</p><note type="other">Dataset #Total #Positive #Negative Training set 17,218 8,889 8,329 Validation set 1,000 507 493 Test set 1,000 504 496</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Details</head><p>We use accuracy as our evaluation metric instead of the F-score, precision, and recall because the positive and negative examples in our dataset are well balanced. Since the author names are different from the common scientific words in the paper, we sep- arately build up vocabulary for authors and text words of source papers with the size of 20,000 and 50,000, respectively.</p><p>We use the training strategies mentioned in <ref type="bibr" target="#b26">Zhang and Wallace (2015)</ref> for CNN classifier to tune the hyper-parameters based on the accuracy on the validation set. The word or author embed- ding is randomly initialized and can be learned during training. The size of word embedding or author embedding is 128 and the batch size is 32. Adam optimizer ( <ref type="bibr" target="#b10">Kingma and Ba, 2014</ref>) is used to minimize cross entropy loss function. We ap- ply dropout regularization ( <ref type="bibr" target="#b21">Srivastava et al., 2014</ref>) to avoid overfitting and clip the gradients ( <ref type="bibr" target="#b17">Pascanu et al., 2013</ref>) to the maximum norm of 5.0.</p><p>During training, we train the model for a fixed number of epochs and monitor its performance on the validation set after every 50 updates. Once training is finished, we select the model with the highest accuracy on the validation set as our final model and evaluate its performance on the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines</head><p>We compare our model with the following base- lines:</p><p>• Randomly predict (RP): We randomly de- cide whether the source paper can be ac- cepted. In other words, the probability of ac- ceptance of every source paper is always 0.5 using this strategy.</p><p>•  <ref type="table">Table 2</ref>: Comparison between our proposed model and the baselines on the test set. Our proposed model is denoted as MHCNN.</p><p>to predict the labels based on the tf-idf fea- tures of the text.</p><p>• Neural networks models: We apply three representative neural network models: CNN <ref type="bibr" target="#b9">(Kim, 2014)</ref>, LSTM <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997)</ref>, and C-LSTM ( <ref type="bibr" target="#b27">Zhou et al., 2015)</ref>. We concatenate all modules of the source paper into a long text sequence as the input to the neural network models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>In this subsection, we present the results of evalu- ation by comparing our proposed method with the baselines. <ref type="table">Table 2</ref> reports experimental results of various models. As is shown in <ref type="table">Table 2</ref>, the pro- posed MHCNN outperforms all the above men- tioned baselines. The best baseline model SVM achieves the accuracy of 61.6%, while the pro- posed model achieves the accuracy of 67.7%. In addition, our MHCNN outperforms other repre- sentative deep-learning models by a large margin. For instance, the proposed MHCNN achieves an improvement of 6.4% accuracy over the traditional CNN. This shows that our MHCNN can learn bet- ter representation by considering modularized hi- erarchical structure in the source paper. Our pro- posed MHCNN aims to divide a long text into sev- eral modules and using attention mechanism to aggregate the representations of each module to form a final high-level representation of a com- plete source paper. By incorporating knowledge of the structure of the source paper and automatically selecting the most informative words, the model is capable of making more accurate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis and Discussions</head><p>Here we perform further analysis on the model and experiment results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Accuracy Decline MHCNN 67.7% −− w/o Attention 66.8%* ↓0.9% w/o Module 61.3%* ↓6.4% <ref type="table">Table 3</ref>: Ablation Study. The symbol * indicates that the difference compared to MHCNN is signif- icant with p ≤ 0.05 under t-test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Exploration on Internal Structure of the Model</head><p>As is shown in <ref type="table">Table 2</ref>, our MHCNN model out- performs all baselines by a large margin. Com- pared with the basic CNN model, the proposed model has a modularized hierarchical structure and uses multiple attention mechanisms. In order to explore the impact of internal structure of the model, we remove the modularized hierarchical structure and attention mechanisms in turn. The performance is shown in <ref type="table">Table 3</ref>. "w/o Attention" means that we still use modularized hierarchical structure while do not use any attention mecha- nism. "w/o Module" means that we do not use both attention mechanism and modularized hier- archical structure, which is the same as the CNN model in the baselines. As is shown in <ref type="table">Table 3</ref>, the accuracy of the model drops by 0.9% when the attention mecha- nism is removed from the model. This shows that there are differences in the contribution of textual content. For instance, the abstract of a source paper is more important than its title. Attention mechanism can automatically decide the relative weights of modules, which makes model predic- tions more accurate. However, the accuracy of the model drops by 6.4% when we remove the modu- larized hierarchical structure, which is much larger than 0.9%. It shows that the modularized hierar- chical structure of the model is of great help to ob- tain better representations by incorporating knowl- edge of the structure of the source paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Impact of Modules of the Source Paper</head><p>One interesting issue is which part of the source paper best determines whether it can be accepted.</p><p>To explore this issue, we subtract each module from complete source papers in turn and observe the change in the performance of the model. The experimental result is shown in <ref type="table" target="#tab_1">Table 4</ref>. As is shown in <ref type="table" target="#tab_1">Table 4</ref>, the performance of the model shows different degrees of decline when we remove different modules of the source paper. This shows that there are differences in the con- tribution of different modules of the source paper to its acceptance, which further illustrates the rea- sonableness of our use of modularized hierarchi- cal structure and attention mechanism. All the de- clines are significant with p ≤ 0.05 under the t- test.</p><formula xml:id="formula_8">Contexts Accuracy Decline Full data 67.7% −− w/o Title 66.6%* ↓1.1% w/o Abstract 65.5%* ↓2.2% w/o Authors 64.6%* ↓3.1% w/o Introduction 65.7%* ↓2.0% w/o Related work 66.0%* ↓1.7% w/o Methods 66.2%* ↓1.5% w/o Conclusion 65.0%* ↓2.7%</formula><p>When we remove authors module, the accuracy drops by 3.1%, which is the largest decline. This shows that the authors of the source paper largely determines whether it can be accepted. Obviously, a source paper written by a proficient scholar tends to be good work, which has a higher probability of being accepted. Except for authors, the two most significant modules affecting the probabil- ity of being accepted are conclusions and abstract. Because they are the essence of the entire source paper, which can directly reflect the quality of the source paper. However, the methods module of the source paper has little effect on the probability of being accepted according to <ref type="table" target="#tab_1">Table 4</ref>. The rea- son may be that the methods of different source papers vary widely, which means that there ex- ists high variance in this module. Therefore, our model may not do well in capturing a unified inter- nal pattern to make prediction. The impact of the title is the smallest and the accuracy of the model drops by only 1.1% when title is removed from the source paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>The most relevant task for our work is automatic essay scoring (AES). There are two main types of methods for the AES task: traditional machine learning algorithms and neural network models.</p><p>Most traditional methods for the AES task use supervised learning algorithms, including classi- fication <ref type="bibr" target="#b11">(Larkey, 1998;</ref><ref type="bibr" target="#b19">Rudner and Liang, 2002;</ref><ref type="bibr" target="#b24">Yannakoudakis et al., 2011;</ref><ref type="bibr" target="#b3">Chen and He, 2013)</ref>, regression ( <ref type="bibr" target="#b1">Attali and Burstein, 2004;</ref><ref type="bibr" target="#b18">Phandi et al., 2015;</ref><ref type="bibr" target="#b25">Zesch et al., 2015)</ref> and so on. How- ever, they all require lots of manual features, for instance, bag of words, spelling errors, or lengths, which can be time-consuming and requires a large amount of expertise.</p><p>In recent years, some neural network models have also been used for the AES task, which have achieved great success. <ref type="bibr" target="#b0">Alikaniotis et al. (2016)</ref> proposed to use the LSTM model with a linear regression output layer to predict the score. Taghipour and Ng (2016) applied the CNN model followed by a recurrent layer to extract local fea- tures and model sequence dependencies. A two- layer CNN model was proposed by <ref type="bibr" target="#b6">Dong and Zhang (2016)</ref> to cover more high-level and ab- stract information.  further pro- posed to add attention mechanism to the pooling layer to automatically decide which part is more important in determining the quality of the essay. <ref type="bibr" target="#b20">Song et al. (2017)</ref> proposed a multi-label neural sequence labeling approach for discourse mode identification and showed that features extracted by this method can further improve the AES task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we propose the task of automatic academic paper rating (AAPR), which aims to au- tomatically determine whether to accept academic papers. We propose a novel modularized hierar- chical CNN for this task to make use of the struc- ture of a source paper. Experimental results show that the proposed model outperforms various base- lines by a large margin. In addition, we find that the conclusion and abstract parts have the most influence on whether the source paper can be ac- cepted when setting aside the factor of authors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>ACNN ACNN Softmax Layer ACNN AP (a) Modularized hierarchical convolutional neural network. Convolution Attentive Pooling (b) Attention-based convolutional neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overview of our model. ACNN denotes attention-based CNN, whose basic structure is shown in (b). AP denotes attentive pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Ablation Study. The symbol * indicates 
that the difference compared to full data is signifi-
cant with p ≤ 0.05 under t-test. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>This work is supported in part by National Natu-ral Science Foundation of China <ref type="figure">(</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Alikaniotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04289</idno>
		<title level="m">Automatic text scoring using neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated essay scoring with e-rater v. 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yigal</forename><surname>Attali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETS Research Report Series</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling scientific influence for research trending topic prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automated essay scoring by maximizing human-machine agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1741" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic features for essay scoring-an empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1072" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attentionbased recurrent convolutional neural network for automatic essay scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic essay grading using text categorization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leah S Larkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cascading multiway attentions for document-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="634" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A unified graph model for personalized query-oriented reference paper recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanqi</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexian</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1509" to="1512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Grading essays by computer: Progress report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis B Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the invitational Conference on Testing Problems</title>
		<meeting>the invitational Conference on Testing Problems</meeting>
		<imprint>
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The use of the computer in analyzing student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis B Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International review of education</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="225" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flexible domain adaptation for automated essay scoring using correlated linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Phandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kian</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="431" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automated essay scoring using bayes&apos; theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahung</forename><surname>Rudner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Technology, Learning and Assessment</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discourse mode identification in essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiji</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="112" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A neural approach to automated essay scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Taghipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1882" to="1891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05804</idno>
		<title level="m">Minimal effort back propagation for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A new dataset and method for automatically grading esol texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Task-independent features for automated essay grading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wojatzki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Scholtenakoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Tenth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="224" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A sensitivity analysis of (and practitioners&apos; guide to) convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<idno>abs/1510.03820</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A C-LSTM neural network for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chonglin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">C M</forename><surname>Lau</surname></persName>
		</author>
		<idno>abs/1511.08630</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
