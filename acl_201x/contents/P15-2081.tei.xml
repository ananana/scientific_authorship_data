<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Fixed-Size Ordinally-Forgetting Encoding Method for Neural Network Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei, Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<addrLine>4700 Keele Street</addrLine>
									<postCode>M3J 1P3</postCode>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbin</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<addrLine>4700 Keele Street</addrLine>
									<postCode>M3J 1P3</postCode>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei, Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Dai</surname></persName>
							<email>lrdai@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei, Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Fixed-Size Ordinally-Forgetting Encoding Method for Neural Network Language Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="495" to="500"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose the new fixed-size ordinally-forgetting encoding (FOFE) method, which can almost uniquely encode any variable-length sequence of words into a fixed-size representation. FOFE can model the word order in a sequence using a simple ordinally-forgetting mechanism according to the positions of words. In this work, we have applied FOFE to feedforward neural network language models (FNN-LMs). Experimental results have shown that without using any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform not only the standard fixed-input FNN-LMs but also the popular recurrent neural network (RNN) LMs.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language models play an important role in many applications like speech recognition, machine translation, information retrieval and nature lan- guage understanding. Traditionally, the back-off n-gram models <ref type="bibr" target="#b0">(Katz, 1987;</ref><ref type="bibr" target="#b1">Kneser, 1995)</ref> are the standard approach to language modeling. Re- cently, neural networks have been successfully ap- plied to language modeling, yielding the state- of-the-art performance in many tasks. In neural network language models (NNLM), the feedfor- ward neural networks (FNN) and recurrent neu- ral networks (RNN) <ref type="bibr" target="#b6">(Elman, 1990</ref>) are two pop- ular architectures. The basic idea of NNLMs is to use a projection layer to project discrete words into a continuous space and estimate word con- ditional probabilities in this space, which may be smoother to better generalize to unseen contexts. FNN language models (FNN-LM) <ref type="bibr" target="#b4">(Bengio and Ducharme, 2001;</ref><ref type="bibr" target="#b5">Bengio, 2003)</ref> usually use a lim- ited history within a fixed-size context window to predict the next word. RNN language mod- els (RNN-LM) <ref type="bibr" target="#b7">(Mikolov, 2010;</ref><ref type="bibr" target="#b9">Mikolov, 2012</ref>) adopt a time-delayed recursive architecture for the hidden layers to memorize the long-term depen- dency in language. Therefore, it is widely re- ported that RNN-LMs usually outperform FNN- LMs in language modeling. While RNNs are the- oretically powerful, the learning of RNNs needs to use the so-called back-propagation through time (BPTT) <ref type="bibr" target="#b2">(Werbos, 1990)</ref> due to the internal recur- rent feedback cycles. The BPTT significantly in- creases the computational complexity of the learn- ing algorithms and it may cause many problems in learning, such as gradient vanishing and ex- ploding <ref type="bibr" target="#b3">(Bengio, 1994)</ref>. More recently, some new architectures have been proposed to solve these problems. For example, the long short term memory (LSTM) RNN <ref type="bibr" target="#b13">(Hochreiter, 1997</ref>) is an enhanced architecture to implement the recur- rent feedbacks using various learnable gates, and it has obtained promising results on handwriting recognition <ref type="bibr" target="#b14">(Graves, 2009)</ref> and sequence model- ing <ref type="bibr" target="#b15">(Graves, 2013)</ref>.</p><p>Comparing with RNN-LMs, FNN-LMs can be learned in a simpler and more efficient way. How- ever, FNN-LMs can not model the long-term de- pendency in language due to the fixed-size input window. In this paper, we propose a novel encod- ing method for discrete sequences, named fixed- size ordinally-forgetting encoding (FOFE), which can almost uniquely encode any variable-length word sequence into a fixed-size code. Relying on a constant forgetting factor, FOFE can model the word order in a sequence based on a sim- ple ordinally-forgetting mechanism, which uses the position of each word in the sequence. Both the theoretical analysis and the experimental sim- ulation have shown that FOFE can provide al- most unique codes for variable-length word se- quences as long as the forgetting factor is prop- erly selected. In this work, we apply FOFE to neural network language models, where the fixed- size FOFE codes are fed to FNNs as input to predict next word, enabling FNN-LMs to model long-term dependency in language. Experiments on two benchmark tasks, Penn Treebank Corpus (PTB) and Large Text Compression Benchmark (LTCB), have shown that FOFE-based FNN-LMs can not only significantly outperform the stan- dard fixed-input FNN-LMs but also achieve better performance than the popular RNN-LMs with or without using LSTM. Moreover, our implementa- tion also shows that FOFE based FNN-LMs can be learned very efficiently on GPUs without the complex BPTT procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach: FOFE</head><p>Assume vocabulary size is K, NNLMs adopt the 1-of-K encoding vectors as input. In this case, each word in vocabulary is represented as a one- hot vector e âˆˆ R K . The 1-of-K representation is a context independent encoding method. When the 1-of-K representation is used to model a word in a sequence, it can not model its history or context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Fixed-size Ordinally Forgetting Encoding</head><p>We propose a simple context-dependent encoding method for any sequence consisting of discrete symbols, namely fixed-size ordinally-forgetting encoding (FOFE). Given a sequence of words (or any discrete symbols), S = {w 1 , w 2 , Â· Â· Â· , w T }, each word w t is first represented by a 1-of-K rep- resentation e t , from the first word t = 1 to the end of the sequence t = T , FOFE encodes each par- tial sequence (history) based on a simple recursive formula (with z 0 = 0) as:</p><formula xml:id="formula_0">z t = Î± Â· z tâˆ’1 + e t (1 â‰¤ t â‰¤ T )<label>(1)</label></formula><p>where z t denotes the FOFE code for the partial sequence up to w t , and Î± (0 &lt; Î± &lt; 1) is a con- stant forgetting factor to control the influence of the history on the current position. Let's take a simple example here, assume we have three sym- bols in vocabulary, e.g., A, B, C, whose 1-of- Obviously, FOFE can encode any variable- length discrete sequence into a fixed-size code. Moreover, it is a recursive context dependent en- coding method that smartly models the order in- formation by various powers of the forgetting fac- tor. Furthermore, FOFE has an appealing property in modeling natural languages that the far-away context will be gradually forgotten due to Î± &lt; 1 and the nearby contexts play much larger role in the resultant FOFE codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Uniqueness of FOFE codes</head><p>Given the vocabulary (of K symbols), for any se- quence S with a length of T , based on the FOFE code z T computed as above, if we can always de- code the original sequence S unambiguously (per- fectly recovering S from z T ), we say FOFE is unique.</p><p>Theorem 1 If the forgetting factor Î± satisfies 0 &lt; Î± â‰¤ 0.5, FOFE is unique for any K and T .</p><p>The proof is simple because if the FOFE code has a value Î± t in its i-th element, we may de- termine the word w i occurs in the position t of S without ambiguity since no matter how many times w i occurs in the far-away contexts (&lt; t), they do not sum to Î± t (due to Î± â‰¤ 0.5). If w i ap- pears in any closer context (&gt; t), the i-th element must be larger than Î± t .</p><p>Theorem 2 For 0.5 &lt; Î± &lt; 1, given any finite values of K and T , FOFE is almost unique every- where for Î± âˆˆ (0.5, 1.0), except only a finite set of countable choices of Î±.</p><p>Refer to <ref type="bibr">(Zhang et. al., 2015a</ref>) for the complete proof. Based on Theorem 2, FOFE is unique al- most everywhere between (0.5, 1.0) only except a countable set of isolated choices of Î±. In practice, the chance to exactly choose these isolated values between (0.5, 1.0) is extremely slim, realistically almost impossible due to quantization errors in the system. To verify this, we have run simulation ex- periments for all possible sequences up to T = 20 symbols to count the number of collisions. Each collision is defined as the maximum element-wise difference between two FOFE codes (generated from two different sequences) is less than a small threshold . In <ref type="figure" target="#fig_2">Figure 2</ref>, we have shown the num- ber of collisions (out of the total 2 20 tested cases) for various Î± values when = 0.01, 0.001 and 0.0001. <ref type="bibr">1</ref> The simulation experiments have shown that the chance of collision is extremely small even when we allow a word to appear any times in the context. Obviously, in a natural language, a word normally does not appear repeatedly within a near context. Moreover, we have run the simulation to examine whether collisions actually occur in two real text corpora, namely PTB (1M words) and LTCB (160M words), using = 0.01, we have not observed a single collision for nine different Î± values between [0.55, 1.0] (incremental 0.05).  efficient for the parallel computation platform like GPUs. Here, we will show that the FOFE compu- tation can be efficiently implemented as sentence- by-sentence matrix multiplications, which are suitable for the mini-batch based stochastic gra- dient descent (SGD) method running on GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Implement FOFE for FNN-LMs</head><p>Given a sentence, S = {w 1 , w 2 , Â· Â· Â· , w T }, where each word is represented by a 1-of-K code as e t (1 â‰¤ t â‰¤ T ). The FOFE codes for all par- tial sequences in S can be computed based on the following matrix multiplication:</p><formula xml:id="formula_1">S = ï£® ï£¯ ï£¯ ï£¯ ï£¯ ï£¯ ï£¯ ï£¯ ï£° 1 Î± 1 Î± 2 Î± 1 . . . . . . 1 Î± T âˆ’1 Â· Â· Â· Î± 1 ï£¹ ï£º ï£º ï£º ï£º ï£º ï£º ï£º ï£» ï£® ï£¯ ï£¯ ï£¯ ï£¯ ï£¯ ï£¯ ï£¯ ï£° e 1 e 2 e 3 . . . e T ï£¹ ï£º ï£º ï£º ï£º ï£º ï£º ï£º ï£» = MV</formula><p>where V is a matrix arranging all 1-of-K codes of the words in the sentence row by row, and M is a T -th order lower triangular matrix. Each row vector of S represents a FOFE code of the partial sequence up to each position in the sentence. This matrix formulation can be easily extended to a mini-batch consisting of several sentences. Assume that a mini-batch is composed of N se- quences, L = {S 1 S 2 Â· Â· Â· S N }, we can compute the FOFE codes for all sentences in the mini-batch as follows:</p><formula xml:id="formula_2">Â¯ S = ï£® ï£¯ ï£¯ ï£¯ ï£¯ ï£° M 1 M 2 . . . M N ï£¹ ï£º ï£º ï£º ï£º ï£» ï£® ï£¯ ï£¯ ï£¯ ï£¯ ï£° V 1 V 2 . . . V N ï£¹ ï£º ï£º ï£º ï£º ï£» = Â¯ M Â¯ V.</formula><p>When feeding the FOFE codes to FNN as shown in <ref type="figure" target="#fig_1">Figure 1</ref>, we can compute the activation signals (assume f is the activation function) in the first hidden layer for all histories in S as follows:</p><formula xml:id="formula_3">H = f ( Â¯ M Â¯ V)UW+b = f Â¯ M( Â¯ VU)W+b</formula><p>where U denotes the word embedding matrix that projects the word indices onto a continuous low- dimensional continuous space. As above, Â¯ VU can be done efficiently by looking up the embed- ding matrix. Therefore, for the computational ef- ficiency purpose, we may apply FOFE to the word embedding vectors instead of the original high- dimensional one-hot vectors. In the backward pass, we can calculate the gradients with the stan- dard back-propagation (BP) algorithm rather than BPTT. As a result, FOFE based FNN-LMs are the same as the standard FNN-LMs in terms of com- putational complexity in training, which is much more efficient than RNN-LMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We have evaluated the FOFE method for NNLMs on two benchmark tasks: i) the Penn Treebank (PTB) corpus of about 1M words, following the same setup as <ref type="bibr" target="#b8">(Mikolov, 2011</ref>). The vocabu- lary size is limited to 10k. The preprocess- ing method and the way to split data into train- ing/validation/test sets are the same as <ref type="bibr" target="#b8">(Mikolov, 2011)</ref>. ii) The Large Text Compression Bench- mark (LTCB) <ref type="bibr" target="#b17">(Mahoney, 2011)</ref>. In LTCB, we use the enwik9 dataset, which is composed of the first 10 9 bytes of enwiki-20060303-pages-articles.xml. We split it into three parts: training (153M), val- idation (8.9M) and test (8.9M) sets. We limit the vocabulary size to 80k for LTCB and replace all out-of-vocabulary words by &lt;UNK&gt;. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental results on PTB</head><p>We have first evaluated the performance of the traditional FNN-LMs, taking the previous several words as input, denoted as n-gram FNN-LMs here. We have trained neural networks with a linear pro- jection layer (of 200 hidden nodes) and two hid- den layers (of 400 nodes per layer). All hidden units in networks use the rectified linear activation function, i.e., f (x) = max(0, x). The nets are initialized based on the normalized initialization  in <ref type="bibr" target="#b16">(Glorot, 2010)</ref>, without using any pre-training. We use SGD with a mini-batch size of 200 and an initial learning rate of 0.4. The learning rate is kept fixed as long as the perplexity on the validation set decreases by at least 1. After that, we continue six more epochs of training, where the learning rate is halved after each epoch. The performance (in perplexity) of several n-gram FNN-LMs (from bi- gram to 6-gram) is shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>For the FOFE-FNNLMs, the net architecture and the parameter setting are the same as above. The mini-batch size is also 200 and each mini- batch is composed of several sentences up to 200 words (the last sentence may be truncated). All sentences in the corpus are randomly shuffled at the beginning of each epoch. In this experiment, we first investigate how the forgetting factor Î± may affect the performance of LMs. We have trained two FOFE-FNNLMs: i) 1st-order (using z t as input to FNN for each time t; ii) 2nd-order (using both z t and z tâˆ’1 as input for each time t, with a forgetting factor varying between [0.0, 1.0]. Experimental results in <ref type="figure" target="#fig_6">Figure 4</ref> have shown that a good choice of Î± lies between [0.5, 0.8]. Us- ing a too large or too small forgetting factor will hurt the performance. A too small forgetting fac- tor may limit the memory of the encoding while a too large Î± may confuse LM with a far-away his- tory. In the following experiments, we set Î± = 0.7 for the rest experiments in this paper.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we have summarized the perplexi- ties on the PTB test set for various models. The proposed FOFE-FNNLMs can significantly out- perform the baseline FNN-LMs using the same architecture. For example, the perplexity of the baseline bigram FNNLM is 176, while the FOFE-  FNNLM can improve to 116. Moreover, the FOFE-FNNLMs can even overtake a well-trained RNNLM (400 hidden units) in <ref type="bibr" target="#b8">(Mikolov, 2011)</ref> and an LSTM in <ref type="bibr" target="#b15">(Graves, 2013)</ref>. It indicates FOFE-FNNLMs can effectively model the long- term dependency in language without using any recurrent feedback. At last, the 2nd-order FOFE- FNNLM can provide further improvement, yield- ing the perplexity of 108 on PTB. It also outper- forms all higher-order FNN-LMs (4-gram, 5-gram and 6-gram), which are bigger in model size. To our knowledge, this is one of the best reported re- sults on PTB without model combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental results on LTCB</head><p>We have further examined the FOFE based FNN- LMs on a much larger text corpus, i.e. LTCB, which contains articles from Wikipedia. We have trained several baseline systems: i) two n-gram LMs (3-gram and 5-gram) using the modified Kneser-Ney smoothing without count cutoffs; ii) several traditional FNN-LMs with different model sizes and input context windows (bigram, trigram, 4-gram and 5-gram); iii) an RNN-LM with one hidden layer of 600 nodes using the toolkit in <ref type="bibr" target="#b7">(Mikolov, 2010)</ref>, in which we have further used a spliced sentence bunch in ( <ref type="bibr" target="#b10">Chen et al. 2014</ref>) to speed up the training on GPUs. Moreover, we have examined four FOFE based FNN-LMs with various model sizes and input window sizes (two 1st-order FOFE models and two 2nd-order ones). For all NNLMs, we have used an output layer of the full vocabulary (80k words). In these exper- iments, we have used an initial learning rate of 0.01, and a bigger mini-batch of 500 for FNN- LMMs and of 256 sentences for the RNN and FOFE models. Experimental results in <ref type="table" target="#tab_1">Table 2</ref> have shown that the FOFE-based FNN-LMs can significantly outperform the baseline FNN-LMs (including some larger higher-order models) and also slightly overtake the popular RNN-based LM, yielding the best result (perplexity of 107) on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we propose the fixed-size ordinally- forgetting encoding (FOFE) method to almost uniquely encode any variable-length sequence into a fixed-size code. In this work, FOFE has been successfully applied to neural network language modeling. Next, FOFE may be combined with neural networks ( <ref type="bibr" target="#b21">Zhang et. al., 2015b</ref>) for other NLP tasks, such as sen- tence modeling/matching, paraphrase detection, machine translation, question and answer and etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>In this case, the FOFE code for the se- quence {ABC} is [Î± 2 , Î±, 1], and that of {ABCBC} is [Î± 4 , Î± + Î± 3 , 1 + Î± 2 ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The FOFE-based FNN language model.</figDesc><graphic url="image-1.png" coords="2,329.10,62.81,174.61,171.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Numbers of collisions in simulation.</figDesc><graphic url="image-2.png" coords="3,88.37,62.81,185.53,139.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The architecture of a FOFE based neural network language model (FOFE-FNNLM) is shown in Fig- ure 1. It is similar to regular bigram FNN-LMs ex- cept that it uses a FOFE code to feed into neural network LM at each time. Moreover, the FOFE can be easily scaled to higher orders like n-gram NNLMs. For example, Figure 3 is an illustration of a second order FOFE-based neural network lan- guage model. FOFE is a simple recursive encoding method but a direct sequential implementation may not be 1 When we use a bigger value for Î±, the magnitudes of the resultant FOFE codes become much larger. As a result, the number of collisions (as measured by a fixed absolute thresh- old ) becomes smaller.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Diagram of 2nd-order FOFE FNN-LM.</figDesc><graphic url="image-3.png" coords="3,329.10,62.81,174.62,169.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2</head><label></label><figDesc>Matlab codes are available at https://wiki.eecs. yorku.ca/lab/MLL/projects:fofe:start for readers to reproduce all results reported in this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Perplexities of FOFE FNNLMs as a function of the forgetting factor.</figDesc><graphic url="image-4.png" coords="4,323.65,62.81,185.53,150.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Perplexities on PTB for various LMs. 
Model 
Test PPL 
KN 5-gram (Mikolov, 2011) 
141 
FNNLM (Mikolov, 2012) 
140 
RNNLM (Mikolov, 2011) 
123 
LSTM (Graves, 2013) 
117 
bigram FNNLM 
176 
trigram FNNLM 
131 
4-gram FNNLM 
118 
5-gram FNNLM 
114 
6-gram FNNLM 
113 
1st-order FOFE-FNNLM 
116 
2nd-order FOFE-FNNLM 
108 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Perplexities on LTCB for various lan-
guage models. [M*N] denotes the sizes of the in-
put context window and projection layer. 
Model 
Architecture 
Test PPL 
KN 3-gram 
-
156 
KN 5-gram 
-
132 
[1*200]-400-400-80k 
241 
[2*200]-400-400-80k 
155 
FNN-LM [2*200]-600-600-80k 
150 
[3*200]-400-400-80k 
131 
[4*200]-400-400-80k 
125 
RNN-LM 
[1*600]-80k 
112 
[1*200]-400-400-80k 
120 
FOFE 
[1*200]-600-600-80k 
115 
FNN-LM [2*200]-400-400-80k 
112 
[2*200]-600-600-80k 
107 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the Science and Technology Development of Anhui Province, China (Grants No. 2014z02006) and the Funda-mental Research Funds for the Central Universi-ties from China, as well as an NSERC Discov-ery grant from Canadian federal government. We appreciate Dr. Barlas Oguz at Microsoft for his insightful comments and constructive suggestions on Theorem 2.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Estimation of probabilities from sparse data for the language model component of a speech recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slava</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="400" to="401" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
	<note>ASSP)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>of International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Back-propagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rejean</forename><surname>Ducharme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rejean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffery</forename><surname>Elman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
		</imprint>
	</monogr>
	<note>Cognitive science</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>KarafiÃ¡t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>of International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SLT</title>
		<meeting>of SLT</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient GPUbased training of recurrent neural network language models using spliced sentence bunch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Temporalkernel recurrent neural networks. Neural Networks</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="239" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporal kernel neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Zhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>of International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8247" to="8251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glorot</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AISTATS</title>
		<meeting>of AISTATS</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Large Text Compression Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Mahoney</surname></persName>
		</author>
		<ptr target="http://mattmahoney.net/dc/textdata.html" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Personal Communications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbin</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.01504</idno>
		<title level="m">Junfeng Hou and LiRong Dai. 2015a. A Fixed-Size Encoding Method for Variable-Length Sequences with its Application to Neural Network Language Models</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.00702</idno>
		<title level="m">Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Probe and Learn Neural Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Dai</surname></persName>
		</author>
		<title level="m">The New HOPE Way to Learn Neural Networks. Proc. of Deep Learning Workshop at ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
