<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Neural Local Coherence Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien</forename><surname>Dat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Informatics Institute University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">Qatar Computing Research Institute HBKU</orgName>
								<address>
									<country>Qatar Foundation</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
							<email>t.d.nguyen@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Informatics Institute University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">Qatar Computing Research Institute HBKU</orgName>
								<address>
									<country>Qatar Foundation</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
							<email>sjoty@hbku.edu.qa</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Informatics Institute University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">Qatar Computing Research Institute HBKU</orgName>
								<address>
									<country>Qatar Foundation</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Neural Local Coherence Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1320" to="1330"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/P17-1121</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a local coherence model based on a convolutional neural network that operates over the entity grid representation of a text. The model captures long range entity transitions along with entity-specific features without loosing generalization, thanks to the power of distributed representation. We present a pairwise ranking method to train the model in an end-to-end fashion on a task and learn task-specific high level features. Our evaluation on three different coherence assessment tasks demonstrates that our model achieves state of the art results outperforming existing models by a good margin.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Motivation</head><p>What distinguishes a coherent text from a random sequence of sentences is that it binds the sentences together to express a meaning as a whole -the in- terpretation of a sentence usually depends on the meaning of its neighbors. Coherence models that can distinguish a coherent from incoherent texts have a wide range of applications in text genera- tion, summarization, and coherence scoring.</p><p>Several formal theories of coherence have been proposed <ref type="bibr" target="#b24">(Mann and Thompson, 1988a;</ref><ref type="bibr" target="#b13">Grosz et al., 1995;</ref><ref type="bibr" target="#b0">Asher and Lascarides, 2003)</ref>, and their principles have inspired development of many existing coherence models ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b22">Lin et al., 2011;</ref>). Among these models, the entity grid ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008)</ref>, which is based on Centering The- ory ( <ref type="bibr" target="#b13">Grosz et al., 1995)</ref>, is arguably the most pop- ular, and has seen a number of improvements over the years. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the entity grid model represents a text by a grid that captures how * Both authors contributed equally to this work. grammatical roles of different entities change from sentence to sentence. The grid is then converted into a feature vector containing probabilities of local entity transitions, which enables machine learning models to learn the degree of text coher- ence. Extensions of this basic grid model incorpo- rate entity-specific features <ref type="bibr" target="#b8">(Elsner and Charniak, 2011)</ref>, multiple ranks <ref type="bibr" target="#b9">(Feng and Hirst, 2012)</ref>, and coherence relations <ref type="bibr" target="#b10">(Feng et al., 2014)</ref>.</p><p>While the entity grid and its extensions have been successful in many applications, they are limited in several ways. First, they use discrete representation for grammatical roles and features, which prevents the model from considering suffi- ciently long transitions ( <ref type="bibr" target="#b3">Bengio et al., 2003)</ref>. Sec- ond, feature vector computation in existing models is decoupled from the target task, which limits the model's capacity to learn task-specific features.</p><p>In this paper, we propose a neural architecture for coherence assessment that can capture long range entity transitions along with arbitrary entity- specific features. Our model obtains generaliza- tion through distributed representations of entity transitions and entity features. We also present an end-to-end training method to learn task-specific high level features automatically in our model. We evaluate our approach on three different evaluation tasks: discrimination, insertion, and summary coherence rating, proposed previously for evaluating coherence models ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b8">Elsner and Charniak, 2011</ref>). Discrim- ination and insertion involve identifying the right order of the sentences in a text with different lev- els of difficulty. In the summary coherence rat- ing task, we compare the rankings, given by the model, against human judgments of coherence.</p><p>The experimental results show that our neu- ral models consistently improve over the non- neural counterparts (i.e., existing entity grid mod- els) yielding absolute gains of about 4% on dis-crimination, up to 2.5% on insertion, and more than 4% on summary coherence rating. Further- more, our model achieves state of the art results in all these tasks. We have released our source code for research purposes. <ref type="bibr">1</ref> The remainder of this paper is organized as fol- lows. We describe entity grid, its extensions, and its limitations in Section 2. In Section 3, we present our neural model. We describe evaluation tasks and results in Sections 4 and 5. We give a brief account of related work in Section 6. Finally, we conclude with future directions in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Entity Grid and Its Extensions</head><p>Motivated by Centering Theory ( <ref type="bibr" target="#b13">Grosz et al., 1995)</ref>, <ref type="bibr" target="#b2">Barzilay and Lapata (2008)</ref> proposed an entity-based model for representing and assessing text coherence. Their model represents a text by a two-dimensional array called entity grid that cap- tures transitions of discourse entities across sen- tences. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the rows of the grid correspond to sentences, and the columns corre- spond to discourse entities appearing in the text. They consider noun phrases (NP) as entities, and employ a coreference resolver to detect mentions of the same entity (e.g., Obama, the president). Each entry G i,j in the entity grid represents the syntactic role that entity e j plays in sentence s i , which can be one of: subject (S), object (O), or other (X). In addition, entities not appearing in a sentence are marked by a special symbol (-). If an entity appears more than once with different gram- matical roles in the same sentence, the role with the highest rank (S O X) is considered.</p><p>To represent the entity grid using a feature vec- tor, <ref type="bibr" target="#b2">Barzilay and Lapata (2008)</ref> compute proba- bility for each local entity transition of length k (i.e., {S, O, X, −} k ), and represent each grid by a vector of 4 k transitions probabilities. To dis- tinguish between transitions of important entities from unimportant ones, they consider the salience of the entities, which they quantify by their oc- currence frequency in the document. Assessment of text coherence is then formulated as a ranking problem in an SVM preference ranking framework <ref type="bibr" target="#b15">(Joachims, 2002)</ref>.</p><p>Subsequent studies proposed to extend the ba- sic entity grid model. <ref type="bibr" target="#b11">Filippova and Strube (2007)</ref> attempted to improve the model by grouping en-</p><formula xml:id="formula_0">1 https://github.com/datienguyen/cnn_ coherence/ UNIT PRODUCTS RESEARCH COMPANY PARTS CONTROLS INDUSTRY ELECTRONICS TERM CONCERN AEROSPACE EMPLOYEES SERVICES LOS ANGELES EATON s 0 O − X X − − − − − − − X − − X s 1 − − − − − − − − S − − − − − − s 2 − O − − − − X − − − − O O X − s 3 − − − − X X − X − O X − − − S s0</formula><p>: Eaton Corp. said it sold its Pacific Sierra Research unit to a company formed by employees of that unit.</p><p>s1: Terms were not disclosed.</p><p>s2: Pacific Sierra, based in Los Angeles, has 200 employ- ees and supplies professional services and advanced products to industry.</p><p>s3: Eaton is an automotive parts, controls and aerospace electronics concern. tities based on semantic relatedness, but did not get significant improvement. <ref type="bibr" target="#b8">Elsner and Charniak (2011)</ref> proposed a number of improvements. They initially show significant improvement by includ- ing non-head nouns (i.e., nouns that do not head NPs) as entities in the grid. <ref type="bibr">2</ref> Then, they extend the grid to distinguish between entities of different types by incorporating entity-specific features like named entity, noun class, modifiers, etc. These ex- tensions led to the best results reported so far. The Entity grid and its extensions have been successfully applied to many downstream tasks including coherence rating ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008)</ref>, essay scoring ( <ref type="bibr" target="#b5">Burstein et al., 2010)</ref>, story generation <ref type="bibr" target="#b26">(McIntyre and Lapata, 2010)</ref>, and read- ability assessment <ref type="bibr" target="#b29">(Pitler et al., 2010;</ref><ref type="bibr" target="#b2">Barzilay and Lapata, 2008)</ref>. They have also been critical com- ponents in state-of-the-art sentence ordering mod- els ( <ref type="bibr" target="#b32">Soricut and Marcu, 2006;</ref><ref type="bibr" target="#b8">Elsner and Charniak, 2011;</ref><ref type="bibr" target="#b22">Lin et al., 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Limitations of Entity Grid Models</head><p>Despite its success, existing entity grid models are limited in several ways.</p><p>• Existing models use discrete representation for grammatical roles and features, which leads to the so-called curse of dimensionality problem <ref type="bibr" target="#b3">(Bengio et al., 2003</ref>). In particular, to model transitions of length k with R different grammatical roles, the basic entity grid model needs to compute R k tran-sition probabilities from a grid. One can imagine that the estimated distribution becomes sparse as k increases. This prevents the model from consider- ing longer transitions -existing models use k ≤ 3. This problem is exacerbated when we want to in- clude entity-specific features, as the number of pa- rameters grows exponentially with the number of features ( <ref type="bibr" target="#b8">Elsner and Charniak, 2011</ref>).</p><p>• Existing models compute feature representa- tions from entity grids in a task-agnostic way. In other words, feature extraction is decoupled from the target downstream tasks. This can limit the model's capacity to learn task-specific features. Therefore, models that can be trained in an end-to- end fashion on different target tasks are desirable.</p><p>In the following section, we present a neural ar- chitecture that allows us to capture long range en- tity transitions along with arbitrary entity-specific features without loosing generalization. We also present an end-to-end training method to learn task-specific features automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Neural Coherence Model</head><p>Figure 2 summarizes our neural architecture for modeling local coherence, and how it can be trained in a pairwise fashion. The architecture takes a document as input, and first extracts its en- tity grid. <ref type="bibr">3</ref> The first layer of the neural network transforms each grammatical role in the grid into a distributed representation, a real-valued vector. The second layer computes high-level features by going over each column (transitions) of the grid. The following layer selects the most important high-level features, which are in turn used for co- herence scoring. The features computed at differ- ent layers of the network are automatically trained by backpropagation to be relevant to the task. In the following, we elaborate on the layers of the neural network model.</p><p>(I) Transforming grammatical roles into fea- ture vectors: Grammatical roles are fed to our model as indices taken from a finite vocabulary V. In the simplest scenario, V contains {S, O, X, −}. However, we will see in Section 3.1 that as we in- clude more entity-specific features, V can contain more symbols. The first layer of our network maps each of these indices into a distributed representa- tion R d by looking up a shared embedding matrix E ∈ R |V|×d . We consider E a model parameter to be learned by backpropagation on a given task. We can initialize E randomly or using pretrained vectors trained on a general coherence task.</p><p>Given an entity grid G with columns represent- ing entity transitions over sentences in a docu- ment, the lookup layer extracts a d-dimensional vector for each entry G i,j from E. More formally,</p><formula xml:id="formula_1">L(G) = E(G 1,1 ) · · · E(G i,j ) · · · E(G m,n )</formula><p>(1) where E(G i,j ) refers to the row in E that corre- sponds to the grammatical role G i,j ∈ V; m is the total number of sentences and n is the total num- ber of entities in the document. The output L(G) is a tensor in R m×n×d , which is fed to the next layer of the network as we describe below.</p><p>(II) Modeling entity transitions: The vectors produced by the lookup layer are combined by subsequent layers of the network to generate a coherence score for the document. To compose higher-level features from the embedding vectors, we make the following modeling assumptions:</p><p>• Similar to existing entity grid models, we as- sume there is no spatio-temporal relation between the entities in a document. In other words, columns in a grid are treated independently.</p><p>• We are interested in modeling entity transitions of arbitrary lengths in a location-invariant way. This means, we aim to compose local patches of entity transitions into higher-level representations, while treating the patches independently of their position in the entity grid.</p><p>Under these assumptions, the natural choice to tackle this problem is to use a convolutional ap- proach, used previously to solve other NLP tasks <ref type="bibr" target="#b6">(Collobert et al., 2011;</ref><ref type="bibr" target="#b18">Kim, 2014</ref>).</p><p>Convolution layer: A convolution operation in- volves applying a filter w ∈ R k.d (i.e., a vector of weight parameters) to each entity transition of length k to produce a new abstract feature</p><formula xml:id="formula_2">h t = f (w T L t:t+k−1,j + b t )<label>(2)</label></formula><p>where L t:t+k−1,j denotes the concatenation of k vectors in the lookup layer representing a transi- tion of length k for entity e j in the grid, b t is a bias term, and f is a nonlinear activation function, e.g., <ref type="bibr">ReLU (Nair and Hinton, 2010</ref>) in our model. We apply this filter to each possible k-length transitions of different entities in the grid to gener- ate a feature map,</p><formula xml:id="formula_3">h i = [h 1 , · · · , h m.n+k−1 ].</formula><p>We repeat this process N times with N different filters to get N different feature maps <ref type="figure" target="#fig_1">(Figure 2)</ref>. No- tice that we use a wide convolution ( <ref type="bibr" target="#b16">Kalchbrenner et al., 2014)</ref>, as opposed to narrow, to ensure that the filters reach entire columns of a grid, including the boundary entities. This is done by performing zero-padding, where out-of-range (i.e., for t &lt; 0 or t &gt; {m, n}) vectors are assumed to be zero.</p><p>Convolutional filters learn to compose local transition features of a grid into higher-level rep- resentations automatically. Since it operates over the distributed representation of grid entries, com- pared to traditional grid models, the transition length k can be sufficiently large (e.g., 5 − 8 in our experiments) to capture long-range tran- sitional dependencies without overfitting on the training data. Moreover, unlike existing grid mod- els that compute transition probabilities from a single document, embedding vectors and convo- lutional filters are learned from all training docu- ments, which helps the neural framework to obtain better generalization and robustness.</p><p>Pooling layer: After the convolution, we apply a max-pooling operation to each feature map.</p><formula xml:id="formula_4">m = [µ p (h 1 ), · · · , µ p (h N )]<label>(3)</label></formula><p>where µ p (h i ) refers to the max operation applied to each non-overlapping 4 window of p features in the feature map h i . Max-pooling reduces the out- put dimensionality by a factor of p, and it drives the model to capture the most salient local features from each feature map in the convolutional layer.</p><p>Coherence scoring: Finally, the max-pooled features are used in the output layer of the network to produce a coherence score y ∈ R.</p><formula xml:id="formula_5">y = v T m + b (4)</formula><p>where v is the weight vector and b is a bias term.</p><p>Why it works: Intuitively, each filter detects a specific transition pattern (e.g., 'SS-O-X' for a co- herent text), and if this pattern occurs somewhere in the grid, the resulting feature map will have a large value for that particular region and small val- ues for other regions. By applying max pooling on this feature map, the network then discovers that the transition appeared in the grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Incorporating Entity-Specific Features</head><p>Our model as described above neuralizes the basic entity grid model that considers only entity transi- tions without distinguishing between types of the entities. However, as Elsner and Charniak (2011) pointed out entity-specific features could be cru- cial for modeling local coherence. One simple way to incorporate entity-specific features into our model is to attach the feature value (e.g., named entity type) with the grammatical role in the grid.</p><p>For example, if an entity e j of type PERSON ap- pears as a subject (S) in sentence s i , the grid entry G i,j can be encoded as PERSON-S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>Our neural model assigns a coherence score to an input document d based on the degree of lo- cal coherence observed in its entity grid G. Let y = φ(G|θ) define our model that transforms an input grid G to a coherence score y through a se- quence of lookup, convolutional, pooling, and lin- ear projection layers with parameter set θ. The parameter set θ includes the embedding matrix E, the filter matrix W , the weight vector v, and the biases. We use a pairwise ranking approach (Col- lobert et al., 2011) to learn θ. The training set comprises ordered pairs</p><formula xml:id="formula_6">(d i , d j )</formula><p>, where document d i exhibits a higher de- gree of coherence than document d j . As we will see in Section 4 such orderings can be obtained automatically or through manual annotation. In training, we seek to find θ that assigns a higher coherence score to d i than to d j . We minimize the following ranking objective with respect to θ:</p><formula xml:id="formula_7">J (θ) = max{0, 1 − φ(G i |θ) + φ(G j |θ)} (5)</formula><p>where G i and G j are the entity grids correspond- ing to documents d i and d j , respectively. Notice that (also shown in <ref type="figure" target="#fig_1">Figure 2</ref>) the network shares its layers (and hence θ) to obtain φ(G i |θ) and φ(G j |θ) from a pair of input grids (G i , G j ).</p><p>Barzilay and Lapata (2008) adopted a similar ranking criterion using an SVM preference kernel learner as they argue coherence assessment is best seen as a ranking problem as opposed to classifi- cation (coherent vs. incoherent). Also, the ranker gives a scoring function φ that a text generation system can use to compare alternative hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Tasks</head><p>We evaluate the effectiveness of our coherence models on two different evaluation tasks: sentence ordering and summary coherence rating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sentence Ordering</head><p>Following Elsner and Charniak (2011), we eval- uate our models on two sentence ordering tasks: discrimination and insertion.</p><p>In the discrimination task ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008)</ref>, a document is compared to a random per- Sections # Doc. # Pairs Avg. # Sen.  mutation of its sentences, and the model is con- sidered correct if it scores the original document higher than the permuted one. We use 20 permu- tations of each document in the test set in accor- dance with previous work. In the insertion task (Elsner and Charniak, 2011), we evaluate models based on their ability to locate the original position of a sentence pre- viously removed from a document. To measure this, each sentence in the document is removed in turn, and an insertion place is located for which the model gives the highest coherence score to the document. The insertion score is then computed as the average fraction of sentences per document reinserted in their actual position.</p><p>Discrimination can be easier for longer docu- ments, since a random permutation is likely to be different than the original one. Insertion is a much more difficult task since the candidate documents differ only by the position of one sentence.  <ref type="table">Table 1</ref> gives basic statistics about the dataset. Following previous works, we use 20 random permutations of each article, and we exclude permutations that match the original document. <ref type="bibr">5</ref> The fourth column (# Pairs) in <ref type="table">Table 1</ref> shows the resulting number of (original, permuted) pairs used for training our model and for testing in the discrimination task.</p><p>Some previous studies ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008;</ref>) used the AIRPLANES and the EARTHQUAKES corpora, which contain re- ports on airplane crashes and earthquakes, respec- tively. Each of these corpora contains 100 articles for training and 100 articles for testing. The av- erage number of sentences per article in these two corpora is 10.4 and 11.5, respectively.</p><p>We preferred the WSJ corpus for several rea- sons. First and most importantly, the WSJ cor- pus is larger than other corpora (see <ref type="table">Table 1</ref>). A large training set is crucial for learning effective deep learning models <ref type="bibr" target="#b6">(Collobert et al., 2011)</ref>, and a large enough test set is necessary to make a gen- eral comment about model performance. Second, as Elsner and Charniak (2011) pointed out, texts in AIRPLANES and EARTHQUAKES are constrained in style, whereas WSJ documents are more like normal informative articles. Third, we could re- produce results on this dataset for the competing systems (e.g., entity grid and its extensions) using the publicly available Brown coherence toolkit. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Summary Coherence Rating</head><p>We further evaluate our models on the summary coherence rating task proposed by <ref type="bibr" target="#b2">Barzilay and Lapata (2008)</ref>, where we compare rankings given by a model to a pair of summaries against rankings elicited from human judges.</p><p>Dataset: The summary dataset was extracted from the Document Understanding Conference (DUC'03), which contains 6 clusters of multi- document summaries produced by human experts and 5 automatic summarization systems. Each cluster has 16 summaries of a document with pair- wise coherence rankings given by humans judges; see ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008</ref>) for details on the annotation method. There are 144 pairs of sum- maries for training and 80 pairs for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we present our experiments -the models we compare, their settings, and the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Models Compared</head><p>We compare our coherence model against a ran- dom baseline and several existing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random:</head><p>The Random baseline makes a ran- dom decision for the evaluation tasks.</p><p>Graph-based Model: This is the graph-based unsupervised model proposed by <ref type="bibr" target="#b14">Guinaudeau and Strube (2013)</ref>. We use the implementation from the cohere 7 toolkit ( <ref type="bibr" target="#b30">Smith et al., 2016)</ref>, and run it on the test set with syntactic projection (command line option 'projection=3') for graph construction. This setting yielded best scores for this model. <ref type="bibr">(2014)</ref> proposed this neural model for measuring <ref type="bibr">6</ref> https://bitbucket.org/melsner/browncoherence 7 https://github.com/karins/CoherenceFramework text coherence. The model first encodes each sen- tence in a document into a fixed-length vector us- ing a recurrent or a recursive neural network. Then it computes the coherence score of the document by aggregating the scores estimated for each win- dow of three sentences in the document. We used the implementation made publicly available by the authors. <ref type="bibr">8</ref> We trained the model on our WSJ cor- pus with 512, 1024 and 1536 minibatch sizes for a maximum of 25 epochs. <ref type="bibr">9</ref> The model that used minibatch size of 512 and completed 23 epochs achieved the best accuracy on the DEV set. We ap- plied this model to get the scores on the TEST set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distributed Sentence Model: Li and Hovy</head><p>Grid-all nouns (E&amp;C): This is the simple ex- tension of the original entity grid model, where all nouns are considered as entities. Elsner and Char- niak (2011) report significant gains by considering all nouns as opposed to only head-nouns. Results for this model were obtained by training the base- line entity grid model (command line option '-n') in the Brown coherence toolkit on our dataset.</p><p>Extended grid (E&amp;C): This represents the ex- tended entity grid model of Elsner and Charniak (2011) that uses 9 entity-specific features; 4 of them were computed from external corpora. This model considers all nouns as entities. For this sys- tem, we train the extended grid model (command line option '-f') in the Brown coherence toolkit.</p><p>Grid-CNN: This is our proposed neural exten- sion of the basic entity grid (all nouns), where we only consider entity transitions as input.</p><p>Extended Grid-CNN: This corresponds to our neural model that incorporates entity-specific fea- tures following the method described in Section 3.1. To keep the model simple, we include only three entity-specific features from (Elsner and Charniak, 2011) that are easy to compute and do not require any external corpus. The features are: (i) named entity type, (ii) salience as deter- mined by occurrence frequency of the entity, and Batch Emb. Dropout Filter Win. Pool <ref type="table" target="#tab_2">Grid-CNN  128  100  0.5  150  6  6  Ext. Grid-CNN  32  100  0.5  150  5  6   Table 2</ref>: Optimal hyper-parameter setting for our neural models based on development set accuracy.</p><p>(iii) whether the entity has a proper mention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Settings for Neural Models</head><p>We held out 10% of the training documents to form a development set (DEV) on which we tune the hyper-parameters of our neural models. For discrimination and insertion tasks, the resulting DEV set contains 138 articles and 2,678 pairs af- ter removing the permutations that match the orig- inal documents. For the summary rating task, DEV contains 14 pairs of summaries.</p><p>We implement our models in Theano <ref type="bibr">(Theano Development Team, 2016)</ref>. We use rectified lin- ear units (ReLU) as activations (f ). The embed- ding matrix is initialized with samples from uni- form distribution U (−0.01, 0.01), and the weight matrices are initialized with samples from glorot- uniform distribution <ref type="bibr" target="#b12">(Glorot and Bengio, 2010)</ref>.</p><p>We train the models by optimizing the pair- wise ranking loss in Equation 5 using the gradient- based online learning algorithm RMSprop with parameters (ρ and ) set to the values suggested by <ref type="bibr" target="#b35">Tieleman and Hinton (2012)</ref>. <ref type="bibr">10</ref> We use up to 25 epochs. To avoid overfitting, we use dropout ( <ref type="bibr" target="#b33">Srivastava et al., 2014</ref>) of hidden units, and do early stopping by observing accuracy on the DEV set -if the accuracy does not increase for 10 consecutive epochs, we exit with the best model recorded so far. We search for optimal minibatch size in {16, 32, 64, 128}, embedding size in {80, 100, 200}, dropout rate in {0.2, 0.3, 0.5}, filter number in {100, 150, 200, 300}, window size in {2, 3, 4, 5, 6, 7, 8}, and pooling length in {3, 4, 5, 6, 7}. <ref type="table">Table 2</ref> shows the optimal hyper- parameter setting for our models. The best model on DEV is then used for the final evaluation on the TEST set. We run each experiment five times, each time with a different random seed, and we report the average of the runs to avoid any randomness in results. Statistical significance tests are done using an approximate randomization test based on the accuracy. We used SIGF V. <ref type="bibr">2 (Padó, 2006</ref>) with  Coherence evaluation results on Discrimination and Insertion tasks. † indicates a neural model is significantly superior to its non- neural counterpart with p-value &lt; 0.01.</p><p>10,000 iterations. <ref type="table" target="#tab_2">Table 3</ref> shows the results on discrimination and insertion tasks. The graph-based model gets the lowest scores. This is not surprising considering that this model works in an unsupervised way. The distributed sentence model surprisingly performed poorly on our dataset. Among the existing mod- els, the grid models get the best scores on both tasks. This demonstrates that entity transition, as a method to capture local coherence, is more ef- fective than the sentence representation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on Sentence Ordering</head><p>Neuralization of the existing grid models yields significant improvements in most cases. The Grid- CNN model delivers absolute improvements of about 4% in discrimination and 1% in insertion over the basic grid model. When we compare our Extended Grid-CNN with its non-neural counter- part Extended Grid, we observe similar gains in discrimination and more gains (2.5%) in insertion. Note that the Extended Grid-CNN yields these im- provements considering only a subset of the Ex- tended Grid features. This demonstrates the effec- tiveness of distributed representation and convolu- tional feature learning method. Compared to the discrimination task, gain in the insertion task is less verbose. There could be two reasons for this. First, as mentioned before, inser- tion is a harder task than discrimination. Second, our models were not trained specifically on the in- sertion task. The model that is trained to distin- guish an original document from its random per- mutation may learn features that are not specific enough to distinguish documents when only one sentence differs. In the future, it will be interesting Acc Pre-trained Grid-CNN 86.3 86.3 Pre-trained Ext. Grid-CNN 87.5 87.5 <ref type="table">Table 4</ref>: Evaluation results on the Summary Co- herence Rating task.</p><p>to see how the model performs when it is trained on the insertion task directly. <ref type="table">Table 4</ref> presents the results on the summary co- herence rating task, where we compare our mod- els with the reported results of the graph-based method ( <ref type="bibr" target="#b14">Guinaudeau and Strube, 2013</ref>) and the initial entity grid model ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008)</ref> on the same experimental setting. <ref type="bibr">11</ref> The ex- tended grid model does not use pairwise training, therefore could not be trained on the summariza- tion dataset. Since there are not many training in- stances, our neural models may not learn well for this task. Therefore, we also present versions of our model, where we use pre-trained models from discrimination task on WSJ corpus (last two rows in the table ). The pre-trained models are then fine- tuned on the summary rating task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results on Summary Coherence Rating</head><p>We can observe that even without pre-training our models outperform existing models, and pre- training gives further improvements. Specifically, Pre-trained Grid-CNN gives an improvement of 2.5% over the Grid model, and including entity features pushes the improvement further to 3.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Barzilay and <ref type="bibr" target="#b1">Lapata (2005</ref><ref type="bibr" target="#b2">Lapata ( , 2008</ref> introduced the entity grid representation of discourse to model lo- cal coherence that captures the distribution of dis- course entities across sentences in a text. They also introduced three tasks to evaluate the perfor- mance of coherence models: discrimination, sum- mary coherence rating, and readability.</p><p>A number of extensions of the basic entity grid model has been proposed. <ref type="bibr" target="#b8">Elsner and Charniak (2011)</ref> included entity-specific features to distin- guish between entities. <ref type="bibr" target="#b9">Feng and Hirst (2012)</ref> used the basic grid representation, but improved its learning to rank scheme. Their model learns not only from original document and its permuta- tions but also from ranking preferences among the permutations themselves. <ref type="bibr" target="#b14">Guinaudeau and Strube (2013)</ref> convert a standard entity grid into a bi- partite graph representing entity occurrences in sentences. To model local entity transition, the method constructs a directed projection graph rep- resenting the connection between adjacent sen- tences. Two sentences have a connected edge if they share at least one entity in common. The co- herence score of the document is then computed as the average out-degree of sentence nodes.</p><p>In addition, there are some approaches that model text coherence based on coreferences and discourse relations. <ref type="bibr" target="#b7">Elsner and Charniak (2008)</ref> proposed the discourse-new model by taking into account mentions of all referring expression (i.e., NPs) whether they are first mention (discourse- new) or subsequent (discourse-old) mentions. Given a document, they run a maximum-entropy classifier to detect each NP as a label L np ∈ {new, old}. The coherence score of the docu- ment is then estimated by np:N P s P (L np |np). In this work, they also estimate text coherence through pronoun coreference modeling. <ref type="bibr" target="#b22">Lin et al. (2011)</ref> assume that a coherent text has certain dis- course relation patterns. Instead of modeling en- tity transitions, they model discourse role transi- tions between sentences. In a follow up work, <ref type="bibr" target="#b10">Feng et al. (2014)</ref> trained the same model but us- ing features derived from deep discourse struc- tures annotated with Rhetorical Structure Theory or RST ( <ref type="bibr" target="#b25">Mann and Thompson, 1988b</ref>) relations. Louis and Nenkova (2012) introduced a coher- ence model based on syntactic patterns in text by assuming that sentences in a coherent discourse should share the same structural syntactic patterns.</p><p>In recent years, there has been a growing in- terest in neuralizing traditional NLP approaches - language modeling ( <ref type="bibr" target="#b3">Bengio et al., 2003)</ref>, sequence tagging (Collobert et al., 2011), syntactic parsing ( <ref type="bibr" target="#b31">Socher et al., 2013)</ref>, and discourse parsing ( ), etc. Following this tradition, in this paper we propose to neuralize the popular entity grid models.  also proposed a neural framework to compute the coherence score of a document by estimating coherence probability for every window of L sentences (in their experi- ments, L = 3). First, they use a recurrent or a recursive neural network to compute the represen- tation for each sentence in L from its words and their pre-trained embeddings. Then the concate- nated vector is passed through a non-linear hidden layer, and finally the output layer decides if the window of sentences is a coherent text or not. Our approach is fundamentally different from their ap- proach; our model operates over entity grids, and we use convolutional architecture to model suffi- ciently long entity transitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We presented a local coherence model based on a convolutional neural network that operates over the distributed representation of entity transitions in the grid representation of a text. Our architec- ture can model sufficiently long entity transitions, and can incorporate entity-specific features with- out loosing generalization power. We described a pairwise ranking approach to train the model on a target task and learn task-specific features. Our evaluation on discrimination, insertion and sum- mary coherence rating tasks demonstrates the ef- fectiveness of our approach yielding the best re- sults reported so far on these tasks.</p><p>In future, we would like to include other sources of information in our model. Our initial plan is to include rhetorical relations, which has been shown to benefit existing grid models <ref type="bibr" target="#b10">(Feng et al., 2014</ref>). We would also like to extend our model to other forms of discourse, especially, asynchronous con- versations, where participants communicate with each other at different times (e.g., forum, email).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Entity grid representation (top) for a document (below) from WSJ (id: 0079).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Neural architecture for modeling local coherence and the pairwise training method.</figDesc><graphic url="image-1.png" coords="4,94.68,62.81,408.18,196.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>TRAIN</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Dataset:</head><label></label><figDesc>For sentence ordering tasks, we use the Wall Street Journal (WSJ) portion of Penn Treebank, as used by Elsner and Charniak (2008, 2011); Lin et al. (2011); Feng et al. (2014).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> They match the nouns to detect coreferent entities.</note>

			<note place="foot" n="3"> For clarification, pairwise input as shown in the figure is required only to train the model.</note>

			<note place="foot" n="4"> We set the stride size to be the same as the pooling length p to get non-overlapping regions.</note>

			<note place="foot" n="5"> Short articles may produce many matches.</note>

			<note place="foot" n="8"> http://cs.stanford.edu/ bdlijiwei/code/ 9 Our WSJ corpus is about 14 times larger than their ACCIDENT or EARTHQUAKE corpus (1378 vs. 100 training articles), and the articles in our corpus are generally longer than the articles in their corpus (on average 22 vs. 10 sentences per article). Also, the vocabulary in our corpus is much larger than their vocabulary (45462 vs. 4758). Considering these factors and the fact that their Java-based implementation does not support GPU and parallelization, it takes quite long to train and to validate their model on our dataset. In our experiments, depending on the minibatch size, it took approximately 3-5 days to complete only one epoch of training!</note>

			<note place="foot" n="10"> Other adaptive algorithms, e.g., ADAM (Kingma and Ba, 2014), ADADELTA (Zeiler, 2012) gave similar results.</note>

			<note place="foot" n="11"> Since we do not have access to the output of their systems, we could not do a significance test for this task.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Regina Barzilay and Mirella Lapata for making their summarization data available and Micha Elsner for making his coherence toolkit publicly available. We also thank the three anony-mous ACL reviewers and the program chairs for their insightful comments on the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Asher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lascarides</surname></persName>
		</author>
		<title level="m">Logics of Conversation</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling local coherence: An entity-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics<address><addrLine>Ann Arbor</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="141" to="148" />
		</imprint>
	</monogr>
	<note>Michigan, ACL &apos;05</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling local coherence: An entity-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using entity-based features to model coherence in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slava</forename><surname>Andreyev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California, HLT &apos;10</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="681" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Coreference-inspired coherence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers<address><addrLine>Columbus, Ohio, HLT-Short &apos;08</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="41" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extending the entity grid with entity-specific features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="125" to="129" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extending the entity-based coherence model with multiple ranks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Vanessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="315" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The impact of deep hierarchical discourse structures in the evaluation of text coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Wei Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extending the entity-grid coherence model to semantically related entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh European Workshop on Natural Language Generation</title>
		<meeting>the Eleventh European Workshop on Natural Language Generation<address><addrLine>Germany, ENLG &apos;07</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="139" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR W&amp;CP: Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting><address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Centering: A framework for modeling the local coherence of discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">J</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="225" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph-based local coherence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Guinaudeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August 2013</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August 2013<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="93" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, Edmonton</title>
		<meeting>the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, Edmonton<address><addrLine>Alberta, Canada, KDD &apos;02</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A model of coherence based on distributed sentence representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1218" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2039" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recursive deep models for discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2061" to="2069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatically evaluating text coherence using discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Oregon</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="997" to="1006" />
		</imprint>
	</monogr>
	<note>Portland</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A coherence model based on syntactic patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1157" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rhetorical Structure Theory: Toward a Functional Theory of Text Organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rhetorical structure theory: Toward a functional theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Plot induction and evolutionary search for story generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Mcintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Uppsala</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1562" to="1572" />
		</imprint>
	</monogr>
	<note>Sweden, ACL &apos;10</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.icml2010.org/papers/432.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<editor>Johannes Frnkranz and Thorsten Joachims</editor>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">User&apos;s guide to sigf: Significance testing by approximate randomisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic evaluation of linguistic quality in multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden, ACL &apos;10</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="544" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cohere: A toolkit for local coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><forename type="middle">Sim</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association (ELRA)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association (ELRA)<address><addrLine>Portoroz, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P13-1045" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discourse generation using utility-trained coherence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL on Main Conference Poster Sessions</title>
		<meeting>the COLING/ACL on Main Conference Poster Sessions<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="803" to="810" />
		</imprint>
	</monogr>
	<note>COLING-ACL &apos;06</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<idno>abs/1605.02688</idno>
		<ptr target="http://arxiv.org/abs/1605.02688" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Theano Development Team</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">RMSprop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>COURSERA: Neural Networks</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>CoRR abs/1212.5701</idno>
		<ptr target="http://arxiv.org/abs/1212.5701" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
