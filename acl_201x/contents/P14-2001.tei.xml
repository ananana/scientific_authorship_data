<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring the Relative Role of Bottom-up and Top-down Information in Phoneme Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdellah</forename><surname>Fourtassi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire de Sciences Cognitives et Psycholinguistique</orgName>
								<orgName type="institution" key="instit1">ENS/EHESS</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schatz</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire de Sciences Cognitives et Psycholinguistique</orgName>
								<orgName type="institution" key="instit1">ENS/EHESS</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">SIERRA Project-Team</orgName>
								<orgName type="institution" key="instit2">INRIA/ENS</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">JHU</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire de Sciences Cognitives et Psycholinguistique</orgName>
								<orgName type="institution" key="instit1">ENS/EHESS</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring the Relative Role of Bottom-up and Top-down Information in Phoneme Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1" to="6"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We test both bottom-up and top-down approaches in learning the phonemic status of the sounds of English and Japanese. We used large corpora of spontaneous speech to provide the learner with an input that models both the linguistic properties and statistical regularities of each language. We found both approaches to help discriminate between allophonic and phone-mic contrasts with a high degree of accuracy , although top-down cues proved to be effective only on an interesting subset of the data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Developmental studies have shown that, during their first year, infants tune in on the phonemic cat- egories (consonants and vowels) of their language, i.e., they lose the ability to distinguish some within-category contrasts <ref type="bibr" target="#b13">(Werker and Tees, 1984)</ref> and enhance their ability to distinguish between- category contrasts ( <ref type="bibr" target="#b6">Kuhl et al., 2006</ref>). Current work in early language acquisition has proposed two competing hypotheses that purport to account for the acquisition of phonemes. The bottom-up hypothesis holds that infants converge on the lin- guistic units of their language through a similarity- based distributional analysis of their input <ref type="bibr" target="#b9">(Maye et al., 2002;</ref><ref type="bibr" target="#b12">Vallabha et al., 2007)</ref>. In contrast, the top-down hypothesis emphasizes the role of higher level linguistic structures in order to learn the lower level units <ref type="bibr" target="#b3">(Feldman et al., 2013;</ref><ref type="bibr" target="#b8">Martin et al., 2013)</ref>. The aim of the present work is to explore how much information can ideally be derived from both hypotheses.</p><p>The paper is organized as follows. First we de- scribe how we modeled phonetic variation from audio recordings, second we introduce a bottom- up cue based on acoustic similarity and top- down cues based of the properties of the lexicon.</p><p>We test their performance in a task that consists in discriminating within-category contrasts from between-category contrasts. Finally we discuss the role and scope of each cue for the acquisition of phonemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Modeling phonetic variation</head><p>In this section, we describe how we modeled the representation of speech sounds putatively pro- cessed by infants, before they learn the relevant phonemic categories of their language. Following <ref type="bibr" target="#b10">Peperkamp et al. (2006)</ref>, we make the assumption that this input is quantized into context-dependent phone-sized unit we call allophones. Consider the example of the allophonic rule that applies to the The phoneme /r/ surfaces as voiced ( <ref type="bibr">[K]</ref>) before a voiced obstruent like in [kanaK Zon] ("canard jaune", yellow duck) and as voiceless ( <ref type="bibr">[X]</ref>) before a voiceless obstruent as in [kanaX puXpK] ("ca- nard pourpre", purple duck). Assuming speech sounds are coded as allophones, the challenge fac- ing the learner is to distinguish the allophonic vari- ation ( <ref type="bibr">[K]</ref>, <ref type="bibr">[X]</ref>) from the phonemic variation (re- lated to a difference in the meaning) like the con- trast ( <ref type="bibr">[K]</ref>, <ref type="bibr">[l]</ref>).</p><p>Previous work has generated allophonic varia- tion using random contexts <ref type="bibr" target="#b8">(Martin et al., 2013)</ref>. This procedure does not take into account the fact that contexts belong to natural classes. In addition, it does not enable to compute an acoustic distance. Here, we generate linguistically and acoustically controlled allophones using Hidden Markov Mod- els (HMMs) trained on audio recordings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Corpora</head><p>We use two speech corpora: the Buckeye Speech corpus <ref type="bibr" target="#b11">(Pitt et al., 2007)</ref>, which consists of 40 hours of spontaneous conversations with 40 speak- ers of American English, and the core of the Cor- pus of Spontaneous Japanese ( <ref type="bibr" target="#b7">Maekawa et al., 2000</ref>) which also consists of about 40 hours of recorded spontaneous conversations and public speeches in different fields. Both corpora are time- aligned with phonetic labels. Following Boruta (2012), we relabeled the japanese corpus using 25 phonemes. For English, we used the phonemic version which consists of 45 phonemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Input generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">HMM-based allophones</head><p>In order to generate linguistically and acoustically plausible allophones, we apply a standard Hidden Markov Model (HMM) phoneme recognizer with a three-state per phone architecture to the signal, as follows.</p><p>First, we convert the raw speech waveform of the corpora into successive vectors of Mel Fre- quency Cepstrum Coefficients (MFCC), computed over 25 ms windows, using a period of 10 ms (the windows overlap). We use 12 MFCC coeffi- cients, plus the energy, plus the first and second or- der derivatives, yielding 39 dimensions per frame. Second, we start HMM training using one three- state model per phoneme. Third, each phoneme model is cloned into context-dependent triphone models, for each context in which the phoneme actually occurs (for example, the phoneme /A/ oc- curs in the context <ref type="bibr">[d-A-g]</ref> as in the word /dAg/ ("dog"). The triphone models are then retrained on only the relevant subset of the data, corresponding to the given triphone context. These detailed mod- els are clustered back into inventories of various sizes (from 2 to 20 times the size of the phone- mic inventory) using a linguistic feature-based de- cision tree, and the HMM states of linguistically similar triphones are tied together so as to max- imize the likelihood of the data. Finally, the tri- phone models are trained again while the initial gaussian emission models are replaced by mix- ture of gaussians with a progressively increasing number of components, until each HMM state is modeled by a mixture of 17 diagonal-covariance gaussians. The HMM were built using the HMM Toolkit (HTK: <ref type="bibr" target="#b14">Young et al., 2006</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Random allophones</head><p>As a control, we also reproduce the random al- lophones of <ref type="bibr" target="#b8">Martin et al. (2013)</ref>, in which allo- phonic contexts are determined randomly: for a given phoneme /p/, the set of all possible con- texts is randomly partitioned into a fixed number n of subsets. In the transcription, the phoneme /p/ is converted into one of its allophones (p 1 ,p 2 ,..,p n ) depending on the subset to which the current con- text belongs.</p><p>3 Bottom-up and top-down hypotheses</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Acoustic cue</head><p>The bottom-up cue is based on the hypothesis that instances of the same phoneme are likely to be acoustically more similar than instances of two different phonemes (see Cristia and Seidl, in press) for a similar proposition). In order to provide a proxy for the perceptual distance between al- lophones, we measure the information theoretic distance between the acoustic HMMs of these al- lophones. The 3-state HMMs of the two allo- phones were aligned with Dynamic Time Warping (DTW), using as a distance between pairs of emit- ting states, a symmetrized version of the Kullback- Leibler (KL) divergence measure (each state was approximated by a single non-diagonal Gaussian):</p><formula xml:id="formula_0">A(x, y) = (i,j)∈DTW (x,y) KL(N x i ||N y j ) + KL(N y j ||N x i )</formula><p>Where {(i, j) ∈ DTW (x, y)} is the set of in- dex pairs over the HMM states that correspond to the optimal DTW path in the comparison between phone model x and y, and N x i the full covariance Gaussian distribution for state i of phone x. For obvious reasons, the acoustic distance cue cannot be computed for Random allophones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Lexical cues</head><p>The top-down information we use in this study, is based on the insight of <ref type="bibr" target="#b8">Martin et al. (2013)</ref>. It rests on the idea that true lexical minimal pairs are not very frequent in human languages, as compared to minimal pairs due to mere phonological processes. In fact, the latter creates variants (alternants) of the same lexical item since adjacent sounds condition the realization of the first and final phoneme. For example, as shown in <ref type="figure" target="#fig_0">figure 1</ref>, the phoneme /r/ sur- faces as <ref type="bibr">[X]</ref> or [K] depending on whether or not the next sound is a voiceless obstruent. Therefore, the lexical item /kanar/ surfaces as <ref type="bibr">[kanaX]</ref> or <ref type="bibr">[kanaK]</ref>. The lexical cue assumes that a pair of words dif- fering in the first or last segment (like <ref type="bibr">[kanaX]</ref> and <ref type="bibr">[kanaK]</ref>) is more likely to be the result of a phono- logical process triggered by adjacent sounds, than a true semantic minimal pair.</p><p>However, this strategy clearly gives rise to false alarms in the (albeit relatively rare) case of true minimal pairs like <ref type="bibr">[kanaX]</ref> ("duck") and <ref type="bibr">[kanal]</ref> ("canal"), where ( <ref type="bibr">[X]</ref>, <ref type="bibr">[l]</ref>) will be mistakenly la- beled as allophonic.</p><p>In order to mitigate the problem of false alarms, we also use Boruta (2011)'s continuous version, where each pair of phones is characterized by the number of lexical minimal pairs it forms.</p><formula xml:id="formula_1">B (x, y) = |(Ax, Ay) ∈ L 2 | + |(xA, yA) ∈ L 2 |</formula><p>where {Ax ∈ L} is the set of words in the lex- icon L that end in the phone x, and {(Ax, Ay) ∈ L 2 } is the set of phonological minimal pairs in L × L that vary on the final segment.</p><p>In addition, we introduce another cue that could be seen as a normalization of Boruta's cue:</p><formula xml:id="formula_2">N (x, y) = |(Ax,Ay)∈L 2 |+|(xA,yA)∈L 2 | |{Ax∈L}|+|{Ay∈L}|+|{xA∈L}|+|{yA∈L}|</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task</head><p>For each corpus we list all the possible pairs of attested allophones. Some of these pairs are allo- phones of the same phoneme (allophonic pair) and others are allophones of different phonemes (non- allophonic pairs). The task is a same-different classification, whereby each of these pairs is given a score from the cue that is being tested. A good cue gives higher scores to allophonic pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>We use the same evaluation procedure as in <ref type="bibr" target="#b8">Martin et al. (2013)</ref>. It is carried out by computing the area under the curve of the Receiver Operat- ing Characteristic (ROC). A value of 0.5 repre- sents chance and a value of 1 represents perfect performance.</p><p>In order to lessen the potential influence of the structure of the corpus (mainly the order of the ut- terances) on the results, we use a statistical resam- pling scheme. The corpus is divided into small blocks (of 20 utterances each). In each run, we draw randomly with replacement from this set of blocks a sample of the same size as the original corpus. This sample is then used to retrain the acoustic models and generate a phonetic inven- tory that we use to re-transcribe the corpus and re-compute the cues. We report scores averaged over 5 such runs. <ref type="table" target="#tab_0">Table 1</ref> shows the classification scores for the lex- ical cues when we vary the inventory size from 2 allophones per phoneme in average, to 20 al- lophones per phoneme, using the Random allo- phones. The top-down scores are very high, repli- cating Martin et al.'s results, and even improving the performance using Boruta's cue and our new Normalized cue.   <ref type="table" target="#tab_1">Table 2</ref> shows the results for HMM-based allo- phones. The acoustic score is very accurate for both languages and is quite robust to variation. Top-down cues, on the other hand, perform, sur- prisingly, almost at chance level in distinguish- ing between allophonic and non-allophonic pairs. A similar discrepancy for the case of Japanese was actually noted, but not explained, in Boruta (2012).  When we list all possible pairs of allophones in the inventory, some of them correspond to lexi-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>cal alternants ([X], [K]) → ([kanaX] and [kanaK]), others to true minimal pairs ([K], [l]) → ([kanaK]</head><p>and <ref type="bibr">[kanal]</ref>), and yet others will simply not gen- erate lexical variation at all, we will call those: invisible pairs. For instance, in English, /h/ and /N/ occur in different syllable positions and thus cannot appear in any minimal pair. As defined above, top-down cues are set to 0 in such pairs (which means that they are systematically classi- fied as non-allophonic). This is a correct decision for /h/ vs. /N/, but not for invisible pairs that also happen to be allophonic, resulting in false nega- tives. In tables 3, we show that, indeed, invisible pairs is a major issue, and could explain to a large extent the pattern of results found above. In fact, the proportion of visible allophonic pairs ("allo" column) is way lower for HMM-based allophones. This means that the majority of allophonic pairs in the HMM case are invisible, and therefore, will be mistakenly classified as non-allophonic.  There are basically two reasons why an allo- phonic pair would be invisible ( will not generate lexical alternants). The first one is the absence of evidence, e.g., if the edges of the word with the underlying phoneme do not appear in enough con- texts to generate the corresponding variants. This happens when the corpus is so small that no word ending with, say, /r/ appears in both voiced and voiceless contexts. The second, is when the allo- phones are triggered on maximally different con- texts (on the right and the left) as illustrated below:</p><formula xml:id="formula_3">/p/ → [p 1 ] / A B [p 2 ] / C D</formula><p>When A doesn't overlap with C and B does not overlap with D, it becomes impossible for the pair ([p 1 ], [p 2 ]) to generate a lexical minimal pair. This is simply because a pair of allophones needs to share at least one context to be able to form vari- ants of a word (the second or penultimate segment of this word).</p><p>When asked to split the set of contexts in two distinct categories that trigger [p 1 ] and [p 2 ] (i.e., A B and C D), the random procedure will of- ten make A overlap with B and C overlap with D because it is completely oblivious to any acous- tic or linguistic similarity, thus making it always possible for the pair of allophones to generate lex- ical alternants. A more realistic categorization (like the HMM-based one), will naturally tend to minimize within-category distance, and maximize between-category distance. Therefore, we will have less overlap, making the chances of the pair to generate a lexical pair smaller. The more al- lophones we have, the bigger is the chance to end up with non-overlapping categories (invisible allo- phonic pairs), and the more mistakes will be made, as shown in <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Restricting the role of top-down cues</head><p>The analysis above shows that top-down cues can- not be used to classify all contrasts. The approxi- mation that consists in considering all pairs that do not generate lexical pairs as non-allophonic, does not scale up to realistic input. A more intuitive, but less ambitious, assumption is to restrict the scope of top-down cues to contrasts that do gen- erate lexical variation (lexical alternants or true minimal pairs). Thus, they remain completely ag- nostic to the status of invisible pairs. This restric- tion makes sense since top-down information boils down to knowing whether two word forms belong to the same lexical category (reducing variation to allophony), or to two different categories (varia- tion is then considered non-allophonic). Phonetic variation that does not cause lexical variation is, in this particular sense, orthogonal to our knowledge about the lexicon.</p><p>We test this hypothesis by applying the cues only to the subset of pairs that are associated with at least one lexical minimal pair. We vary the num- ber of allophones per phoneme on the one hand <ref type="table" target="#tab_5">(Table 4</ref>) and the size of the input on the other hand <ref type="table">(Table 5)</ref>. We refer to this subset by an aster- isk (*), by which we also mark the cues that apply to it. Notice that, in this new framing, the M cue is completely uninformative since it assigns the same value to all pairs.</p><p>As predicted, the cues perform very well on this subset, especially the N cue. The combination of top-down and bottom-up cues shows that the for- mer is always useful, and that these two sources of    information are not completely redundant. How- ever, the scope of top-down cues (the proportion of the subset * ) shrinks as we increase the number of allophones. <ref type="table">Table 5</ref> shows that this problem can, in principle, be mitigated by increasing the amount of data available to the learner. As we were limited to only 40 hours of speech, we generated an artifi- cial corpus that uses the same lexicon but with all possible word orders so as to maximize the num- ber of contexts in which words appear. This artifi- cial corpus increases the proportion of the subset, but we are still not at 100 % coverage, which ac- cording the analysis above, is due (at least in part) to the irreducible set of non-overlapping pairs.</p><formula xml:id="formula_4">4 - English Japanese - - Individual cues Combination - Individual cues Combination Allo./phon. * (%) A A* B* N* A*+B* A*+N* * (%) A A* B* N* A</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this study we explored the role of both bottom- up and top-down hypotheses in learning the phonemic status of the sounds of two typologically different languages. We introduced a bottom-up cue based on acoustic similarity, and we used al- ready existing top-down cues to which we pro- vided a new extension. We tested these hypothe- ses on English and Japanese, providing the learner with an input that mirrors closely the linguistic and acoustic properties of each language. We showed, on the one hand, that the bottom-up cue is a very reliable source of information, across differ- ent levels of variation and even with small amount of data. Top-down cues, on the other hand, were found to be effective only on a subset of the data, which corresponds to the interesting contrasts that cause lexical variation. Their role becomes more relevant as the learner gets more linguistic experi- ence, and their combination with bottom-up cues shows that they can provide non-redundant infor- mation. Note, finally, that even if this work is based on a more realistic input compared to previ- ous studies, it still uses simplifying assumptions, like ideal word segmentation, and no low-level acoustic variability. Those assumptions are, how- ever, useful in quantifying the information that can ideally be extracted from the input, which is a nec- essary preliminary step before modeling how this input is used in a cognitively plausible way. Inter- ested readers may refer to ) for a more learning- oriented approach, where some of the assumptions made here about high level representations are re- laxed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Allophonic variation of French /r/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5 :</head><label>5</label><figDesc>Same-different scores for different cues and their combinations with HMM-allophones, as a function of corpus size. * (%) refers to the proportion of the subset of contrasts associated with at least one minimal pair. The cues applied to this subset are marked with an asterisk (*)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Same-different scores for top-down cues on</head><label>1</label><figDesc></figDesc><table>Random allophones, as a function of the average number of 

allophones per phoneme. M=Martin et al., B=Boruta, N= 

Normalized 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Same-different scores for bottom-up and top-down 

cues on HMM-based allophones, as a function of the 

average number of allophones per phoneme. A=Acoustic, 

M=Martin et al., B=Boruta, N= Normalized 

5 Analysis 

5.1 Why does the performance drop for 
realistic allophones? 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Proportion (in %) of allophonic pairs (allo), and 

non-allophonic pairs (¬ allo) associated with at least one 

lexical minimal pair, in Random and HMM allophones. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 : Same-different scores for different cues and their combinations with HMM-allophones, as a function of average</head><label>4</label><figDesc></figDesc><table>number of allophones per phonemes. 

-
English 
Japanese 
-
-
Individual cues 
Combination 
-
Individual cues 
Combination 
Size (hours) * (%) 
A 
A* 
B* 
N* 
A*+B* A*+N* * (%) 
A 
A* 
B* 
N* 
A*+B* A*+N* 
1 
9.87 
0.885 0.907 0.741 0.915 0.927 
0.969 34.78 0.890 0.883 0.835 0.915 0.889 
0.934 
4 
18.3 
0.918 0.958 0.798 0.917 0.967 
0.989 48.00 0.917 0.939 0.860 0.937 0.938 
0.973 
8 
21.3 
0.916 0.964 0.837 0.942 0.971 
0.992 51.71 0.915 0.940 0.889 0.937 0.954 
0.977 
20 
24.4 
0.911 0.960 0.827 0.936 0.969 
0.994 58.12 0.921 0.954 0.865 0.912 0.945 
0.971 
40 
26.6 
0.916 0.965 0.840 0.950 0.971 
0.994 60.92 0.885 0.909 0.859 0.906 0.918 
0.946 
∞ 
34.82 
-
-
-
-
-
-
72.16 
-
-
-
-
-
-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project is funded in part by the Euro-pean Research Council (ERC-2011-AdG-295810 BOOTPHON), the Agence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, ANR-10-IDEX-0001-02 PSL*), the Fondation de France, the Ecole de Neurosciences de Paris, and the Région Ile de France (DIM cerveau et pensée). We thank Luc Boruta, Sanjeev Khudanpur, Isabelle Dautriche, Sharon Peperkamp and Benoit Crabbé for highly useful discussions and contributions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combining Indicators of Allophony</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Boruta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ACL-SRW</title>
		<meeting>ACL-SRW</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="88" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Indicateurs d&apos;allophonie et de phonémicité. Doctoral dissertation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Boruta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Université Paris-Diderot-Paris VII</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">In press. The hyperarticulation hypothesis of infant-directed speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cristia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Child Language</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A role for the developing lexicon in phonetic category acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomi</forename><forename type="middle">H</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="751" to="778" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A rudimentary lexicon and semantics help bootstrap phoneme acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdellah</forename><surname>Fourtassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 18th Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-consistency as an inductive bias in early language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdellah</forename><surname>Fourtassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Dunbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Meeting of the Cognitive Science Society</title>
		<meeting>the 36th Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Infants show a facilitation effect for native language phonetic perception between 6 and 12 months</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><forename type="middle">K</forename><surname>Kuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erica</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshisada</forename><surname>Deguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="13" to="21" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Shigeru Kiritani, and Paul Iverson</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spontaneous speech corpus of japanese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kikuo</forename><surname>Maekawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanae</forename><surname>Koiso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadaoki</forename><surname>Furui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<meeting><address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="947" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning phonemes with a protolexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Peperkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="124" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Infant sensitivity to distributional information can affect phonetic discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Werker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gerken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="101" to="111" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The acquisition of allophonic rules: Statistical learning with linguistic constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Peperkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rozenn</forename><forename type="middle">Le</forename><surname>Calvez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pierre</forename><surname>Nadal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="31" to="41" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Pitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dilley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiesling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fosler-Lussier</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Buckeye corpus of conversational speech</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised learning of vowel categories from infant-directed speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Vallabha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Werker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">33</biblScope>
			<biblScope unit="page">13273</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Crosslanguage speech perception: Evidence for perceptual reorganization during the first year of life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><forename type="middle">F</forename><surname>Werker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Tees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infant Behavior and Development</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="63" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kershaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Odell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ollason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Valtchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Woodland</surname></persName>
		</author>
		<title level="m">The HTK Book Version 3.4</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
