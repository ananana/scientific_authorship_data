<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combining Natural Logic and Shallow Reasoning for Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University Stanford</orgName>
								<orgName type="institution" key="instit2">Stanford University Stanford</orgName>
								<orgName type="institution" key="instit3">Stanford University Stanford</orgName>
								<address>
									<postCode>94305, 94305, 94305</postCode>
									<region>CA, CA, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neha</forename><surname>Nayak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University Stanford</orgName>
								<orgName type="institution" key="instit2">Stanford University Stanford</orgName>
								<orgName type="institution" key="instit3">Stanford University Stanford</orgName>
								<address>
									<postCode>94305, 94305, 94305</postCode>
									<region>CA, CA, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University Stanford</orgName>
								<orgName type="institution" key="instit2">Stanford University Stanford</orgName>
								<orgName type="institution" key="instit3">Stanford University Stanford</orgName>
								<address>
									<postCode>94305, 94305, 94305</postCode>
									<region>CA, CA, CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combining Natural Logic and Shallow Reasoning for Question Answering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="442" to="452"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Broad domain question answering is often difficult in the absence of structured knowledge bases, and can benefit from shallow lexical methods (broad coverage) and logical reasoning (high precision). We propose an approach for incorporating both of these signals in a unified framework based on natural logic. We extend the breadth of inferences afforded by natural logic to include relational entailment (e.g., buy → own) and meronymy (e.g., a person born in a city is born the city&apos;s country). Furthermore, we train an evaluation function-akin to gameplaying-to evaluate the expected truth of candidate premises on the fly. We evaluate our approach on answering multiple choice science questions, achieving strong results on the dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering is an important task in NLP, and becomes both more important and more diffi- cult when the answers are not supported by hand- curated knowledge bases. In these cases, view- ing question answering as textual entailment over a very large premise set can offer a means of gen- eralizing reliably to open domain questions.</p><p>A natural approach to textual entailment is to treat it as a logical entailment problem. How- ever, this high-precision approach is not feasible in cases where a formal proof is difficult or impossi- ble. For example, consider the following hypothe- sis (H) and its supporting premise (P) for the ques- tion Which part of a plant produces the seeds?: P: Ovaries are the female part of the flower, which pro- duces eggs that are needed for making seeds.</p><p>H: A flower produces the seeds.</p><p>This requires a relatively large amount of infer- ence: the most natural atomic fact in the sentence is that ovaries produce eggs. These inferences are feasible in a limited domain, but become difficult the more open-domain reasoning they require. In contrast, even a simple lexical overlap classifier could correctly predict the entailment. In fact, such a bag-of-words entailment model has been shown to be surprisingly effective on the Recog- nizing Textual Entailment (RTE) challenges <ref type="bibr" target="#b27">(MacCartney, 2009</ref>). On the other hand, such methods are also notorious for ignoring even trivial cases of nonentailment that are easy for natural logic, e.g., recognizing negation in the example below: P: Eating candy for dinner is an example of a poor health habit.</p><p>H: Eating candy is an example of a good health habit.</p><p>We present an approach to leverage the bene- fits of both methods. Natural logic -a proof the- ory over the syntax of natural language -offers a framework for logical inference which is already familiar to lexical methods. As an inference sys- tem searches for a valid premise, the candidates it explores can be evaluated on their similarity to a premise by a conventional lexical classifier.</p><p>We therefore extend a natural logic inference engine in two key ways: first, we handle rela- tional entailment and meronymy, increasing the total number of inferences that can be made. We further implement an evaluation function which quickly provides an estimate for how likely a can- didate premise is to be supported by the knowl- edge base, without running the full search. This can then more easily match a known premise de- spite still not matching exactly.</p><p>We present the following contributions: (1) we extend the classes of inferences NaturalLI can per- form on real-world sentences by incorporating re- lational entailment and meronymy, and by operat-ing over dependency trees; (2) we augment Nat- uralLI with an evaluation function to provide an estimate of entailment for any query; and (3) we run our system over the Aristo science questions corpus, achieving the strong results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We briefly review natural logic and NaturalLI - the existing inference engine we use. Much of this paper will extend this system, with additional inferences (Section 3) and a soft lexical classifier (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Natural Logic</head><p>Natural logic is a formal proof theory that aims to capture a subset of logical inferences by appeal- ing directly to the structure of language, without needing either an abstract logical language (e.g., Markov Logic Networks; <ref type="bibr" target="#b32">Richardson and Domingos (2006)</ref>) or denotations (e.g., semantic pars- ing; <ref type="bibr" target="#b22">Liang and Potts (2015)</ref>). We use the logic in- troduced by the NatLog system <ref type="bibr" target="#b24">(MacCartney and Manning, 2007;</ref><ref type="bibr" target="#b25">2008;</ref><ref type="bibr" target="#b27">2009)</ref>, which was in turn based on earlier theoretical work on Monotonicity Calculus <ref type="bibr" target="#b37">(van Benthem, 1986;</ref><ref type="bibr" target="#b34">Sánchez Valencia, 1991)</ref>. We adopt the precise semantics of <ref type="bibr" target="#b20">Icard and Moss (2014)</ref>; we refer the reader to this paper for a more thorough introduction to the formalism.</p><p>At a high level, natural logic proofs operate by mutating spans of text to ensure that the mutated sentence follows from the original -each step is much like a syllogistic inference. Each mutation in the proof follows three steps:</p><p>1. An atomic lexical relation is induced by ei- ther inserting, deleting or mutating a span in the sentence. For example, in <ref type="figure">Figure 1</ref>, mu- tating The to No induces the relation; mu- tating cat to carnivore induces the relation. The relations ≡ and are variants of entail- ment; and are variants of negation. 2. This lexical relation between words is pro- jected up to yield a relation between sen- tences, based on the polarity of the token. For instance, The cat eats animals some carni- vores eat animals. We explain this in more detail below. 3. These sentence level relations are joined together to produce a relation between a premise, and a hypothesis multiple mutations away. For example in <ref type="figure">Figure 1</ref>, if we join , ≡, , and , we get negation ( ).</p><p>The notion of projecting a relation from a lexi- cal item to a sentence is important to understand. 1 To illustrate, cat animal, and some cat meows some animal meows (recall, denotes entail- ment), but no cat barks no animal barks. De- spite differing by the same lexical relation, the sentence-level relation is different in the two cases.</p><p>We appeal to two important concepts: mono- tonicity -a property of arguments to natural lan- guage operators; and polarity -a property of to- kens. From the example above, some is monotone in its first argument (i.e., cat or animal), and no is antitone in its first argument. This means that the first argument to some is allowed to mutate up the specified hierarchy (e.g., hypernymy), whereas the first argument to no is allowed to mutate down.</p><p>Polarity is a property of tokens in a sentence de- termined by the operators acting on it. All lexical items have upward polarity by default; monotone operators -like some, several, or a few -preserve polarity. Antitone operators -like no, not, and all (in its first argument) -reverse polarity. For ex- ample, mice in no cats eat mice has downward po- larity, whereas mice in no cats don't eat mice has upward polarity (it is in the scope of two down- ward monotone operators).</p><p>As a final note, although we refer to the mono- tonicity calculus described above as natural logic, this formalism is only one of many possible nat- ural logics. For example, <ref type="bibr" target="#b29">McAllester and Givan (1992)</ref> introduce a syntax for first order logic which they call Montagovian syntax. This syntax has two key advantages over first order logic: first, the "quantifier-free" version of the syntax (roughly equivalent to the monotonicity calculus we use) is computationally efficient while still handling limited quantification. Second, the syntax more closely mirrors that of natural language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">NaturalLI</head><p>We build our extensions within the framework of NaturalLI, introduced by <ref type="bibr" target="#b0">Angeli and Manning (2014)</ref>. NaturalLI casts inference as a search prob- lem: given a hypothesis and an arbitrarily large corpus of text, it searches through the space of lex- ical mutations (e.g., cat → carnivore), with asso- ciated costs, until a premise is found.</p><p>An example search using NaturalLI is given in <ref type="figure">Figure 1</ref>. The relations along the edges denote re-  <ref type="figure">Figure 1</ref>: An illustration of NaturalLI searching for a candidate premise to support the hypothesis at the root of the tree. We are searching from a hypothesis no carnivores eat animals, and find a contradicting premise the cat ate a mouse. The edge labels denote Natural Logic inference steps.</p><p>lations between the associated sentences -i.e., the projected lexical relations from Section 2.2. Im- portantly, and in contrast with traditional entail- ment systems, NaturalLI searches over an arbitrar- ily large knowledge base of textual premises rather than a single premise/hypothesis pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Improving Inference in NaturalLI</head><p>We extend NaturalLI in three ways to improve its coverage. We adapt the search algorithm to oper- ate over dependency trees rather than the surface forms (Section 3.1). We enrich the class of in- ferences warranted by natural logic beyond hyper- nymy and operator rewording to also encompass meronymy and relational entailment (Section 3.2). Lastly, we handle token insertions during search more elegantly (Section 3.3).</p><p>The general search algorithm in NaturalLI is parametrized as follows: First, an order is cho- sen to traverse the tokens in a sentence. For ex- ample, the original paper traverses tokens left-to- right. At each token, one of three operations can be performed: deleting a token (corresponding to inserting a word in the proof derivation), mutating a token, and inserting a token (corresponding to deleting a token in the proof derivation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Natural logic over Dependency Trees</head><p>Operating over dependency trees rather than a to- ken sequence requires reworking (1) the semantics of deleting a token during search, and (2) the order in which the sentence is traversed.</p><p>We recently defined a mapping from Stanford Dependency relations to the associated lexical re- lation deleting the dependent subtree would in- duce ( <ref type="bibr" target="#b1">Angeli et al., 2015</ref>). We adapt this mapping to yield the relation induced by inserting a given dependency edge, corresponding to our deletions in search; we also convert the mapping to use <ref type="bibr">Universal Dependencies (de Marneffe et al., 2014)</ref>. This now lends a natural deletion operation: at a given node, the subtree rooted at that node can be deleted to induce the associated natural logic rela- tion.</p><p>For example, we can infer that all truly notori- ous villains have lairs from the premise all villains have lairs by observing that deleting an amod arc induces the relation , which in the downward polarity context of villains ↓ projects to (entail- ment):</p><formula xml:id="formula_0">All ↑ truly ↓ notorious ↓ villains ↓ have ↑ lairs ↑ . operator nsubj amod advmod dobj</formula><p>An admittedly rare but interesting subtlety in the order we chose to traverse the tokens in the sentence is the effect mutating an operator has on the polarity of its arguments. For example, mu- tating some to all changes the polarity of its first argument. There are cases where we must mutate the argument to the operator before the operator itself, as well as cases where we must mutate the operator before its arguments. Consider, for in- stance: where we must first mutate some to all. There- fore, our traversal first visits each operator, then performs a breadth-first traversal of the tree, and then visits each operator a second time.</p><formula xml:id="formula_1">P</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Meronymy and Relational Entailment</head><p>Although natural logic and the underlying mono- tonicity calculus has only been explored in the context of hypernymy, the underlying framework can be applied to any partial order.</p><p>Natural language operators can be defined as a mapping from denotations of objects to truth val- ues.  dered by the subset operator, corresponding to or- dering by hypernymy over the words. <ref type="bibr">2</ref> However, hypernymy is not the only useful partial ordering over denotations. We include two additional or- derings as motivating examples: relational entail- ment and meronymy.</p><p>Relational Entailment For two verbs v 1 and v 2 , we define v 1 ≤ v 2 if the first verb entails the sec- ond. In many cases, a verb v 1 may entail a verb v 2 even if v 2 is not a hypernym of v 1 . For exam- ple, to sell something (hopefully) entails owning that thing. Apart from context-specific cases (e.g., orbit entails launch only for man-made objects), these hold largely independent of context. Note that the usual operators apply to relational entail- ments -if all cactus owners live in Arizona then all cactus sellers live in Arizona.</p><p>This information was incorporated using data from VERBOCEAN ( <ref type="bibr" target="#b8">Chklovski and Pantel, 2004)</ref>, adapting the confidence weights as transition costs. VERBOCEAN uses lexicosyntactic pat- terns to score pairs of verbs as candidate par- ticipants in a set of relations. We approximate the VERBOCEAN relations stronger -than(v 1 , v 2 ) (e.g., to kill is stronger than to wound) and happens-before(v 2 , v 1 ) (e.g., buying happens be- fore owning) to indicate that v 1 entails v 2 . These verb entailment transitions are incorporated us- ing costs derived from the original weights from <ref type="bibr" target="#b8">Chklovski and Pantel (2004)</ref>.</p><p>Meronymy The most salient use-case for meronymy is with locations. For example, if Obama was born in Hawaii, then we know that Obama was born in America, because Hawaii is a meronym of (part of) America. Unlike relational entailment and hypernymy, meronymy is operated on by a distinct set of operators: if Hawaii is an island, we cannot necessarily entail that America is an island.</p><p>We semi-automatically collect a set of 81 op- erators (e.g., born in, visited) which then com- pose in the usual way with the conventional op- erators (e.g., some, all). These operators consist of dependency paths of length 2 that co-occurred in newswire text with a named entity of type PER- SON and two different named entities of type LO- CATION, such that one location was a meronym of the other. All other operators are considered non- monotone with respect to the meronym hierarchy.</p><p>Note that these are not the only two orders that can be incorporated into our framework; they just happen to be two which have lexical resources available and are likely to be useful in real-world entailment tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Removing the Insertion Transition</head><p>Inserting words during search poses an inherent problem, as the space of possible words to insert at any position is on the order of the size of the vocabulary. In NaturalLI, this was solved by keep- ing a trie of possible insertions, and using that to prune this space. This is both computationally slow and adapts awkwardly to a search over de- pendency trees.</p><p>Therefore, this work instead opts to perform a bidirectional search: when constructing the knowledge base, we add not only the original sentence but also all entailments with subtrees deleted. For example, a premise of some furry cats have tails would yield two facts for the knowledge base: some furry cats have tails as well as some cats have tails. For this, we use the process de- scribed in <ref type="bibr" target="#b1">Angeli et al. (2015)</ref> to generate short entailed sentences from a long utterance using nat- ural logic. This then leaves the reverse search to only deal with mutations and inference insertions, which are relatively easier.</p><p>The new challenge this introduces, of course, is the additional space required to store the new facts. To mitigate this, we hash every fact into a 64 bit integer, and store only the hashed value in the knowledge base. We construct this hash func- tion such that it operates over a bag of edges in the dependency tree. This has two key properties: it allows us to be invariant to the word order of of the sentence, and more importantly it allows us to run our search directly over modifications to this hash function.</p><p>To elaborate, we notice that each of the two classes of operations our search is performing are done locally over a single dependency edge. When adding an edge, we can simply take the XOR of the hash saved in the parent state and the hash of the added edge. When mutating an edge, we XOR the hash of the parent state with the edge we are mutating, and again with the mutated edge. In this way, each search node need only carry an 8 byte hash, local information about the edge cur- rently being considered (8 bytes), global infor- mation about the words deleted during search (5 bytes), a 3 byte backpointer to recover the infer- ence path, and 8 bytes of operator metadata -32 bytes in all, amounting to exactly half a cache line on our machines. This careful attention to data structures and memory layout turn out to have a large impact on runtime efficiency. More details are given in Angeli (2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">An Evaluation Function for NaturalLI</head><p>There are many cases -particularly as the length of the premise and the hypothesis grow -where despite our improvements NaturalLI will fail to find any supporting premises; for example: P: Food serves mainly for growth, energy and body re- pair, maintenance and protection.</p><p>H: Animals get energy for growth and repair from food.</p><p>In addition to requiring reasoning with multi- ple implicit premises (a concomitant weak point of natural logic), a correct interpretation of the sen- tence requires fairly nontrivial nonlocal reasoning: Food serves mainly for x → Animals get x from food.</p><p>Nonetheless, there enough lexical clues in the sentence that even a simple entailment classifier would get the example correct. We build such a classifier and adapt it as an evaluation function in- side NaturalLI in case no premises are found dur- ing search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A Standalone Entailment Classifier</head><p>Our entailment classifier is designed to be as do- main independent as possible; therefore we de- fine only 5 unlexicalized real-valued features, with an optional sixth feature encoding the score out- put by the Solr information extraction system (in turn built upon Lucene). In fact, this classifier is a stronger baseline than it may seem: evaluating the system on RTE-3 ( <ref type="bibr" target="#b17">Giampiccolo et al., 2007</ref>) yielded 63.75% accuracy -2 points above the me- dian submission.</p><p>All five of the core features are based on an alignment of keyphrases between the premise and the hypothesis. A keyphrase is defined as a span of text which is either (1) a possibly empty se- quence of adjectives and adverbs followed by a sequence of nouns, and optionally followed by ei- ther of or the possessive marker ('s), and another noun (e.g., sneaky kitten or pail of water); (2) a possibly empty sequence of adverbs followed by a verb (e.g., quietly pounce); or (3) a gerund fol- lowed by a noun (e.g., flowing water). The verb to be is never a keyphrase. We make a distinction between a keyphrase and a keyword -the latter is a single noun, adjective, or verb.</p><p>We then align keyphrases in the premise and hypothesis by applying a series of sieves. First, all exact matches are aligned to each other. Then, prefix or suffix matches are aligned, then if either keyphrase contains the other they are aligned as well. Last, we align a keyphrase in the premise p i to a keyphrase in the hypothesis h k if there is an alignment between p i−1 and h k−1 and between p i+1 and h k+1 . This forces any keyphrase pair which is "sandwiched" between aligned pairs to be aligned as well. An example alignment is given in <ref type="figure">Figure 3</ref>.</p><p>Features are extracted for the number of align- ments, the numbers of alignments which do and do not match perfectly, and the number of keyphrases in the premise and hypothesis which were not aligned. A feature for the Solr score of the premise given the hypothesis is optionally included; we re- visit this issue in the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">An Evaluation Function for Search</head><p>A version of the classifier constructed in Sec- tion 4.1, but over keywords rather than keyphrases can be incorporated directly into NaturalLI's search to give a score for each candidate premise Heat energy is being transferred when a stove is used to boil water in a pan.</p><p>When you heat water on a stove, thermal energy is transferred. <ref type="figure">Figure 3</ref>: An illustration of an alignment between a premise and a hypothesis. Keyphrases can be multiple words (e.g., heat energy), and can be approximately matched (e.g., to thermal energy). In the premise, used, boil and pan are unaligned. Note that heat water is incorrectly tagged as a compound noun.</p><p>visited. This can be thought of as analogous to the evaluation function in game-playing search -even though an agent cannot play a game of Chess to completion, at some depth it can apply an evalua- tion function to its leaf states.</p><p>Using keywords rather than keyphrases is in general a hindrance to the fuzzy alignments the system can produce. Importantly though, this al- lows the feature values to be computed incremen- tally as the search progresses, based on the score of the parent state and the mutation or deletion being performed. For instance, if we are delet- ing a word which was previously aligned perfectly to the premise, we would subtract the weight for a perfect and imperfect alignment, and add the weight for an unaligned premise keyphrase. This has the same effect as applying the trained clas- sifier to the new state, and uses the same weights learned for this classifier, but requires substantially less computation.</p><p>In addition to finding entailments from candi- date premises, our system also allows us to en- code a notion of likely negation. We can consider the following two statements na¨ıvelyna¨ıvely sharing ev- ery keyword. Each token marked with its polarity:</p><formula xml:id="formula_2">P: some ↑ cats ↑ have ↑ tails ↑ H: no ↑ cats ↓ have ↓ tails ↓</formula><p>However, we note that all of the keyword pairs are in opposite polarity contexts. We can therefore define a pair of keywords as matching in NaturalLI if the following two conditions hold: (1) their lem- matized surface forms match exactly, and (2) they have the same polarity in the sentence. The second constraint encodes a good approximation for nega- tion. To illustrate, consider the polarity signatures of common operators:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Operators</head><p>Subj. polarity Obj. polarity Some, few, etc.</p><formula xml:id="formula_3">↑ ↑ All, every, etc. ↓ ↑ Not all, etc. ↑ ↓ No, not, etc. ↓ ↓ Most, many, etc.</formula><p>- ↑</p><p>We note that most contradictory operators (e.g., some/no; all/not all) induce the exact opposite po- larity on their arguments. Otherwise, pairs of op- erators which share half their signature are usually compatible with each other (e.g., some and all).</p><p>This suggests a criterion for likely negation: If the highest classifier score is produced by a con- tradictory candidate premise, we have reason to believe that we may have found a contradiction. To illustrate with our example, NaturalLI would mutate no cats have tails to the cats have tails, at which point it has found a contradictory candi- date premise which has perfect overlap with the premise some cats have tails. Even had we not found the exact premise, this suggests that the hy- pothesis is likely false.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>This work is similar in many ways to work on rec- ognizing textual entailment -e.g., <ref type="bibr" target="#b35">Schoenmackers et al. (2010)</ref>, <ref type="bibr" target="#b4">Berant et al. (2011)</ref>, <ref type="bibr" target="#b21">Lewis and Steedman (2013)</ref>. In the RTE task, a single premise and a single hypothesis are given as in- put, and a system must return a judgment of either entailment or nonentailment (in later years, nonen- tailment is further split into contradiction and in- dependence). These approaches often rely on alignment features, similar to ours, but do not gen- erally scale to large premise sets (i.e., a compre- hensive knowledge base). The discourse commit- ments in <ref type="bibr" target="#b18">Hickl and Bensley (2007)</ref> can be thought of as similar to the additional entailed facts we add to the knowledge base <ref type="bibr">(Section 3.3)</ref>. In an- other line of work, <ref type="bibr" target="#b36">Tian et al. (2014)</ref> approach the RTE problem by parsing into Dependency Com- positional Semantics (DCS) ( <ref type="bibr" target="#b23">Liang et al., 2011</ref>). This work particularly relevant in that it also in- corporates an evaluation function (using distribu- tional similarity) to augment their theorem prover -although in their case, this requires a transla- tion back and forth between DCS and language. <ref type="bibr">Beltagy et al. (To appear 2016)</ref> takes a similar ap- proach, but encoding distributional information di- rectly in entailment rules in a Markov Logic Net- work ( <ref type="bibr" target="#b32">Richardson and Domingos, 2006</ref>).</p><p>Many systems make use of structured knowl- edge bases for question answering. Semantic parsing methods <ref type="bibr" target="#b39">(Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b23">Liang et al., 2011</ref>) use knowledge bases like Freebase to find support for a complex ques- tion. Knowledge base completion (e.g., <ref type="bibr" target="#b7">Chen et al. (2013)</ref>, <ref type="bibr" target="#b6">Bordes et al. (2011)</ref>, or <ref type="bibr" target="#b33">Riedel et al. (2013)</ref>) can be thought of as entailment, predict- ing novel knowledge base entries from the origi- nal database. In contrast, this work runs inference over arbitrary text without needing a structured knowledge base. Open IE ( <ref type="bibr" target="#b38">Wu and Weld, 2010;</ref><ref type="bibr" target="#b28">Mausam et al., 2012</ref>) QA approaches -e.g., <ref type="bibr" target="#b15">Fader et al. (2014)</ref> are closer to operating over plain text, but still requires structured extractions.</p><p>Of course, this work is not alone in attempting to incorporate strict logical reasoning into ques- tion answering systems. The COGEX system ( <ref type="bibr" target="#b30">Moldovan et al., 2003</ref>) incorporates a theorem prover into a QA system, boosting overall per- formance on the TREC QA task. Similarly, Wat- son ( <ref type="bibr" target="#b16">Ferrucci et al., 2010)</ref> incorporates logical rea- soning components alongside shallower methods. This work follows a similar vein, but both the theorem prover and lexical classifier operate over text, without requiring either the premises or ax- ioms to be in logical forms.</p><p>On the Aristo corpus we evaluate on, <ref type="bibr" target="#b19">Hixon et al. (2015)</ref> proposes a dialog system to augment a knowledge graph used for answering the ques- tions. This is in a sense an oracle measure, where a human is consulted while answering the ques- tion; although, they show that their additional ex- tractions help answer questions other than the one the dialog was collected for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We evaluate our entailment system on the Regents Science Exam portion of the Aristo dataset <ref type="bibr" target="#b9">(Clark et al., 2013;</ref><ref type="bibr" target="#b12">Clark, 2015)</ref>. The dataset consists of a collection of multiple-choice science questions from the New York Regents 4 th Grade Science Ex- ams <ref type="bibr" target="#b31">(NYSED, 2014)</ref>. Each multiple choice option is translated to a candidate hypotheses. A large corpus is given as a knowledge base; the task is to find support in this knowledge base for the hy- pothesis.</p><p>Our system is in many ways well-suited to the dataset. Although certainly many of the facts re- quire complex reasoning (see Section 6.4), the ma- jority can be answered from a single premise. Un- like FraCaS <ref type="figure">(Cooper et al., 1996)</ref> or the RTE chal- lenges, however, the task does not have explicit premises to run inference from, but rather must in- fer the truth of the hypothesis from a large collec- tion of supporting text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data Processing</head><p>We make use of two collections of unlabeled cor- pora for our experiments. The first of these is the Barron's study guide (BARRON'S), consisting of 1200 sentences. This is the corpus used by <ref type="bibr" target="#b19">Hixon et al. (2015)</ref> for their conversational dia- log engine Knowbot, and therefore constitutes a more fair comparison against their results. How- ever, we also make use of the full SCITEXT cor- pus ( <ref type="bibr" target="#b10">Clark et al., 2014</ref>). This corpus consists of 1 316 278 supporting sentences, including the Bar- ron's study guide alongside simple Wikipedia, dic- tionaries, and a science textbook.</p><p>Since we lose all document context when searching over the corpus with NaturalLI, we first pre-process the corpus to resolve high-precision cases of pronominal coreference, via a set of very simple high-precision sieves. This finds the most recent candidate antecedent (NP or named entity) which, in order of preference, matches either the pronoun's animacy, gender, and number. Filter- ing to remove duplicate sentences and sentences containing non-ASCII characters yields a total of 822 748 facts in the corpus.</p><p>These sentences were then indexed using Solr. The set of promising premises for the soft align- ment in Section 4, as well as the Solr score fea- ture in the lexical classifier (Section 4.1), were obtained by querying Solr using the default sim- ilarity metric and scoring function. On the query side, questions were converted to answers using the same methodology as <ref type="bibr" target="#b19">Hixon et al. (2015)</ref>. In cases where the question contained multiple sen- tences, only the last sentence was considered. As discussed in Section 6.4, we do not attempt rea- soning over multiple sentences, and the last sen- tence is likely the most informative sentence in a longer passage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Training an Entailment Classifier</head><p>To train a soft entailment classifier, we needed a set of positive and negative entailment instances. These were collected on Mechanical Turk. In par- ticular, for each true hypothesis in the training set and for each sentence in the Barron's study guide, we found the top 8 results from Solr and consid- ered these to be candidate entailments. These were then shown to Turkers, who decided whether the premise entailed the hypothesis, the hypothesis en- tailed the premise, both, or neither. Note that each pair was shown to only one Turker, lowering the cost of data collection, but consequently resulting in a somewhat noisy dataset. The data was aug- mented with additional negatives, collected by tak- ing the top 10 Solr results for each false hypothesis in the training set. This yielded a total of 21 306 examples.</p><p>The scores returned from NaturalLI incorporate negation in two ways: if NaturalLI finds a contra- dictory premise, the score is set to zero. If Natu- ralLI finds a soft negation (see Section 4.2), and did not find an explicit supporting premise, the score is discounted by 0.75 -a value tuned on the training set. For all systems, any premise which did not contain the candidate answer to the multi- ple choice query was discounted by a value tuned on the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experimental Results</head><p>We present results on the Aristo dataset in <ref type="table" target="#tab_3">Table 1</ref>, alongside prior work and strong baselines. In all cases, NaturalLI is run with the evaluation func- tion enabled; the limited size of the text corpus and the complexity of the questions would cause the basic NaturalLI system to perform poorly. The test set for this corpus consists of only 68 examples, and therefore both perceived large differences in model scores and the apparent best system should be interpreted cautiously. NaturalLI consistently achieves the best training accuracy, and is more stable between configurations on the test set. For instance, it may be consistently discarding lexi- cally similar but actually contradictory premises that often confuse some subset of the baselines.</p><p>KNOWBOT is the dialog system presented in <ref type="bibr" target="#b19">Hixon et al. (2015</ref>  variants of the system: held-out is the system's performance when it is not allowed to use the di- alog collected from humans for the example it is answering; oracle is the full system. Note that the oracle variant is a human-in-the-loop system. We additionally present three baselines. The first simply uses Solr's IR confidence to rank en- tailment (Solr Only in <ref type="table" target="#tab_3">Table 1</ref>). The max IR score of any premise given a hypothesis is taken as the score for that hypothesis. Furthermore, we report results for the entailment classifier defined in Sec- tion 4.1 (Classifier), optionally including the Solr score as a feature. We also report performance of the evaluation function in NaturalLI applied di- rectly to the premise and hypothesis, without any inference (Evaluation Function).</p><p>Last, we evaluate NaturalLI with the improve- ments presented in this paper (NaturalLI in Ta- ble 1). We additionally tune weights on our train- ing set for a simple model combination with (1) Solr (with weight 6:1 for NaturalLI) and (2) the standalone classifier (with weight 24:1 for Nat- uralLI). Empirically, both parameters were ob- served to be fairly robust.</p><p>To  an associated 500 example training set (and 249 example development set). These are substantially more difficult as they contain a far larger num- ber of questions that require an understanding of a more complex process. Nonetheless, the trend illustrated in <ref type="table" target="#tab_3">Table 1</ref> holds for this larger set, as shown in <ref type="table" target="#tab_5">Table 2</ref>. Note that with a web-scale corpus, accuracy of an IR-based system can be pushed up to 51.4%; a PMI-based solver, in turn, achieves an accuracy of 54.8% -admittedly higher than our best system <ref type="bibr" target="#b11">(Clark et al., 2016)</ref>. <ref type="bibr">3</ref> An in- teresting avenue of future work would be to run NaturalLI over such a large web-scale corpus, and to incorporate PMI-based statistics into the evalu- ation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Discussion</head><p>We analyze some common types of errors made by the system on the training set. The most com- mon error can be attributed to the question requir- ing complex reasoning about multiple premises. 29 of 108 questions in the training set (26%) con- tain multiple premises. Some of these cases can be recovered from (e.g., This happens because the smooth road has less friction.), while others are trivially out of scope for our method (e.g., The vol- ume of water most likely decreased.). Although there is usually still some signal for which answer is most likely to be correct, these questions are fundamentally out-of-scope for the approach. Another class of errors which deserves mention are cases where a system produces the same score for multiple answers. This occurs fairly frequently in the standalone classifier (7% of examples in training; 4% loss from random guesses), and es- pecially often in NaturalLI (11%; 6% loss from random guesses). This offers some insight into why incorporating other models -even with low weight -can offer significant boosts in the per- <ref type="bibr">3</ref> Results from personal correspondence with the authors. formance of NaturalLI. Both this and the previous class could be further mitigated by having a notion of a process, as in <ref type="bibr" target="#b5">Berant et al. (2014)</ref>.</p><p>Other questions are simply not supported by any single sentence in the corpus. For example, A hu- man offspring can inherit blue eyes has no sup- port in the corpus that does not require significant multi-step inferences.</p><p>A remaining chunk of errors are simply classi- fication errors. For example, Water freezing is an example of a gas changing to a solid is marked as the best hypothesis, supported incorrectly by An ice cube is an example of matter that changes from a solid to a liquid to a gas, which after mutating water to ice cube matches every keyword in the hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have improved NaturalLI to be more robust for question answering by running the inference over dependency trees, pre-computing deletions, and incorporating a soft evaluation function for predicting likely entailments when formal support could not be found. Lastly, we show that relational entailment and meronymy can be elegantly incor- porated into natural logic. These features allow us to perform large-scale broad domain question answering, achieving strong results on the Aristo science exams corpus.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>:</head><label></label><figDesc>All felines have a tail H: Some cats have a tail where we must first mutate cat to feline, versus: P: All cats have a tail H: Some felines have a tail</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of monotonicity using different partial orders. (a) The monotonicity of all and some in their first arguments, over a domain of denotations. (b) An illustration of the born in monotone operator over the meronymy hierarchy, and the operator is an island as neither monotone or antitone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>). We report numbers for two</head><label></label><figDesc></figDesc><table>System 
Barron's 
SCITEXT 
Train Test 
Train Test 

KNOWBOT (held-out) 
45 
-
-
-
KNOWBOT (oracle) 
57 
-
-
-

Solr Only 
49 
42 
62 
58 
Classifier 
53 
52 
68 
60 
+ Solr 
53 
48 
66 
64 

Evaluation Function 
52 
54 
61 
63 
+ Solr 
50 
45 
62 
58 
NaturalLI 
52 
51 
65 
61 
+ Solr 
55 
49 
73 
61 
+ Solr + Classifier 
55 
49 
74 
67 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Accuracy on the Aristo science questions 
dataset. All NaturalLI runs include the evalua-
tion function. Results are reported using only the 
Barron's study guide or SCITEXT as the support-
ing KNOWBOT is the dialog system presented in 
Hixon et. al (2015). The held-out version uses ad-
ditional facts from other question's dialogs; the or-
acle version made use of human input on the ques-
tion it was answering. The test set did not exist at 
the time KNOWBOT was published. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results of our baselines and NaturalLI on 
a larger dataset of 250 examples. All NaturalLI 
runs include the evaluation function. 

</table></figure>

			<note place="foot" n="1"> For clarity we describe a simplified semantics here; NaturalLI implements the semantics described in Icard and Moss (2014).</note>

			<note place="foot" n="2"> Truth values are a trivial partial order corresponding to entailment: if t1 ≤ t2 (i.e., t1 t2), and you know that t1 is true, then t2 must be true.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their thoughtful comments. We gratefully acknowl-edge the support of the Allen Institute for Arti-ficial Intelligence, and in particular Peter Clark and Oren Etzioni for valuable discussions, as well as for access to the Aristo corpora and associ-ated preprocessing. We would also like to ac-knowledge the support of the Defense Advanced Research Projects Agency (DARPA) Deep Explo-ration and Filtering of Text (DEFT) Program un-der Air Force Research Laboratory (AFRL) con-tract no. FA8750-13-2-0040. Any opinions, findings, and conclusion or recommendations ex-pressed in this material are those of the authors and do not necessarily reflect the view of AI2, DARPA, AFRL, or the US government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NaturalLI: Natural logic inference for common sense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Leveraging linguistic structure for open domain information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin Jose Johnson</forename><surname>Premkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning Open Domain Knowledge From Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Representing meaning with a combination of logical and distributional models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Islam</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>To appear. Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Global learning of typed entailment rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling biological processes for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vander Linden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP<address><addrLine>Brittany Harding, and Peter Clark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3618</idno>
		<title level="m">Learning new facts from knowledge bases with neural tensor networks and semantic word vectors</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Verbocean: Mining the web for fine-grained semantic verb relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Chklovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A study of the knowledge base requirements for passing an elementary science test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AKBC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Automatic construction of inferencesupporting knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumithra</forename><surname>Bhakthavatsalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Kinkead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>AKBC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Combining retrieval, statistics, and inference to answer elementary science questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Elementary school science and math tests as a driver for AI: Take the Aristo challenge! AAAI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Using the framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dick</forename><surname>Crouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Van Eijck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Van Genabith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Jaspars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Milward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>The FraCaS Consortium</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Universal Stanford dependencies: A cross-linguistic typology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katri</forename><surname>Haverinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Open question answering over curated and extracted knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jmes</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">William</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Schlaefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Welty</surname></persName>
		</author>
		<title level="m">The AI behind Watson. The AI Magazine</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The third PASCAL recognizing textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACL-PASCAL workshop on textual entailment and paraphrasing. Association for Computational Linguistics</title>
		<meeting>of the ACL-PASCAL workshop on textual entailment and paraphrasing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A discourse commitment-based framework for recognizing textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Bensley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning knowledge graphs for question answering through conversational dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Recent progress on monotonicity. Linguistic Issues in Language Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Moss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Combined distributional and logical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="179" to="192" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Corpusbased semantics and pragmatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Linguistics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Natural logic for textual inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Modeling semantic containment and exclusion in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>In Coling</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An extended model of natural logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth international conference on computational semantics</title>
		<meeting>the eighth international conference on computational semantics</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Natural Language Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Open language learning for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Natural language syntax and first-order inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Givan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">COGEX: A logic prover for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Maiorano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The grade 4 elementary-level science test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nysed</forename></persName>
		</author>
		<ptr target="http://www.nysedregents.org/Grade4/Science/home.html" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="107" to="136" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin M</forename><surname>Marlin</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Studies on natural logic and categorial grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Víctor Manuel Sánchez</forename><surname>Valencia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>University of Amsterdam</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning first-order horn clauses from web text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schoenmackers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Logical inference on dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Essays in logical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Van Benthem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Open information extraction using wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel S Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
