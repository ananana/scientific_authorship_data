<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prototype Synthesis for Model Laws</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Burgess</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">UC Berkeley &amp; YouGov</orgName>
								<orgName type="institution" key="instit3">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenia</forename><surname>Giraudy</surname></persName>
							<email>eugenia.giraudy@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">UC Berkeley &amp; YouGov</orgName>
								<orgName type="institution" key="instit3">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Adar</surname></persName>
							<email>eadar@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">UC Berkeley &amp; YouGov</orgName>
								<orgName type="institution" key="instit3">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Prototype Synthesis for Model Laws</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1579" to="1588"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>State legislatures often rely on existing text when drafting new bills. Resource and expertise constraints, which often drive this copying behavior, can be taken advantage of by lobbyists and special interest groups. These groups provide model bills, which encode policy agendas, with the intent that the models become actual law. Unfortunately, model legislation is often opaque to the public-both in source and content. In this paper we present LOBBYBACK, a system that reverse engineers model legislation from observed text. LOBBYBACK identifies clusters of bills which have text reuse and generates &quot;prototypes&quot; that represent a canon-ical version of the text shared between the documents. We demonstrate that LOBBY-BACK accurately reconstructs model legislation and apply it to a dataset of over 550k bills.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Beginning in 2005, a number of states began passing "Stand Your Ground" laws-legal protec- tions for the use of deadly force in self-defense. Within a few years, at least two dozen states im- plemented a version of the this legislation <ref type="bibr" target="#b8">(Garrett and Jansa, 2015)</ref>. Though each state passed its own variant, there is striking similarity in the text of the legislation. While seemingly "viral" the expedient adoption of these laws was not the result of an organic diffusion process, but rather more centralized efforts. An interest group, the American Legislative Exchange Council (ALEC), drafted model legislation (in this case modeled on Florida's law) and lobbied to have the model law enacted in other states. While the influence of the lobbyists through model laws grows, the hidden nature of their original text (and source) creates a troubling opacity.</p><p>Reconstructing such hidden text through analy- sis of observed, potentially highly mutated, copies poses an interesting and challenging NLP prob- lem. We refer to this as the Dark Corpora prob- lem. Since legislatures are not required to cite the source of the text that goes into a drafted bill, the bills that share text are unknown beforehand. The first problem therein lies in identifying clusters of bills with reused text. Once a cluster is identi- fied, a second challenge is the reconstruction of the original or prototype bill that corresponds to the observed text. The usual circumstances under which a model law is adopted by individual states involves "mutation." This may be as simple as modifying parameters to the existing policy (e.g., changing the legal limit allowed of medical mar- ijuana possession to 3.0 ounces from 2.5) or can be more substantial, with significant additions or deletions of different conditions of a policy. Inter- estingly, the need to maintain the objectives of the law creates a pressure to retain a legally meaning- ful structure and precise language-thus changes need to satisfy the existing laws of the state but carry out the intent of the model. Both subtle changes of this type, and more dramatic ones, are of great interest to political scientists. A specific application, for example, may be predicting likely spots for future modifications as additional states adopt the law. Our challenge is to identify and rep- resent "prototype" sentences that capture the sim- ilarity of observed sentences while also capturing the variation.</p><p>In this paper we propose LOBBYBACK, a sys- tem that automatically identifies clusters of docu- ments that exhibit text reuse, and generates "pro- totypes" that represent a canonical version of text shared between the documents. In order to syn-thesize the prototypes, LOBBYBACK first extracts clusters of sentences, where each sentence per- tains to the same policy but can exhibit variation. LOBBYBACK then uses a greedy multi-sequence alignment algorithm to identify an approximation of the optimal alignment between the sentences. Prototype sentences are synthesized by comput- ing a consensus sentence from the multi-sentence alignment. As sentence variants are critical in un- derstanding the effect of the model legislation, we can not simply generate a single "common" sum- mary sentence as a prototype. Rather, LOBBY- BACK creates a data structure that captures this variability in text for display to the end-user.</p><p>With LOBBYBACK, end-users can quickly identify clusters of text reuse to better understand what type of policies are diffused across states. In other applications, sentence prototypes can be used by journalists and researchers to discover previously unknown model legislation and the in- volvement of lobbying organizations. For exam- ple, prototype text can be compared to the lan- guage or policy content of interest groups docu- ments and accompanied with qualitative research it can help discover which lobbyists have drafted this legislation.</p><p>We evaluated LOBBYBACK on the task of re- constructing 122 known model legislation docu- ments. Our system was able to achieve an aver- age of 0.6 F1 score based on the number of pro- totype sentences that had high similarity with sen- tences from the model legislation. We have also run LOBBYBACK on the entire corpus of state leg- islation (571,000 documents) from openstates.org as an open task. The system identified 4,446 clus- ters for which we generated prototype documents. We have released the resulting data set and code at http://github.com/mattburg/LobbyBack. LOB- BYBACK is novel in fully automating and scal- ing the pipeline of model-legislation reconstruc- tion. The output of this pipeline captures both the likely "source sentences" but also the variations of those sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>While no specific system or technique has fo- cused on the problem of legislative document re- construction, we find related work in a number of domains. Multi-document summarization (MDS), for example, can be used to partially model the un- derlying problem-generating a representative doc- ument from multiple sources. Extractive MDS, in particular, is promising in that representative sen- tences are identified.</p><p>Early work in extractive summarization include greedy approaches such as that proposed by <ref type="bibr" target="#b4">Carbonell and Goldstein (1998)</ref>. The algorithm uses an objective function which trades off between rel- evance and redundancy. Global optimization tech- niques attempt to generate "summaries" (selected sets of sentences or utterances) that maximize an objective based on informativeness, redundancy and/or length of summary. These have shown su- perior performance to greedy algorithms ( <ref type="bibr" target="#b25">Yih et al., 2007;</ref><ref type="bibr" target="#b9">Gillick et al., 2009)</ref>. Approaches based on neural networks have recently been proposed for ranking candidate sentences <ref type="bibr" target="#b3">(Cao et al., 2015)</ref>. Graph based methods, such as LexRank ( <ref type="bibr" target="#b6">Erkan and Radev, 2004</ref>), have also proven effective for MDS. Extensions to this approach combine sen- tence ranking with clustering in order to minimize redundancy ( <ref type="bibr" target="#b20">Qazvinian and Radev, 2008;</ref><ref type="bibr" target="#b23">Wan and Yang, 2008;</ref><ref type="bibr" target="#b2">Cai and Li, 2013</ref>). The C-LexRank algorithm <ref type="bibr" target="#b20">(Qazvinian and Radev, 2008)</ref>, in par- ticular, uses this combination and inspired our high level design. Similar to our approach, <ref type="bibr" target="#b24">Wang and Cardie (2013)</ref> propose a method for generat- ing templates of meeting summaries using multi- sequence alignment.</p><p>Though related, it is important to note that the objectives of summarization (informativeness, re- duced redundancy, etc.) are not entirely consistent with our task. For example, using the n-gram co- occurrence based ROUGE score would not be suf- ficient at evaluating LOBBYBACK. Our goal is to accurately reconstruct entire sentences of a hidden document given observed mutations of that docu- ment. Additionally, our goal is not simply to find a representative sentence that reflects the original document, but to capture the similarity and vari- ability of the text within a given "sentence cluster."</p><p>Within the political science and legal studies communities research has focused on manual ap- proaches to both understanding how model legis- lation impacts law and how policy ideas diffuse between bill text. As these studies are time con- suming, there is no large-scale or broad analysis of legislative materials. Rather, researchers have lim- ited their workload by focusing on a single topic (e.g., abortion <ref type="bibr" target="#b19">(Patton, 2003)</ref> and crime <ref type="bibr" target="#b16">(Kent and Carmichael, 2015)</ref>) or a single lobbying group (e.g., ALEC <ref type="bibr" target="#b14">(Jackman, 2013)</ref>. Similarly, those studying policy diffusion across US states have also limited their analysis to a few topics (e.g., same-sex marriage <ref type="bibr" target="#b12">(Haider-Markel, 2001)</ref>).</p><p>Recent attempts to automate the analysis of model legislation has had similar problems, as most researchers have limited their analysis to one interest group or a few relevant topics <ref type="bibr">(HertelFernandez and Kashin, 2015;</ref>. Hertel-Fernandez and Kashin proposed a super- vised model in which they train on hand labeled examples of state bills that borrow text and/or con- cepts from ALEC bills. The problem they focus on is different from ours. The motivation behind LOBBYBACK is that there exists many model leg- islation which we don't have access to and the goal is to try and reconstruct these documents with- out labeled training data. Jansa et al. propose a technique for inferring a network of policy diffu- sion for manually labeled clusters of bills. Both Jansa et al. and Hertel-Fernandaz and Kashin pro- pose techniques that only look at the problem of inferring whether two bills exhibit text reuse but unlike LOBBYBACK they do not attempt to infer whether specific policies (sentences) in the docu- ments are similar/different.</p><p>A related "dark corpora" problem, though at a far smaller scale, is the biblical "Q source" where hidden oral sources are reconstructed through tex- tual analysis <ref type="bibr" target="#b17">(Mournet, 2005</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition</head><p>Policy diffusion is a common phenomenon in state bills <ref type="bibr" target="#b10">(Gray, 1973;</ref><ref type="bibr" target="#b22">Shipan and Volden, 2008;</ref><ref type="bibr" target="#b0">Berry and Berry, 1990;</ref><ref type="bibr" target="#b12">Haider-Markel, 2001</ref>). Unlike members of the (Federal) Congress, few state legislators have the expertise, time, and staff to draft legislation. It is far easier for a legisla- tor to adapt existing legislative text than to write a bill from scratch. As a consequence, state leg- islatures have an increased willingness to adopt legislation drafted by interest groups or by legis- lators in other states ( . In addi- tion to states borrowing text from other legislators and lobbyists, another reason why bills can exhibit text reuse is when a new federal law passes and each state needs to modify its existing policy to conform with the new federal law.</p><p>The result of legislative copying, whether caused by diffusion between states, influence from a lobby or the passing of a new federal law, is sim- ilar: a cluster of bills will share very similar text,  <ref type="figure">Figure 1</ref>: Visualization of a multi-sentence align- ment (fragment) and resulting prototype sentence.</p><p>often varying only by implementation details of a given policy. The goal in constructing a prototype document-a representation of the "original" text- is to synthesize this document from the modified copies. In the case when the bill cluster was in- fluenced by one external source, such as lobby or passage of a federal bill, the ideal prototype doc- ument would capture the language that each bill borrowed from the source document. In the case when their is not one single document that influ- enced a cluster of bills, the prototype will still give a summary of a concise description of the diffused text between bills, providing fast insight into what text was shared and changed within a bill cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">State Legislation Corpus</head><p>We obtained the entire openstates.org corpus of state legislation, which includes 550,000 bills and 200,000 resolutions for all 50 states. While for some states this corpus includes data since 2007, for the majority of states we have data from 2010 on. We do not include data from Puerto Rico, where the text is in Spanish, and from Washing- ton DC, which includes many idiosyncrasies (e.g., correspondence from city commissions introduced as bills). On average, each state introduced 10,524 bills, with an average length of 1205 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LOBBYBACK Architecture</head><p>LOBBYBACK consists of 3 major components. The first component identifies clusters of bills that have text reuse. Then for each of these bill clus- ters, LOBBYBACK extracts and clusters the sen- tences from all documents. For each of the sen- tence clusters, LOBBYBACK synthesizes proto- type sentences in order to capture the similarity and variability of the sentences in the cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Clustering Bills</head><p>Groups of bills with significant text reuse repre- sent candidates for analysis as they have may have all copied from the same model legislation. There are a number of ways one could identify such clus- ters through text mining. In our implementation, we have opted to generate a network representa- tion of the bills and then use a network cluster- ing (i.e., "community-finding") algorithm to gen- erate the bill clusters. In our network representa- tion each node represents a state bill and weighted edges represent the degree to which two bills ex- hibit substantive text reuse. Since most pairs of bills do not have text reuse, we chose to use a network model because community finding algo- rithms work well on sparse data and do not require any parameter choice for the number of clusters. In the context of this paper, text reuse occurs when two state bills share:</p><p>1. Long passages of text, e.g. (sections of bills) that can differ in details. 2. These passages contain text of substantive nature to the topic of the bill (i.e., text that is not boilerplate).</p><p>In addition to text that describes policy, state bills also contain boilerplate text that is common to all bills from a particular state or to a particu- lar topic. Examples of legislative boilerplate in- clude: "Read first time 01/29/16. Referred to Committee on Higher Education" (meta-data de- scribing where the bill is in the legislative pro- cess); and "Safety clause. The general assembly hereby finds, determines, and declares . . . " (a standard clause included in nearly all legislation from Colorado, stating the eligibility of a bill to be petitioned with a referendum).</p><p>In order to identify pairs of bills that exhibit text reuse, we created an inverted index that contained n-grams ranging from size 4-8. We use Elastic- Search to implement the inverted index and com- puted the similarity between bills using the "More like This" (MLT) query <ref type="bibr" target="#b5">(Elastic, 2016)</ref>. The MLT query first selects the 100 highest scoring TF*IDF n-grams from a given document and uses those to form a search query. The MLT query is able to quickly compute the similarity between docu- ments and since it ranks the query terms by using TF*IDF the query text is more likely to be sub- stantive rather then boilerplate. The MLT query we used was configured to only return documents that matched at least 50% of the query's shingles and returned at most 100 documents per query. By implementing the similarity search using a TF*IDF cutoff we were able to scale the similar- ity computation while still maintaining our desire to identify reuse of substantive text.</p><p>The edges of the bill similarity network are computed by calculating pairwise similarity. Each bill is submitted as input for an MLT query and scored matches are returned by the search engine. Since the MLT query extracts n-grams only for the query document, the similarity function be- tween two documents d i and d j is not symmet- ric. We construct a symmetric bill similarity net- work by taking the average score of each (d i , d j ) and its reciprocal (d j , d i ). A non-existent edge is represented as an edge with score 0. We fur- ther reduce the occurrence of false-positive edges by removing all edges with a score lower than 0.1. The resulting network is very sparse, consist- ing of 35,346 bills that have 1 or more neighbors, 125,401 edges, and 3534 connected components that contain an average of 10 bills.</p><p>A specific connected component may contain more than one bill cluster. To isolate these clusters in the bill network we use the InfoMap commu- nity detection algorithm ( <ref type="bibr" target="#b21">Rosvall and Bergstrom, 2008)</ref>. We use the InfoMap algorithm because it has been to shown to be one of the best commu- nity detection algorithms and it is able to detect clusters at a finer granularity than other methods. Our corpus contains both bills that have passed and those that have not. Bills can often be re- introduced in their entirety after failing the previ- ous year. As we do not want to bias the clusters towards bills that are re-introduced more than oth- ers, we filter the clusters such that they only in- clude the earliest bill from each state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Prototype Synthesis</head><p>Once we have identified a cluster of bills that have likely emerged from a single "source" we would like to construct a plausible representation of that source. The prototype synthesizer achieves this by constructing a canonical document that captures the similarity and variability of the con- tent in a given bill cluster. The two main steps in prototype synthesis consists of clustering bill sen- tences and generating prototype sentences from the clusters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Sentence Clustering</head><p>Most state bills have a common structure, con- sisting of an introduction that describes the intent of the bill followed by sections that contain the law to be implemented. Each section of a bill is comprised of self-contained policy, usually con- sisting of a long sentence that describes the pol- icy and the implementation details of that policy. Each document is segmented into these policy sen- tences using the standard Python NLTK sentence extractor. Sentences are cleaned by removing spu- rious white space characters, surrounding punctu- ation and lower-casing each word. Once we have extracted all of the sentences for a given bill clus- ter, we compute the cosine similarity between all pairs of sentences which are represented using a unigram bag-of-words model. We used a simple unweighted bag of words model because in le- gal text stop words can be import 1 In this case, we are generating a similarity "matrix" capturing sentence-sentence similarity.</p><p>Given the similarity matrix, our next goal is to isolate clusters of variant sentences that likely came from the same source sentence. We elected to use the DBSCAN ( <ref type="bibr" target="#b7">Ester et al., 1996</ref>) algorithm to generate these clusters. The DBSCAN algo- rithm provides us with tunable parameters that can isolate better clusters. Specifically, the parameter controls the maximum distance between any two points in the same neighborhood. By varying we are able to control both the number of clus- ters and the amount of sentence variation within a cluster. A second reason for selecting DBSCAN is that the algorithm automatically deals with noisy data points, placing all points that are not close enough to other points in a separate cluster labeled <ref type="bibr">1</ref> The difference between the words "shall" and "may" for instance is important, the former requires that a specific ac- tion be put on a states budget while the later does not "noise." Since many sentences in a given bill clus- ter do not contribute to the reused text between bills, the noise cluster is useful for grouping those sentences together rather than having them be out- siders in "good" clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Multi-Sequence Alignment</head><p>Once we have sentence clusters we then syn- thesize a "prototype" sentence from all of the sen- tences in a given cluster. An ideal prototype "sen- tence" is one that simultaneously captures the sim- ilarity between each sentence in the cluster (the common sentence structures) and the variation be- tween the sentences in a cluster. For a simple pair of (partial) sentences, "The Department of Motor Vehicles retains the right to . . . " and "The Depart- ment of Transportation retains the right to . . . ", a prototype might be of the form, "The Department of { Motor Vehicles, Transportation } retains the rights to . . . " Our "sentence" is not strictly a sin- gle linear piece of text. Rather, we have a data structure that describes alternative sub-strings and captures variant text.</p><p>To generate this structure we propose an algo- rithm that computes an approximation of the opti- mal multi-sentence alignment (MSA) in the clus- ter and then generates a prototype sentence repre- senting a 'consensus' for sentences in the MSA.</p><p>We generate an MSA using a modified version of the iterative pairwise alignment algorithm de- scribed in <ref type="bibr" target="#b11">(Gusfield, 1997</ref>). The greedy algorithm builds a multi-alignment by iteratively applying the Needleman-Wunsch pairwise global alignment algorithm. Needleman-Wunsch computes the op- timal pairwise alignment by maximizing the align- ment score between two sentences. An align- ment score is calculated based on three param- eters: word matches, word mismatches (when a word appears in one sentence but not the other), and gaps (when the algorithm inserts a space in one of the sentences).</p><p>An MSA is generated by the following steps:</p><p>1. Construct an edit-distance matrix for all pairs of sentences 2. Construct an initial alignment between the two sentences with the smallest edit distance 3. Repeat this step k times:</p><p>(a) Select the sentence with the smallest av- erage edit distance to the current MSA. (b) Add the chosen sentence to the MSA by aligning it to the existing MSA.</p><p>The algorithm stops after the alignment has reached a size that is determined as a free param- eter k (user configured, but can be chosen to be the number of sentences in the cluster). As the al- gorithm execution is ordered on the edit distance between the current MSA and the next sentence to be added, the larger the MSA, the more variation we are allowing in the prototype sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Synthesizing Prototype Sentences</head><p>We synthesize a prototype sentence by finding a consensus sentence from all of the aligned sen- tences in the MSA for a given cluster. We achieve this by going through each "column" of the MSA and using the following rules to decide which to- ken will be used in the prototype. A token can be either a word in one of the sentences or a "space" that was inserted during the alignment process.</p><p>1. If there is a token that occurs in the major- ity of alignments (&gt; 50%) then that token is chosen. 2. If no token appears in a majority, then a spe- cial variable token is constructed that dis- plays all of the possible tokens in each sen- tence. For example the 1 st and 3 rd columns in <ref type="figure">Figure 1</ref>. 3. If a space is the majority token chosen then it is shown as a variable token with the second most common token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We provide experiments that evaluate all three components of LOBBYBACK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model Legislation Corpus</head><p>To test the effectiveness of LOBBYBACK in recreating a model bill, we first identified a set of known model bills. Our model legislation cor- pus consists of 1846 model bills that we found by searching on Google using the keywords "model law", "model policy" and "model legislation." Most of the corpus is comprised of bills from the conservative lobby group, American Legisla- tive Exchange Council (ALEC, 708 documents), its liberal counterpart, the State Innovative Ex- change (SIX, 269 documents) and the non-partisan Council of State Governments (CSG, 470 docu- ments), and the remainder (399) from smaller in- terest groups that focus on specific issues.</p><p>Using the clusters we previously described (Section 4.1), we found the most similar cluster to each model bill. This was done by first computing the set of neighbors (ego-network) for a model bill using the same procedure used in creating the bill similarity network. We then matched a bill cluster to the model legislation by finding the bill clus- ter that had the highest Jaccard similarity (based on the overlapping bills in each cluster) with the neighbor set of a model bill. Each test example in our evaluation data set consists of model bill and its corresponding bill cluster. The total number of model legislation documents that had matches in the state bill corpus was 360 documents.</p><p>Once we have an evaluation data set comprised of model bill/cluster pairs our goal is to compare the prototype sentences we infer for a cluster to the model bill that matches that cluster. Since we do not have ground truth on which sentences from the model bill match sentences in the documents that comprise the cluster we need to infer such la- bels. In order to identify which sentences from the model legislation actually get re-used in the bill cluster, we take the following steps:</p><p>1. Extract all sentences from each of the bills in a cluster and the sentences in the correspond- ing model legislation. 2. Compute the pairwise cosine similarity be- tween bill sentences and each of the model bill sentences using the same unigram bag- of-words model described in Section 4.2.1 3. Compute the "oracle" matching M * using the Munkres algorithm <ref type="bibr" target="#b18">(Munkres, 1957)</ref> The Munkres algorithm gives the best possible one-to-one matching between the sentences in the model legislation and the sentences in the bill clus- ters. There are some sentences in the model bill that are never used in actual state legislation (e.g., sentences that describe the intent of a law or in- structions of how to implement a model policy). Therefore we label model bill sentences in M * that match a bill sentence with a score greater than 0.85 as true matches (S * ) 2 . The final set of 122 evaluation examples consists of all model legis- lation/bill cluster pairs where more than 50% of model bill sentences have true matches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>While no specific baseline exists for our prob- lem, we implemented two alternatives to test against. The first, Random-Baseline, was imple- mented simply to show the performance of ran- domly constructing prototype documents. The second, LexRank-Baseline, implements a popular extractive summarization method.</p><p>Random-Baseline -The random-baseline randomly samples sentences from a given bill cluster. The number of sentences it samples is equal to the number of sentences in the optimal matching |M * | LexRank-Baseline -The LexRank baseline uses the exact same clustering algorithm as LOBBY- BACK except instead of synthesizing prototype sentences, it uses the LexRank algorithm ( <ref type="bibr" target="#b6">Erkan and Radev, 2004</ref>) to pick the most salient sentence from each of the sentence clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluating Bill Clusters</head><p>As described in Section 5.1, each model bill is associated with a set of bills that comprise its neighbors in the bill network. In order to evaluate how LOBBYBACK clusters bills we compare the inferred clusters to the corresponding neighbor set for each model bill.</p><p>The inferred cluster for a given model bill can exhibit false positive and false negative errors. False negatives, in which a bill that exists in the neighbor set of a model bill but is not contained in the inferred cluster, are easy to identify (allow- ing us to calculate recall). Precision, on the other hand, is more difficult as any "extra" bills in the inferred cluster are not necessarily incorrect. It is possible that there are bills which do exhibit text re-use with the model legislation but did not match via the ElasticSearch query used to construct the network. We have found that in practice the sec- ond component of LOBBYBACK, which clusters the sentences extracted from a bill cluster, is robust to false-positives due to DBSCAN's treatment of "noisy" data points. Because of this, we are more concerned with recall in the bill clustering module as any data lost in this step propagates through the rest of the system. <ref type="figure" target="#fig_1">Figure 3</ref> shows the precision, recall, and F1 scores for LOBBYBACK coupled with both the Louvain ( <ref type="bibr" target="#b1">Blondel et al., 2008)</ref> and InfoMap <ref type="bibr" target="#b21">(Rosvall and Bergstrom, 2008)</ref> network clustering al- gorithms. Because we do not know with abso- lute certainty which state bills are derived from model legislation, we would like to test our ap- proach at different levels of confidence. To do this we vary the threshold on the similarity scores (edge weights) of the ego-network determined for each model legislation. A normalized weight is computed by taking the score provided by Elas- ticSearch and dividing that edge weight weight by the maximum edge weight in the neighbor set (to control for the unbounded scores provided by ElasticSearch). We vary the threshold for this weight (ranging from 0 to 1) and calculate the pre- cision, recall, and F1 on a neighborhood set that is comprised of all bills that have a weight greater than the threshold. Higher threshold values means fewer state bills are included in the set, but they are increasingly similar to the model bill.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, recall stays high for the majority of threshold values. This indicates that both clustering algorithms do a good job at re- covering the bills that are the most similar to the model legislation. However, the two clustering al- gorithms differ somewhat in precision. Louvain, in particular performs worse, as it suffer from "res- olution" problems. While effective for networks with large clusters, Louvain can not isolate small groups, of which we have many. For this reason, we chose InfoMap as the method to use for the fi- nal implementation of LOBBYBACK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluating Sentence Clusters</head><p>We first evaluate the quality of the sentence clusters using the optimal matching M * described above. For each test example in the evaluation set we generate a prototype document using LOBBY- BACK and each of the baselines described above. We then compute a matching M between the pro- totype sentences and the model bill sentences (us- ing the same procedure described in 5.1), where S is the set of sentences in the prototype and S 0.85 is the set of sentences that match with a score greater than 0.85. We compute precision, P , as P = |S 0.85 |/|S|, and recall, R, as R = |S|/|S * |. <ref type="figure" target="#fig_2">Figure 4</ref> shows precision, recall and F1 scores for both baselines and LOBBYBACK. Each curve is generated by averaging the precision/recall/F1 scores computed for each of the examples in the test set. The x-axis represents the minimum bill cluster size of the test examples for which the score is computed. For example, a minimum clus- ter size would average over all test examples with at least 2 bills in a cluster. LOBBYBACK re- lies on the fact that if text was borrowed from a model bill, then it would have been borrowed by many of the bills in the cluster. By analyzing how LOBBYBACK performs with respect to the min- imum cluster size, we can determine how much evidence LOBBYBACK needs in order to construct clusters that correspond to sentences in the model bills. While the performance of LOBBYBACK and the LexRank baseline substantially improves over the random baseline, the different between LOB- BYBACK and LexRank for this task is negligi- ble. Since our cut-off similarity is 0.85, all sen- tences above the threshold are treated as true posi- tives, making the distinction between the LexRank baseline and system small. LOBBYBACK per- forms a little worse than LexRank for large cluster sizes because it is penalized for having space and variable tokens which don't occur in model bills. Space and variable tokens occur more frequently in prototype sentences in larger clusters because there is more variation in the sentence clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Evaluating Sentence Synthesis</head><p>The experiment in the previous section evalu- ated the quality of the sentence clusters by treat- ing all matching sentences with a similarity greater than 0.85 as true positives. Here we provide an evaluation of the synthesized sentences that LOBBYBACK generates and compare them to the LexRank baseline, which chooses the most salient sentence from each cluster. We evaluate the qual- ity of the synthesized prototype sentences by com- puting the word-based edit-distance between the prototype sentence with its corresponding model bill sentence in S for each test example.</p><p>Since the prototypes contain variable and space tokens which do not occur in the model bill sen- tences we modify the standard edit distance algo- rithm by not penalizing space tokens and allow- ing for any of the tokens that comprise a variable to be positive matches. In addition, we remove punctuation and lowercase all words in all of the sentences, regardless of method. We generate the results in <ref type="table">Table 1</ref> by averaging the edit distance for a configuration of LOBBYBACK or LexRank over sentence clusters produced for each test ex- ample. LOBBYBACK was configured to run with the number of iterations set to the size of the sen- tence cluster.</p><p>We compared both the performance of LOBBY- BACK and LexRank for DBSCAN values of 0.1 and 0.15 as well as computing the average edit dis- tance for different minimum sizes of cluster val- ues. As the table shows, LOBBYBACK obtains a lower edit distance than LexRank in every config-uration and as the size of the clusters increase the gap between the two increases. The goal of LOB- BYBACK is not to be a better summarization algo- rithm than LexRank. By comparing to LexRank and showing that the edit distances are smaller on average, we can conclude that the prototype sen- tences created by LOBBYBACK are capturing the text that is "central" or similar within a given clus- ter. In addition, the prototype sentences produced by LOBBYBACK are superior because they also capture and describe in a succinct way, the vari- ability of the sentences within a cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>One assumption that we made about the nature of state adoption of model legislation is that the legislatures make modifications that largely pre- serve the model language in an effort to preserve policy. However, we currently do not consider cases in which a legislature has intentionally ob- scured the text while still retaining the same mean- ing. While not as frequent as common text reuse, <ref type="bibr" target="#b13">Hertel-Fernandez and Kashin (2015)</ref> observed that some legislatures almost completely changed the text while reusing the concepts. One area of future work would be to try and extend LOBBYBACK to be more robust to these cases. One strategy would be to allow for a more flexible representation of text, such as word vector embeddings. The em- beddings might even be used to extend the multi- sentence alignment to include a penalty based on word distance in embedding space.</p><p>LOBBYBACK performs well on reconstructing model legislation from automatically generated bill clusters. However, there are a number of im- provements that can refine part of the pipeline. A potential change, but one that is more computa- tionally costly, would be to use a deeper parsing of the sentences that we extract from the documents. We used a simple unigram model when comput- ing sentence similarities because we wanted to en- sure that stop words were included-due to their importance in legal text. We suspect that by using a parser we could, for example, weight the sim- ilarity of noun-phrases, yielding a better similar- ity matrix and potentially higher precision/recall. Currently LOBBYBACK does not consider the or- der of the sentences when attempting to construct a prototype document. We envision a future ver- sion of LOBBYBACK that tries to reconstruct the original ordering of the prototype sentences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Min Cluster Size</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper we present LOBBYBACK, a system to reconstruct the "dark corpora" that is comprised of model bills which are copied (and modified) by resource constrained state legislatures. LOB- BYBACK first identifies clusters of text reuse in a large corpora of state legislation and then gener- ates prototype sentences that summarizes the sim- ilarity and variation of the copied text in a bill cluster. We believe that by open-sourcing LOB- BYBACK and releasing our data of prototype bills to the public, journalists and legal scholars can use our findings to better understand the origination of U.S state laws.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An sample state bill segment from Michigan Senate Bill 571</figDesc><graphic url="image-1.png" coords="5,72.00,62.81,218.26,114.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Precision, Recall, and F1 scores for the bill clustering component of LOBBYBACK configured with both the Louvain and InfoMap clustering algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Precision, Recall, and F1 scores of LOBBYBACK and baselines.</figDesc></figure>

			<note place="foot" n="2"> A threshold of 0.85 was found effective in prior work (Garrett and Jansa, 2015) and we observed good separation between matching sentences and non-matches in our data-set.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Mike Cafarella, Joe Walsh and Rayid Ghani for helpful discus-sions, the Sunlight Foundation for providing ac-cess to data and the anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">State lottery adoptions as policy innovations: An event history analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frances Stokes</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">D</forename><surname>Berry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="395" to="415" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast unfolding of communities in large networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Loup</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Lambiotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lefebvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">10008</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ranking through clustering: An integrated approach to multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1424" to="1433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ranking with recursive neural networks and its application to multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence, AAAI&apos;15</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2153" to="2159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The use of MMR, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;98</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Elasticsearch reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elastic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">GÃ¼nes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrg</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining, KDD&apos;96</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interest group influence in policy diffusion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><forename type="middle">N</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">M</forename><surname>Jansa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">State Politics &amp; Policy Quarterly</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="387" to="417" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A global optimization framework for meeting summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Korbinian</forename><surname>Riedhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP &apos;09</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="4769" to="4772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Innovation in the states: A diffusion study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="1174" to="1185" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gusfield</surname></persName>
		</author>
		<title level="m">Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Policy diffusion as a geographical expansion of the scope of political conflict: Same-sex marriage bans in the 1990s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donald P Haider-Markel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">State Politics &amp; Policy Quarterly</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="26" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Capturing business power across the states with text reuse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hertel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kashin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the Midwest Political Science Association</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="16" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Alecs influence over lawmaking in state legislatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Molly</forename><surname>Jackman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brookings Institute Blog</title>
		<imprint>
			<date type="published" when="2013-12-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Copy and paste lawmaking: The diffusion of policy language across american state legislatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">R</forename><surname>Joshua M Jansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia H</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">Working Paper</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Legislative responses to wrongful conviction: Do partisan principals and advocacy efforts influence statelevel criminal justice policy? Social science research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">T</forename><surname>Kent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carmichael</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="147" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Oral tradition and literary dependency: Variability and stability in the synoptic tradition and Q</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Terence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mournet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mohr Siebeck</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Algorithms for the assignment and transportation problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munkres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Society of Industrial and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="38" />
			<date type="published" when="1957-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The Effect of U.S. Supreme Court Intervention on the Innovation and Diffusion of Post-Roe Abortion Policies in the American States</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana Jill</forename><surname>Patton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
		</imprint>
		<respStmt>
			<orgName>University of Kentucky</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scientific paper summarization using citation summary networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahed</forename><surname>Qazvinian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics, COLING &apos;08</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maps of random walks on complex networks reveal community structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rosvall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">T</forename><surname>Bergstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1118" to="1123" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The mechanisms of policy diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Shipan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Political Science</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="840" to="857" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-document summarization using cluster-based link analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;08</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Domainindependent abstract generation for focused meeting summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics, ACL&apos;13</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1395" to="1405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-document summarization by maximizing informative content-words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artifical Intelligence, IJCAI&apos;07</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1776" to="1782" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
