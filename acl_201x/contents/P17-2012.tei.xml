<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Parse and Translate Improves Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<addrLine>7-3-1 Hongo, Bunkyo-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">New York University</orgName>
								<address>
									<postCode>10012</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Parse and Translate Improves Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="72" to="78"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2012</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>There has been relatively little attention to incorporating linguistic prior to neu-ral machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neu-ral machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural Machine Translation (NMT) has enjoyed impressive success without relying on much, if any, prior linguistic knowledge. Some of the most recent studies have for instance demonstrated that NMT systems work comparably to other systems even when the source and target sentences are given simply as flat sequences of characters ( <ref type="bibr" target="#b21">Lee et al., 2016;</ref><ref type="bibr" target="#b7">Chung et al., 2016</ref>) or statistically, not linguistically, motivated subword units ( <ref type="bibr">Wu et al., 2016)</ref>. <ref type="bibr" target="#b30">Shi et al. (2016)</ref> recently made an observation that the encoder of NMT captures syntactic properties of a source sen- tence automatically, indirectly suggesting that ex- plicit linguistic prior may not be necessary.</p><p>On the other hand, there have only been a couple of recent studies showing the potential benefit of explicitly encoding the linguistic prior into NMT.  for in- stance proposed to augment each source word with its corresponding part-of-speech tag, lemmatized form and dependency label. <ref type="bibr" target="#b11">Eriguchi et al. (2016)</ref> instead replaced the sequential encoder with a tree-based encoder which computes the represen- tation of the source sentence following its parse tree. <ref type="bibr" target="#b31">Stahlberg et al. (2016)</ref> let the lattice from a hierarchical phrase-based system guide the decod- ing process of neural machine translation, which results in two separate models rather than a single end-to-end one. Despite the promising improve- ments, these explicit approaches are limited in that the trained translation model strictly requires the availability of external tools during inference time. More recently, researchers have proposed meth- ods to incorporate target-side syntax into NMT models. <ref type="bibr" target="#b2">Alvarez-Melis and Jaakkola (2017)</ref> have proposed a doubly-recurrent neural network that can generate a tree-structured sentence, but its ef- fectiveness in a full scale NMT task is yet to be shown. <ref type="bibr" target="#b0">Aharoni and Goldberg (2017)</ref> introduced a method to serialize a parsed tree and to train the serialized parsed sentences.</p><p>We propose to implicitly incorporate linguis- tic prior based on the idea of multi-task learn- ing <ref type="bibr" target="#b4">(Caruana, 1998;</ref><ref type="bibr" target="#b8">Collobert et al., 2011</ref>). More specifically, we design a hybrid decoder for NMT, called NMT+RNNG 1 , that combines a usual con- ditional language model and a recently pro- posed recurrent neural network grammars (RN- NGs, <ref type="bibr" target="#b10">Dyer et al., 2016</ref>). This is done by plugging in the conventional language model decoder in the place of the buffer in RNNG, while sharing a sub- set of parameters, such as word vectors, between the language model and RNNG. We train this hy- brid model to maximize both the log-probability of a target sentence and the log-probability of a parse action sequence. We use an external parser <ref type="bibr" target="#b3">(Andor et al., 2016</ref>) to generate target parse actions, but unlike the previous explicit approaches, we do not need it during test time.</p><p>We evaluate the proposed NMT+RNNG on four language pairs ({JP, Cs, De, Ru}-En). We observe significant improvements in terms of BLEU scores on three out of four language pairs and RIBES scores on all the language pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation</head><p>Neural machine translation is a recently proposed framework for building a machine translation sys- tem based purely on neural networks. It is of- ten built as an attention-based encoder-decoder network ( <ref type="bibr" target="#b5">Cho et al., 2015</ref>) with two recurrent networks-encoder and decoder-and an atten- tion model. The encoder, which is often imple- mented as a bidirectional recurrent network with long short-term memory units (LSTM, Hochre- iter and Schmidhuber, 1997) or gated recurrent units (GRU, <ref type="bibr" target="#b6">Cho et al., 2014</ref>), first reads a source sentence represented as a sequence of words x = (x 1 , x 2 , . . . , x N ). The encoder returns a sequence of hidden states h = (h 1 , h 2 , . . . , h N ). Each hid- den state h i is a concatenation of those from the forward and backward recurrent network:</p><formula xml:id="formula_0">h i = − → h i ; ← − h i , where</formula><formula xml:id="formula_1">− → h i = − → f enc ( − → h i−1 , V x (x i )), ← − h i = ← − f enc ( ← − h i+1 , V x (x i )).</formula><p>V x (x i ) refers to the word vector of the i-th source word. The decoder is implemented as a conditional re- current language model which models the target sentence, or translation, as</p><formula xml:id="formula_2">log p(y|x) = j log p(y j |y &lt;j , x),</formula><p>where y = (y 1 , . . . , y M ). Each of the conditional probabilities in the r.h.s is computed by</p><formula xml:id="formula_3">p(y j = y|y &lt;j , x) = sof tmax(W y ˜ s j ),<label>(1)</label></formula><formula xml:id="formula_4">˜ s j = tanh(W c [s j ; c j ]),<label>(2)</label></formula><formula xml:id="formula_5">s j = f dec (s j−1 , [V y (y j−1 ); ˜ s j−1 ]),<label>(3)</label></formula><p>where f dec is a recurrent activation function, such as LSTM or GRU, and W y is the output word vec- tor of the word y. c j is a time-dependent context vector that is computed by the attention model using the se- quence h of hidden states from the encoder. The attention model first compares the current hidden state s j against each of the hidden states and as- signs a scalar score: <ref type="bibr">- ong et al., 2015)</ref>. These scores are then normal- ized across the hidden states to sum to 1, that is</p><formula xml:id="formula_6">β i,j = exp(h i W d s j ) (Lu</formula><formula xml:id="formula_7">α i,j = β i,j i β i,j</formula><p>. The time-dependent context vector is then a weighted-sum of the hidden states with these attention weights: c j = i α i,j h i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Recurrent Neural Network Grammars</head><p>A recurrent neural network grammar <ref type="bibr">(RNNG, Dyer et al., 2016</ref>) is a probabilistic syntax-based language model. Unlike a usual recurrent lan- guage model (see, e.g., <ref type="bibr" target="#b23">Mikolov et al., 2010)</ref>, an RNNG simultaneously models both tokens and their tree-based composition. This is done by having a (output) buffer, stack and action his- tory, each of which is implemented as a stack LSTM (sLSTM, <ref type="bibr" target="#b9">Dyer et al., 2015)</ref>. At each time step, the action sLSTM predicts the next action based on the (current) hidden states of the buffer, stack and action sLSTM. That is,</p><formula xml:id="formula_8">p(a t = a|a &lt;t ) ∝ e W a f action (h buffer t ,h stack t ,h action t ) , (4)</formula><p>where W a is the vector of the action a. If the se- lected action is shift, the word at the beginning of the buffer is moved to the stack. When the re- duce action is selected, the top-two words in the stack are reduced to build a partial tree. Addi- tionally, the action may be one of many possible non-terminal symbols, in which case the predicted non-terminal symbol is pushed to the stack. The hidden states of the buffer, stack and action sLSTM are correspondingly updated by</p><formula xml:id="formula_9">h buffer t = StackLSTM(h buffer top , V y (y t−1 )), (5) h stack t = StackLSTM(h stack top , r t ), h action t = StackLSTM(h action top , V a (a t−1 )),</formula><p>where V y and V a are functions returning the target word and action vectors. The input vector r t of the stack sLSTM is computed recursively by</p><formula xml:id="formula_10">r t = tanh(W r [r d ; r p ; V a (a t )]),</formula><p>where r d and r p are the corresponding vectors of the parent and dependent phrases, respec- tively ( <ref type="bibr" target="#b9">Dyer et al., 2015)</ref>. This process is iter- ated until a complete parse tree is built. Note that the original paper of RNNG ( <ref type="bibr" target="#b10">Dyer et al., 2016)</ref> uses constituency trees, but we employ depen- dency trees in this paper. Both types of trees are represented as a sequence of the three types of ac- tions in a transition-based parsing model. When the complete sentence is provided, the buffer simply summarizes the shifted words. When the RNNG is used as a generator, the buffer further generates the next word when the selected action is shift. The latter can be done by replacing the buffer with a recurrent language model, which is the idea on which our proposal is based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning to Parse and Translate</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">NMT+RNNG</head><p>Our main proposal in this paper is to hybridize the decoder of the neural machine translation and the RNNG. We continue from the earlier observation that we can replace the buffer of RNNG to a recur- rent language model that simultaneously summa- rizes the shifted words as well as generates future words. We replace the RNNG's buffer with the neural translation model's decoder in two steps.</p><p>Construction First, we replace the hidden state of the buffer h buffer (in Eq. <ref type="formula">(5)</ref>) with the hidden state of the decoder of the attention-based neural machine translation from Eq. (3). As is clear from those two equations, both the buffer sLSTM and the translation decoder take as input the previous hidden state (h buffer top and s j−1 , respectively) and the previously decoded word (or the previously shifted word in the case of the RNNG's buffer), and returns its summary state. The only difference is that the translation decoder additionally consid- ers the state˜sstate˜ state˜s j−1 . Once the buffer of the RNNG is replaced with the NMT decoder in our proposed model, the NMT decoder is also under control of the actions provided by the RNNG. <ref type="bibr">2</ref> Second, we let the next word prediction of the translation de- coder as a generator of RNNG. In other words, the generator of RNNG will output a word, when asked by the shift action, according to the condi- tional distribution defined by the translation de- coder in Eq. (1). Once the buffer sLSTM is re- placed with the neural translation decoder, the ac- tion sLSTM naturally takes as input the translation decoder's hidden state when computing the action conditional distribution in Eq. (4). We call this hy- brid model NMT+RNNG.</p><p>Learning and Inference After this integration, our hybrid NMT+RNNG models the conditional distribution over all possible pairs of transla- tion and its parse given a source sentence, i.e., p(y, a|x). Assuming the availability of parse annotation in the target-side of a parallel cor- pus, we train the whole model jointly to maxi- mize E (x,y,a)∼data [log p(y, a|x)]. In doing so, we notice that there are two separate paths through which the neural translation decoder receives er- ror signal. First, the decoder is updated in or- der to maximize the conditional probability of the correct next word, which has already existed in the original neural machine translation. Second, the decoder is updated also to maximize the con- ditional probability of the correct parsing action, which is a novel learning signal introduced by the proposed hybridization. Furthermore, the second learning signal affects the encoder as well, encour- aging the whole neural translation model to be aware of the syntactic structure of the target lan- guage. Later in the experiments, we show that this additional learning signal is useful for translation, even though we discard the RNNG (the stack and action sLSTMs) in the inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Knowledge Distillation for Parsing</head><p>A major challenge in training the proposed hybrid model is that there is not a parallel corpus aug- mented with gold-standard target-side parse, and vice versa. In other words, we must either parse the target-side sentences of an existing parallel corpus or translate sentences with existing gold- standard parses. As the target task of the proposed model is translation, we start with a parallel cor- pus and annotate the target-side sentences. It is however costly to manually annotate any corpus of reasonable size <ref type="table">(Table 6</ref> in <ref type="bibr" target="#b1">Alonso et al., 2016)</ref>.</p><p>We instead resort to noisy, but automated an- notation using an existing parser. This approach of automated annotation can be considered along the line of recently proposed techniques of knowl- edge distillation <ref type="bibr" target="#b13">(Hinton et al., 2015</ref>) and distant supervision ( <ref type="bibr" target="#b24">Mintz et al., 2009</ref>). In knowledge dis- tillation, a teacher network is trained purely on a training set with ground-truth annotations, and the annotations predicted by this teacher are used to train a student network, which is similar to our ap- proach where the external parser could be thought of as a teacher and the proposed hybrid network's RNNG as a student. On the other hand, what we Train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dev. Test Voc. (src, tgt, act) Cs-En 134,453 2,656 2,999 (33,867, 27,347, 82) De-En 166,313 2,169 2,999 (33,820, 30,684, 80) Ru-En 131,492 2,818 2,998 (32,442, 27,979, 82) Jp-En</head><p>100,000 1,790 1,812 (23,509, 28,591, 80) <ref type="table">Table 1</ref>: Statistics of parallel corpora.</p><p>propose here is a special case of distant supervi- sion in that the external parser provides noisy an- notations to otherwise an unlabeled training set. Specifically, we use SyntaxNet, released by An- dor et al. <ref type="formula" target="#formula_3">(2016)</ref>, on a target sentence. <ref type="bibr">3</ref> We convert a parse tree into a sequence of one of three tran- sition actions (SHIFT, REDUCE-L, REDUCE-R). We label each REDUCE action with a correspond- ing dependency label and treat it as a more fine- grained action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Language Pairs and Corpora</head><p>We compare the proposed NMT+RNNG against the baseline model on four different language pairs-Jp-En, Cs-En, De-En and Ru-En. The ba- sic statistics of the training data are presented in <ref type="table">Table 1</ref>. We mapped all the low-frequency words to the unique symbol "UNK" and inserted a spe- cial symbol "EOS" at the end of both source and target sentences. Ja We use the ASPEC corpus ("train1.txt") from the WAT'16 Jp-En translation task. We tokenize each Japanese sentence with <ref type="bibr">KyTea (Neubig et al., 2011</ref>) and preprocess according to the recommen- dations from <ref type="bibr">WAT'16 (WAT, 2016)</ref>. We use the first 100K sentence pairs of length shorter than 50 for training. The vocabulary is constructed with all the unique tokens that appear at least twice in the training corpus. We use "dev.txt" and "test.txt" provided by WAT'16 respectively as development and test sets.</p><p>Cs, De and Ru We use News Commentary v8. We removed noisy metacharacters and used the to- kenizer from Moses ( <ref type="bibr" target="#b19">Koehn et al., 2007)</ref> to build a vocabulary of each language using unique tokens that appear at least 6, 6 and 5 times respectively for Cs, Ru and De. The target-side (English) vocab- ulary was constructed with all the unique tokens appearing more than three times in each corpus. We also excluded the sentence pairs which include empty lines in either a source sentence or a target sentence. We only use sentence pairs of length 50 or less for training. We use "newstest2015" and "newstest2016" as development and test sets re- spectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Models, Learning and Inference</head><p>In all our experiments, each recurrent network has a single layer of LSTM units of 256 dimensions, and the word vectors and the action vectors are of 256 and 128 dimensions, respectively. To re- duce computational overhead, we use BlackOut ( <ref type="bibr" target="#b16">Ji et al., 2015</ref>) with 2000 negative samples and α = 0.4. When employing BlackOut, we shared the negative samples of each target word in a sen- tence in training time <ref type="bibr" target="#b12">(Hashimoto and Tsuruoka, 2017)</ref>, which is similar to the previous work <ref type="bibr">(Zoph et al., 2016)</ref>. For the proposed NMT+RNNG, we share the target word vectors between the decoder (buffer) and the stack sLSTM.</p><p>Each weight is initialized from the uniform dis- tribution [−0.1, 0.1]. The bias vectors and the weights of the softmax and BlackOut are initial- ized to be zero. The forget gate biases of LSTMs and Stack-LSTMs are initialized to 1 as recom- mended in <ref type="bibr">Józefowicz et al. (2015)</ref>. We use stochastic gradient descent with minibatches of 128 examples. The learning rate starts from 1.0, and is halved each time the perplexity on the de- velopment set increases. We clip the norm of the gradient ( <ref type="bibr" target="#b27">Pascanu et al., 2012</ref>) with the thresh- old set to 3.0 (2.0 for the baseline models on Ru- En and Cs-En to avoid <ref type="bibr">NaN and Inf)</ref>. When the perplexity of development data increased in train- ing time, we halved the learning rate of stochastic gradient descent and reloaded the previous model. The RNNG's stack computes the vector of a de- pendency parse tree which consists of the gener- ated target words by the buffer. Since the complete parse tree has a "ROOT" node, the special token of the end of a sentence ("EOS") is considered as the ROOT. We use beam search in the inference time, with the beam width selected based on the devel- opment set performance.</p><p>It took about 15 minutes per epoch and about 20 minutes respectively for the baseline and the pro- posed model to train a full JP-EN parallel corpus in our implementation. 4   <ref type="table">Table 3</ref>: Effect of each component in RNNG.</p><note type="other">De-En Ru-En Cs-En Jp-En</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results and Analysis</head><p>In <ref type="table" target="#tab_1">Table 2</ref>, we report the translation qualities of the tested models on all the four language pairs. We report both BLEU ( <ref type="bibr" target="#b26">Papineni et al., 2002</ref>) and RIBES ( <ref type="bibr" target="#b15">Isozaki et al., 2010)</ref>. Except for De- En, measured in BLEU, we observe the statis- tically significant improvement by the proposed NMT+RNNG over the baseline model. It is worth- while to note that these significant improvements have been achieved without any additional param- eters nor computational overhead in the inference time.</p><p>Ablation Since each component in RNNG may be omitted, we ablate each component in the pro- posed NMT+RNNG to verify their necessity. 5 As shown in <ref type="table">Table 3</ref>, we see that the best performance could only be achieved when all the three compo- nents were present. Removing the stack had the most adverse effect, which was found to be the case for parsing as well by <ref type="bibr" target="#b20">Kuncoro et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated Sentences with Parsed Actions</head><p>The decoder part of our proposed model consists of two components: the NMT decoder to gener- threads on Intel(R) Xeon(R) CPU E5-2680 v2 @2.80GHz) <ref type="bibr">5</ref> Since the buffer is the decoder, it is not possible to com- pletely remove it. Instead we simply remove the dependency of the action distribution on it. <ref type="figure">Figure 1</ref>: An example of translation and its depen- dency relations obtained by our proposed model. ate a translated sentence and the RNNG decoder to predict its parsing actions. The proposed model can therefore output a dependency structure along with a translated sentence. <ref type="figure">Figure 1</ref> shows an example of JP-EN translation in the development dataset and its dependency parse tree obtained by the proposed model. The special symbol ("EOS") is treated as the root node ("ROOT") of the parsed tree. The translated sentence was generated by using beam search, which is the same setting of NMT+RNNG shown in <ref type="table">Table 3</ref>. The parsing ac- tions were obtained by greedy search. The re- sulting dependency structure is mostly correct but contains a few errors; for example, dependency re- lation between "The" and " transition" should not be "pobj".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a hybrid model, to which we refer as NMT+RNNG, that combines the decoder of an attention-based neural translation model with the RNNG. This model learns to parse and translate si- multaneously, and training it encourages both the encoder and decoder to better incorporate linguis- tic priors. Our experiments confirmed its effec- tiveness on four language pairs ({JP, Cs, De, Ru}- En). The RNNG can in principle be trained with- out ground-truth parses, and this would eliminate the need of external parsers completely. We leave the investigation into this possibility for future re- search.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>BLEU and RIBES scores by the baseline 
and proposed models on the test set. We use the 
bootstrap resampling method from Koehn (2004) 
to compute the statistical significance. We use  † to 
mark those significant cases with p &lt; 0.005. 

Jp-En (Dev) 
BLEU 
NMT+RNNG 18.60 
w/o Buffer 
18.02 
w/o Action 
17.94 
w/o Stack 
17.58 
NMT 
17.75 

</table></figure>

			<note place="foot" n="1"> Our code is available at https://github.com/ tempra28/nmtrnng.</note>

			<note place="foot" n="2"> The j-th hidden state in Eq. (3) is calculated only when the action (shift) is predicted by the RNNG. This is why our proposed model can handle the sequences of words and actions which have different lengths.</note>

			<note place="foot" n="3"> When the target sentence is parsed as data preprocessing, we use all the vocabularies in a corpus and do not cut off any words. We use the plain SyntaxNet and do not train it furthermore.</note>

			<note place="foot" n="4"> We run all the experiments on multi-core CPUs (10</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Yuchen Qiao and Kenjiro Taura for their help to speed up the implementations of training and also Kazuma Hashimoto for his valuable com-ments and discussions. This work was supported by JST CREST Grant Number JPMJCR1513 and JSPS KAKENHI Grant Number 15J12597 and</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards string-to-tree neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">From noisy questions to minecraft texts: Annotation challenges in extreme syntax scenario</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Héctor Martínez Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoˆıtbenoˆıt</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT)</title>
		<meeting>the 2nd Workshop on Noisy User-generated Text (WNUT)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tree-structured decoding with doubly-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Melis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2442" to="2452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="95" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Describing multimedia content using attention-based encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1875" to="1886" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A character-level decoder without explicit segmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1693" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Noah</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Noah</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tree-to-sequence attentional neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="823" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neural Machine Translation with SourceSide Latent Graph Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02265</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic evaluation of translation quality for distant language pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="944" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Blackout: Speeding up recurrent neural network language models with very large vocabularies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Shihao Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadathur</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What do recurrent neural network grammars learn about syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1249" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fully character-level neural machine translation without explicit segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.03017</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaňjaň</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010). International Speech Communication Association</title>
		<meeting>the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010). International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointwise prediction for robust, adaptable japanese morphological analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Understanding the exploding gradient problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.5063abs/1211.5063</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Linguistic input features improve neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="83" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Does string-based neural mt learn source syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1526" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Syntactically guided neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Waite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="299" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wat</forename></persName>
		</author>
		<ptr target="http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/dataPreparationJE.html" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
