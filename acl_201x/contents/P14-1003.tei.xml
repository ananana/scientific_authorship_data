<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text-level Discourse Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 23-25</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>MOE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>MOE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>MOE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>HongKong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Text-level Discourse Dependency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="25" to="35"/>
							<date type="published">June 23-25</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Previous researches on Text-level discourse parsing mainly made use of constituency structure to parse the whole document into one discourse tree. In this paper, we present the limitations of constituency based discourse parsing and first propose to use dependency structure to directly represent the relations between elementary discourse units (EDUs). The state-of-the-art dependency parsing techniques, the Eisner algorithm and maximum spanning tree (MST) algorithm, are adopted to parse an optimal discourse dependency tree based on the arc-factored model and the large-margin learning techniques. Experiments show that our discourse dependency parsers achieve a competitive performance on text-level discourse parsing.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It is widely agreed that no units of the text can be understood in isolation, but in relation to their context. Researches in discourse parsing aim to acquire such relations in text, which is funda- mental to many natural language processing ap- plications such as question answering, automatic summarization and so on.</p><p>One important issue behind discourse parsing is the representation of discourse structure. Rhe- torical Structure Theory (RST) ( <ref type="bibr" target="#b13">Mann and Thompson, 1988)</ref>, one of the most influential discourse theories, posits a hierarchical genera- tive tree representation, as illustrated in <ref type="figure">Figure 1</ref>. The leaves of a tree correspond to contiguous text spans called Elementary Discourse Units (EDUs) <ref type="bibr">1</ref> . The adjacent EDUs are combined into <ref type="bibr">1</ref> EDU segmentation is a relatively trivial step in discourse parsing. Since our work focus here is not EDU segmenta- tion but discourse parsing. We assume EDUs are already known. the larger text spans by rhetorical relations (e.g., Contrast and Elaboration) and the larger text spans continue to be combined until the whole text constitutes a parse tree. The text spans linked by rhetorical relations are annotated as either nucleus or satellite depending on how sali- ent they are for interpretation. It is attractive and challenging to parse the whole text into one tree.</p><p>Since such a hierarchical discourse tree is analogous to a constituency based syntactic tree except that the constituents in the discourse trees are text spans, previous researches have explored different constituency based syntactic parsing techniques (eg. CKY and chart parsing) and var- ious features (eg. length, position et al.) for dis- course parsing <ref type="bibr" target="#b22">(Soricut and Marcu, 2003;</ref><ref type="bibr">Joty et al., 2012;</ref><ref type="bibr" target="#b20">Reitter, 2003;</ref><ref type="bibr" target="#b11">LeThanh et al., 2004;</ref><ref type="bibr" target="#b0">Baldridge and Lascarides, 2005;</ref><ref type="bibr" target="#b23">Subba and Di Eugenio, 2009;</ref><ref type="bibr" target="#b21">Sagae, 2009;</ref><ref type="bibr" target="#b9">Hernault et al., 2010b;</ref><ref type="bibr" target="#b7">Feng and Hirst, 2012)</ref>. However, the ex- isting approaches suffer from at least one of the following three problems. First, it is difficult to design a set of production rules as in syntactic parsing, since there are no determinate genera- tive rules for the interior text spans. Second, the different levels of discourse units (e.g. EDUs or larger text spans) occurring in the generative process are better represented with different fea- tures, and thus a uniform framework for dis- course analysis is hard to develop. Third, to reduce the time complexity of the state-of-the-art constituency based parsing techniques, the ap- proximate parsing approaches are prone to trap in local maximum.</p><p>In this paper, we propose to adopt the depend- ency structure in discourse representation to overcome the limitations mentioned above. Here is the basic idea: the discourse structure consists of EDUs which are linked by the binary, asym- metrical relations called dependency relations. A dependency relation holds between a subordinate EDU called the dependent, and another EDU on which it depends called the head, as illustrated in <ref type="figure">Figure 2</ref>. Each EDU has one head. So, the de- pendency structure can be seen as a set of head- dependent links, which are labeled by functional relations. Now, we can analyze the relations be- tween EDUs directly, without worrying about any interior text spans. Since dependency trees contain much fewer nodes and on average they are simpler than constituency based trees, the current dependency parsers can have a relatively low computational complexity. Moreover, con- cerning linearization, it is well known that de- pendency structures can deal with non-projective relations, while constituency-based models need the addition of complex mechanisms like trans- formations, movements and so on. In our work, we adopt the graph based dependency parsing techniques learned from large sets of annotated dependency trees. The Eisner (1996) algorithm and maximum spanning tree (MST) algorithm are used respectively to parse the optimal projec- tive and non-projective dependency trees with the large-margin learning technique <ref type="bibr" target="#b4">(Crammer and Singer, 2003)</ref>. To the best of our knowledge, we are the first to apply the dependency structure and introduce the dependency parsing techniques into discourse analysis.</p><p>The rest of this paper is organized as follows. Section 2 formally defines discourse dependency structure and introduces how to build a discourse dependency treebank from the existing RST cor- pus. Section 3 presents the discourse parsing ap- proach based on the Eisner and MST algorithms. Section 4 elaborates on the large-margin learning technique as well as the features we use. Section 5 discusses the experimental results. Section 6 introduces the related work and Section 7 con- cludes the paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Discourse Dependency Structure and Tree Bank</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Discourse Dependency Structure</head><p>Similar to the syntactic dependency structure defined by <ref type="bibr" target="#b15">McDonald (2005a</ref><ref type="bibr" target="#b16">McDonald ( , 2005b</ref>, we insert an artificial EDU e 0 in the beginning for each document and label the dependency relation link- ing from e 0 as ROOT. This treatment will sim- plify both formal definitions and computational implementations. Normally, we assume that each EDU should have one and only one head except for e 0 . A labeled directed arc is used to represent the dependency relation from one head to its de- pendent. Then, discourse dependency structure can be formalized as the labeled directed graph, where nodes correspond to EDUs and labeled arcs correspond to labeled dependency relations.</p><p>We assume that the text 2 T is composed of n+1 EDUs including the artificial e 0 . That is T=e 0 e 1 e 2 … e n . Let R={r 1 ,r 2 , … ,r m } denote a finite set of functional relations that hold be- tween two EDUs. Then a discourse dependency graph can be denoted by G=&lt;V, A&gt; where V de- notes a set of nodes and A denotes a set of la- beled directed arcs, such that for the text T=e 0 e 1 e 2 … e n and the label set R the following holds:</p><p>(1) V = { e 0 , e 1 , e 2 , … e n } (2) A  V R  V, where &lt;e i , r, e j &gt;A represents an arc from the head e i to the dependent e j labeled with the relation r. (3) If &lt;e i , r, e j &gt;A then &lt;e k , r', e j &gt;A for all ki (4) If &lt;e i , r, e j &gt;A then &lt;e i , r', e j &gt;A for all r'r</p><p>The third condition assures that each EDU has one and only one head and the fourth tells that only one kind of dependency relation holds be- tween two EDUs. According to the definition, we illustrate all the 9 possible unlabeled depend- ency trees for a text containing three EDUs in <ref type="figure">Figure 2</ref>. The dependency trees 1' to 7' are pro- jective while 8' and 9' are non-projective with crossing arcs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Our Discourse Dependency Treebank</head><p>To automatically conduct discourse dependency parsing, constructing a discourse dependency treebank is fundamental. It is costly to manually construct such a treebank from scratch. Fortu- nately, RST Discourse Treebank (RST-DT) <ref type="bibr" target="#b2">(Carlson et al., 2001</ref>) is an available resource to help with.</p><p>A RST tree constitutes a hierarchical structure for one document through rhetorical relations. A total of 110 fine-grained relations (e.g. Elabora- tion-part-whole and List) were used for tagging RST-DT. They can be categorized into 18 classes (e.g. Elaboration and Joint). All these relations can be hypotactic ("mononuclear") or paratactic ("multi-nuclear"). A hypotactic relation holds between a nucleus span and an adjacent satellite span, while a paratactic relation connects two or more equally important adjacent nucleus spans. For convenience of computation, we convert the n-ary (n&gt;2) RST trees 3 to binary trees through adding a new node for the latter n-1 nodes and assume each relation is connected to only one nucleus <ref type="bibr">4</ref> . This departure from the original theory is not such a major step as it may appear, since any nucleus is known to contribute to the essen- tial meaning. Now, each RST tree can be seen as a headed constituency based binary tree where the nuclei are heads and the children of each node are linearly ordered. Given three EDUs 5 , <ref type="figure">Figure 1</ref> shows the possible 8 headed constituen- cy based trees where the superscript * denotes the heads (nuclei). We use dependency trees to simulate the headed constituency based trees.</p><p>Contrasting <ref type="figure">Figure 1</ref> with <ref type="figure">Figure 2</ref>, we use dependency tree 1' to simulate binary trees 1 and 8, and dependency tress 2'-7' to simulate binary trees 2-7 correspondingly. The rhetorical rela- tions in RST trees are kept as the functional rela- tions which link the two EDUs in dependency trees. With this kind of conversion, we can get our discourse dependency treebank. It is worth noting that the non-projective trees like 8' and 9' do not exist in our dependency treebank, though they are eligible according to the definition of discourse dependency graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discourse Dependency Parsing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Overview</head><p>As stated above, T=e 0 e 1 …e n represents an input text (document) where e i denotes the i th EDU of T. We use V to denote all the EDU nodes and VRV -0 (V -0 =V-{e 0 }) denote all the possible discourse dependency arcs. The goal of discourse dependency parsing is to parse an optimal span- ning tree from VRV -0 . Here we follow the arc factored method and define the score of a de- pendency tree as the sum of the scores of all the arcs in the tree. Thus, the optimal dependency tree for T is a spanning tree with the highest score and obtained through the function DT(T,w): ) denotes the score of the arc &lt;e i , r, e j &gt; which is calculated according to its feature representation f(e i ,r,e j ) and a weight vector w. Next, two basic problems need to be solved: how to find the dependency tree with the highest score for T given all the arc scores (i.e. a parsing problem), and how to learn and compute the scores of arcs according to a set of arc features (i.e. a learning problem).</p><formula xml:id="formula_0">0 0 0 ,, ,, ( , ) ( , ) ( , , ) ( , , ) f T T i j T T i j T G V R V G V R V i j e r e G G V R V i j e r e G T DT T argmax</formula><p>The following of this section addresses the first problem. Given the text T, we first reduce the multi-digraph composed of all possible arcs to the digraph. The digraph keeps only one arc &lt;e i , r, e j &gt; between two nodes which satisfies</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(</head><p>) . Thus, we can proceed with a reduction from labeled parsing to unlabeled parsing. Next, two algorithms, i.e. the Eisner algorithm and MST algorithm, are pre- sented to parse the projective and non-projective unlabeled dependency trees respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Eisner Algorithm</head><p>It is well known that projective dependency pars- ing can be handled with the Eisner algorithm <ref type="bibr">(1996)</ref> which is based on the bottom-up dynamic programming techniques with the time complexi- ty of O(n 3 ). The basic idea of the Eisner algo- rithm is to parse the left and right dependents of an EDU independently and combine them at a later stage. This reduces the overhead of index- ing heads. Only two binary variables, i.e. c and d, are required to specify whether the heads occur leftmost or rightmost and whether an item is complete.</p><formula xml:id="formula_1">Eisner(T, ) Input: Text T=e 0 e 1 … e n ; Arc scores (e i ,e j ) 1 Instantiate E[i, i, d, c]=0.0 for all i, d, c 2 For m := 1 to n 3 For i := 1 to n 4 j = i + m 5</formula><p>if j&gt; n then break; 6 # Create subgraphs with c=0 by adding arcs</p><formula xml:id="formula_2">7 E[i, j, 0, 0]=max iqj (E[i,q,1,1]+E[q+1,j,0,1]+(e j ,e i )) 8 E[i, j, 1, 0]=max iqj (E[i,q,1,1]+E[q+1,j,0,1]+(e i ,e j )) 9 # Add corresponding left/right subgraphs 10 E[i, j, 0, 1]=max iqj (E[i,q,0,1]+E[q,j,0,0] 11 E[i, j, 1, 1]=max iqj (E[i,q,1,0]+E[q,j,1,1])</formula><p>Figure 3: Eisner Algorithm <ref type="figure">Figure 3</ref> shows the pseudo-code of the Eisner algorithm. A dynamic programming table E <ref type="bibr">[i,j,d,c]</ref> is used to represent the highest scored subtree spanning e i to e j . d indicates whether e i is the head (d=1) or e j is head (d=0). c indicates whether the subtree will not take any more de- pendents (c=1) or it needs to be completed (c=0). The algorithm begins by initializing all length- one subtrees to a score of 0.0. In the inner loop, the first two steps (Lines 7 and 8) are to construct the new dependency arcs by taking the maximum over all the internal indices (iqj) in the span, and calculating the value of merging the two sub- trees and adding one new arc. The last two steps (Lines 10 and 11) attempt to achieve an optimal left/right subtree in the span by adding the corre- sponding left/right subtree to the arcs that have been added previously. This algorithm considers all the possible subtrees. We can then get the optimal dependency tree with the score E[0,n,1,1] .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Maximum Spanning Tree Algorithm</head><p>As the bottom-up Eisner Algorithm must main- tain the nested structural constraint, it cannot parse the non-projective dependency trees like 8' and 9' in <ref type="figure">Figure 2</ref>. However, the non-projective dependency does exist in real discourse. For ex- ample, the earlier text mainly talks about the top- ic A with mentioning the topic B, while the latter text gives a supplementary explanation for the topic B. This example can constitute a non- projective tree and its pictorial diagram is exhib- ited in <ref type="figure" target="#fig_2">Figure 4</ref>. Following the work of McDon- ald (2005b), we formalize discourse dependency parsing as searching for a maximum spanning tree (MST) in a directed graph.  Chu and <ref type="bibr" target="#b3">Liu (1965)</ref> and <ref type="bibr" target="#b5">Edmonds (1967)</ref> in- dependently proposed the virtually identical al- gorithm named the Chu-Liu/Edmonds algorithm, for finding MSTs on directed graphs <ref type="bibr" target="#b16">(McDonald et al. 2005b</ref>). <ref type="figure" target="#fig_3">Figure 5</ref> shows the details of the Chu-Liu/Edmonds algorithm for discourse pars- ing. Each node in the graph greedily selects the incoming arc with the highest score. If one tree results, the algorithm ends. Otherwise, there must exist a cycle. The algorithm contracts the identified cycle into a single node and recalcu- lates the scores of the arcs which go in and out of the cycle. Next, the algorithm recursively call itself on the contracted graph. Finally, those arcs which go in or out of one cycle will recover themselves to connect with the original nodes in V. Like <ref type="bibr" target="#b16">McDonald et al. (2005b)</ref>, we adopt an efficient implementation of the Chu- Liu/Edmonds algorithm that is proposed by Tar- jan (1997) with O(n 2 ) time complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chu-Liu-Edmonds(G, )</head><p>Input: Text T=e 0 e 1 … e n ; Arc scores (e i ,e j )</p><formula xml:id="formula_3">1 A' = {&lt;e i , e j &gt;| e i = argmax (e i ,e j ); 1j|V|} 2 G' = (V, A') 3</formula><p>If G' has no cycles, then return G' 4</p><p>Find an arc set A C that is a cycle in G' 5</p><formula xml:id="formula_4">&lt;G C , ep&gt; = contract(G, A C , ) 6 G = (V, A)=Chu-Liu-Edmonds(G C , ) 7</formula><p>For the arc &lt;e i ,e C &gt; where ep(e i ,e C )=e j : 8</p><p>A=AA C {&lt;e i ,e j )}-{&lt;e i ,e C &gt;, &lt;a(e j ),e j &gt;} 9</p><p>For the arc &lt;e C , e i &gt; where ep(e C ,e i )=e j : 10</p><p>A=A{&lt;e j ,e i &gt;}-{&lt;e C ,e i &gt;} 11 V = V 12 Return G</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contract(G=(V,A), A C , )</head><p>1 Let G C be the subgraph of G excluding nodes in C 2 Add a node e C to G C denoting the cycle C 3 For e j V-C :   </p><formula xml:id="formula_5">e i C &lt;e i ,e j &gt;A 4 Add arc &lt;e C ,e j &gt; to G C with ep(e C ,e j )= (e i ,e j ) 5 (e C ,e j ) = (ep(e C ,e j ),e j ) 6 For e i V-C: e j C (e i ,e j )A 7 Add arc &lt;e i ,e C &gt; to G C with ep(e i ,e C )= = [(e i ,e j )-(a(e i ),e j )] 8 (e i ,e C ) =(e i ,e j )-(a(e i ),e j )+score(C) 9 Return &lt;G C , ep&gt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) Length: The length of each EDU. (5) Syntactic: POS tags of the dominating nodes as defined in Soricut and Marcu (2003) are ex- tracted as features. We use the syntactic trees</head><note type="other">from the Penn Treebank to find the dominating nodes,. (6) Semantic similarity: We compute the se- mantic relatedness between the two EDUs based on WordNet. The word pairs are extracted from (e i , e j ) and their similarity is calculated. Then, we can get a weighted complete bipartite graph where words are deemed as nodes and similarity as weights. From this bipartite graph, we get the maximum weighted matching and use the aver- aged weight of the matches as the similarity be- tween e i and e j . In particular, we use path_similarity, wup_similarity, res_similarity, jcn_similarity and lin_similarity provided by the nltk.wordnet.similarity (Bird et. al., 2009) pack- age for calculating word similarity.</note><p>As for relations, we experiment two sets of relation labels from RST-DT. One is composed of 19 coarse-grained relations and the other 111 fine-grained relations 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MIRA based Learning</head><p>Margin Infused Relaxed Algorithm (MIRA) is an online algorithm for multiclass classification and is extended by <ref type="bibr" target="#b25">Taskar et al. (2003)</ref> to cope with structured classification. y in each iteration. On each update, MIRA attempts to keep the norm of the change to the weight vector as small as possible, which is subject to con- structing the correct dependency tree under con- sideration with a margin at least as large as the loss of the incorrect dependency trees. We define the loss of a discourse dependency tree ' i y (de- noted by <ref type="bibr">( , ')</ref> ii L yy ) as the number of the EDUs that have incorrect heads. Since there are expo- nentially many possible incorrect dependency trees and thus exponentially many margin con- straints, here we relax the optimization and stay with a single best dependency tree <ref type="bibr">'</ref> ( , )</p><formula xml:id="formula_6">j ii DT T </formula><p>yw which is parsed under the weight vector w j . In this algorithm, the successive up- dated values of w are accumulated and averaged to avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Preparation</head><p>We test our methods experimentally using the discourse dependency treebank which is built as in Section 2.  <ref type="table" target="#tab_3">Elaboration-additional  2912  312  Attribution  2474  329  Elaboration-object-attribute-e  2274  250  List  1690  206  Same-unit  1230  127  Elaboration-additional-e  747  69  Circumstance  545  80  Explanation-argumentative  524  70  Purpose  430  43  Contrast  358  64   Table 2</ref>: 10 Highest Distributed Fine-grained Relations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Feature Influence on Two Relation Sets</head><p>So far, researches on discourse parsing avoid adopting too fine-grained relations and the rela- tion sets containing around 20 labels are widely used.</p><p>In our experiments, we observe that adopt- ing a fine-grained relation set can even be helpful to building the discourse trees.   Based on the MIRA leaning algorithm, the Eisner algorithm and MST algorithm are used to parse the test documents respectively. Referring to the evaluation of syntactic dependency parsing, we use unlabeled accuracy to calculate the ratio of EDUs that correctly identify their heads, la- beled accuracy the ratio of EDUs that have both correct heads and correct relations. <ref type="table" target="#tab_3">Table 3 and  Table 4</ref> show the performance on two relation sets. The numbers (1-6) represent the corre- sponding feature types described in Section 4.1.</p><p>From <ref type="table" target="#tab_3">Table 3 and Table 4</ref>, we can see that the addition of more feature types, except the 6 th fea- ture type (semantic similarity), can promote the performance of relation labeling, whether using the coarse-grained 19 relations and the fine- grained 111 relations. As expected, the first and second types of features (WORD and POS) are the ones which play an important role in building and labeling the discourse dependency trees. These two types of features attain similar per- formance on two relation sets. The Eisner algo- rithm can achieve unlabeled accuracy around 0.36 and labeled accuracy around 0.26, while MST algorithm achieves unlabeled accuracy around 0.20 and labeled accuracy around 0.14.</p><p>The third feature type (Position) is also very helpful to discourse parsing. With the addition of this feature type, both unlabeled accuracy and labeled accuracy exhibit a marked increase. Es- pecially, when applying MST algorithm on dis- course parsing, unlabeled accuracy rises from around 0.20 to around 0.73. This result is con- sistent with Hernault's work (2010b) whose ex- periments have exhibited the usefulness of those position-related features. The other two types of features which are related to length and syntactic parsing, only promote the performance slightly.</p><p>As we employed the MIRA learning algorithm, it is possible to identify which specific features are useful, by looking at the weights learned to each feature using the training data. <ref type="table" target="#tab_5">Table 5</ref> se- lects 10 features with the highest weights in ab- solute value for the parser which uses the coarse- grained relations, while <ref type="table" target="#tab_6">Table 6</ref> selects the top 10 features for the parser using the fine-grained relations. Each row denotes one feature: the left part before the symbol "&amp;" is from one of the 6 feature types and the right part denotes a specific relation. From <ref type="table" target="#tab_5">Table 5 and Table 6</ref>, we can see that some features are reasonable. For example, The sixth feature in <ref type="table" target="#tab_5">Table 5</ref> represents that the dependency relation is preferred to be labeled Explanation with the fact that "because" is the first word of the dependent EDU. From these two tables, we also observe that most of the heavily weighted features are usually related to those highly distributed relations. When using the coarse-grained relations, the popular relations (eg. Elaboration, Attribution and Joint) are al- ways preferred to be labeled. When using the fine-grained relations, the large relations includ- ing List and Elaboration-object-attribute-e are given the precedence of labeling. This phenome- non is mainly caused by the sparseness of the training corpus and the imbalance of relations. To solve this problem, the augment of training corpus is necessary.    Unlike previous discourse parsing approaches, our methods combine tree building and relation labeling into a uniform framework naturally. This means that relations play a role in building the dependency tree structure. From <ref type="table" target="#tab_3">Table 3 and  Table 4</ref>, we can see that fine-grained relations are more helpful to building unlabeled discourse trees more than the coarse-grained relations. The best result of unlabeled accuracy using 111 rela- tions is 0.7506, better than the best performance (0.7447) using 19 relations. We can also see that the labeled accuracy using the fine-grained rela- tions can achieve 0.4309, only 0.06 lower than the best labeled accuracy (0.4915) using the coarse-grained relations.</p><p>In addition, comparing the MST algorithm with the Eisner algorithm, <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table" target="#tab_4">Table 4</ref> show that their performances are not significant- ly different from each other. But we think that MST algorithm has more potential in discourse dependency parsing, because our converted dis- course dependency treebank contains only pro- jective trees and somewhat suppresses the MST algorithm to exhibit its advantage of parsing non- projective trees. In fact, we observe that some non-projective dependencies produced by the MST algorithm are even reasonable than what they are in the dependency treebank. Thus, it is important to build a manually labeled discourse dependency treebank, which will be our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with Other Systems</head><p>The state-of-the-art discourse parsing methods normally produce the constituency based dis- course trees. To comprehensively evaluate the performance of a labeled constituency tree, the blank tree structure ('S'), the tree structure with nuclearity indication ('N'), and the tree structure with rhetorical relation indication but no nuclear- ity indication ('R') are evaluated respectively using the F measure (Marcu 2000).</p><p>To compare our discourse parsers with others, we adopt MIRA and Eisner algorithm to conduct discourse parsing with all the 6 types of features and then convert the produced projective de- pendency trees to constituency based trees through their correspondence as stated in Section 2. Our parsers using two relation sets are named Our-coarse and Our-fine respectively. The in- putted EDUs of our parsers are from the standard segmentation of RST-DT. Other text-level dis- course parsing methods include: (1) Percep- coarse: we replace MIRA with the averaged per- ceptron learning algorithm and the other settings are the same with Our-coarse; (2) HILDA- manual and HILDA-seg are from Hernault (2010b)'s work, and their inputted EDUs are from RST-DT and their own EDU segmenter respectively; (3) LeThanh indicates the results given by <ref type="bibr">LeThanh el al. (2004)</ref>, which built a multi-level rule based parser and used 14 rela- tions evaluated on 21 documents from RST-DT; (4) Marcu denotes the results given by Mar- cu(2000)'s decision-tree based parser which used 15 relations evaluated on unspecified documents. <ref type="table" target="#tab_8">Table 7</ref> shows the performance comparison for all the parsers mentioned above. Human de- notes the manual agreement between two human annotators. From this table, we can see that both our parsers perform better than all the other parsers as a whole, though our parsers are not developed directly for constituency based trees. Our parsers do not exhibit obvious advantage than HILDA-manual on labeling the blank tree structure, because our parsers and HILDA- manual all perform over 94% of Human and this performance level somewhat reaches a bottle- neck to promote more. However, our parsers outperform the other parsers on both nuclearity and relation labeling. Our-coarse achieves 94.2% and 91.8% of the human F-scores, on labeling nuclearity and relation respectively, while Our- fine achieves 95.2% and 87.6%. We can also see that the averaged perceptron learning algorithm, though simple, can achieve a comparable per- formance, better than HILDA-manual. The parsers HILDA-seg, LeThanh and Marcu use their own automatic EDU segmenters and exhibit a relatively low performance. This means that EDU segmentation is important to a practical discourse parser and worth further investigation.   Macro-averaged F-score is not influenced by the number of instances that are contained in each relation. Weight-averaged F-score (WAFS) weights the performance of each relation by the number of its existing instances. <ref type="table" target="#tab_9">Table 8</ref> com- pares our parser Our-coarse with other parsers HILDA-manual, <ref type="bibr" target="#b7">Feng (Feng and Hirst, 2012)</ref> and <ref type="bibr">Baseline. Feng (Feng and Hirst, 2012)</ref> can be seen as a strengthened version of HILDA which adopts more features and conducts feature selection. Baseline always picks the most fre- quent relation (i.e. Elaboration). From the results, we find that Our-coarse consistently provides superior performance for most relations over other parsers, and therefore results in higher MAFS and WAFS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>So far, the existing discourse parsing techniques are mainly based on two well-known treebanks. One is the Penn Discourse TreeBank (PDTB) ( <ref type="bibr" target="#b18">Prasad et al., 2007</ref>) and the other is RST-DT.</p><p>PDTB adopts the predicate-arguments repre- sentation by taking an implicit/explicit connec- tive as a predication of two adjacent sentences (arguments). Then the discourse relation between each pair of sentences is annotated independently to characterize its predication. A majority of re- searches regard discourse parsing as a classifica- tion task and mainly focus on exploiting various linguistic features and classifiers when using PDTB <ref type="bibr" target="#b28">(Wellner et al., 2006;</ref><ref type="bibr" target="#b17">Pitler et al., 2009;</ref><ref type="bibr" target="#b27">Wang et al., 2010)</ref>. However, the predicate- arguments annotation scheme itself has such a limitation that one can only obtain the local dis- course relations without knowing the rich context.</p><p>In contrast, RST and its treebank enable peo- ple to derive a complete representation of the whole discourse. Researches have begun to in- vestigate how to construct a RST tree for the given text. Since the RST tree is similar to the constituency based syntactic tree except that the constituent nodes are different, the syntactic parsing techniques have been borrowed for dis- course parsing <ref type="bibr" target="#b22">(Soricut and Marcu, 2003;</ref><ref type="bibr" target="#b0">Baldridge and Lascarides, 2005;</ref><ref type="bibr" target="#b21">Sagae, 2009;</ref><ref type="bibr" target="#b9">Hernault et al., 2010b;</ref><ref type="bibr" target="#b7">Feng and Hirst, 2012)</ref>. <ref type="bibr" target="#b22">Soricut and Marcu (2003)</ref> use a standard bottom- up chart parsing algorithm to determine the dis- course structure of sentences. <ref type="bibr" target="#b0">Baldridge and Lascarides (2005)</ref> model the process of discourse parsing with the probabilistic head driven parsing techniques. Sagae (2009) apply a transition based constituent parsing approach to construct a RST tree for a document. <ref type="bibr" target="#b9">Hernault et al. (2010b)</ref> de- velop a greedy bottom-up tree building strategy for discourse parsing. The two adjacent text spans with the closest relations are combined in each iteration. As the extension of Hernault's work, <ref type="bibr" target="#b7">Feng and Hirst (2012)</ref> further explore var- ious features aiming to achieve better perfor- mance. However, as analyzed in Section 1, there exist three limitations with the constituency based discourse representation and parsing. We innovatively adopt the dependency structure, which can be benefited from the existing RST- DT, to represent the discourse. To the best of our knowledge, this work is the first to apply de- pendency structure and dependency parsing techniques in discourse analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper, we present the benefits and feasi- bility of applying dependency structure in text- level discourse parsing. Through the correspond- ence between constituency-based trees and de- pendency trees, we build a discourse dependency treebank by converting the existing RST-DT. Based on dependency structure, we are able to directly analyze the relations between the EDUs without worrying about the additional interior text spans, and apply the existing state-of-the-art dependency parsing techniques which have a relatively low time complexity. In our work, we use the graph based dependency parsing tech- niques learned from the annotated dependency trees. The Eisner algorithm and the MST algo- rithm are applied to parse the optimal projective and non-projective dependency trees respectively based on the arc-factored model. To calculate the score for each arc, six types of features are ex- plored to represent the arcs and the feature weights are learned based on the MIRA learning technique. Experimental results exhibit the effec- tiveness of the proposed approaches. In the fu- ture, we will focus on non-projective discourse dependency parsing and explore more effective features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>where G T means a possible spanning tree with ( , ) T score T G and (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Pictorial Diagram of Non-projective Trees</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Chu-Liu/Edmonds MST Algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Following (Feng and Hirst, 2012 ;</head><label>2012</label><figDesc>Lin et al., 2009; Hernault et al., 2010b), we explore the following 6 feature types combined with relations to repre- sent each labeled arc &lt;e i , r, e j &gt; . (1) WORD: The first one word, the last one word, and the first bigrams in each EDU, the pair of the two first words and the pair of the two last words in the two EDUs are extracted as features. (2) POS: The first one and two POS tags in each EDU, and the pair of the two first POS tags in the two EDUs are extracted as features. (3) Position: These features concern whether the two EDUs are included in the same sentence, and the positions where the two EDUs are located in one sentence, one paragraph, or one document.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: MIRA based Learning Figure 6 gives the pseudo-code of the MIRA algorithm (McDonld et al., 2005b). This algorithm is designed to update the parameters w using a single training instance   , i T i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Features Weight 1 Last two words in dependent EDU are "ap- peals court" &amp; List 0.576 2 First two words in head EDU are "I 'd" &amp; Attribution 0.385 3 First two words in dependent EDU is "that the" &amp; Elaboration-object-attribute-e 0.348 4 First POS in head EDU is "DT" &amp; List -0.323 5 Last word in dependent EDU is "in" &amp; List -0.286 6 First word in dependent EDU is "racked" &amp; Elaboration-object-attribute-e 0.445 7 First two word pairs are &lt;"In an","But even"&gt; &amp; List -0.252 8 Dependent EDU has a dominating node tagged "CD"&amp; Elaboration-object-attribute-e -0.244 9 First two words in dependent EDU are "pa- tents disputes" &amp; Purpose 0.231 10 First word in dependent EDU is "to" &amp; Purpose 0.230</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>The training part of the corpus is composed of 342 documents and contains 18,765 EDUs, while the test part consists of 38 docu- ments and 2,346 EDUs. The number of EDUs in each document ranges between 2 and 304. Two sets of relations are adopted. One is composed of 19 relations and Table 1 shows the number of each relation in the training and test corpus. The other is composed of 111 relations. Due to space limitation, Table 2 only lists the 10 highest- distributed relations with regard to their frequen- cy in the training corpus. The following experiments are conducted: (1) to measure the parsing performance with differ- ent relation sets and different feature types; (2) to compare our parsing methods with the state-of- the-art discourse parsing methods.</figDesc><table>Relations 
Train Test Relations Train Test 

Elaboration 
6879 
796 
Temporal 
426 
73 
Attribution 
2641 
343 
ROOT 
342 
38 
Joint 
1711 
212 
Compari. 
273 
29 
Same-unit 
1230 
127 
Condition 
258 
48 
Contrast 
944 
146 
Manner. 
191 
27 
Explanation 849 
110 
Summary 
188 
32 
Background 786 
111 
Topic-Cha. 187 
13 
Cause 
785 
82 
Textual 
147 
9 
Evaluation 
502 
80 
TopicCom. 126 
24 
Enablement 500 
46 
Total 
18765 2346 

Table 1: Coarse-grained Relation Distribution 

Relations 
Train Test 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Here, we conduct experiments on two relation sets that contain 19 and 111 labels respectively. At the same time, different feature types are tested their effects on discourse parsing.</figDesc><table>Method Features 
Unlabeled 
Acc. 

Labeled 
Acc. 
Eisner 
1+2 
0.3602 
0.2651 
1+2+3 
0.7310 
0.4855 
1+2+3+4 
0.7370 
0.4868 
1+2+3+4+5 
0.7447 
0.4957 
1+2+3+4+5+6 0.7455 
0.4983 
MST 
1+2 
0.1957 
0.1479 
1+2+3 
0.7246 
0.4783 
1+2+3+4 
0.7280 
0.4795 
1+2+3+4+5 
0.7340 
0.4915 
1+2+3+4+5+6 0.7331 
0.4851 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance Using Coarse-grained Re-
lations. 

Method Feature types 
Unlabeled 
Acc. 

Labeled 
Acc. 
Eisner 
1+2 
0.3743 
0.2421 
1+2+3 
0.7451 
0.4079 
1+2+3+4 
0.7472 
0.4041 
1+2+3+4+5 
0.7506 
0.4254 
1+2+3+4+5+6 0.7485 
0.4288 
MST 
1+2 
0.2080 
0.1300 
1+2+3 
0.7366 
0.4054 
1+2+3+4 
0.7468 
0.4071 
1+2+3+4+5 
0.7494 
0.4288 
1+2+3+4+5+6 0.7460 
0.4309 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance Using Fine-grained Rela-
tions. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Top 10 Feature Weights for Coarse-
grained Relation Labeling (Eisner Algorithm) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Top 10 Feature Weights for Coarse-
grained Relation Labeling (Eisner Algorithm) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><head>Table 7 : Full Parser Evaluation</head><label>7</label><figDesc></figDesc><table>MAFS WAFS Acc 
Our-coarse 
0.454 0.643 66.84 
Percep-coarse 
0.438 0.633 65.37 
Feng 
0.440 0.607 65.30 
HILDA-manual 0.428 0.604 64.18 
Baseline 
-
-
35.82 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="true"><head>Table 8 : Relation Labeling Performance</head><label>8</label><figDesc></figDesc><table>To further compare the performance of rela-
tion labeling, we follow Hernault el al. (2010a) 
and use Macro-averaged F-score (MAFS) to 
evaluate each relation. Due to space limitation, 
we do not list the F scores for each relation. 
</table></figure>

			<note place="foot" n="2"> The two terms &quot;text&quot; and &quot;document&quot; are used interchangeably and represent the same meaning. 3 According to our statistics, there are totally 381 n-ary relations in RST-DT. 4 We set the first nucleus as the only nucleus.</note>

			<note place="foot" n="5"> We can easily get all possible headed binary trees for one more complex text containing more than three EDUs, by extending the 8 possible situations for three EDUs.</note>

			<note place="foot" n="6"> 19 relations include the original 18 relation in RST-DT plus one artificial ROOT relation. The 111 relations also include the ROOT relation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by National </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Probabilistic Head-driven Parsing for Discourse Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lascarides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on Computational Natural Language Learning</title>
		<meeting>the Ninth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="96" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python-Analyzing Text with the Natural Language Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>O&apos;Reilly</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Building a Discourse-tagged Corpus in the Framework of Rhetorical Structure Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">E</forename><surname>Okurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second SIGdial Workshop on Dis</title>
		<meeting>the Second SIGdial Workshop on Dis</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On the Shortest Arborescence of a Directed Graph, Science Sinica</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoeng-Jin</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tseng-Hong</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965" />
			<biblScope unit="page" from="1396" to="1400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Ultraconservative Online Algorithms for Multiclass Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Optimum Branchings, J. Research of the National Bureau of Standards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Edmonds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Three New Probabilistic Models for Dependency Parsing: An Exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Text-level Discourse Parsing with Rich Linguistic Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Vanessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Semi-supervised Approach to Improve Classification of Infrequent Discourse Relations Using Feature Vector Extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Hernault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-10" />
			<biblScope unit="page" from="399" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">HILDA: A Discourse Parser Using Support Vector Machine Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Hernault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Prendinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Duverle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue and Discourse</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Novel Discriminative Framework for Sentencelevel Discourse Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL &apos;12 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generating Discourse Structures for Written Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huong</forename><surname>Lethanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geetha</forename><surname>Abeysinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Huyck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computational Linguistics</title>
		<meeting>the 20th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="329" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recognizing Implicit Discourse Relations in the Penn Discourse Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Method in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Method in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rhetorical Structure Theory: Toward a Functional Theory of Text Organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The Theory and Practice of Discourse Parsing and Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online Large-Margin Training of Dependency Parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43rd Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Ribarov</surname></persName>
		</author>
		<title level="m">Non-projective Dependency Parsing using Spanning Tree Algorithms, Proceedings of HLT/EMNLP</title>
		<imprint>
			<date type="published" when="2005-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic Sense Prediction for Implicit Discourse Relations in Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 47th ACL</title>
		<meeting>of the 47th ACL</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="683" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
		<idno>2.0</idno>
		<title level="m">The Penn Discourse Treebank</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Annotation Manual. The PDTB Research Group</title>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simple Signals for Complex Rhetorics: On Rhetorical Analysis with Richfeature Support Vector Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Reitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LDV Forum</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="38" to="52" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analysis of discourse structure with syntactic dependencies and data-driven shiftreduce parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Parsing Technologies</title>
		<meeting>the 11th International Conference on Parsing Technologies</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="81" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sentence level discourse parsing using syntactic and lexical information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
		<meeting>the 2003 Conference of the North American Chapter</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An effective discourse parser that uses rich linguistic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Subba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">Di</forename><surname>Eugenio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="566" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Finding Optimum Branchings, Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert Endre Tarjan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="page" from="25" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Max-margin Markov Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">D-LTAG: Extending Lexicalized TAG to Discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="751" to="779" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kernel based Discourse Relation Recognition with Temporal Ordering Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wen Ting Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL&apos;10</title>
		<meeting>of ACL&apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="710" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Classification of Discourse Coherence Relations: an Exploratory Study Using Multiple Knowledge Sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wellner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc.of the 7th SIGDIAL Workshop on Discourse and Dialogue</title>
		<meeting>.of the 7th SIGDIAL Workshop on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="117" to="125" />
		</imprint>
	</monogr>
	<note>Anna Rumshisky and Roser Sauri</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
