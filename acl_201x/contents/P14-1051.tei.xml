<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ReNew: A Semi-Supervised Framework for Generating Domain-Specific Lexicons and Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhang</surname></persName>
							<email>zzhang13@ncsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science North</orgName>
								<orgName type="department" key="dep2">Department of Computer Science North</orgName>
								<orgName type="institution" key="instit1">Carolina State University Raleigh</orgName>
								<orgName type="institution" key="instit2">Carolina State University Raleigh</orgName>
								<address>
									<postCode>27695-8206, 27695-8206</postCode>
									<region>NC, NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munindar</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
							<email>singh@ncsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science North</orgName>
								<orgName type="department" key="dep2">Department of Computer Science North</orgName>
								<orgName type="institution" key="instit1">Carolina State University Raleigh</orgName>
								<orgName type="institution" key="instit2">Carolina State University Raleigh</orgName>
								<address>
									<postCode>27695-8206, 27695-8206</postCode>
									<region>NC, NC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ReNew: A Semi-Supervised Framework for Generating Domain-Specific Lexicons and Sentiment Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="542" to="551"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The sentiment captured in opinionated text provides interesting and valuable information for social media services. However, due to the complexity and diversity of linguistic representations, it is challenging to build a framework that accurately extracts such sentiment. We propose a semi-supervised framework for generating a domain-specific sentiment lexicon and inferring sentiments at the segment level. Our framework can greatly reduce the human effort for building a domain-specific sentiment lexicon with high quality. Specifically, in our evaluation, working with just 20 manually labeled reviews, it generates a domain-specific sentiment lexicon that yields weighted average F-Measure gains of 3%. Our sentiment classification model achieves approximately 1% greater accuracy than a state-of-the-art approach based on elementary discourse units.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatically extracting sentiments from user- generated opinionated text is important in build- ing social media services. However, the complex- ity and diversity of the linguistic representations of sentiments make this problem challenging.</p><p>High-quality sentiment lexicons can improve the performance of sentiment analysis models over general-purpose lexicons <ref type="bibr" target="#b1">(Choi and Cardie, 2009)</ref>. More advanced methods such as ( <ref type="bibr" target="#b7">Kanayama and Nasukawa, 2006</ref>) adopt domain knowledge by ex- tracting sentiment words from the domain-specific corpus. However, depending on the context, the same word can have different polarities even in the same domain <ref type="bibr" target="#b11">(Liu, 2012)</ref>.</p><p>In respect to sentiment classification, <ref type="bibr" target="#b15">Pang et al. (2002)</ref> infer the sentiments using basic features, such as bag-of-words. To capture more complex linguistic phenomena, leading approaches <ref type="bibr" target="#b14">(Nakagawa et al., 2010;</ref><ref type="bibr" target="#b6">Jo and Oh, 2011;</ref><ref type="bibr" target="#b8">Kim et al., 2013</ref>) apply more advanced models but assume one document or sentence holds one sentiment. However, this is often not the case. Sentiments can change within one document, one sentence, or even one clause. Also, existing approaches in- fer sentiments without considering the changes of sentiments within or between clauses. However, these changes can be successfully exploited for in- ferring fine-grained sentiments.</p><p>To address the above shortcomings of lexicon and granularity, we propose a semi-supervised framework named ReNew. (1) Instead of us- ing sentences, ReNew uses segments as the basic units for sentiment classification. Segments can be shorter than sentences and therefore help cap- ture fine-grained sentiments. (2) ReNew leverages the relationships between consecutive segments to infer their sentiments and automatically generates a domain-specific sentiment lexicon in a semi-su- pervised fashion. (3) To capture the contextual sentiment of words, ReNew uses dependency re- lation pairs as the basic elements in the generated sentiment lexicon.  Consider a part of a review from Tripadvisor. <ref type="bibr">1</ref> We split it into six segments with sentiment labels. ". . . (1: POS) The hotel was clean and comfortable. (2: POS) Service was friendly (3: POS) even providing us a late-morning check-in. (4: POS) The room was quiet and comfortable, (5: NEG) but it was beginning to show a few small signs of wear and tear. . . . " <ref type="figure" target="#fig_1">Figure 1</ref> visualizes the sentiment changes within the text. The sentiment remains the same across Segments 1 to 4. The sentiment transi- tion between Segments 4 and 5 is indicated by the transition cue "but"-which signals conflict and contradiction. Assuming we know Segment 4 is positive, given the fact that Segment 5 starts with "but," we can infer with high confidence that the sentiment in Segment 5 changes to neutral or nega- tive even without looking at its content. After clas- sifying the sentiment of Segment 5 as NEG, we associate the dependency relation pairs {"sign", "wear"} and {"sign", "tear"} with that sentiment.</p><p>ReNew can greatly reduce the human effort for building a domain-specific sentiment lexicon with high quality. Specifically, in our evaluation on two real datasets, working with just 20 manu- ally labeled reviews, ReNew generates a domain- specific sentiment lexicon that yields weighted av- erage F-Measure gains of 3%. Additionally, our sentiment classification model achieves approxi- mately 1% greater accuracy than a state-of-the- art approach based on elementary discourse units ( <ref type="bibr" target="#b10">Lazaridou et al., 2013)</ref>.</p><p>The rest of this paper is structured as follows. Section 2 introduces some essential background. Section 3 illustrates ReNew. Section 4 presents our experiments and results. Section 5 reviews some related work. Section 6 concludes this pa- per and outlines some directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Let us introduce some of the key terminology used in ReNew. A segment is a sequence of words that represents at most one sentiment. A seg- ment can consist of multiple consecutive clauses, up to a whole sentence. Or, it can be shorter than a clause. A dependency relation defines a binary relation that describes whether a pairwise syntactic relation among two words holds in a sen- tence. In ReNew, we exploit the Stanford typed dependency representations <ref type="bibr" target="#b2">(de Marneffe et al., 2006</ref>) that use triples to formalize dependency re- lations. A domain-specific sentiment lexicon con- tains three lists of dependency relations, associ- ated respectvely with positive, neutral, or negative sentiment.</p><p>Given a set of reviews, the tasks of senti- ment analysis in ReNew are (1) splitting each re- view into segments, (2) associating each segment with a sentiment label (positive, neutral, nega- tive), and (3) automatically generating a domain- specific sentiment lexicon. We employ Condi- tional Random Fields ( <ref type="bibr" target="#b9">Lafferty et al., 2001</ref>) to pre- dict the sentiment label for each segment. Given a sequence of segments ¯ x = (x 1 , · · · , x n ) and a se- quence of sentiment labels ¯ y = (y 1 , · · · , y n ), the CRFs model p(¯ y|¯ x) as follows.</p><formula xml:id="formula_0">p(¯ y|¯ x) = 1 Z(¯ x) exp J j (ω j · F j (¯ x, ¯ y)) F j (¯ x, ¯ y) = n i=1 f j (y i−1 , y i , ¯ x, i)</formula><p>where ω is a set of weights learned in the train- ing process to maximize p(¯ y|¯ x). Z(¯ x) is a nor- malization constant that is the sum of all possible label sequences. And, F j is a feature function that sums f j over i ∈ (1, n), where n is the length of ¯ y, and f j can have arbitrary dependencies on the observation sequence ¯ x and neighboring labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bootstrapping Process</head><p>Sentiment Labeling or Learner Retraining  a general sentiment lexicon and a small labeled training dataset. We use a general sentiment lexi- con and the training dataset as prior knowledge to build the initial learners. On each iteration in the bootstrapping process, additional unlabeled data is first segmented. Sec- ond, the learners predict labels for segments based on current knowledge. Third, the lexicon gener- ator determines which newly learned dependency relation triples to promote to the lexicon. At the end of each iteration, the learners are retrained via the updated lexicon so as to classify better on the next iteration. After labeling all of the data, we obtain the final version of our learners along with a domain-specific lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Rule-Based Segmentation Algorithm</head><p>Algorithm 1 Rule-based segmentation.</p><p>Require: Review dataset T 1: for all review r in T do 2:</p><p>Remove HTML tags 3:</p><p>Expand typical abbreviations 4:</p><p>Mark special name-entities <ref type="bibr">5:</ref> for all sentence m in r do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>while m contains a transition cue and m is not empty do Add the remaining part in m as segment s into segment list <ref type="bibr">12:</ref> end for 13: end for</p><p>The algorithm starts with a review dataset T. Each review r from dataset T is first normalized by a set of hard-coded rules (lines 2-4) to remove unnecessary punctuations and HTML tags, expand typical abbreviations, and mark special name enti- ties (e.g., replace a URL by #LINK# and replace a monetary amount "$78.99" by #MONEY#).</p><p>After the normalization step, it splits each re- view r into sentences, and each sentence into sub- clauses (lines 6-10) provided transition cues oc- cur. In effect, the algorithm converts each review into a set of segments.</p><p>Note that ReNew captures and uses the senti- ment changes. Therefore, our segmentation algo- rithm considers only two specific types of transi- tion cues including contradiction and emphasis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentiment Labeling</head><p>ReNew starts with a small labeled training set. Knowledge from this initial training set is not suf- ficient to build an accurate sentiment classification model or to generate a domain-specific sentiment lexicon. Unlabeled data contains rich knowledge, and it can be easily obtained. To exploit this re- source, on each iteration, the sentiment labeling component, as shown in <ref type="figure" target="#fig_5">Figure 3</ref>, labels the data by using multiple learners and a label integrator. We have developed a forward (FR) and a back- ward relationship (BR) learner to learn relation- ships among segments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">FR and BR Learners</head><p>The FR learner learns the relationship between the current segment and the next. Given the senti- ment label and content of a segment, it tries to find the best possible sentiment label of the next seg- ment. The FR Learner tackles the following situa- tion where two segments are connected by a tran- sition word, but existing knowledge is insufficient to infer the sentiment of the second segment. For instance, consider the following review sentence. <ref type="bibr">2</ref> (1) The location is great, (2) but the staff was pretty ho-hum about everything from checking in, to AM hot coffee, to PM bar.</p><p>The sentence contains two segments. We can easily infer the sentiment polarity of Segment 1 based on the word "great" that is commonly in- cluded in many general sentiment lexicons. For Segment 2, without any context information, it is difficult to infer its sentiment. Although the word "ho-hum" indicates a negative polarity, it is not a frequent word. However, the conjunc- tion "but" clearly signals a contrast. So, given the fact that the former segment is positive, a pre- trained FR learner can classify the latter as neg- ative. The Backward Relationship (BR) learner does the same but with the segments in each re- view in reverse order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Label Integrator</head><p>Given the candidate sentiment labels suggested by the two learners, the label integrator first selects the label with confidence greater than or equal to a preset threshold. Segments are left unlabeled if their candidate labels belong to mutually exclusive categories with the same confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Lexicon Generator</head><p>In each iteration, after labeling a segment, the lexi- con generator identifies new triples automatically. As shown in <ref type="figure" target="#fig_7">Figure 4</ref>, this module contains two parts: a Triple Extractor and a Lexicon Integra- tor. For each sentiment, the Triple Extractor (TE) extracts candidate dependency relation triples us- ing a novel rule-based approach. The Lexicon Integrator (LI) evaluates the proposed candidates and promotes the most supported candidates to the corresponding sentiment category in the domain- specific lexicon.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Triple Extractor (TE)</head><p>The TE follows the steps below, for segments that contain only one clause, as demonstrated in <ref type="figure" target="#fig_8">Figure 5</ref> for "The staff was slow and defi- nitely not very friendly." The extracted triples are root nsubj(slow, staff), nsubj(slow, staff), and nsubj(not friendly, staff).</p><p>1. Generate a segment's dependency parse tree. 2. Identify the root node of each clause in the segment. <ref type="table" target="#tab_0">Table 1</ref>. 4. Apply the rules in <ref type="table" target="#tab_1">Table 2</ref> to add or modify triples. 5. Suggest the types of triples marked L in <ref type="table" target="#tab_0">Ta- ble 1</ref> to the lexicon integrator.  <ref type="table" target="#tab_0">Table 1</ref> describes all seven types of triples used in the domain-specific lexicon. Among them, amod, acomp, and nsubj are as in <ref type="bibr" target="#b2">(de Marneffe et al., 2006</ref>). And, root amod captures the root node of a sentence when it also appears in the ad- jectival modifier triple, similarly for root acomp and root nsubj. We observe that the word of the root node is often related to the sentiment of a sen- tence and this is especially true when this word also appears in the adjectival modifier, adjectival complement, or negation modifier triple. <ref type="bibr" target="#b19">Zhang et al. (2010)</ref> propose the no pattern that describes a word pair whose first word is "No" followed by a noun or noun phrase. They show that this pattern is a useful indicator for sentiment analysis. In our dataset, in addition to "No," we observe the frequent usage of "Nothing" followed by an adjective. For example, users may express a negative feeling about a hotel using sentence such as "Nothing special." Therefore, we create the neg pattern to capture a larger range of possible word pairs. In ReNew, neg pattern is "No" or "Nothing" followed by a noun or noun phrase or an adjective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Remove all triples except those marked E in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Lexicon Integrator (LI)</head><p>The Lexicon Integrator promotes candidate triples with a frequency greater than or equal to a preset 545 threshold. The frequency list is updated in each iteration. The LI first examines the prior knowl- edge represented as an ordered list of the gover- nors of all triples, each is attached with an ordered list of its dependents. Then, based on the triples promoted in this iteration, the order of the gover- nors and their dependents is updated. Triples are not promoted if their governors or dependents ap- pear in a predetermined list of stopwords.</p><p>The LI promotes triples by respecting mutual exclusion and the existing lexicon. In particular, it does not promote triples if they exist in multiple sentiment categories or if they already belong to a different sentiment category.</p><p>Finally, for each sentiment, we obtain seven sorted lists corresponding to amod, acomp, nsubj, root amod, root acomp, root nsubj, and neg pattern. These lists form the domain-specific sentiment lexicon. </p><formula xml:id="formula_1">w i = w dep + " neg(w gov , w dep ); + w i w i = w gov ; R 2</formula><p>Build Relationships word w i and w j ; amod(w gov , w i ) (conj and, amod) conj and(w i , w j ); amod(w gov , w j ) amod(w gov , w i ); R 3</p><p>Build Relationships word w i and w j ; acomp(w gov , w i ) (conj and, acomp) conj and(w i , w j ); acomp(w gov , w j ) acomp(w gov , w i ); R 4</p><p>Build Relationships word w i and w j ; nsubj(w i , w dep ) (conj and, nsubj) conj and(w i , w j ); nsubj(w j , w dep ) nsubj(w i , w dep );</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learner Retraining</head><p>At the end of each iteration, ReNew retrains each learner as shown in <ref type="figure" target="#fig_9">Figure 6</ref>. Newly labeled seg- ments are selected by a filter. Then, given an up- dated lexicon, learners are retrained to perform better on the next iteration. Detailed description of the filter and learner are presented below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Filter</head><p>The filter seeks to prevent labeling errors from accumulating during bootstrapping. In ReNew, newly acquired training samples are segments with labels that are predicted by old learners. Each predicted label is associated with a confidence value. The filter is applied to select those labeled segments with confidence greater than or equal to a preset threshold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learner Retraining</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Learner</head><p>As Section 3.2 describes, ReNew uses learners to capture different types of relationships among seg- ments to classify sentiment by leveraging these relationships. Each learner contains two com- ponents: a feature extractor and a classification model. To train a learner, the feature extractor first converts labeled segments into feature vectors Grammar: part-of-speech tag of every word, the type of phrases and clauses (if known). Opinion word: To exploit a general sentiment lexicon, we use two binary features indicat- ing the presence or absence of a word in the positive or negative list in a general sentiment lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependency relation: The lexicon generated by</head><p>ReNew uses the Stanford typed dependency representation as its structure. Transition cue: For tracking the changes of the sentiment, we exploit seven types of transi- tion cues, as shown in <ref type="table" target="#tab_2">Table 3</ref>. Punctuation, special name-entity, and seg- ment position: Some punctuation symbols, such as "!", are reliable carriers of senti- ments. We mark special named-entities, such as time, money, and so on. In addition, we use segment positions (beginning, middle, and end) in reviews as features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To assess ReNew's effectiveness, we prepare two hotel review datasets crawled from Tripadvisor. One dataset contains a total of 4,017 unlabeled re- views regarding 802 hotels from seven US cities. The reviews are posted by 340 users, each of whom contributes at least ten reviews. The other dataset contains 200 reviews randomly selected from Tripadvisor. We collected ground-truth la- bels for this dataset by inviting six annotators in two groups of three. Each group labeled the same 100 reviews. We obtained the labels for each segment consist as positive, neutral, or nega- tive. Fleiss' kappa scores for the two groups were 0.70 and 0.68, respectively, indicating substantial agreement between our annotators. The results we present in the remainder of this section rely upon the following parameter values.</p><p>The confidence thresholds used in the Label In- tegrator and filter are both set to 0.9 for positive labels and 0.7 for negative and neutral labels. The minimum frequency used in the Lexicon Integra- tor for selecting triples is set to 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Feature Function Evaluation</head><p>Our first experiment evaluates the effects of dif- ferent combinations of features. To do this, we first divide all features into four basic feature sets: T (transition cues), P (punctuations, special name- entities, and segment positions), G (grammar), and OD (opinion words and dependency relations). We train 15 sentiment classification models using all basic features and their combinations. <ref type="figure" target="#fig_10">Figure 7</ref> shows the results of a 10-fold cross validation on the 200-review dataset (light grey bars show the accuracy of the model trained without using tran- sition cue features). The feature OD yields the best accuracy, fol- lowed by G, P, and T. Although T yields the worst accuracy, incorporating it improves the resulting accuracy of the other features, as shown by the dark grey bars. In particular, the accuracy of OD is markedly improved by adding T. The model trained using all the feature sets yields the best ac- curacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relationship Learners Evaluation</head><p>Our second experiment evaluates the impact of the relationship learners and the label integrator. To this end, we train and compare sentiment classifi- cation models using three configurations. The first configuration (FW-L) uses only the FR learner; the second (BW-L) only the BR learner. ALL-L uses both the FR and BR learners, together with a label integrator. We evaluate them with 10-fold cross validation on the 200-review dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy</head><p>Macro F-score Micro F-score 0.46 0.48 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FW-L BW-L Both</head><p>Figure 8: Comparison among the learners. <ref type="figure">Figure 8</ref> reports the accuracy, macro F-score, and micro F-score. It shows that the BR learner produces better accuracy and a micro F-score than the FR learner but a slightly worse macro F-score. Jointly considering both learners with the label in- tegrator achieves better results than either alone. The results demonstrate the effectiveness of our sentiment labeling component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Domain-Specific Lexicon Assessment</head><p>Our third experiment evaluates the quality of the domain-specific lexicon automatically generated by ReNew. To do this, we first transform each of the 200 labeled reviews into feature vectors. Then we retrain Logistic Regression models us- ing WEKA ( <ref type="bibr" target="#b4">Hall et al., 2009</ref>). Note that we use only the features extracted from the lexicons them- selves. This is important because to compare only the lexicons' impact on sentiment classification, we need to avoid the effect of other factors, such as syntax, transition cues, and so on. We com- pare models trained using (1) our domain-specific lexicon, (2) Affective Norms for English Words (ANEW) <ref type="bibr" target="#b0">(Bradley and Lang, 1999</ref>), and (3) Lin- guistic Inquiry and Word Count (LIWC) <ref type="bibr" target="#b18">(Tausczik and Pennebaker, 2010)</ref>. ANEW and LIWC are well-known general sentiment lexicons. <ref type="table" target="#tab_3">Table 4</ref> shows the results obtained by 10-fold cross validation. Each weighted average is com- puted according to the number of segments in each class. The table shows the significant advan- tages of the lexicon generated by ReNew. ANEW achieves the highest recall for the positive class, but the lowest recalls in the negative and neutral classes. Regarding the neutral class, both ANEW and LIWC achieve poor results. The weighted av- erage measures indicate our lexicon has the high- est overall quality.</p><p>Our domain-specific lexicon contains dis- tinguishable aspects associated with sentiment words. For example, the aspect "staff" is associ- ated with positive words (e.g., "nice," "friendli," "help," "great," and so on) and negative words (e.g., "okai," "anxiou," "moodi," "effici," and so on). We notice that some positive words also occur on the negative side. This may be for two reasons. First, some sentences that contain positive words may convey a negative sentiment, such as "The staff should be more efficient." Second, the boot- strapping process in ReNew may introduce some wrong words by mistakenly labeling the sentiment of the segments. These challenges suggest useful directions for the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Lexicon Generation and Sentiment Classification</head><p>Our fourth experiment evaluates the robustness of ReNew's lexicon generation process as well as the performance of the sentiment classification mod- els using these lexicons. We first generate ten domain-specific lexicons by repeatedly following these steps: For the first iteration, (1) build a train- ing dataset by randomly selecting 20 labeled re- views (about 220 segments) and (2) train the learn- ers using the training dataset and LIWC. For each iteration thereafter, (1) label 400 reviews from the unlabeled dataset (4,071 reviews) and (2) update the lexicon and retrain the learners. After labeling all of the data, output a domain-specific lexicon.</p><p>To evaluate the benefit of using domain-specific sentiment lexicons, we train ten sentiment classifi- cation models using the ten lexicons and then com- pare them, pairwise, against models trained with the general sentiment lexicon LIWC. Each model consists of an FR learner, a BR learner, and a la- bel integrator. Each pairwise comparison is eval- uated on a testing dataset with 10-fold cross vali- dation. Each testing dataset consists of 180 ran- domly selected reviews (about 1,800 segments). For each of the pairwise comparisons, we conduct a paired t-test to determine if the domain-specific sentiment lexicon can yield better results. <ref type="figure">Figure 9</ref> shows the pairwise comparisons of ac- curacy between the two lexicons. Each group of bars represents the accuracy of two sentiment classification models trained using LIWC (CRFs- General) and the generated domain-specific lexi- con (CRFs-Domain), respectively. The solid line corresponds to a baseline model that takes the ma- jority classification strategy. Based on the dis- tribution of the datasets, the majority class of all datasets is positive. We can see that models using either the general lexicon or the domain-specific lexicon achieve higher accuracy than the baseline model. Domain-specific lexicons produce signif- icantly higher accuracy than general lexicons. In the figures below, we indicate significance to 10%, 5%, and 1% as '·', ' * ', and ' * * ', respectively.  ReNew starts with LIWC and a labeled dataset and generates ten lexicons and sentiment classifi- cation models by iteratively learning 4,017 unla- beled reviews without any human guidance. The above results show that the generated lexicons contain more domain-related information than the general sentiment lexicons. Also, note that the la- beled datasets we used contain only 20 labeled re- views. This is an easy requirement to meet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with Previous Work</head><p>Our fifth experiment compares ReNew with <ref type="bibr">Lazaridou et al.'s (2013)</ref> approach for sentiment classification using discourse relations. Like Re- New, Lazaridou et al.'s approach works on the sub sentential level. However, it differs from Re- New in three aspects. First, the basic units of their model are elementary discourse units (EDUs) from Rhetorical Structure Theory (RST) ( <ref type="bibr" target="#b12">Mann and Thompson, 1988)</ref>. Second, their model con- siders the forward relationship between EDUs, whereas ReNew captures both forward and back- ward relationship between segments. Third, they use a generative model to capture the transition distributions over EDUs whereas ReNew uses a discriminative model to capture the transition se- quences of segments.</p><p>EDUs are defined as minimal units of text and consider many more relations than the two types (1) My husband called the front desk (2) to com- plain.</p><p>Unfortunately, EDU (1) lacks sentiment and EDU (2) lacks the topic. Although Lazaridou et al.'s model can capture the forward relationship between any two consecutive EDUs, it cannot han- dle such cases because their model assumes that each EDU is associated with a topic and a senti- ment. In contrast, ReNew finds just one segment in the above sentence.</p><p>Just to compare with Lazaridou et al., we ap- ply our sentiment labeling component at the level of EDUs. Their labeled dataset contains 65 re- views, corresponding to 1,541 EDUs. Since this dataset is also extracted from Tripadvisor, we use the domain-specific lexicon automatically learned by ReNew based on our 4,071 unlabeled reviews. Follow the same training and testing regimen (10- fold cross validation), we compare ReNew with their model. As shown in <ref type="table" target="#tab_4">Table 5</ref>, ReNew outper- forms their approach on their dataset: Although ReNew is not optimized for EDUs, it achieves bet- ter accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Two bodies of work are relevant. First, to gener- ate sentiment lexicons, existing approaches com- monly generate a sentiment lexicon by extend- ing dictionaries or sentiment lexicons. <ref type="bibr" target="#b5">Hu and Liu (2004)</ref>, manually collect a small set of sen- timent words and expand it iteratively by search- ing synonyms and antonyms in WordNet <ref type="bibr" target="#b13">(Miller, 1995)</ref>. <ref type="bibr" target="#b16">Rao and Ravichandran (2009)</ref> formalize the problem of sentiment detection as a semi- supervised label propagation problem in a graph. Each node represents a word, and a weighted edge between any two nodes indicates the strength of the relationship between them. Esuli and Sebas- tiani (2006) use a set of classifiers in a semi- supervised fashion to iteratively expand a manu- ally defined lexicon. Their lexicon, named Senti- WordNet, comprises the synset of each word ob- tained from WordNet. Each synset is associated with three sentiment scores: positive, negative, and objective.</p><p>Second, for sentiment classification, <ref type="bibr" target="#b14">Nakagawa et al. (2010)</ref> introduce a probabilistic model that uses the interactions between words within one sentence for inferring sentiments. <ref type="bibr" target="#b17">Socher et al. (2011)</ref> introduce a semi-supervised approach that uses recursive autoencoders to learn the hierarchi- cal structure and sentiment distribution of a sen- tence. <ref type="bibr" target="#b6">Jo and Oh (2011)</ref> propose a probabilis- tic generative model named ASUM that can ex- tract aspects coupled with sentiments. <ref type="bibr" target="#b8">Kim et al. (2013)</ref> extend ASUM by enabling its probabilis- tic model to discover a hierarchical structure of the aspect-based sentiments. The above works apply sentence-level sentiment classification and their models are not able to capture the relation- ships between or among clauses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>The leading lexical approaches to sentiment anal- ysis from text are based on fixed lexicons that are painstakingly built by hand. There is little a priori justification that such lexicons would port across application domains. In contrast, ReNew seeks to automate the building of domain-specific lexi- cons beginning from a general sentiment lexicon and the iterative application of CRFs. Our results are promising. ReNew greatly reduces the human effort for generating high-quality sentiment lexi- cons together with a classification model. In fu- ture work, we plan to apply ReNew to additional sentiment analysis problems such as review qual- ity analysis and sentiment summarization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Segments in a Tripadvisor review.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The ReNew framework schematically.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 illustrates</head><label>2</label><figDesc>Figure 2 illustrates ReNew. Its inputs include</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sentiment labeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Lexicon generator module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Extracting sentiment triples from a segment that contains one clause. (a) The initial dependency parse tree. (b) Remove nonsentiment triples. (c) Handle negation triples. (d) Build relationships.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Retrain a relationship learner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Accuracy using different features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 9: Accuracy with different lexicons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 Figure 11 :</head><label>1011</label><figDesc>Figure 10 and 11 show the pairwise comparisons of macro and micro F-score together with the results of the paired t-tests. We can see that the domain-specific lexicons (dark-grey bars) consistently yield better results than their corresponding general lexicons (light-grey bars).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Dependency relation types used in ex-
tracting (E) and domain-specific lexicon (L). 

Types 
Explanation 
E L 

amod 
adjectival modifier 
√ √ 

acomp 
adjectival complement 
√ √ 

nsubj 
nominal subject 
√ √ 

neg 
negation modifier 
√ 

conj and 
words coordinated by "and" 
√ 

or similar 
prep with 
words coordinated by "with" 
√ 

root 
root node 
√ 

root amod 
amod root node 
√ 

root acomp acomp root node 
√ 

root nsubj 
nsubj root node 
√ 

neg pattern "neg" pattern 
√ 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Rules for extracting sentiment triples.</head><label>2</label><figDesc></figDesc><table>Rule Function 
Condition 
Result 

R 1 
Handle Negation 
word w i ; 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : A list of transition types used in ReNew.</head><label>3</label><figDesc></figDesc><table>Transition Types 
Examples 

Agreement, Addition, and Similarity 
also, similarly, as well as, . . . 
Opposition, Limitation, and Contradiction but, although, in contrast, . . . 
Cause, Condition, and Purpose 
if, since, as/so long as, . . . 
Examples, Support, and Emphasis 
including, especially, such as, . . . 
Effect, Consequence, and Result 
therefore, thus, as a result . . . 
Conclusion, Summary, and Restatement 
overall, all in all, to sum up, . . . 
Time, Chronology, and Sequence 
until, eventually, as soon as, . . . 

for training a CRF-based sentiment classification 
model. The feature extractor generates five kinds 
of features as below. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 : Comparison results of different lexicons.</head><label>4</label><figDesc></figDesc><table>ANEW 
LIWC 
ReNew 

Precision Recall F-Measure Precision Recall F-Measure Precision Recall F-Measure 

Positive 
0.59 
0.994 
0.741 
0.606 
0.975 
0.747 
0.623 
0.947 
0.752 
Negative 
0.294 
0.011 
0.021 
0.584 
0.145 
0.232 
0.497 
0.202 
0.288 
Neutral 
0 
0 
0 
0 
0 
0 
0.395 
0.04 
0.073 

Weighted average 
0.41 
0.587 
0.44 
0.481 
0.605 
0.489 
0.551 
0.608 
0.518 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 : Comparison of our framework with pre- vious work on sentiment classification.</head><label>5</label><figDesc></figDesc><table>Method 
Accuracy 

EDU-Model (Lazaridou et al.) 0.594 
ReNew (our method) 
0.605 

of transition cues underlying our segments. We 
posit that EDUs are too fine-grained for sentiment 
analysis. Consider the following sentence from 
Lazaridou et al.'s dataset with its EDUs identified. 
</table></figure>

			<note place="foot" n="1"> http://www.tripadvisor.com/ShowUserReviews-g32655d81765-r100000013</note>

			<note place="foot" n="2"> http://www.tripadvisor.com/ShowUserReviews-g60763d93589-r10006597</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Thanks to Chung-Wei Hang, Chris Healey, James Lester, Steffen Heber, and the anonymous review-ers for helpful comments. This work is supported by the Army Research Laboratory in its Net-work Sciences Collaborative Technology Alliance (NS-CTA) under Cooperative Agreement Number W911NF-09-2-0053 and by an IBM Ph.D. Schol-arship and an IBM Ph.D. fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Affective norms for English words (ANEW): Instruction manual and affective ratings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>Gainesville, FL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>The Center for Research in Psychophysiology, University of Florida</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report C-1</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adapting a polarity lexicon using integer linear programming for domain-specific sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14 th Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 14 th Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="590" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5 th Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the 5 th Conference on Language Resources and Evaluation (LREC)<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SentiWordNet: A publicly available lexical resource for opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5 th Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the 5 th Conference on Language Resources and Evaluation (LREC)<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="417" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The WEKA data mining software: An update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10 th International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the 10 th International Conference on Knowledge Discovery and Data Mining (KDD)<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Aspect and sentiment unification model for online review analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohan</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><forename type="middle">Haeyun</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4 th ACM International Conference on Web Search and Data Mining (WSDM)</title>
		<meeting>the 4 th ACM International Conference on Web Search and Data Mining (WSDM)<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fully automatic lexicon expansion for domainoriented sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuya</forename><surname>Nasukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11 th Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 11 th Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="355" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A hierarchical aspectsentiment model for online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixia</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27 th AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 27 th AAAI Conference on Artificial Intelligence (AAAI)<address><addrLine>Bellevue</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="804" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18 th International Conference on Machine Learning (ICML)</title>
		<meeting>the 18 th International Conference on Machine Learning (ICML)<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Bayesian model for joint unsupervised induction of sentiment, aspect and discourse representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Sporleder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51 st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 51 st Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1630" to="1639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Synthesis Lectures on Human Language Technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
			<pubPlace>San Rafael, CA</pubPlace>
		</imprint>
	</monogr>
	<note>Sentiment Analysis and Opinion Mining</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rhetorical structure theory: Toward a functional theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dependency tree-based sentiment classification using CRFs with hidden variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuji</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11 th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 11 th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)<address><addrLine>Los Angeles</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="786" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Thumbs up?: Sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7 th Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 7 th Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semisupervised polarity lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delip</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12 th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the 12 th Conference of the European Chapter of the Association for Computational Linguistics (EACL)<address><addrLine>Athens</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="675" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16 th Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 16 th Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Edinburgh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The psychological meaning of words: LIWC and computerized text analysis methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Tausczik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennebaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Language and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="54" />
			<date type="published" when="2010-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Extracting and ranking product features in opinion documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suk</forename><forename type="middle">Hwan</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eamonn O&amp;apos;brien-Strain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23 rd International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 23 rd International Conference on Computational Linguistics (COLING)<address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1462" to="1470" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
