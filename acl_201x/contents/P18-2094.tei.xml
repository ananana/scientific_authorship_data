<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Data Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="592" to="598"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>592</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>One key task of fine-grained sentiment analysis of product reviews is to extract product aspects or features that users have expressed opinions on. This paper fo-cuses on supervised aspect extraction using deep learning. Unlike other highly sophisticated supervised deep learning models , this paper proposes a novel and yet simple CNN model 1 employing two types of pre-trained embeddings for aspect extraction: general-purpose embeddings and domain-specific embeddings. Without using any additional supervision, this model achieves surprisingly good results, outper-forming state-of-the-art sophisticated existing methods. To our knowledge, this paper is the first to report such double em-beddings based CNN model for aspect extraction and achieve very good results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aspect extraction is an important task in sentiment analysis ( <ref type="bibr">Hu and Liu, 2004</ref>) and has many applica- tions ( <ref type="bibr">Liu, 2012)</ref>. It aims to extract opinion targets (or aspects) from opinion text. In product reviews, aspects are product attributes or features. For ex- ample, from "Its speed is incredible" in a laptop review, it aims to extract "speed".</p><p>Aspect extraction has been performed using su- pervised ( <ref type="bibr">Jakob and Gurevych, 2010;</ref><ref type="bibr">Chernyshevich, 2014;</ref><ref type="bibr" target="#b9">Shu et al., 2017</ref>) and unsupervised ap- proaches ( <ref type="bibr">Hu and Liu, 2004;</ref><ref type="bibr" target="#b22">Zhuang et al., 2006;</ref><ref type="bibr">Mei et al., 2007;</ref><ref type="bibr" target="#b6">Qiu et al., 2011;</ref><ref type="bibr" target="#b19">Yin et al., 2016;</ref><ref type="bibr">He et al., 2017)</ref>. Recently, supervised deep learn- ing models achieved state-of-the-art performances ( <ref type="bibr">Li and Lam, 2017)</ref>. Many of these models use handcrafted features, lexicons, and complicated neural network architectures ( <ref type="bibr" target="#b5">Poria et al., 2016;</ref><ref type="bibr" target="#b14">Wang et al., 2016</ref><ref type="bibr" target="#b15">Wang et al., , 2017</ref><ref type="bibr">Li and Lam, 2017)</ref>. Al- though these approaches can achieve better per- formances than their prior works, there are two other considerations that are also important. (1) Automated feature (representation) learning is al- ways preferred. How to achieve competitive per- formances without manually crafting features is an important question. (2) According to Occam's ra- zor principle <ref type="bibr" target="#b0">(Blumer et al., 1987)</ref>, a simple model is always preferred over a complex model. This is especially important when the model is deployed in a real-life application (e.g., chatbot), where a complex model will slow down the speed of infer- ence. Thus, to achieve competitive performance whereas keeping the model as simple as possible is important. This paper proposes such a model.</p><p>To address the first consideration, we propose a double embeddings mechanism that is shown cru- cial for aspect extraction. The embedding layer is the very first layer, where all the information about each word is encoded. The quality of the em- beddings determines how easily later layers (e.g., LSTM, CNN or attention) can decode useful infor- mation. Existing deep learning models for aspect extraction use either a pre-trained general-purpose embedding, e.g., <ref type="bibr">GloVe (Pennington et al., 2014</ref>), or a general review embedding ( <ref type="bibr" target="#b5">Poria et al., 2016)</ref>. However, aspect extraction is a complex task that also requires fine-grained domain embeddings. For example, in the previous example, detecting "speed" may require embeddings of both "Its" and "speed". However, the criteria for good embed- dings for "Its" and "speed" can be totally differ- ent. "Its" is a general word and the general em- bedding (trained from a large corpus) is likely to have a better representation for "Its". But, "speed" has a very fine-grained meaning (e.g., how many instructions per second) in the laptop domain, whereas "speed" in general embeddings or general review embeddings may mean how many miles per second. So using in-domain embeddings is im- portant even when the in-domain embedding cor- pus is not large. Thus, we leverage both general embeddings and domain embeddings and let the rest of the network to decide which embeddings have more useful information.</p><p>To address the second consideration, we use a pure Convolutional Neural Network (CNN) ( <ref type="bibr">LeCun et al., 1995</ref>) model for sequence labeling. Al- though most existing models use LSTM <ref type="bibr">(Hochreiter and Schmidhuber, 1997</ref>) as the core building block to model sequences ( <ref type="bibr">Liu et al., 2015;</ref><ref type="bibr">Li and Lam, 2017)</ref>, we noticed that CNN is also success- ful in many NLP tasks <ref type="bibr">(Kim, 2014;</ref><ref type="bibr" target="#b20">Zhang et al., 2015;</ref><ref type="bibr">Gehring et al., 2017)</ref>. One major draw- back of LSTM is that LSTM cells are sequentially dependent. The forward pass and backpropaga- tion must serially go through the whole sequence, which slows down the training/testing process 2 . One challenge of applying CNN on sequence la- beling is that convolution and max-pooling opera- tions are usually used for summarizing sequential inputs and the outputs are not well-aligned with the inputs. We discuss the solutions in Section 3.</p><p>We call the proposed model Dual Embeddings CNN (DE-CNN). To the best of our knowledge, this is the first paper that reports a double embed- ding mechanism and a pure CNN-based sequence labeling model for aspect extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sentiment analysis has been studied at document, sentence and aspect levels ( <ref type="bibr">Liu, 2012;</ref><ref type="bibr">Pang and Lee, 2008;</ref><ref type="bibr">Cambria and Hussain, 2012)</ref>. This work focuses on the aspect level ( <ref type="bibr">Hu and Liu, 2004</ref>). Aspect extraction is one of its key tasks, and has been performed using both unsupervised and supervised approaches. The unsupervised ap- proach includes methods such as frequent pattern mining ( <ref type="bibr">Hu and Liu, 2004;</ref><ref type="bibr" target="#b4">Popescu and Etzioni, 2005</ref>), syntactic rules-based extraction ( <ref type="bibr" target="#b22">Zhuang et al., 2006</ref>; <ref type="bibr" target="#b13">Wang and Wang, 2008;</ref><ref type="bibr" target="#b6">Qiu et al., 2011</ref>), topic modeling ( <ref type="bibr">Mei et al., 2007;</ref><ref type="bibr" target="#b11">Titov and McDonald, 2008;</ref><ref type="bibr">Lin and He, 2009;</ref><ref type="bibr">Moghaddam and Ester, 2011</ref>), word alignment ( <ref type="bibr">Liu et al., 2013</ref>) and label propagation ( <ref type="bibr" target="#b21">Zhou et al., 2013;</ref><ref type="bibr" target="#b8">Shu et al., 2016)</ref>.</p><p>Traditionally, the supervised approach <ref type="bibr">(Jakob and Gurevych, 2010;</ref><ref type="bibr">Mitchell et al., 2013;</ref><ref type="bibr" target="#b9">Shu et al., 2017</ref>) uses Conditional Random Fields (CRF) ( <ref type="bibr">Lafferty et al., 2001</ref>). Recently, deep neural networks are applied to learn better fea- tures for supervised aspect extraction, e.g., us- ing LSTM <ref type="bibr" target="#b16">(Williams and Zipser, 1989;</ref><ref type="bibr">Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr">Liu et al., 2015)</ref> and attention mechanism ( <ref type="bibr" target="#b15">Wang et al., 2017;</ref><ref type="bibr">He et al., 2017</ref>) together with manual features ( <ref type="bibr" target="#b5">Poria et al., 2016;</ref><ref type="bibr" target="#b14">Wang et al., 2016)</ref>. Further, ( <ref type="bibr" target="#b14">Wang et al., 2016</ref><ref type="bibr" target="#b15">Wang et al., , 2017</ref><ref type="bibr">Li and Lam, 2017</ref>) also pro- posed aspect and opinion terms co-extraction via a deep network. They took advantage of the gold- standard opinion terms or sentiment lexicon for as- pect extraction. The proposed approach is close to ( <ref type="bibr">Liu et al., 2015)</ref>, where only the annotated data for aspect extraction is used. However, we will show that our approach is more effective even compared with baselines using additional supervi- sions and/or resources.</p><p>The proposed embedding mechanism is related to cross domain embeddings ( <ref type="bibr">Bollegala et al., 2015</ref><ref type="bibr">Bollegala et al., , 2017</ref> and domain-specific embeddings ( <ref type="bibr">Xu et al., 2018a,b)</ref>. However, we require the domain of the domain embeddings must exactly match the domain of the aspect extraction task. CNN ( <ref type="bibr">LeCun et al., 1995;</ref><ref type="bibr">Kim, 2014</ref>) is recently adopted for named entity recognition ( <ref type="bibr" target="#b10">Strubell et al., 2017)</ref>. CNN classifiers are also used in sentiment analysis ( <ref type="bibr" target="#b5">Poria et al., 2016;</ref><ref type="bibr">Chen et al., 2017)</ref>. We adopt CNN for sequence labeling for aspect extraction because CNN is simple and parallelized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>The proposed model is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. It has 2 embedding layers, 4 CNN layers, a fully- connected layer shared across all positions of words, and a softmax layer over the labeling space Y = {B, I, O} for each position of inputs. Note that an aspect can be a phrase and B, I indicate the beginning word and non-beginning word of an aspect phrase and O indicates non-aspect words.</p><p>Assume the input is a sequence of word indexes x = (x 1 , . . . , x n ). This sequence gets its two corresponding continuous representations x g and x d via two separate embedding layers (or embed- ding matrices) W g and W d . The first embedding matrix W g represents general embeddings pre- trained from a very large general-purpose corpus (usually hundreds of billions of tokens). The sec- ond embedding matrix W d represents domain em- beddings pre-trained from a small in-domain cor- pus, where the scope of the domain is exactly the domain that the training/testing data belongs to. As a counter-example, if the training/testing data is in the laptop domain, then embeddings from the electronics domain are considered to be out-of- domain embeddings (e.g., the word "adapter" may represent different types of adapters in electronics rather than exactly a laptop adapter). That is, only laptop reviews are considered to be in-domain.</p><p>We do not allow these two embedding layers trainable because small training examples may lead to many unseen words in test data. If em- beddings are tunable, the features for seen words' embeddings will be adjusted (e.g., forgetting use- less features and infusing new features that are re- lated to the labels of the training examples). And the CNN filters will adjust to the new features ac- cordingly. But the embeddings of unseen words from test data still have the old features that may be mistakenly extracted by CNN.</p><p>Then we concatenate two embeddings x (1) = x g ⊕ x d and feed the result into a stack of 4 CNN layers. A CNN layer has many 1D-convolution fil- ters and each (the r-th) filter has a fixed kernel size k = 2c+1 and performs the following convolution </p><formula xml:id="formula_0">x (l+1) i,r = max 0, ( c j=−c w (l) j,r x (l) i+j ) + b (l) r ,<label>(1)</label></formula><p>where l indicates the l-th CNN layer. We apply each filter to all positions i = 1 : n. So each fil- ter computes the representation for the i-th word along with 2c nearby words in its context. Note that we force the kernel size k to be an odd num- ber and set the stride step to be 1 and further pad the left c and right c positions with all zeros. In this way, the output of each layer is well-aligned with the original input x for sequence labeling purposes. For the first (l = 1) CNN layer, we employ two different filter sizes. For the rest 3 CNN (l ∈ {2, 3, 4}) layers, we only use one fil- ter size. We will discuss the details of the hyper- parameters in the experiment section. Finally, we apply a fully-connected layer with weights shared across all positions and a softmax layer to com- pute label distribution for each word. The out- put size of the fully-connected layer is |Y| = 3. We apply dropout after the embedding layer and each ReLU activation. Note that we do not apply any max-pooling layer after convolution layers be- cause a sequence labeling model needs good rep- resentations for every position and max-pooling operation mixes the representations of different positions, which is undesirable (we show a max- pooling baseline in the next section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Following the experiments of a recent aspect ex- traction paper ( <ref type="bibr">Li and Lam, 2017)</ref>, we conduct experiments on two benchmark datasets from Se- mEval challenges ( <ref type="bibr" target="#b3">Pontiki et al., 2014</ref><ref type="bibr" target="#b2">Pontiki et al., , 2016</ref> as shown in <ref type="table">Table 4</ref>.1. The first dataset is from the laptop domain on subtask 1 of SemEval-2014 Task 4. The second dataset is from the restaurant do- main on subtask 1 (slot 2) of SemEval-2016 Task 5. These two datasets consist of review sentences with aspect terms labeled as spans of characters.</p><p>We use NLTK 3 to tokenize each sentence into a sequence of words. For the general-purpose embeddings, we use the glove.840B.300d embeddings ( <ref type="bibr" target="#b1">Pennington et al., 2014</ref>), which are pre-trained from a corpus of 840 billion tokens that cover almost all web pages. <ref type="bibr">5</ref> . We set the embedding dimensions to 100 and the number of iterations to 30 (for a small embed- ding corpus, embeddings tend to be under-fitted), and keep the rest hyper-parameters as the defaults in fastText. We further use fastText to compose out-of-vocabulary word embeddings via subword N-gram embeddings.</p><note type="other">These embeddings have 300 dimensions. For domain-specific embeddings, we collect a laptop review corpus and a restaurant review corpus and use fastText (Bojanowski et al., 2016) to train do- main embeddings. The laptop review corpus con- tains all laptop reviews from the Amazon Review Dataset (He and McAuley, 2016). The restaurant review corpus is from the Yelp Review Dataset Challenge 4 . We only use reviews from restaurant categories that the second dataset is selected from</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head><p>We perform a comparison of DE-CNN with three groups of baselines using the standard evaluation of the datasets 6 7 . The results of the first two groups are copied from ( <ref type="bibr">Li and Lam, 2017</ref>). The first group uses single-task approaches.</p><p>CRF is conditional random fields with basic features <ref type="bibr">8</ref> and GloVe word embedding( <ref type="bibr" target="#b1">Pennington et al., 2014)</ref>.</p><p>IHS RD (Chernyshevich, 2014) and NLANGP (Toh and Su, 2016) are best systems in the original challenges ( <ref type="bibr" target="#b3">Pontiki et al., 2014</ref><ref type="bibr" target="#b2">Pontiki et al., , 2016</ref> BiLSTM-CNN-CRF (Reimers and Gurevych, 2017) is the state-of-the-art from the Named En- tity Recogntion (NER) community. We use this baseline 9 to demonstrate that a NER model may need further adaptation for aspect extraction.</p><p>The second group uses multi-task learning and also take advantage of gold-standard opinion terms/sentiment lexicon.</p><p>RNCRF ( <ref type="bibr" target="#b14">Wang et al., 2016</ref>) is a joint model with a dependency tree based recursive neural net- work and CRF for aspect and opinion terms co- extraction. Besides opinion annotations, it also uses handcrafted features.</p><p>CMLA ( <ref type="bibr" target="#b15">Wang et al., 2017</ref>) is a multi-layer coupled-attention network that also performs as- pect and opinion terms co-extraction. It uses gold- standard opinion labels in the training data.</p><p>MIN ( <ref type="bibr">Li and Lam, 2017</ref>) is a multi-task learn- ing framework that has (1) two LSTMs for jointly extraction of aspects and opinions, and (2) a third LSTM for discriminating sentimental and non- sentimental sentences. A sentiment lexicon and high precision dependency rules are employed to find opinion terms.</p><p>The third group is the variations of DE-CNN. GloVe-CNN only uses glove.840B.300d to show that domain embeddings are important.</p><p>Domain-CNN does not use the general embed- dings to show that domain embeddings alone are not good enough as the domain corpus is limited for training good general words embeddings.</p><p>MaxPool-DE-CNN adds max-pooling in the last CNN layer. We use this baseline to show that the max-pooling operation used in the traditional CNN architecture is harmful to sequence labeling.</p><p>DE-OOD-CNN replaces the domain embed- dings with out-of-domain embeddings to show that a large out-of-domain corpus is not a good replacement for a small in-domain corpus for do- main embeddings. We use all electronics reviews as the out-of-domain corpus for the laptop and all the Yelp reviews for restaurant.</p><p>DE-Google-CNN replaces the glove embed- dings with GoogleNews embeddings 10 , which are pre-trained from a smaller corpus (100 billion to- kens). We use this baseline to demonstrate that general embeddings that are pre-trained from a larger corpus performs better.</p><p>DE-CNN-CRF replaces the softmax activation with a CRF layer <ref type="bibr">11</ref> . We use this baseline to  demonstrate that CRF may not further improve the challenging performance of aspect extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hyper-parameters</head><p>We hold out 150 training examples as validation data to decide the hyper-parameters. The first CNN layer has 128 filters with kernel sizes k = 3 (where c = 1 is the number of words on the left (or right) context) and 128 filters with kernel sizes k = 5 (c = 2). The rest 3 CNN layers have 256 filters with kernel sizes k = 5 (c = 2) per layer. The dropout rate is 0.55 and the learning rate of Adam optimizer <ref type="bibr">(Kingma and Ba, 2014</ref>) is 0.0001 because CNN training tends to be unstable. <ref type="table">Table 4</ref>.3 shows that DE-CNN performs the best. The double embedding mechanism improves the performance and in-domain embeddings are im- portant. We can see that using general embeddings (GloVe-CNN) or domain embeddings (Domain- CNN) alone gives inferior performance. We fur- ther notice that the performance on Laptops and Restaurant domains are quite different. Lap- tops has many domain-specific aspects, such as "adapter". So the domain embeddings for Lap- tops are better than the general embeddings. The Restaurant domain has many very general aspects like "staff", "service" that do not deviate much from their general meanings. So general embed- dings are not bad. Max pooling is a bad op- eration as indicated by MaxPool-DE-CNN since the max pooling operation loses word positions. DE-OOD-CNN's performance is poor, indicating that making the training corpus of domain embed- dings to be exactly in-domain is important. DE- Google-CNN uses a much smaller training corpus for general embeddings, leading to poorer perfor- mance than that of DE-CNN. Surprisingly, we no- tice that the CRF layer (DE-CNN-CRF) does not help. In fact, the CRF layer can improve 1-2% when the laptop's performance is about 75%. But it doesn't contribute much when laptop's perfor- mance is above 80%. CRF is good at modeling label dependences (e.g., label I must be after B), but many aspects are just single words and the ma- jor types of errors (mentioned later) do not fall in what CRF can solve. Note that we did not tune the hyperparameters of DE-CNN-CRF for practi- cal purpose because training the CRF layer is ex- tremely slow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and Analysis</head><p>One important baseline is BiLSTM-CNN-CRF, which is markedly worse than our method. We believe the reason is that this baseline leverages dependency-based embeddings( <ref type="bibr">Levy and Goldberg, 2014</ref>), which could be very important for NER. NER models may require further adapta- tions (e.g., domain embeddings) for opinion texts.</p><p>DE-CNN has two major types of errors. One type comes from inconsistent labeling (e.g., for the restaurant data, the same aspect is sometimes labeled and sometimes not). Another major type of errors comes from unseen aspects in test data that require the semantics of the conjunction word "and" to extract. For example, if A is an aspect and when "A and B" appears, B should also be ex- tracted but not. We leave this to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a CNN-based aspect extraction model with a double embeddings mechanism without extra supervision. Experimental results demon- strated that the proposed method outperforms state-of-the-art methods with a large margin. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of DE-CNN: red vectors are zero vectors; purple triangles are CNN filters.</figDesc><graphic url="image-1.png" coords="3,73.13,62.81,216.00,203.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>16 Restaurant 2000/1743 676/622 Table 1: Dataset description with the number of sentences(#S.) and number of aspect terms(#A.)</head><label></label><figDesc></figDesc><table>Description 
Training 
Testing 
#S./#A. 
#S./#A. 
SemEval-14 Laptop 
3045/2358 800/654 
SemEval-operation and ReLU activation: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison results in F 1 score: numbers 
in the third group are averaged scores of 5 runs as 
in (Li and Lam, 2017). * indicates the result is 
statistical significant at the level of 0.05. 

</table></figure>

			<note place="foot" n="1"> The code of this paper can be found at https://www. cs.uic.edu/ ˜ hxu/.</note>

			<note place="foot" n="2"> We notice that a GPU with more cores has no training time gain on a low-dimensional LSTM because extra cores are idle and waiting for the other cores to sequentially compute cells.</note>

			<note place="foot" n="3"> http://www.nltk.org/ 4 https://www.yelp.com/dataset/ challenge 5 http://www.cs.cmu.edu/ ˜ mehrbod/RR/ Cuisines.wht 6 http://alt.qcri.org/semeval2014/task4 7 http://alt.qcri.org/semeval2016/task5 8 http://sklearn-crfsuite.readthedocs. io/en/latest/tutorial.html</note>

			<note place="foot" n="9"> https://github.com/UKPLab/ emnlp2017-bilstm-cnn-crf 10 https://code.google.com/archive/p/ word2vec/ 11 https://github.com/allenai/allennlp</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Occam&apos;s razor. Information processing letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Ehrenfeucht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Haussler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="377" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 5: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alsmadi</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orphée</forename><surname>De Clercq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international workshop on semantic evaluation (SemEval-2016)</title>
		<meeting>the 10th international workshop on semantic evaluation (SemEval-2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
		<respStmt>
			<orgName>Computational Linguistics and Dublin City University</orgName>
		</respStmt>
	</monogr>
	<note>Association for</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extracting product features and opinions from reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana-Maria</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-EMNLP &apos;05</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="339" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Aspect extraction for opinion mining with a deep convolutional neural network. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Opinion word expansion and target extraction through double propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="27" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lifelong-rl: Lifelong relaxation labeling for separating entities and aspects in opinion targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annice</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lifelong learning crf for supervised aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Short Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="148" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast and accurate entity recognition with iterated dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2670" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A joint model of text and aspect ratings for sentiment summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL &apos;08: HLT</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="308" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nlangp at semeval2016 task 5: Improving aspect based sentiment analysis using neural network features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international workshop on semantic evaluation (SemEval-2016)</title>
		<meeting>the 10th international workshop on semantic evaluation (SemEval-2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="282" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bootstrapping both product features and opinion words from chinese customer reviews with cross-inducing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP &apos;08</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="289" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokui</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06679</idno>
		<title level="m">Recursive neural conditional random fields for aspect-based sentiment analysis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coupled multi-layer attentions for co-extraction of aspect and opinion terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokui</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3316" to="3322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lifelong domain word embedding via metalearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dual attention network for product compatibility and function satisfiability analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unsupervised word and dependency path embeddings for aspect term extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaimeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07843</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Collective opinion target extraction in Chinese microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP &apos;13</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1840" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Movie review mining and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;06</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
