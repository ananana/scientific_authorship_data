<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Decision-Theoretic Approach to Natural Language Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Mckinley</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of EECS Case</orgName>
								<orgName type="department" key="dep2">Department of EECS Case Western Reserve University Cleveland</orgName>
								<orgName type="institution">Western Reserve University Cleveland</orgName>
								<address>
									<region>OH, OH</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Ray</surname></persName>
							<email>sray@case.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of EECS Case</orgName>
								<orgName type="department" key="dep2">Department of EECS Case Western Reserve University Cleveland</orgName>
								<orgName type="institution">Western Reserve University Cleveland</orgName>
								<address>
									<region>OH, OH</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Decision-Theoretic Approach to Natural Language Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="552" to="561"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We study the problem of generating an En-glish sentence given an underlying prob-abilistic grammar, a world and a communicative goal. We model the generation problem as a Markov decision process with a suitably defined reward function that reflects the communicative goal. We then use probabilistic planning to solve the MDP and generate a sentence that, with high probability, accomplishes the communicative goal. We show empirically that our approach can generate complex sentences with a speed that generally matches or surpasses the state of the art. Further, we show that our approach is anytime and can handle complex communicative goals, including negated goals.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Suppose someone wants to tell their friend that they saw a dog chasing a cat. Given such a com- municative goal, most people can formulate a sen- tence that satisfies the goal very quickly. Fur- ther, they can easily provide multiple similar sen- tences, differing in details but all satisfying the general communicative goal, with no or very lit- tle error. Natural language generation (NLG) de- velops techniques to extend similar capabilities to automated systems. In this paper, we study the re- stricted NLG problem: given a grammar, lexicon, world and a communicative goal, output a valid English sentence that satisfies this goal. The prob- lem is restricted because in our work, we do not consider the issue of how to fragment a complex goal into multiple sentences (discourse planning).</p><p>Though restricted, this NLG problem is still dif- ficult. A key source of difficulty is the nature of the grammar, which is generally large, probabilis- tic and ambiguous. Some NLG techniques use sampling strategies <ref type="bibr" target="#b7">(Knight and Hatzivassiloglou, 1995)</ref> where a set of sentences is sampled from a data structure created from an underlying gram- mar and ranked according to how well they meet the communicative goal. Such approaches natu- rally handle statistical grammars, but do not solve the generation problem in a goal-directed manner. Other approaches view NLG as a planning prob- lem ( <ref type="bibr" target="#b10">Koller and Stone, 2007)</ref>. Here, the commu- nicative goal is treated as a predicate to be sat- isfied, and the grammar and vocabulary are suit- ably encoded as logical operators. Then auto- mated classical planning techniques are used to derive a plan which is converted into a sentence. This is an elegant formalization of NLG, however, restrictions on what current planning techniques can do limit its applicability. A key limitation is the logical nature of automated planning systems, which do not handle probabilistic grammars, or force ad-hoc approaches for doing so <ref type="bibr" target="#b0">(Bauer and Koller, 2010)</ref>. A second limitation comes from re- strictions on the goal: it may be difficult to en- sure that some specific piece of information should not be communicated, or to specify preferences over communicative goals, or specify general con- ditions, like that the sentence should be readable by a sixth grader. A third limitation comes from the search process: without strong heuristics, most planners get bogged down when given commu- nicative goals that require chaining together long sequences of operators ( <ref type="bibr" target="#b9">Koller and Petrick, 2011</ref>).</p><p>In our work, we also view NLG as a plan- ning problem. However, we differ in that our underlying formalism for NLG is a suitably de- fined Markov decision process (MDP). This set- ting allows us to address the limitations outlined above: it is naturally probabilistic, and handles probabilistic grammars; we are able to specify complex communicative goals and general criteria through a suitably-defined reward function; and, as we show in our experiments, recent develop- ments in fast planning in large MDPs result in a generation system that can rapidly deal with very specific communicative goals. Further, our sys- tem has several other desirable properties: it is an anytime approach; with a probabilistic grammar, it can naturally be used to sample and generate mul- tiple sentences satisfying the communicative goal; and it is robust to large grammar sizes. Finally, the decision-theoretic setting allows for a precise tradeoff between exploration of the grammar and vocabulary to find a better solution and exploita- tion of the current most promising (partial) solu- tion, instead of a heuristic search through the solu- tion space as performed by standard planning ap- proaches.</p><p>Below, we first describe related work, followed by a detailed description of our approach. We then empirically evaluate our approach and a state-of- the-art baseline in several different experimental settings and demonstrate its effectiveness at solv- ing a variety of NLG tasks. Finally, we discuss future extensions and conclude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Two broad lines of approaches have been used to attack the general NLG problem. One direction can be thought of as "overgeneration and rank- ing." Here some (possibly probabilistic) struc- ture is used to generate multiple candidate sen- tences, which are then ranked according to how well they satisfy the generation criteria. This in- cludes work based on chart generation and pars- ing <ref type="bibr" target="#b16">(Shieber, 1988;</ref><ref type="bibr" target="#b5">Kay, 1996)</ref>. These generators assign semantic meaning to each individual token, then use a set of rules to decide if two words can be combined. Any combination which contains a semantic representation equivalent to the input at the conclusion of the algorithm is a valid out- put from a chart generation system. Other exam- ples of this idea are the HALogen/Nitrogen sys- tems <ref type="bibr" target="#b11">(Langkilde-Geary, 2002</ref>). HALogen uses a two-phase architecture where first, a "forest" data structure that compactly summarizes possible ex- pressions is constructed. The structure allows for a more efficient and compact representation com- pared to lattice structures that were previously used in statistical sentence generation approaches. Using dynamic programming, the highest ranked sentence from this structure is then output. Many other systems using similar ideas exist, e.g. ( <ref type="bibr" target="#b18">White and Baldridge, 2003;</ref><ref type="bibr" target="#b13">Lu et al., 2009)</ref>.</p><p>A second line of attack formalizes NLG as an AI planning problem. SPUD ( <ref type="bibr" target="#b17">Stone et al., 2003)</ref>, a system for NLG through microplanning, con- siders NLG as a problem which requires realiz- ing a deliberative process of goal-directed activ- ity. Many such NLG-as-planning systems use a pipeline architecture, working from their com- municative goal through discourse planning and sentence generation. In discourse planning, in- formation to be conveyed is selected and split into sentence-sized chunks. These sentence-sized chunks are then sent to a sentence generator, which itself is usually split into two tasks, sen- tence planning and surface realization ( <ref type="bibr" target="#b9">Koller and Petrick, 2011</ref>). The sentence planner takes in a sentence-sized chunk of information to be con- veyed and enriches it in some way. This is then used by a surface realization module which en- codes the enriched semantic representation into natural language. This chain is sometimes referred to as the "NLG Pipeline" <ref type="bibr" target="#b15">(Reiter and Dale, 2000</ref>).</p><p>Another approach, called integrated generation, considers both sentence generation portions of the pipeline together ( <ref type="bibr" target="#b10">Koller and Stone, 2007)</ref>. This is the approach taken in some modern generators like CRISP ( <ref type="bibr" target="#b10">Koller and Stone, 2007)</ref> and PCRISP ( <ref type="bibr" target="#b0">Bauer and Koller, 2010)</ref>. In these generators, the input semantic requirements and grammar are en- coded in PDDL ( <ref type="bibr" target="#b3">Fox and Long, 2003)</ref>, which an off-the-shelf planner such as Graphplan <ref type="bibr" target="#b1">(Blum and Furst, 1997</ref>) uses to produce a list of applications of rules in the grammar. These generators generate parses for the sentence at the same time as the sen- tence, which keeps them from generating realiza- tions that are grammatically incorrect, and keeps them from generating grammatical structures that cannot be realized properly.</p><p>In the NLG-as-planning framework, the choice of grammar representation is crucial in treating NLG as a planning problem; the grammar pro- vides the actions that the planner will use to gener- ate a sentence. Tree Adjoining Grammars (TAGs) are a common choice <ref type="bibr" target="#b10">(Koller and Stone, 2007;</ref><ref type="bibr" target="#b0">Bauer and Koller, 2010)</ref>. TAGs are tree-based grammars consisting of two sets of trees, called initial trees and auxiliary or adjoining trees. An entire initial tree can replace a leaf node in the sen- tence tree whose label matches the label of the root of the initial tree in a process called "substitution." Auxiliary trees, on the other hand, encode recur- sive structures of language. Auxiliary trees have, at a minimum, a root node and a foot node whose labels match. The foot node must be a leaf of the auxiliary tree. These trees are used in a three-step process called "adjoining". The first step finds an adjoining location by searching through our sen- tence to find any subtree with a root whose label matches the root node of the auxiliary tree. In the second step, the target subtree is removed from the sentence tree, and placed in the auxiliary tree as a direct replacement for the foot node. Finally, the modified auxiliary tree is placed back in the sen- tence tree in the original target location. We use a variation of TAGs in our work, called a lexicalized TAG (LTAG), where each tree is associated with a lexical item called an anchor.</p><p>Though the NLG-as-planning approaches are elegant and appealing, a key drawback is the diffi- culty of handling probabilistic grammars, which are readily handled by the overgeneration and ranking strategies. Recent approaches such as PCRISP ( <ref type="bibr" target="#b0">Bauer and Koller, 2010</ref>) attempt to rem- edy this, but do so in a somewhat ad-hoc way, by transforming the probabilities into costs, because they rely on deterministic planning to actually re- alize the output. In this work, we directly address this by using a more expressive underlying formal- ism, a Markov decision process (MDP). We show empirically that this modification has other bene- fits as well, such as being anytime and an ability to handle complex communicative goals beyond those that deterministic planners can handle.</p><p>We note that prior work exists that uses MDPs for NLG <ref type="bibr" target="#b12">(Lemon, 2011)</ref>. That work differs from ours in several key respects: (i) it considers NLG at a coarse level, for example choosing the type of utterance (in a dialog context) and how to fill in specific slots in a template, (ii) the source of un- certainty is not language-related but comes from things like uncertainty in speech recognition, and (iii) the MDPs are solved using reinforcement learning and not planning, which is impractical in our setting. However, that work does consider NLG in the context of the broader task of dialog management, which we leave for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sentence Tree Realization with UCT</head><p>In this section, we describe our approach, called Sentence Tree Realization with UCT (STRUCT). We describe the inputs to STRUCT, followed by the underlying MDP formalism and the probabilis- tic planning algorithm we use to generate sen- tences in this MDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Inputs to STRUCT</head><p>STRUCT takes three inputs in order to generate a single sentence. These inputs are a grammar (in- cluding a lexicon), a communicative goal, and a world specification.</p><p>STRUCT uses a first-order logic-based seman- tic model in its communicative goal and world specification. This model describes named "en- tities," representing general things in the world. Entities with the same name are considered to be the same entity. These entities are described us- ing first-order logic predicates, where the name of the predicate represents a statement of truth about the given entities. In this semantic model, the communicative goal is a list of these predicates with variables used for the entity names. For in- stance, a communicative goal of 'red(d), dog(d)' (in English, "say anything about a dog which is red.") would match a sentence with the seman- tic representation 'red(subj), dog(subj), cat(obj), chased(subj, obj)', like "The red dog chased the cat", for instance.</p><p>A grammar contains a set of PTAG trees, di- vided into two sets (initial and adjoining). These trees are annotated with the entities in them. En- tities are defined as any element anchored by pre- cisely one node in the tree which can appear in a statement representing the semantic content of the tree. In addition to this set of trees, the grammar contains a list of words which can be inserted into those trees, turning the PTAG into an PLTAG. We refer to this list as a lexicon. Each word in the lexicon is annotated with its first-order logic se- mantics with any number of entities present in its subtree as the arguments.</p><p>A world specification is simply a list of all state- ments which are true in the world surrounding our generation. Matching entity names refer to the same entity. We use the closed world assumption, that is, any statement not present in our world is false. Before execution begins, our grammar is pruned to remove entries which cannot possibly be used in generation for the given problem, by tran-chased(subj,obj), dog(subj) chased(subj,obj) S VP NP−subj det N−self "dog" "chased" sitively discovering all predicates that hold about the entities mentioned in the goal in the world, and eliminating all trees not about any of these. This often allows STRUCT to be resilient to large grammar sizes, as our experiments will show.</p><formula xml:id="formula_0">V−self NP−obj "dog" N−self det NP dog(self) V−self NP−obj NP−subj</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Specification of the MDP</head><p>We formulate NLG as a planning problem on a Markov decision process (MDP) <ref type="bibr" target="#b14">(Puterman, 1994)</ref>. An MDP is a tuple (S, A, T, R, γ) where S is a set of states, A is a set of actions avail- able to an agent, T :</p><formula xml:id="formula_1">S × A × S → [0, 1]</formula><p>is a possibly stochastic function defining the probabil- ity T (s, a, s ) with which the environment tran- sitions to s when the agent does a in state s. R : S × A × S → R is a real-valued reward function that specifies the utility of performing ac- tion a in state s to reach another state. Finally, γ is a discount factor that allows planning over in- finite horizons to converge. In such an MDP, the agent selects actions at each state to optimize the expected infinite-horizon discounted reward. In the MDP we use for NLG, we must define each element of the tuple in such a way that a plan in the MDP becomes a sentence in a natural lan- guage. Our set of states, therefore, will be par- tial sentences which are in the language defined by our PLTAG input. There are an infinite number of these states, since TAG adjoins can be repeated indefinitely. Nonetheless, given a specific world and communicative goal, only a fraction of this MDP needs to be explored, and, as we show be- low, a good solution can often be found quickly us- ing a variation of the UCT algorithm ( <ref type="bibr" target="#b8">Kocsis and Szepesvari, 2006</ref>).</p><p>Our set of actions consist of all single substitu- tions or adjoins at a particular valid location in the tree (example shown in <ref type="figure" target="#fig_0">Figure 1</ref>). Since we are us- ing PLTAGs in this work, this means every action adds a word to the partial sentence. In situations where the sentence is complete (no nonterminals without children exist), we add a dummy action that the algorithm may choose to stop generation and emit the sentence. Based on these state and action definitions, the transition function takes a mapping between a partial sentence / action pair and the partial sentences which can result from one particular PLTAG adjoin / substitution, and re- turns the probability of that rule in the grammar.</p><p>In order to control the search space, we restrict the structure of the MDP so that while substitu- tions are available, only those operations are con- sidered when determining the distribution over the next state, without any adjoins. We do this is in order to generate a complete and valid sentence quickly. This allows STRUCT to operate as an anytime algorithm, described further below.</p><p>The immediate value of a state, intuitively, de- scribes closeness of an arbitrary partial sentence to our communicative goal. Each partial sentence is annotated with its semantic information, built up using the semantic annotations associated with the PLTAG trees. Thus we use as a reward a measure of the match between the semantic annotation of the partial tree and the communicative goal. That is, the larger the overlap between the predicates, the higher the reward. For an exact reward signal, when checking this overlap, we need to substitute each combination of entities in the goal into predi- cates in the sentence so we can return a high value if there are any mappings which are both possible (contain no statements which are not present in the grounded world) and mostly fulfill the goal (con- tain most of the goal predicates). However, this is combinatorial; also, most entities within sen- tences do not interact (e.g. if we say "the white rabbit jumped on the orange carrot," the whiteness of the rabbit has nothing to do with the carrot), and finally, an approximate reward signal gener- ally works well enough unless we need to emit nested subclauses. Thus as an approximation, we use a reward signal where we simply count how many individual predicates overlap with the goal with some entity substitution. In the experiments, we illustrate the difference between the exact and approximate reward signals.</p><p>The final component of the MDP is the discount factor. We generally use a discount factor of 1; this is because we are willing to generate lengthy sentences in order to ensure we match our goal. A discount factor of 1 can be problematic in gen- eral since it can cause rewards to diverge, but since there are a finite number of terms in our reward function (determined by the communicative goal and the fact that because of lexicalization we do not loop), this is not a problem for us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Probabilistic Planner</head><p>We now describe our approach to solving the MDP above to generate a sentence. Determining the op- timal policy at every state in an MDP is polyno- mial in the size of the state-action space <ref type="bibr" target="#b2">(Brafman and Tennenholtz, 2003)</ref>, which is intractable in our case. But for our application, we do not need to find the optimal policy. Rather we just need to plan in an MDP to achieve a given communica- tive goal. Is it possible to do this without explor- ing the entire state-action space? Recent work an- swers this question affirmatively. New techniques such as sparse sampling ( <ref type="bibr" target="#b6">Kearns et al., 1999</ref>) and UCT ( <ref type="bibr" target="#b8">Kocsis and Szepesvari, 2006</ref>) show how to generate near-optimal plans in large MDPs with a time complexity that is independent of the state space size. Using the UCT approach with a suit- ably defined MDP (explained above) allows us to naturally handle probabilistic grammars as well as formulate NLG as a planning problem, unify- ing the distinct lines of attack described in Sec- tion 2. Further, the theoretical guarantees of UCT translate into fast generation in many cases, as we demonstrate in our experiments.</p><p>Online planning in MDPs as done by UCT fol- lows two steps. From each state encountered, we construct a lookahead tree and use it to estimate the utility of each action in this state. Then, we take the best action, the system transitions to the next state and the procedure is repeated. In order to build a lookahead tree, we use a "rollout policy." This policy has two components: if it encounters a state already in the tree, it follows a "tree pol- icy," discussed further below. If it encounters a new state, the policy reverts to a "default" pol- icy that randomly samples an action. In all cases, any rewards received during the rollout search are backed up. Because this is a Monte Carlo esti- mate, typically, we run several simultaneous trials, and we keep track of the rewards received by each choice and use this to select the best action at the root.</p><p>The tree policy needed by UCT for a state s is the action a in that state which maximizes:</p><formula xml:id="formula_2">P (s, a) = Q(s, a) + c lnN (s) N (s, a)<label>(1)</label></formula><p>Algorithm 1 STRUCT algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Require: Number of simulations numT rials,</head><p>Depth of lookahead maxDepth, time limit T Ensure: Generated sentence tree 1: bestSentence ← nil 2: while time limit not reached do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>state ← empty sentence tree <ref type="bibr">4:</ref> while state not terminal do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>for numT rials do  <ref type="figure">, a)</ref> is the estimated value of a as ob- served in the tree search, computed as a sum over future rewards observed after (s, a). N (s) and N (s, a) are visit counts for the state and state- action pair. Thus the second term is an exploration term that biases the algorithm towards visiting ac- tions that have not been explored enough. c is a constant that trades off exploration and exploita- tion. This essentially treats each action decision as a bandit problem; previous work shows that this approach can efficiently select near-optimal actions at each state.</p><p>We use a modified version of UCT in order to increase its usability in the MDP we have defined. First, because we receive frequent, reasonably ac- curate feedback, we favor breadth over depth in the tree search. That is, it is more important in our case to try a variety of actions than to pursue a sin- gle action very deep. Second, UCT was originally used in an adversarial environment, and so is bi- ased to select actions leading to the best average reward rather than the action leading to the best overall reward. This is not true for us, however, so we choose the latter action instead. With the MDP definition above, we use our modified UCT to find a solution sentence (Algo- rithm 1). After every action is selected and ap- plied, we check to see if we are in a state in which the algorithm could terminate (i.e. the sentence has no nonterminals yet to be expanded). If so, we determine if this is the best possibly-terminal state we have seen so far. If so, we store it, and continue the generation process. Whenever we reach a terminal state, we begin again from the start state of the MDP. Because of the struc- ture restriction above (substitution before adjoin), STRUCT generates a valid sentence quickly. This enables STRUCT to perform as an anytime algo- rithm, which if interrupted will return the highest- value complete and valid sentence it has found. This also allows partial completion of communica- tive goals if not all goals can be achieved simulta- neously in the time given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Evaluation</head><p>In this section, we compare STRUCT to a state- of-the-art NLG system, CRISP, 1 and evaluate three hypotheses: (i) STRUCT is comparable in speed and generation quality to CRISP as it gen- erates increasingly large referring expressions, (ii) STRUCT is comparable in speed and generation quality to CRISP as the size of the grammar which they use increases, and (iii) STRUCT is capable of communicating complex propositions, includ- ing multiple concurrent goals, negated goals, and nested subclauses.</p><p>For these experiments, STRUCT was imple- mented in Python 2.7. We used a 2010 version of CRISP which uses a Java-based GraphPlan imple- mentation. All of our experiments were run on a 4-core AMD Phenom II X4 995 processor clocked at 3.2 GHz. Both systems were given access to 8 <ref type="bibr">1</ref> We were unfortunately unable to get the PCRISP system to compile, and so we could not evaluate it.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison to CRISP</head><p>We begin by describing experiments comparing STRUCT to CRISP. For these experiments, we use the approximate reward function for STRUCT.</p><p>Referring Expressions We first evaluate CRISP and STRUCT on their ability to gen- erate referring expressions.</p><p>Following prior work ( <ref type="bibr" target="#b9">Koller and Petrick, 2011</ref>), we consider a series of sentence generation problems which re- quire the planner to generate a sentence like "The Adj 1 Adj 2 ... Adj k dog chased the cat.", where the string of adjectives is a string that distin- guishes one dog (whose identity is specified in the problem description) from all other entities in the world. In this experiment, maxDepth was set equal to 1, since each action taken improved the sentence in a way measurable by our reward func- tion. numT rials was set equal to k(k + 1), since this is the number of adjoining sites available in the final step of generation, times the number of potential words to adjoin. This allows us to en- sure successful generation in a single loop of the STRUCT algorithm.</p><p>The experiment has two parameters: j, the number of adjectives in the grammar, and k, the number of adjectives necessary to distinguish the entity in question from all other entities. We set j = k and show the results in <ref type="figure" target="#fig_1">Figure 2</ref>. We ob- serve that CRISP was able to achieve sub-second or similar times for all expressions of less than length 5, but its generation times increase ex- ponentially past that point, exceeding 100 sec- onds for some plans at length 10. At length 15, CRISP failed to generate a referring expression; after 90 minutes the Java garbage collector termi- nated the process. STRUCT (the "STRUCT final" line) performs much better and is able to generate much longer referring expressions without failing. Later experiments had successful referring expres- sion generation of lengths as high as 25. The "STRUCT initial" curve shows the time taken by STRUCT to come up with the first complete sen- tence, which partially solves the goal and which (at least) could be output if generation was inter- rupted and no better alternative was found. As can be seen, this always happens very quickly.</p><p>Grammar Size. We next evaluate STRUCT and CRISP's ability to handle larger grammars. This experiment is set up in the same way as the one above, with the exception of l "distracting" words, words which are not useful in the sentence to be generated. l is defined as j − k. In these ex- periments, we vary l between 0 and 50. <ref type="figure">Figure 3a</ref> shows the results of these experiments. We ob- serve that CRISP using GraphPlan, as previously reported in ( <ref type="bibr" target="#b9">Koller and Petrick, 2011)</ref>, handles an increase in number of unused actions very well. Prior work reported a difference on the order of single milliseconds moving from j = 1 to j = 10. We report similar variations in CRISP runtime as j increases from 10 to 60: runtime increases by approximately 10% over that range.</p><p>No Pruning. If we do not prune the gram- mar (as described in Section 3.1), STRUCT's per- formance is similar to CRISP using the FF plan- ner ( <ref type="bibr" target="#b4">Hoffmann and Nebel, 2001</ref>), also profiled in ( <ref type="bibr" target="#b9">Koller and Petrick, 2011</ref>), which increased from 27 ms to 4.4 seconds over the interval from j = 1 to j = 10. STRUCT's performance is less sensi- tive to larger grammars than this, but over the same interval where CRISP increases from 22 seconds of runtime to 27 seconds of runtime, STRUCT in- creases from 4 seconds to 32 seconds. This is due almost entirely to the required increase in the value of numT rials as the grammar size increases. At the low end, we can use numT rials = 20, but at l = 50, we must use numT rials = 160 in order to ensure perfect generation as soon as possible. Note that, as STRUCT is an anytime algorithm, valid sentences are available very early in the gen- eration process, despite the size of the set of ad- joining trees. This time does not change substan- tially with increases in grammar size. However, the time to perfect this solution does.</p><p>With Pruning. STRUCT's performance im- proves significantly if we allow for pruning. This experiment involving distracting words is an ex- ample of a case where pruning will perform well. When we apply pruning, we find that STRUCT is able to ignore the effect of additional distract- ing words. Experiments showed roughly constant times for generation for j = 1 through j = 5000. Our experiments do not show any significant im- pact on runtime due to the pruning procedure it- self, even on large grammars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Complex Communicative Goals</head><p>In the next set of experiments, we illustrate that STRUCT can solve a variety of complex commu- nicative goals such as negated goals, conjuctions and goals requiring nested subclauses to be out- put.</p><p>Multiple Goals. We first evaluate STRUCT's ability to accomplish multiple communicative goals when generating a single sentence. In this experiment, we modify the problem from the pre- vious section. In that section, the referred-to dog was unique, and it was therefore possible to pro- duce a referring expression which identified it un- ambiguously. In this experiment, we remove this condition by creating a situation in which the gen- erator will be forced to ambiguously refer to sev- eral dogs. We then add to the world a number of adjectives which are common to each of these possible referents. Since these adjectives do not further disambiguate their subject, our generator should not use them in its output. We then encode these adjectives into communicative goals, so that they will be included in the output of the genera- tor despite not assisting in the accomplishment of disambiguation. For example, assume we had two black cats, and we wanted to say that one of them was sleeping, but we wanted to emphasize that it was a black cat. We would have as our goal both "sleeps(c)" and "black(c)". We want the genera- tor to say "the black cat sleeps", instead of simply "the cat sleeps."</p><p>We find that, in all cases, these otherwise use- less adjectives are included in the output of our generator, indicating that STRUCT is successfully balancing multiple communicative goals. As we show in <ref type="figure">figure 3b</ref> (the "Positive Goals" curve) , the presence of additional satisfiable semantic goals does not substantially affect the time required for generation. We are able to accomplish this task with the same very high frequency as the CRISP <ref type="table">558   0   5   10   15   20   25   30   35   10  20  30  40  50  60  Time</ref>   <ref type="figure">Figure 3</ref>: STRUCT experiments (see text for details).  comparisons, as we use the same parameters.</p><p>Negated Goals. We now evaluate STRUCT's ability to generate sentences given negated com- municative goals. We again modify the prob- lem used earlier by adding to our lexicon several new adjectives, each applicable only to the tar- get of our referring expression. Since our target can now be referred to unambiguously using only one adjective, our generator should just select one of these new adjectives (we experimentally con- firmed this). We then encode these adjectives into negated communicative goals, so that they will not be included in the output of the generator, despite allowing a much shorter referring expression. For example, assume we have a tall spotted black cat, a tall solid-colored white cat, and a short spotted brown cat, but we wanted to refer to the first one without using the word "black".</p><p>We find that these adjectives which should have been selected immediately are omitted from the output, and that the sentence generated is the best possible under the constraints. This demonstrates that STRUCT is balancing these negated commu- nicative goals with its positive goals. <ref type="figure">Figure 3b</ref> (the "Negative Goals" curve) shows the impact of negated goals on the time to generation. Since this experiment alters the grammar size, we see the time to final generation growing linearly with grammar size. The increased time to generate can be traced directly to this increase in grammar size. This is a case where pruning does not help us in re- ducing the grammar size; we cannot optimistically prune out words that we do not plan to use. Doing so might reduce the ability of STRUCT to produce a sentence which partially fulfills its goals.</p><p>Nested subclauses. Next, we evaluate STRUCT's ability to generate sentences with nested subclauses. An example of such a sentence is "The dog which ate the treat chased the cat." This is a difficult sentence to generate for several reasons. The first, and clearest, is that there are words in the sentence which do not help to in- crease the score assigned to the partial sentence. Notably, we must adjoin the word "which" to "the dog" during the portion of generation where the sentence reads "the dog chased the cat". This de- cision requires us to do planning deeper than one level in the MDP, which increases the number of simulations STRUCT requires in order to get the correct result. In this case, we require lookahead further into the tree than depth 1. We need to know that using "which" will allow us to further specify which dog is chasing the cat; in order to do this we must use at least d = 3. Our reward function must determine this with, at a minimum, the actions corresponding to "which", "ate", and "treat". For these experiments, we use the exact reward function for STRUCT.</p><p>Despite this issue, STRUCT is capable of gen- erating these sentences. <ref type="figure">Figure 3c</ref> shows the score of STRUCT's generated output over time for two nested clauses. Notice that, because the exact re- ward function is being used, the time to generate is longer in this experiment. To the best of our knowledge, CRISP is not able to generate sen- tences of this form due to an insufficiency in the way it handles TAGs, and consequently we present our results without this baseline.</p><p>Conjunctions. Finally, we evaluate STRUCT's ability to generate sentences including conjunc- tions. We introduce the conjunction "and", which allows for the root nonterminal of a new sentence ('S') to be adjoined to any other sentence. We then provide STRUCT with multiple goals. Given sufficient depth for the search (d = 3 was suf- ficient for our experiments, as our reward signal is fine-grained), STRUCT will produce two sen- tences joined by the conjunction "and". Again, we follow prior work in our experiment design <ref type="bibr" target="#b9">(Koller and Petrick, 2011</ref>).</p><p>As we can see in <ref type="figure" target="#fig_3">Figures 4a, 4b</ref>, and 4c, STRUCT successfully generates results for con- junctions of up to five sentences. This is not a hard upper bound, but generation times begin to be im- practically large at that point. Fortunately, human language tends toward shorter sentences than these unwieldy (but technically grammatical) sentences.</p><p>STRUCT increases in generation time both as the number of sentences increases and as the num- ber of objects per sentences increases. We com- pare our results to those presented in ( <ref type="bibr" target="#b9">Koller and Petrick, 2011)</ref> for CRISP with the FF Planner. They attempted to generate sentences with three entities and failed to find a result within their 4 GB memory limit. As we can see, CRISP gener- ates a result slightly faster than STRUCT when we are working with a single entity, but works much much slower for two entities and cannot generate results for a third entity. According to <ref type="bibr">Koller's findings, this</ref> is because the search space grows by a factor of the universe size with the addition of another entity ( <ref type="bibr" target="#b9">Koller and Petrick, 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed STRUCT, a general-purpose natural language generation system which is comparable to current state-of-the-art generators. STRUCT formalizes the generation problem as an MDP and applies a version of the UCT algorithm, a fast online MDP planner, to solve it. Thus, STRUCT naturally handles probabilistic gram- mars. We demonstrate empirically that STRUCT is anytime, comparable to existing generation-as- planning systems in certain NLG tasks, and is also capable of handling other, more complex tasks such as negated communicative goals.</p><p>Though STRUCT has many interesting prop- erties, many directions for exploration remain. Among other things, it would be desirable to in- tegrate STRUCT with discourse planning and di- alog systems. Fortunately, reinforcement learn- ing has already been investigated in such con- texts <ref type="bibr" target="#b12">(Lemon, 2011)</ref>, indicating that an MDP- based generation procedure could be a natural fit in more complex generation systems. This is a pri- mary direction for future work. A second direction is that, due to the nature of the approach, STRUCT is highly amenable to parallelization. None of the experiments reported here use parallelization, however, to be fair to CRISP. We plan to paral- lelize STRUCT in future work, to take advantage of current multicore architectures. This should ob- viously further reduce generation time.</p><p>STRUCT is open source and available from github.com upon request.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example tree substitution operation in STRUCT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Experimental comparison between STRUCT and CRISP: Generation time vs. length of referring expression</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Time to Generate (seconds) Number of Sentences STRUCT, 3 entities (c) Three entities ("The man gave the girl the book and ...").</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Time taken by STRUCT to generate sentences with conjunctions with varying numbers of entities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>to Generate (seconds)</head><label></label><figDesc></figDesc><table>Adjoining Grammar Size 

CRISP 
STRUCT 
STRUCT (pruning) 

(a) Effect of grammar size 

3.5 

4 

4.5 

5 

5.5 

6 

6.5 

7 

7.5 

8 

1 
2 
3 
4 
5 
6 
7 
8 
9 
Time to Generate (seconds) 

Number of Goals 

Positive Goals 
Negative Goals 

(b) Effect of multiple/ negated goals 

0 

200 

400 

600 

800 

1000 

1200 

2 
4 
6 
8 
10 
12 
14 
16 
18 

Score 

Time (seconds) 

Generated Score 
Best Available Score 

(c) Effect of nested subclauses 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by NSF CNS-1035602. SR was supported in part by CWRU award OSA110264. The authors are grateful to Umang Banugaria for help with the STRUCT im-plementation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sentence generation as planning with probabilistic LTAG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Tree Adjoining Grammar and Related Formalisms</title>
		<meeting>the 10th International Workshop on Tree Adjoining Grammar and Related Formalisms<address><addrLine>New Haven, CT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fast planning through planning graph analysis. Artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Furst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="281" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">R-MAX-a general polynomial time algorithm for near-optimal reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="213" to="231" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PDDL2.1: An extension to PDDL for expressing temporal planning domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="124" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The FF planning system: fast plan generation through heuristic search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="253" to="302" />
			<date type="published" when="2001-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Chart generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th annual meeting on Association for Computational Linguistics, ACL &apos;96</title>
		<meeting>the 34th annual meeting on Association for Computational Linguistics, ACL &apos;96<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="200" to="204" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A sparse sampling algorithm for near-optimal planning in large Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Lawrence Erlbaum Associates Ltd</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1324" to="1331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Two-level, many-paths generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual meeting on Association for Computational Linguistics</title>
		<meeting>the 33rd annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="252" to="260" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bandit based Monte-Carlo planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levente</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Szepesvari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth European Conference on Machine Learning</title>
		<meeting>the Seventeenth European Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="282" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Experiences with planning for natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">P A</forename><surname>Petrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="40" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sentence generation as a planning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">336</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An empirical verification of coverage and correctness for a general-purpose sentence generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Langkilde-Geary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Natural Language Generation Workshop</title>
		<meeting>the 12th International Natural Language Generation Workshop</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning what to say and how to say it: joint optimization of spoken dialogue management and natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="221" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Natural language generation with tree conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="400" to="409" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Markov decision processes: Discrete stochastic dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Building Natural Language Generation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-01" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A uniform architecture for parsing and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th conference on Computational linguistics</title>
		<meeting>the 12th conference on Computational linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1988" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="614" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microplanning with communicative intentions: The SPUD system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonia</forename><surname>Bleam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="311" to="381" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adapting chart realization to CCG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th European Workshop on Natural Language Generation</title>
		<meeting>the 9th European Workshop on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
