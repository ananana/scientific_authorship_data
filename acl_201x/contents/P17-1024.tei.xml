<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FOIL it! Find One mismatch between Image and Language caption</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Shekhar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yauhen</forename><surname>Klimovich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aur√©lie</forename><surname>Herbelot</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
						</author>
						<title level="a" type="main">FOIL it! Find One mismatch between Image and Language caption</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="255" to="265"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1024</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we aim to understand whether current language and vision (LaVi) models truly grasp the interaction between the two modalities. To this end, we propose an extension of the MS-COCO dataset, FOIL-COCO, which associates images with both correct and &apos;foil&apos; captions, that is, descriptions of the image that are highly similar to the original ones, but contain one single mistake (&apos;foil word&apos;). We show that current LaVi models fall into the traps of this data and perform badly on three tasks: a) caption classification (correct vs. foil); b) foil word detection; c) foil word correction. Humans , in contrast, have near-perfect performance on those tasks. We demonstrate that merely utilising language cues is not enough to model FOIL-COCO and that it challenges the state-of-the-art by requiring a fine-grained understanding of the relation between text and image.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most human language understanding is grounded in perception. There is thus growing interest in combining information from language and vision in the NLP and AI communities.</p><p>So far, the primary testbeds of Language and Vision (LaVi) models have been 'Visual Question Answering' (VQA) (e.g. <ref type="bibr" target="#b2">Antol et al. (2015)</ref>; <ref type="bibr" target="#b20">Malinowski and Fritz (2014)</ref>; <ref type="bibr" target="#b21">Malinowski et al. (2015)</ref>; ; <ref type="bibr" target="#b24">Ren et al. (2015)</ref>) and 'Im- age Captioning' (IC) (e.g. <ref type="bibr" target="#b13">Hodosh et al. (2013)</ref>; <ref type="bibr" target="#b8">Fang et al. (2015)</ref>; <ref type="bibr" target="#b4">Chen and Lawrence Zitnick (2015)</ref>; <ref type="bibr" target="#b6">Donahue et al. (2015)</ref>; <ref type="bibr" target="#b17">Karpathy and Fei-Fei (2015)</ref>; <ref type="bibr" target="#b26">Vinyals et al. (2015)</ref>). Whilst some models have seemed extremely successful on those tasks, it remains unclear how the re- ported results should be interpreted and what those <ref type="figure">Figure 1</ref>: Is the caption correct or foil (T1)? If it is foil, where is the mistake (T2) and which is the word to correct the foil one (T3)? models are actually learning. There is an emerg- ing feeling in the community that the VQA task should be revisited, especially as many current dataset can be handled by 'blind' models which use language input only, or by simple concate- nation of language and vision features ( <ref type="bibr" target="#b0">Agrawal et al., 2016;</ref><ref type="bibr" target="#b14">Jabri et al., 2016;</ref><ref type="bibr" target="#b30">Zhang et al., 2016;</ref><ref type="bibr" target="#b10">Goyal et al., 2016a</ref>). In IC too, <ref type="bibr" target="#b12">Hodosh and Hockenmaier (2016)</ref> showed that, contrarily to what prior research had suggested, the task is far from been solved, since IC models are not able to dis- tinguish between a correct and incorrect caption.</p><p>Such results indicate that in current datasets, language provides priors that make LaVi models successful without truly understanding and inte- grating language and vision. But problems do not stop at biases.  also point out that current data 'conflate multiple sources of er- ror, making it hard to pinpoint model weaknesses', thus highlighting the need for diagnostic datasets. Thirdly, existing IC evaluation metrics are sensi- tive to n-gram overlap and there is a need for mea- sures that better simulate human judgments <ref type="bibr" target="#b13">(Hodosh et al., 2013;</ref><ref type="bibr" target="#b7">Elliott and Keller, 2014;</ref><ref type="bibr" target="#b1">Anderson et al., 2016)</ref>.</p><p>Our paper tackles the identified issues by proposing an automatic method for creating a large dataset of real images with minimal lan- guage bias and some diagnostic abilities. Our dataset, FOIL (Find One mismatch between Im- age and Language caption), <ref type="bibr">1</ref> consists of images associated with incorrect captions. The captions are produced by introducing one single error (or 'foil') per caption in existing, human-annotated data <ref type="figure">(Figure 1</ref>). This process results in a chal- lenging error-detection/correction setting (because the caption is 'nearly' correct). It also provides us with a ground truth (we know where the error is) that can be used to objectively measure the perfor- mance of current models.</p><p>We propose three tasks based on widely ac- cepted evaluation measures: we test the ability of the system to a) compute whether a caption is compatible with the image (T1); b) when it is in- compatible, highlight the mismatch in the caption (T2); c) correct the mistake by replacing the foil word (T3).</p><p>The dataset presented in this paper (Section 3) is built on top of MS-COCO ( <ref type="bibr" target="#b18">Lin et al., 2014)</ref>, and contains 297,268 datapoints and 97,847 im- ages. We will refer to it as FOIL-COCO. We eval- uate two state-of-the-art VQA models: the popular one by <ref type="bibr" target="#b2">Antol et al. (2015)</ref>, and the attention-based model by <ref type="bibr" target="#b19">Lu et al. (2016)</ref>, and one popular IC model by ( . We show that those models perform close to chance level, while hu- mans can perform the tasks accurately (Section 4). Section 5 provides an analysis of our results, al- lowing us to diagnose three failures of LaVi mod- els. First, their coarse representations of language and visual input do not encode suitably structured information to spot mismatches between an utter- ance and the corresponding scene (tested by T1). Second, their language representation is not fine- grained enough to identify the part of an utterance that causes a mismatch with the image as it is (T2). Third, their visual representation is also too poor to spot and name the visual area that corresponds to a captioning error (T3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The image captioning (IC) and visual question answering (VQA) tasks are the most relevant to our work. In IC ( <ref type="bibr" target="#b8">Fang et al., 2015;</ref><ref type="bibr" target="#b4">Chen and Lawrence Zitnick, 2015;</ref><ref type="bibr" target="#b6">Donahue et al., 2015;</ref><ref type="bibr" target="#b17">Karpathy and Fei-Fei, 2015;</ref><ref type="bibr" target="#b26">Vinyals et al., 2015;</ref>, the goal is to generate a caption for a given image, such that it is both semantically and syntactically correct, and properly describes the content of that image. In VQA ( <ref type="bibr" target="#b2">Antol et al., 2015;</ref><ref type="bibr" target="#b20">Malinowski and Fritz, 2014;</ref><ref type="bibr" target="#b21">Malinowski et al., 2015;</ref><ref type="bibr" target="#b24">Ren et al., 2015)</ref>, the system attempts to answer open-ended ques- tions related to the content of the image. There is a wealth of literature on both tasks, but we only discuss here the ones most related to our work and refer the reader to the recent surveys by <ref type="bibr" target="#b3">(Bernardi et al., 2016;</ref><ref type="bibr" target="#b28">Wu et al., 2016)</ref>.</p><p>Despite their success, it remains unclear whether state-of-the-art LaVi models capture vi- sion and language in a truly integrative fashion. We could identify three types of arguments sur- rounding the high performance of LaVi models:</p><p>(i) Triviality of the LaVi tasks: Recent work has shown that LaVi models heavily rely on lan- guage priors <ref type="bibr" target="#b24">(Ren et al., 2015;</ref><ref type="bibr" target="#b0">Agrawal et al., 2016;</ref><ref type="bibr" target="#b16">Kafle and Kanan, 2016)</ref>. Even simple cor- relation and memorisation can result in good per- formance, without the underlying models truly un- derstanding visual content ( <ref type="bibr" target="#b14">Jabri et al., 2016;</ref><ref type="bibr" target="#b12">Hodosh and Hockenmaier, 2016)</ref>. <ref type="bibr" target="#b30">Zhang et al. (2016)</ref> first unveiled that there exists a huge bias in the popular VQA dataset by <ref type="bibr" target="#b2">Antol et al. (2015)</ref>: they showed that almost half of all the questions in this dataset could be answered correctly by using the question alone and ignoring the image completely. In the same vein,  proposed a simple baseline for the task of VQA. This baseline simply concatenates the Bag of Words (BoW) features from the question and Convolutional Neural Networks (CNN) fea- tures from the image to predict the answer. They showed that such a simple method can achieve comparable performance to complex and deep ar- chitectures. <ref type="bibr" target="#b14">Jabri et al. (2016)</ref> proposed a similar model for the task of multiple choice VQA, and suggested a cross-dataset generalization scheme as an evaluation criterion for VQA systems. We com- plement this research by introducing three new tasks with different levels of difficulty, on which LaVi models can be evaluated sequentially.</p><p>(ii) Need for diagnostics: To overcome the bias uncovered in previous datasets, several re- search groups have started proposing tasks which involve distinguishing distractors from a ground- truth caption for an image. <ref type="bibr" target="#b30">Zhang et al. (2016)</ref> in- troduced a binary VQA task along with a dataset composed of sets of similar artificial images, al- lowing for more precise diagnostics of a system's errors. <ref type="bibr" target="#b10">Goyal et al. (2016a)</ref> balanced the dataset of <ref type="bibr" target="#b2">Antol et al. (2015)</ref>, collecting a new set of com- plementary natural images which are similar to ex- isting items in the original dataset, but result in different answers to a common question. <ref type="bibr" target="#b12">Hodosh and Hockenmaier (2016)</ref> also proposed to evalu- ate a number of state-of-the-art LaVi algorithms in the presence of distractors. Their evaluation was however limited to a small dataset (namely, <ref type="bibr">Flickr30K (Young et al., 2014)</ref>) and the caption generation was based on a hand-crafted scheme using only inter-dataset distractors.</p><p>Most related to our paper is the work by <ref type="bibr" target="#b5">Ding et al. (2016)</ref>. Like us, they propose to extend the MS-COCO dataset by generating decoys from human-created image captions. They also suggest an evaluation apparently similar to our T1, requir- ing the LaVi system to detect the true target cap- tion amongst the decoys. Our efforts, however, differ in some substantial ways. First, their tech- nique to create incorrect captions (using BLEU to set an upper similarity threshold) is so that many of those captions will differ from the gold descrip- tion in more than one respect. For instance, the caption two elephants standing next to each other in a grass field is associated with the decoy a herd of giraffes standing next to each other in a dirt field (errors: herd, giraffe, dirt) or with animals are gathering next to each other in a dirt field (error: dirt; infelicities: animals and gathering, which are both pragmatically odd). Clearly, the more the caption changes in the decoy, the easier the task becomes. In contrast, the foil captions we propose only differ from the gold description by one word and are thus more challenging. Secondly, the auto- matic caption generation of Ding et al means that 'correct' descriptions can be produced, resulting in some confusion in human responses to the task. We made sure to prevent such cases, and human performance on our dataset is thus close to 100%. We note as well that our task does not require any complex instructions for the annotation, indicat- ing that it is intuitive to human beings (see ¬ß4). Thirdly, their evaluation is a multiple-choice task, where the system has to compare all captions to understand which one is closest to the image. This is arguably a simpler task than the one we propose, where a caption is given and the system is asked to classify it as correct or foil: as we show in ¬ß4, detecting a correct caption is much easier than de- tecting foils. So evaluating precision on both gold and foil items is crucial.</p><p>Finally,  proposed CLEVR, a dataset for the diagnostic evaluation of VQA systems. This dataset was designed with the explicit goal of enabling detailed analysis of dif- ferent aspects of visual reasoning, by minimising dataset biases and providing rich ground-truth rep- resentations for both images and questions.</p><p>(iii) Lack of objective evaluation metrics: The evaluation of Natural Language Generation (NLG) systems is known to be a hard prob- lem. It is further unclear whether the quality of LaVi models should be measured using met- rics designed for language-only tasks. <ref type="bibr" target="#b7">Elliott and Keller (2014)</ref> performed a sentence-level correla- tion analysis of NLG evaluation measures against expert human judgements in the context of IC. Their study revealed that most of those metrics were only weakly correlated with human judge- ments. In the same line of research, <ref type="bibr" target="#b1">Anderson et al. (2016)</ref> showed that the most widely-used metrics for IC fail to capture semantic proposi- tional content, which is an essential component of human caption evaluation. They proposed a se- mantic evaluation metric called SPICE, that mea- sures how effectively image captions recover ob- jects, attributes and the relations between them. In this paper, we tackle this problem by proposing tasks which can be evaluated based on objective metrics for classification/detection error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>In this section, we describe how we automati- cally generate FOIL-COCO datapoints, i.e. im- age, original and foil caption triples. We used the training and validation Microsoft's Common Ob- jects in Context (MS-COCO) dataset ( <ref type="bibr" target="#b18">Lin et al., 2014</ref>) (2014 version) as our starting point. In MS-COCO, each image is described by at least five descriptions written by humans via Amazon Mechanical Turk (AMT). The images contains 91 common object categories (e.g. dog, elephant, bird, . . . and car, bicycle, airplane, . . . ), from 11 supercategories (Animal, Vehicle, resp.), with 82 of them having more than 5K labeled instances. In total there are 123,287 images with captions (82,783 for training and 40,504 for validation). <ref type="bibr">2</ref> Our data generation process consists of four <ref type="bibr">2</ref> The MS-COCO test set is not available for download.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>257</head><p>nr. of datapoints nr. unique images nr. of tot. captions nr. target <ref type="table" target="#tab_2">::foil pairs  Train  197,788  65,697  395,576  256  Test  99,480  32,150</ref> 198,960 216 <ref type="table">Table 1</ref>: Composition of FOIL-COCO.</p><p>main steps, as described below. The last two steps are illustrated in <ref type="figure">Figure 2</ref>.</p><p>1. Generation of replacement word pairs We want to replace one noun in the original caption (the target) with an incorrect but similar word (the foil). To do this, we take the labels of MS- COCO categories, and we pair together words belonging to the same supercategory (e.g., bicy- cle::motorcycle, bicycle::car, bird::dog). We use as our vocabulary 73 out of the 91 MS-COCO cat- egories, leaving out those categories that are multi- word expressions (e.g. traffic light). We thus ob- tain 472 target::foil pairs.</p><p>2. Splitting of replacement pairs into train- ing and testing To avoid the models learning trivial correlations due to replacement frequency, we randomly split, within each supercategory, the candidate target::foil pairs which are used to gen- erate the captions of the training vs. test sets. We obtain 256 pairs, built out of 72 target and 70 foil words, for the training set, and 216 pairs, contain- ing 73 target and 71 foil words, for the test set.</p><p>3. Generation of foil captions We would like to generate foil captions by replacing only target words which refer to visually salient objects. To this end, given an image, we replace only those tar- get words that occur in more than one MS-COCO caption associated with that image. Moreover, we want to use foils which are not visually present, i.e. that refer to visual content not present in the image. Hence, given an image, we only replace a word with foils that are not among the labels (objects) annotated in MS-COCO for that image. We use the images from the MS-COCO training and validation sets to generate our training and test sets, respectively. We obtain 2,229,899 for train- ing and 1,097,012 captions for testing.</p><p>4. Mining the hardest foil caption for each image To eliminate possible visual-language dataset bias, out of all foil captions generated in step 3, we select only the hardest one. For this pur- pose, we need to model the visual-language bias of the dataset. To this end, we use Neuraltalk 3 <ref type="bibr" target="#b17">(Karpathy and Fei-Fei, 2015</ref>), one of the state- of-the-art image captioning systems, pre-trained on MS-COCO. Neuraltalk is based on an LSTM which takes as input an image and generates a sen- tence describing its content. We obtain a neural network N that implicitly represents the visual- language bias through its weights. We use N to approximate the conditional probability of a cap- tion C given a dataset T and and an image I (P (C|I, T )). This is obtained by simply using the loss l(C, N (I)) i.e., the error obtained by compar- ing the pseudo-ground truth C with the sentence predicted by N : P (C|I, T ) = 1 ‚àí l(C, N (I)) (we refer to <ref type="bibr" target="#b17">(Karpathy and Fei-Fei, 2015</ref>) for more details on how l() is computed). P (C|I, T ) is used to select the hardest foil among all the pos- sible foil captions, i.e. the one with the highest probability according to the dataset bias learned by N . Through this process, we obtain 197,788 and 99,480 original::foil caption pairs for the training and test sets, respectively. None of the target::foil word pairs are filtered out by this mining process.</p><p>The final FOIL-COCO dataset consists of 297,268 datapoints <ref type="bibr">(197,</ref><ref type="bibr">788</ref> in training and 99,480 in test set). All the 11 MS-COCO supercat- egories are represented in our dataset and contain 73 categories from the 91 MS-COCO ones (4.8 categories per supercategory on average.) Further details are reported in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>We conduct three tasks, as presented below:</p><p>Task 1 (T1): Correct vs. foil classification Given an image and a caption, the model is asked to mark whether the caption is correct or wrong. The aim is to understand whether LaVi models can spot mismatches between their coarse representa- tions of language and visual input.</p><p>Task 2 (T2): Foil word detection Given an im- age and a foil caption, the model has to detect the foil word. The aim is to evaluate the understanding of the system at the word level. In order to system- atically check the system's performance with dif- ferent prior information, we test two different set- <ref type="figure">Figure 2</ref>: The main aspects of the foil caption generation process. Left column: some of the original COCO captions associated with an image. In bold we highlight one of the target words (bicycle), chosen because it is mentioned by more than one annotator. Middle column: For each original caption and each chosen target word, different foil captions are generated by replacing the target word with all possible candidate foil replacements. Right column: A single caption is selected amongst all foil candidates. We select the 'hardest' caption, according to Neuraltalk model, trained using only the original captions.</p><p>tings: the foil has to be selected amongst (a) only the nouns or (b) all content words in the caption.</p><p>Task 3 (T3): Foil word correction Given an image, a foil caption and the foil word, the model has to detect the foil and provide its correction. The aim is to check whether the system's visual representation is fine-grained enough to be able to extract the information necessary to correct the error. For efficiency reasons, we operationalise this task by asking models to select a correction from the set of target words, rather than the whole dataset vocabulary (viz. more than 10K words).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Models</head><p>We evaluate both VQA and IC models against our tasks. For the former, we use two of the three mod- els evaluated in ( <ref type="bibr" target="#b10">Goyal et al., 2016a</ref>) against a bal- anced VQA dataset. For the latter, we use the mul- timodal bi-directional LSTM, proposed in ( , and adapted for our tasks. HieCoAtt: We use the Hierarchical Co- Attention model proposed by ( <ref type="bibr" target="#b19">Lu et al., 2016</ref>) that co-attends to both the image and the question to solve the task. In particular, we evaluate the 'alternate' version, i.e. the model that sequentially alternates between generating some attention over the image and question. It does so in a hierarchical way by starting from the word-level, then going to the phrase and then to the entire sentence-level. These levels are combined recursively to produce the distribution over the foil vs. correct captions.</p><p>IC-Wang: Amongst the IC models, we choose the multimodal bi-directional LSTM (Bi-LSTM) model proposed in ( . This model predicts a word in a sentence by considering both the past and future context, as sentences are fed to the LSTM in forward and backward order. The model consists of three modules: a CNN for encoding image inputs, a Text-LSTM (T-LSTM) for encoding sentence inputs, a Multimodal LSTM (M-LSTM) for embedding visual and textual vec- tors to a common semantic space and decoding to sentence. The bidirectional LSTM is implemented with two separate LSTM layers.</p><p>Baselines: We compare the SoA models above against two baselines. For the classification task, we use a Blind LSTM model followed by a fully connected layer and softmax and train it only on captions as input to predict the answer. In addition, we evaluate the CNN+LSTM model, where visual and textual features are simply concatenated.</p><p>The models at work on our three tasks For the classification task (T1), the baselines and VQA models can be applied directly. We adapt the gen- erative IC model to perform the classification task as follows. Given a test image I and a test cap- tion, for each word w t in the test caption, we remove the word and use the model to gener- ate new captions in which the w t has been re- placed by the word v t predicted by the model (w 1 ,...,w t‚àí1 , v t , w t‚àí1 ,...,w n ). We then compare the conditional probability of the test caption with all the captions generated from it by replacing w t with v t . When all the conditional probabilities of the generated captions are lower than the one as- signed to the test caption the latter is classified as good, otherwise as foil. For the other tasks, the models have been trained on T1. To perform the foil word detection task (T2), for the VQA models, we apply the occlusion method. Follow- ing ( <ref type="bibr" target="#b11">Goyal et al., 2016b</ref>), we systematically oc- clude subsets of the language input, forward prop- agate the masked input through the model, and compute the change in the probability of the an- swer predicted with the unmasked original input. For the IC model, similarly to T1, we sequentially generate new captions from the foil one by replac- ing, one by one, the words in it and computing the conditional probability of the foil caption and the one generated from it. The word whose replace- ment generate the caption with the highest con- ditional probabilities is taken to be the foil word. Finally, to evaluate the models on the error cor- rection task (T3), we apply the linear regression method over all the target words and select the tar- get word which has the highest probability of mak- ing that wrong caption correct with respect to the given image.</p><p>Upper-bound Using Crowdflower, we collected human answers from 738 native English speak- ers for 984 image-caption pairs randomly selected from the test set. Subjects were given an image and a caption and had to decide whether it was cor- rect or wrong (T1). If they thought it was wrong, they were required to mark the error in the cap- tion (T2). We collected 2952 judgements (i.e. 3 judgements per pair and 4 judgements per rater) and computed human accuracy in T1 when con- sidering as answer (a) the one provided by at least 2 out of 3 annotators (majority) and (b) the one provided by all 3 annotators (unanimity). The same procedure was adopted for computing ac- curacies in T2. Accuracies in both T1 an T2 are reported in <ref type="table">Table 2</ref>. As can be seen, in the ma- jority setting annotators are quasi-perfect in classi- fying captions (92.89%) and detecting foil words (97.00%). Though lower, accuracies in the una- nimity setting are still very high, with raters pro- viding the correct answer in 3 out of 4 cases in both tasks. Hence, although we have collected hu- man answers only on a rather small subset of the test set, we believe their results are representative of how easy the tasks are for humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>As shown in <ref type="table">Table 2</ref>, the FOIL-COCO dataset is challenging. On T1, for which the chance level is 50.00%, the 'blind', language-only model, does badly with an accuracy of 55.62% (25.04% on foil captions), demonstrating that language bias is minimal. By adding visual information, CNN+LSTM, the overall accuracy increases by 5.45% (7.94% on foil captions.) reaching 61.07% (resp. 32.98%). Both SoA VQA and IC models do significantly worse than humans on both T1 and T2. The VQA systems show a strong bias towards correct captions and poor overall perfor- mance. They only identify 34.51% (LSTM +norm I) and 36.38% (HieCoAtt) of the incorrect cap- tions (T1). On the other hand, the IC model tends to be biased toward the foil captions, on which it achieves an accuracy of 45.44%, higher than the VQA models. But the overall accuracy (42.21%) is poorer than the one obtained by the two base- lines. On the foil word detection task, when con- sidering only nouns as possible foil word, both the IC and the LSTM+norm I models perform close to chance level, and the HieCoAtt performs some- what better, reaching 38.79%. Similar results are obtained when considering all words in the caption as possible foil. Finally, the VQA models' accu- racy on foil word correction (T3) is extremely low, at 4.7% (LSTM +norm I) and 4.21% (HieCoAtt). The result on T3 makes it clear that the VQA sys- tems are unable to extract from the image rep-resentation the information needed to correct the foil: despite being told which element in the cap- tion is wrong, they are not able to zoom into the correct part of the image to provide a correction, or if they are, cannot name the object in that region. The IC model performs better compared to the other models, having an accuracy that is 20,78% higher than chance level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T1: Classification task</head><p>Overall  <ref type="table">Table 2</ref>: T1: Accuracy for the classification task, relatively to all image-caption pairs (overall) and by type of caption (correct vs. foil); T2: Accu- racy for the foil word detection task, when the foil is known to be among the nouns only or when it is known to be among all the content words; T3: Accuracy for the foil word correction task when the correct word has to be chosen among any of the target words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>We performed a mixed-effect logistic regression analysis in order to check whether the behavior of the best performing models in T1, namely the VQA models, can be predicted by various linguis- tic variables. We included: 1) semantic similar- ity between the original word and the foil (com- puted as the cosine between the two corresponding word2vec embeddings ( <ref type="bibr" target="#b22">Mikolov et al., 2013)</ref>); 2) frequency of original word in FOIL-COCO cap- tions; 3) frequency of the foil word in FOIL- COCO captions; 4) length of the caption (number of words). The mixed-effect model was performed to get rid of possible effects due to either object supercategory (indoor, food, vehicle, etc.) or tar- get::foil pair (e.g., zebra::giraffe, boat::airplane, etc.). For both LSTM + norm I and HieCoAtt, word2vec similarity, frequency of the original word, and frequency of the foil word turned out to be highly reliable predictors of the model's re- sponse. The higher the values of these variables, the more the models tend to provide the wrong output. That is, when the foil word (e.g. cat) is semantically very similar to the original one (e.g. dog), the models tend to wrongly classify the cap- tion as 'correct'. The same holds for frequency values. In particular, the higher the frequency of both the original word and the foil one, the more the models fail. This indicates that systems find it difficult to distinguish related concepts at the text- vision interface, and also that they may tend to be biased towards frequently occurring concepts, 'seeing them everywhere' even when they are not present in the image. Caption length turned out to be only a partially reliable predictor in the LSTM + norm I model, whereas it is a reliable predictor in HieCoAtt. In particular, the longer the caption, the harder for the model to spot that there is a foil word that makes the caption wrong.</p><p>As revealed by the fairly high variance ex- plained by the random effect related to target::foil pairs in the regression analysis, both models per- form very well on some target::foil pairs, but fail on some others (see leftmost part of <ref type="table">Table 4</ref> for same examples of easy/hard target::foil pairs). Moreover, the variance explained by the random effect related to object supercategory is reported in <ref type="table" target="#tab_2">Table 3</ref>. As can be seen, for some supercategories accuracies are significatively higher than for oth- ers (compare, e.g., 'electronic' and 'outdoor').</p><p>In a separate analysis, we also checked whether there was any correlation between results and the position of the foil in the sentence, to ensure the models did not profit from any undesirable arti- facts of the data. We did not find any such corre- lation.   <ref type="table">Table 4</ref>: Easiest and hardest target::foil pairs: T1 (caption classification) and T2 (foil word detection).</p><p>To better understand results on T2, we per- formed an analysis investigating the performance of the VQA models on different target::foil pairs. As reported in <ref type="table">Table 4</ref> (right), both models per- form nearly perfectly with some pairs and very badly with others. At first glance, it can be no- ticed that LSTM + norm I is very effective with pairs involving vehicles (airplane, truck, etc.), whereas HieCoAtt seems more effective with pairs involving animate nouns (i.e. animals), though more in depth analysis is needed on this point. More interestingly, some pairs that are found to be predicted almost perfectly by LSTM + I norm, namely boat::airplane, zebra::giraffe, and drier::scissors, turn out to be among the Bottom-5 cases in HieCoAtt. This suggests, on the one hand, that the two VQA models use different strategies to perform the task. On the other hand, it shows that our dataset does not contain cases that are a priori easy for any model.</p><p>The results of IC-Wang on T3 are much higher than LSTM + norm I and HieCoAtt, although it is outperformed by or is on par with HieCoAtton on T1-T2. Our interpretation is that this behaviour is related to the discriminative/generative nature of our tasks. Specifically, T1 and T2 are discrimina- tive tasks and LSTM + norm I and HieCoAtt are discriminative models. Conversely, T3 is a gen- erative task (a word needs to be generated) and IC-Wang is a generative model. It would be in- teresting to test other IC models on T3 and com- pare their results against the ones reported here. However, note that IC-Wang is 'tailored' for T3 because it takes as input the whole sentence (mi- nus the word to be generated), while common se- quential IC approaches can only generate a word depending on the previous words in the sentence.</p><p>As far as human performance is concerned, both T1 and T2 turn out to be extremely easy. In T1, image-caption pairs were correctly judged as correct/wrong in overall 914 out of 984 cases (92.89%) in the majority setting. In the unanim-ity setting, the correct response was provided in 751 out of 984 cases (76.32%). Judging foil cap- tions turns out to be slightly easier than judging correct captions in both settings, probably due to the presence of typos and misspellings that some- times occur in the original caption (e.g. raters judge as wrong the original caption People playing ball with a drown and white dog, where 'brown' was misspelled as 'drown'). To better under- stand which factors contribute to make the task harder, we qualitatively analyse those cases where all annotators provided a wrong judgement for an image-caption pair. As partly expected, almost all cases where original captions (thus correct for the given image) are judged as being wrong are cases where the original caption is indeed incor- rect. For example, a caption using the word 'mo- torcycle' to refer to a bicycle in the image is judged as wrong. More interesting are those cases where all raters agreed in considering as correct image-caption pairs that are instead foil. Here, it seems that vagueness as well as certain metaphor- ical properties of language are at play: human annotators judged as correct a caption describing Blue and banana large birds on tree with metal pot (see <ref type="figure" target="#fig_1">Fig 3, left)</ref>, where 'banana' replaced 'orange'. Similarly, all raters judged as correct the caption A cat laying on a bed next to an opened keyboard (see <ref type="figure" target="#fig_1">Fig 3, right)</ref>, where the cat is instead laying next to an opened laptop.</p><p>Focusing on T2, it is interesting to report that among the correctly-classified foil cases, annota- tors provided the target word in 97% and 73.6% of cases in the majority and unanimity setting, re- spectively. This further indicates that finding the foil word in the caption is a rather trivial task for humans. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have introduced FOIL-COCO, a large dataset of images associated with both correct and foil captions. The error production is automatically generated, but carefully thought out, making the task of spotting foils particularly challenging. By associating the dataset with a series of tasks, we al- low for diagnosing various failures of current LaVi systems, from their coarse understanding of the correspondence between text and vision to their grasp of language and image structure.</p><p>Our hypothesis is that systems which, like hu- mans, deeply integrate the language and vision modalities, should spot foil captions quite easily. The SoA LaVi models we have tested fall through that test, implying that they fail to integrate the two modalities. To complete the analysis of these re- sults, we plan to carry out a further task, namely ask the system to detect in the image the area that produces the mismatch with the foil word (the red box around the bird in <ref type="figure">Figure 1</ref>.) This extra step would allow us to fully diagnose the failure of the tested systems and confirm what is implicit in our results from task 3: that the algorithms are unable to map particular elements of the text to their vi- sual counterparts. We note that the addition of this extra step will move this work closer to the tex- tual/visual explanation research (e.g., <ref type="bibr" target="#b23">(Park et al., 2016;</ref><ref type="bibr" target="#b25">Selvaraju et al., 2016)</ref>). We will then have a pipeline able to not only test whether a mistake can be detected, but also whether the system can explain its decision: 'the wrong word is dog be- cause the cyclists are in fact approaching a bird, there, in the image'.</p><p>LaVi models are a great success of recent re- search, and we are impressed by the amount of ideas, data and models produced in this stimulat- ing area. With our work, we would like to push the community to think of ways that models can bet- ter merge language and vision modalites, instead of merely using one to supplement the other.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>LSTM + norm I:</head><label></label><figDesc>We use the best performing VQA model in (Antol et al., 2015) (deeper LSTM + norm I). This model uses a two stack Long- Short Term Memory (LSTM) to encode the ques- tions and the last fully connected layer of VG- GNet to encode images. Both image embedding and caption embedding are projected into a 1024- dimensional feature space. Following (Antol et al., 2015), we have normalised the image feature be- fore projecting it. The combination of these two projected embeddings is performed by a point- wise multiplication. The multi-model represen- tation thus obtained is used for the classification, which is performed by a multi-layer perceptron (MLP) classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Two cases of foil image-caption pairs that are judged as correct by all annotators.</figDesc><graphic url="image-3.png" coords="9,96.57,592.90,166.40,113.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Super -category No. of object</head><label>Super</label><figDesc></figDesc><table>No. of foil 
captions 

Acc. using 
LSTM + norm I 

Acc. using 
HieCoAtt 
outdoor 
2 
107 
2.80 
0.93 
food 
9 
10407 
22.00 
26.59 
indoor 
6 
4911 
30.74 
27.97 
appliance 
5 
2811 
32.72 
34.54 
sports 
10 
16276 
31.57 
31.61 
animal 
10 
21982 
39.03 
43.18 
vehicle 
8 
16514 
34.38 
40.09 
furniture 
5 
13625 
33.27 
33.13 
accessory 
5 
3040 
49.53 
31.80 
electronic 
6 
5615 
45.82 
43.47 
kitchen 
7 
4192 
38.19 
45.34 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Classification Accuracy of foil captions by Super Categories (T1). The No. of the objects and 
the No. of foil captions refer to the test set. The training set has a similar distribution. 

Top-5 
Bottom-5 
T1: LSTM + norm I 
racket::glove 
100 
motorcycle::airplane 
0 
racket::kite 
97.29 
bicycle::airplane 
0 
couch::toilet 
97.11 
drier::scissors 
0 
racket::skis 
95.23 
bus::airplane 
0.35 
giraffe::sheep 
95.09 
zebra::giraffe 
0.43 
T1: HieCoAtt 
tie::handbag 
100 
drier::scissors 
0 
snowboard::glove 
100 
fork::glass 
0 
racket::skis 
100 
handbag::tie 
0 
racket::glove 
100 
motorcycle::airplane 
0 
backpack::handbag 
100 
train::airplane 
0 

Top-5 
Bottom-5 
T2: LSTM + norm I 
drier::scissors 
100 
glove::skis 
0 
zebra::giraffe 
88.98 
snowboard::racket 
0 
boat::airplane 
87.87 
donut::apple 
0 
truck::airplane 
85.71 
glove::surfboard 
0 
train::airplane 
81.93 
spoon::bottle 
0 
T2: HieCoAtt 
zebra::elephant 
94.92 
drier::scissors 
0 
backpack::handbag 94.44 
handbag::tie 
0 
cow::zebra 
93.33 
broccoli:orange 
1.47 
bird::sheep 
93.11 
zebra::giraffe 
1.96 
orange::carrot 
92.37 
boat::airplane 
2.09 

</table></figure>

			<note place="foot" n="1"> The dataset is available from https://foilunitn. github.io/</note>

			<note place="foot" n="3"> https://github.com/karpathy/ neuraltalk</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are greatful to the Erasmus Mundus European Master in Language and Communication Tech-nologies (EM LCT) for the scholarship provided to the third author. Moreover, we gratefully ac-knowledge the support of NVIDIA Corporation with the donation of the GPUs used in our re-search.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the behavior of visual question answering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SPICE: Semantic Propositional Image Caption Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<ptr target="https://github.com/VT-vision-lab/VQA_LSTM_CNN" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic description generation from images: A survey of models, datasets, and evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruket</forename><surname>Cakici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aykut</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkut</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Muscat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res.(JAIR)</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="409" to="442" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mind&apos;s eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2422" to="2431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Understanding image and text simultaneously: a dual vision-language machine comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07833</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Comparing automatic evaluation measures for image description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="452" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1473" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are you talking to a machine? dataset and methods for multilingual image question</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2296" to="2304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Making the V in VQA Matter: Elevating the Role of Image Understanding in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00837</idno>
	</analytic>
	<monogr>
		<title level="m">Visual Question Answering</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards Transparent AI Systems: Interpreting Visual Question Answering Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akrit</forename><surname>Mohapatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML Visualization Workshop</title>
		<meeting>ICML Visualization Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Focused evaluation for image description with binary forcedchoice tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Vision and Language (VL&apos;16)</title>
		<meeting>the 5th Workshop on Vision and Language (VL&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Revisiting visual question answering baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="727" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno>ArXiv:1612.06890</idno>
		<title level="m">CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01465</idno>
		<title level="m">Visual question answering: Datasets, algorithms, and future challenges</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Microsoft COCO: Common Objects in Context. In European Conference on Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical question-image coattention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<ptr target="https://github.com/jiasenlu/HieCoAttenVQA" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2016</title>
		<meeting>NIPS 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A multiworld approach to question answering about realworld scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1682" to="1690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Attentive explanations: Justifying decisions and pointing to the evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<idno>ArXiv:1612.04757</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Trevor Darrell Bernt Schiele, and Marcus Rohrbach</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<idno>ArXiv:1610.02391v2</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image captioning with deep bidirectional LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Meinel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="988" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05910</idno>
		<title level="m">Visual question answering: A survey of methods and datasets</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Yin and yang: Balancing and answering binary visual questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5014" to="5022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Simple baseline for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
