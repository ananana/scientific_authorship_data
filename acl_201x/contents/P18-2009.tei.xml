<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Thematic Similarity Metric Using Triplet Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ein</forename><surname>Liat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosi</forename><surname>Mass</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Halfon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Venezian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Shnayderman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranit</forename><surname>Aharonov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Slonim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Thematic Similarity Metric Using Triplet Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="49" to="54"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>49</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we suggest to leverage the partition of articles into sections, in order to learn thematic similarity metric between sentences. We assume that a sentence is thematically closer to sentences within its section than to sentences from other sections. Based on this assumption, we use Wikipedia articles to automatically create a large dataset of weakly labeled sentence triplets, composed of a pivot sentence , one sentence from the same section and one from another section. We train a triplet network to embed sentences from the same section closer. To test the performance of the learned embeddings, we create and release a sentence clustering benchmark. We show that the triplet network learns useful thematic metrics, that significantly outperform state-of-the-art semantic similarity methods and multipurpose embeddings on the task of thematic clustering of sentences. We also show that the learned embeddings perform well on the task of sentence semantic similarity prediction.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text clustering is a widely studied NLP problem, with numerous applications including collabora- tive filtering, document organization and index- ing ( <ref type="bibr" target="#b0">Aggarwal and Zhai, 2012)</ref>. Clustering can be applied to texts at different levels, from sin- gle words to full documents, and can vary with respect to the clustering goal. In this paper, we fo- cus on the problem of clustering sentences based on thematic similarity, aiming to group together sentences that discuss the same theme, as opposed * * These authors contributed equally to this work.</p><p>to the related task of clustering sentences that rep- resent paraphrases of the same core statement.</p><p>Thematic clustering is important for various use cases. For example, in multi-document summa- rization, one often extracts sentences from mul- tiple documents that have to be organized into meaningful sections and paragraphs. Similarly, within the emerging field of computational argu- mentation ( <ref type="bibr" target="#b6">Lippi and Torroni, 2016)</ref>, arguments may be found in a widespread set of articles ( <ref type="bibr" target="#b5">Levy et al., 2017)</ref>, which further require thematic orga- nization to generate a compelling argumentative narrative.</p><p>We approach the problem of thematic cluster- ing by developing a dedicated sentence similar- ity measure, targeted at a comparative task -The- matic Distance Comparison (TDC): given a pivot sentence, and two other sentences, the task is to determine which of the two sentences is themati- cally closer to the pivot. By training a deep neural network (DNN) to perform TDC, we are able to learn a thematic similarity measure.</p><p>Obtaining annotated data for training the DNN is quite demanding. Hence, we exploit the natural structure of text articles to obtain weakly-labeled data. Specifically, our underlying assumption is that sentences belonging to the same section are typically more thematically related than sentences appearing in different sections. Armed with this observation, we use the partition of Wikipedia ar- ticles into sections to automatically generate sen- tence triplets, where two of the sentences are from the same section, and one is from a different sec- tion. This results in a sizable training set of weakly labeled triplets, used to train a triplet neural net- work <ref type="bibr" target="#b4">(Hoffer and Ailon, 2015)</ref>, aiming to predict which sentence is from the same section as the pivot in each triplet. <ref type="table">Table 1</ref> shows an example of a triplet.</p><p>To test the performance of our network on the-matic clustering of sentences, we create a new clustering benchmark based on Wikipedia sec- tions. We show that our methods, combined with existing clustering algorithms, outperform state-of-the-art general-purpose sentence embed- ding models in the task of reconstructing the orig- inal section structure. Moreover, the embeddings obtained from the triplet DNN perform well also on standard semantic relatedness tasks. The main contribution of this work is therefore in proposing a new approach for learning thematic relatedness between sentences, formulating the related TDC task and creating a thematic clustering benchmark.</p><p>To further enhance research in these directions, we publish the clustering benchmark on the IBM De- bater Datasets webpage 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deep learning via triplet networks was first in- troduced in <ref type="bibr" target="#b4">(Hoffer and Ailon, 2015)</ref>, and has since become a popular technique in metric learn- ing( <ref type="bibr" target="#b20">Zieba and Wang, 2017;</ref><ref type="bibr" target="#b17">Yao et al., 2016;</ref><ref type="bibr" target="#b19">Zhuang et al., 2016)</ref>. However, previous usages of triplet networks were based on supervised data and were applied mainly to computer vision ap- plications such as face verification. Here, for the first time, this architecture is used with weakly- supervised data for solving an NLP related task.</p><p>In <ref type="bibr" target="#b8">(Mueller and Thyagarajan, 2016)</ref>, a supervised approach was used to learn semantic sentence sim- ilarity by a Siamese network, that operates on pairs of sentences. In contrast, here the triplet network is trained with weak supervision, aim- ing to learn thematic relations. By learning from triplets, rather than pairs, we provide the DNN with a context, that is crucial for the notion of similarity. <ref type="bibr" target="#b4">(Hoffer and Ailon, 2015)</ref> show that triplet networks perform better in metric learning than Siamese networks, probably due to this valu- able context. Finally, ( <ref type="bibr" target="#b9">Palangi et al., 2016</ref>) used click-through data to learn sentence similarity on top of web search engine results. Here we propose a different type of weak supervision, targeted at learning thematic relatedness between sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Construction</head><p>We present two weakly-supervised triplet datasets. The first is based on sentences appearing in same vs. different sections, and the second is based on section titles. The datasets are extracted from the Wikipedia version of May 2017.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentence Triplets</head><p>For generating the sentence triplet dataset, we ex- ploit the Wikipedia partitioning into sections and paragraphs, using OpenNLP 2 for sentence extrac- tion. We then apply the following rules and fil- ters, in order to reduce noise and to create a high- quality dataset, 'triplets-sen': i) The maximal dis- tance between the intra-section sentences is lim- ited to three paragraphs. ii) Sentences with less than 5, or more than 50 tokens are filtered out.</p><p>iii) The first and the "Background" sections are re- moved due to their general nature. iv) The follow- ing sections are removed: "External links", "Fur- ther reading", "References", "See also", "Notes", "Citations" and "Authored books". These sections usually list a set of items rather than discuss a spe- cific subtopic of the article's title. v) Only arti- cles with at least five remaining sections are con- sidered, to ensure focusing on articles with rich enough content. An example of a triplet is shown in <ref type="table">Table 1</ref>  <ref type="table">Table 1</ref>: Example of a section-sen triplet from the article 'James Smith McDonnell'. The first two sentences are from the section 'Career' and the third is from 'Early life'</p><p>In use-cases such as multi-document summa- rization( <ref type="bibr" target="#b2">Goldstein et al., 2000</ref>), one often needs to organize sentences originating from different documents. Such sentences tend to be stand- alone sentences, that do not contain the syntactic cues that often exist between adjacent sentences (e.g. co-references, discourse markers etc.). Cor- respondingly, to focus our weakly labeled data on sentences that are typically stand-alone in nature, we consider only paragraph opening sentences.</p><p>An essential part of learning using triplets, is the mining of difficult examples, that prevent quick stagnation of the network ( <ref type="bibr" target="#b3">Hermans et al., 2017)</ref>. Since sentences in the same article essentially dis- cuss the same topic, a deep understanding of se-mantic nuances is necessary for the network to correctly classify the triplets. In an attempt to ob- tain even more challenging triplets, the third sen- tence is selected from an adjacent section. Thus, for a pair of intra-section sentences, we create a maximum of two triplets, where the third sentence is randomly selected from the previous/next sec- tion (if exists). The selection of the third sentence from both previous and next sections is intended to ensure the network will not pick up a signal re- lated to the order of the sentences. In Section 5 we compare our third-sentence-selection method to two alternatives, and examine the effect of the selection method on the model performance.</p><p>Out of the 5.37M Wikipedia articles, 809K yield at least one triplet. We divide these arti- cles into three sets, training (80%), validation and test (10% each). In terms of number of triplets, the training set is composed of 1.78M triplets, whereas the validation and test are composed of 220K and 223K triplets respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Triplets with Section Titles</head><p>Incorporating the section titles into the training data can potentially enhance the network per- formance. Correspondingly, we created another triplets data, 'triplets-titles', where in each triplet the first sentence in the section (the 'pivot') is paired with the section title 3 , as well as with the title of the previous/next sections (if exists), where the former pair is assumed to have greater the- matic similarity. After applying the filters de- scribed above we end up with 1.38M , 172K and 173K triplets for the training, validation and test set respectively. An example of a triplet is shown in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Note, that for this variation of the triplets data, the network is expected to find a sentence embed- ding which is closer to the embedding of the true section title, than to the embedding of the title of the previous/next section. The learned representa- tion is expected to encode information about the themes of the different sections to which the sen- tence can potentially belong. Thus, thematically related sentences are expected to have similar rep- resentations. Figure 1: Triplet Network</p><note type="other">1. Bishop was appointed Minister for Ageing in 2003. 2. Julie Bishop Political career 3. Julie Bishop Early life and career</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sentence Clustering Benchmark (SCB)</head><p>Our main goal is to successfully partition sen- tences into subtopics. Unfortunately, there is still no standard evaluation method for sentence clus- tering, which is considered a very difficult task for humans <ref type="bibr" target="#b1">(Geiss, 2009)</ref>. Correspondingly, we leverage again the partition of Wikipedia articles into sections. We assume that this partition, as performed by the Wikipedia editors, can serve as ground truth for the clustering of the article sen- tences. Based on this assumption we create a sentence clustering benchmark (SCB). SCB in- cludes 692 articles that were not used in the train- ing and validation sets of 'triplet-sen' and 'triplet- titles'. The number of sections (and correspond- ingly clusters) per article ranges from 5 to 12. The number of clustered sentences ranges from 17 to 1614, with an average of 67 sentences per article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model Architecture</head><p>We adopt the triplet network architecture (Hoffer and Ailon, 2015) ( <ref type="figure">Figure 1</ref>) for obtaining sentence embeddings via metric learning as follows.</p><p>Assume a training data of sentences, arranged into triplets (x,x + ,x − ), where the pair (x,x + ) is presumably more similar than the pair (x,x − ). To train the model, each of the three sentences of each triplet, is fed into the same network (Net), as a sequence of word embeddings. The layer out- puts their representations Net(x), Net(x + ) and Net(x − ) respectively. Our objective is to make the representations of x and x + closer than the rep- resentations of x and x − . Thus the next layer uses a distance function, denoted by 'dist', to compute two distances</p><formula xml:id="formula_0">d + = dist(Net(x), Net(x + )) d − = dist(Net(x), Net(x − ))</formula><p>The final layer applies softmax on (d + ,d − ) that re- sults in p(d + ) and p(d − ). Finally, the loss function is given by:</p><formula xml:id="formula_1">loss = |p(d + )| + |1 -(p(d − )|</formula><p>Net is composed of a Bi-directional LSTM with hidden size 300 and 0.8 dropout followed by an attention ( <ref type="bibr" target="#b16">Yang et al., 2016</ref>) layer of size 200. The input to Net are the pre-trained glove word em- beddings of 300d trained on 840B tokens <ref type="bibr" target="#b10">(Pennington et al., 2014</ref>). For dist and the loss func- tion we use the L1 distance, which we found to yield better results than L2 and cosine-similarity. The selected loss function outperformed the pop- ular triplet loss suggested in ( <ref type="bibr" target="#b12">Schroff et al., 2015</ref>). Finally, we use Adam optimizer with initial learn- ing rate of 0.001. Given a sentence s, Net(s) pro- vides a sentence embedding of dimension 600.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Reconstructing Article Sections</head><p>As mentioned, our main objective task is cluster- ing sentences into subtopics. As a preliminary step, we first evaluate our method on the triplet- sen test set. We compare the model trained on triplet-sen to two well known methods. The first, mean-vectors, is simply the mean of the GloVe embeddings of the sentence words ( <ref type="bibr" target="#b14">Tai et al., 2015)</ref>, which is considered a strong unsupervised baseline. The second, skip-thoughts <ref type="bibr">(Ryan Kiros, 2015)</ref>, is among the state-of-the-art unsupervised models for semantic similarity, and the most pop- ular multi-purpose embedding method. We ad- dress two versions of skip-thoughts: one is based on the original 4800-dimensional vectors (skip- thoughts-cs), and the other, skip-thoughts-SICK, is based on the similarity function learned from the SICK semantic similarity dataset, as described in <ref type="bibr">(Ryan Kiros, 2015)</ref>. The aim of assessing skip- thoughts-SICK is to examine how well a state-of- the-art semantic similarity function performs on the thematic clustering task. In the case of mean- vectors and skip-thoughts-CS, the similarity be- tween the sentences is computed using the cosine similarity (CS) between the embedding vectors. <ref type="table">Table 3</ref> indicates that our method, denoted by triplet-sen, clearly outperforms the other tested methods. Surprizingly, skip-thoughts-SICK is in- Method accuracy mean-vectors 0.65 skip-thoughts-CS 0.615 skip-thoughts-SICK 0.547 triplets-sen 0.74 <ref type="table">Table 3</ref>: Results on the triplets data ferior to skip-thoughts-CS. Note that an additional interesting comparison is to a skip-thought ver- sion obtained by learning a linear transformation of the original vectors using the triplet datasets. However, no off-the-shelf algorithm is available for learning such transformation, and we leave this experiment for future work.</p><p>Next we report results on the clustering bench- mark, SCB (Section 3.3). We evaluate three triplet-based models. Triplets-sen and triplets- titles are the models trained on triplets-sen and triplets-titles datasets respectively. Triplets-sen- titles is a concatenation of the representations of our two models. In addition we compare to mean- vectors and skip-thoughts-CS.</p><p>The evaluation procedure is performed as fol- lows: for each method, we first compute for the sentences of each article, a similarity matrix, by calculating the CS between the embedding vectors of all pairs of sentences. We then use Iclust <ref type="bibr">(YomTov and Slonim, 2009;</ref><ref type="bibr" target="#b13">Slonim et al., 2005</ref>) and k-means to cluster the sentences, where the num- ber of clusters is set to the number of sections in SCB 4 . Since the clustering algorithms them- selves are not the focus of this study, we choose the classical, simple k-means, and one more ad- vanced algorithm, Iclust. For the same reason, we also set the number of clusters to the correct number. Finally, we use standard agreement mea- sures, MI, Adjusted MI (AMI) ( <ref type="bibr" target="#b15">Vinh et al., 2009)</ref>, Rand Index (RI) and Adjusted Rand Index (ARI) <ref type="bibr" target="#b11">(Rand, 1971)</ref>, to quantify the agreement between the ground truth and the clustering results.</p><p>As exhibited in <ref type="table">Table 4</ref>, our models signifi- cantly outperform the two other methods for both clustering algorithms, where the best performance is achieved by the concatenated representations (triplets-sen-titles), suggesting the two models, triplets-sen and triplets-titles, learned complemen- tary features. The performance of skip-thoughts- SICK on this task (not shown) was again inferior to skip-thoughts-CS.</p><p>As mentioned in Section 3.1, the third sentence in triplet-sen was selected from the sections adja- cent to the pivot section, aiming to obtain more difficult triplets. We use the clustering task to ex- amine the effect of the selection method on the model performance. We compare to two alterna- tive methods: one that chooses the third sentence from a random section within the same article, and another (triplets-sen-rand-art), that chooses it ran- domly from a random different article. Results show that the first method leads to the same perfor- mance as our method, whereas triplets-sen-rand- art yields inferior results (see <ref type="table">Table 4</ref>). A possi- ble explanation is that the within-article triplets are difficult enough to prevent stagnation of the learn- ing process without the need for further hardening of the task. However, the cross-article triplets are too easy to classify, and do not provide the net- work with the challenge and difficulty required for obtaining high quality representations.  <ref type="table">Table 4</ref>: Results on the clustering task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Semantic Relatedness</head><p>As evident from the clustering results, our mod- els learned well to capture thematic similarity be- tween sentences. Here we investigate the perfor- mance of our model in the more classical task of semantic relatedness of sentences. Specifically, we examine the SemEval 2014 Task 1: seman- tic relatedness SICK dataset ( <ref type="bibr" target="#b7">Marelli et al., 2014</ref>). We adopt the experimental setup of <ref type="bibr">(Ryan Kiros, 2015)</ref> and learn logistic regression classifiers on top of the absolute difference and the component- wise product for all sentence pairs in the train- ing data. The evaluation measures are Pearson r, Spearman ρ, and mean square error (MSE). <ref type="table">Ta- ble 5</ref> shows that like in the clustering task, best re- sults are achieved by the concatenated embedding triplets-sen-titles, which performs in the range be- tween mean-vector and skip-thoughts-SICK.  <ref type="table">Table 5</ref>: Results on the SICK semantic relatedness subtask. <ref type="table">Table 6</ref> presents some examples of predictions of triplets-sen-titles compared to the ground truth and to skip-thoughts-SICK predictions. The first pair is semantically equivalent as both methods de- tect. In the second pair, the first sentence is a nega- tion of the second, but from the thematic point of view they are rather similar, thus assigned a rela- tively high score by our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>sentences</head><p>GT Tr Sk 1. A sea turtle is hunting for fish 4.5 4.2 4.5 2. A sea turtle is hunting for food 1. A sea turtle is not hunting for fish 3.4 4.1 3.8 2. A sea turtle is hunting for fish <ref type="table">Table 6</ref>: Example predictions on the SICK data. GT = groundtruth, Tr=triplets-sen, Sk=skip- thoughts-SICK</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary</head><p>In this paper we suggest a new approach for learn- ing thematic similarity between sentences. We exploit the Wikipedia section structure to gener- ate a large dataset of weakly labeled triplets of sentences with no human involvement. Using a triplet network, we learn a high quality sentence embeddings, tailored to reveal thematic relations between sentences. Furthermore, we take a first step towards exploring the versatility of these em- beddings, by showing their good performance on the semantic similarity task. An interesting direc- tion for future work is further exploring this ver- satility, by examining the performance of the em- beddings on a variety of other NLP tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Example of a triplet from the triplet-titles dataset, generated from the article 'Julie Bishop'.</head><label>2</label><figDesc></figDesc><table>í µí±í µí±í µí±¡ 
í µí±í µí±í µí±¡ 
í µí±í µí±í µí±¡ 

í µí±¥ % 
í µí±¥ 
í µí±¥ &amp; 

dist(í µí±í µí±í µí±¡ í µí±¥ , í µí±í µí±í µí±¡ í µí±¥ % ) 

í µí±í µí±í µí± í µí±  í µí±í µí±¢í µí±í µí±í µí±¡í µí±í µí±í µí± 

dist(í µí±í µí±í µí±¡ í µí±¥ , í µí±í µí±í µí±¡ í µí±¥ &amp; ) 

</table></figure>

			<note place="foot" n="1"> http://www.research.ibm.com/haifa/ dept/vst/debating_data.shtml</note>

			<note place="foot" n="2"> https://opennlp.apache.org/</note>

			<note place="foot" n="3"> We define the section title to be the article title concatenated to the section title. For example, the title of the section &quot;Pricing&quot; in the article &quot;Black Market&quot; is &quot;Black Market Pricing&quot;.</note>

			<note place="foot" n="4"> For k-means, using L1 as the distance metric gave similar results</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Yoav Goldberg for helpful advise.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of text clustering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining text data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="77" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Creating a gold standard for sentence clustering in multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Geiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-IJCNLP Student Research Workshop</title>
		<meeting>the ACL-IJCNLP Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-document summarization by sentence extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhu</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Kantrowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization</title>
		<meeting>the 2000 NAACL-ANLP Workshop on Automatic summarization</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<title level="m">Defense of the Triplet Loss for Person ReIdentification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
		<title level="m">DEEP METRIC LEARNING USING TRIPLET NETWORK</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised corpus-wide claim detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Gretz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sznajder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><surname>Hummel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranit</forename><surname>Aharonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Slonim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Argument Mining</title>
		<meeting>the 4th Workshop on Argument Mining<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Argumentation mining: State of the art and emerging trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Torroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Internet Technology (TOIT)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Siamese recurrent architectures for learning sentence similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Thyagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2786" to="2792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinying</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabab</forename><surname>Ward</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ArXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Objective criteria for the evaluation of clustering methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William M Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical association</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">336</biblScope>
			<biblScope unit="page" from="846" to="850" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Information-based clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurinder</forename><surname>Singh Atwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaper</forename><surname>Tkaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Bialek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>PNAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Information theoretic measures for clusterings comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">X</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning-ICML</title>
		<meeting>the 26th Annual International Conference on Machine Learning-ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep semantic-preserving and ranking-based hashing for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchen</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3931" to="3937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Parallel pairwise clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Yom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Tov</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Slonim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast training of triplet-based deep binary embedding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5955" to="5964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Zieba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02227</idno>
		<title level="m">Training triplet networks with gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
