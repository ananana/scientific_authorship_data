<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Code-switched Text for Lexical Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
							<email>hod.lipson@cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generating Code-switched Text for Lexical Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="562" to="571"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A vast majority of L1 vocabulary acquisition occurs through incidental learning during reading (Nation, 2001; Schmitt et al., 2001). We propose a probabilistic approach to generating code-mixed text as an L2 technique for increasing retention in adult lexical learning through reading. Our model that takes as input a bilingual dictionary and an English text, and generates a code-switched text that optimizes a defined &quot;learnability&quot; metric by constructing a factor graph over lexical mentions. Using an artificial language vocabulary, we evaluate a set of algorithms for generating code-switched text automatically by presenting it to Mechanical Turk subjects and measuring recall in a sentence completion task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Today, an adult trying to learn a new language is likely to embrace an age-old and widely accepted practice of learning vocabulary through curated word lists and rote memorization. Yet, it is not uncommon to find yourself surrounded by speak- ers of a foreign language and instinctively pick up words and phrases without ever seeing the defini- tion in your native tongue. Hearing "pass le sale please" at the dinner table from your in-laws vis- iting from abroad, is unlikely to make you think twice about passing the salt. Humans are extraor- dinarily good at inferring meaning from context, whether this context is your physical surround- ing, or the surrounding text in the paragraph of the word that you don't yet understand.</p><p>Recently, a novel method of L2 language teach- ing had been shown effective in improving adult lexical acquisition rate and retention <ref type="bibr">1</ref> . This tech- 1 authors' unpublished work nique relies on a phenomenon that elicits a nat- ural simulation of L1-like vocabulary learning in adults -significantly closer to L1 learning for L2 learners than any model studied previously. By in- fusing foreign words into text in the learner's na- tive tongue into low-surprisal contexts, the lexi- cal acquisition process is facilitated naturally and non-obtrusively. Incidentally, this phenomenon occurs "in the wild" and is termed code-switching or code-mixing, and refers to the linguistic pattern of bilingual speakers swapping words and phrases between two languages during speech. While this phenomenon had received significant attention from both a socio-linguistic ( <ref type="bibr" target="#b19">Milroy and Muysken, 1995)</ref> and theoretical linguistic perspectives <ref type="bibr" target="#b2">(Belazi et al., 1994;</ref><ref type="bibr" target="#b4">Bhatt, 1997</ref>) (including some computational studies), only recently has it been hypothesizes that "code-switching" is a marking of bilingual proficiency, rather than deficiency <ref type="bibr" target="#b11">(Genesee, 2001)</ref>.</p><p>Until recently it was widely believed that inci- dental lexical acquisition through reading can only occur for words that occur at sufficient density in a single text, so as to elicit the "noticing" ef- fect needed for lexical acquisition to occur <ref type="bibr" target="#b8">(Cobb, 2007)</ref>. Recent neurophysiological findings, how- ever, indicate that even a single incidental expo- sure to a novel word in a sufficiently constrained context is sufficient to trigger an early integra- tion of the word in the brain's semantic network ( <ref type="bibr" target="#b7">Borovsky et al., 2012</ref>).</p><p>An approach explored in this paper, and moti- vated by the above findings, exploits "constrain- ing" contexts in text to introduce novel words. A state-of-the-art approach for generating such text is based on an expert annotator whose job is to decide which words to "switch out" with novel foreign words (from hereon we will refer to the "switched out" word as the source word and to the "switched in" word as the target word). Conse- quently the process is labor-intensive and leads to a "one size fits all solution" that is insensitive to the learner's skill level or vocabulary proficiency. This limitation is also cited in literature as a sig- nificant roadblock to the widespread adaptation of graded reading series <ref type="bibr" target="#b12">(Hill, 2008)</ref>. A reading- based tool that follows the same principle, i.e. by systematic exposure of a learner to an incremen- tally more challenging text, will result in more ef- fective learning <ref type="bibr" target="#b16">(Lantolf and Appel, 1994)</ref>.</p><p>To address the above limitation, we develop an approach for automatically generating such "code- switched" text with an explicit goal of maximizing the lexical acquisition rate in adults. Our method is based on a global optimization approach that incorporates a "knowledge model" of a user with the content of the text, to generate a sequence of lexical "switches". To facilitate the selection of "switch points", we learn a discriminative model for predicting switch point locations on a corpus that we collect for this purpose (and release to the community). Below is a high-level outline of this paper.</p><p>• We formalize our approach within a prob- abilistic graphical model framework, infer- ence in which yields "code-switched" text that maximizes a surrogate to the acquisition rate objective.</p><p>• We compare this global method to sev- eral baseline techniques, including the strong "high-frequency" baseline.</p><p>• We analyze the operating range in which our model is effective and motivate the near- future extension of this approach with the proposed improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our proposed approach to the computational gen- eration of code-switched text, for the purpose of L2 pedagogy, is influenced by a number of fields that studied aspects of this phenomenon from dis- tinct perspectives. In this section, we briefly de- scribe a motivation from the areas of socio-and psycho-linguistics and language pedagogy re- search that indicate the promise of this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Code-switching as a natural phenomenon</head><p>Code-switching (or code-mixing) is a widely stud- ied phenomenon that received significant attention over the course of the last three decades, across the disciplines of sociolinguistics, theoretical and psycholinguistics and even literary and cultural studies (predominantly in the domain of Spanish- English code-switching) ( <ref type="bibr" target="#b17">Lipski, 2005</ref>). Code-switching that occurs naturally in bilin- gual populations, and especially in children, has for a long time been considered a marking of incompetency in the second language. A more recent view on this phenomenon, however, sug- gests that due to the underlying syntactic com- plexity of code-switching, code-switching is ac- tually a marking of bilingual fluency <ref type="bibr" target="#b11">(Genesee, 2001</ref>). More recently, the idea of employing code-switching in the classroom, in a form of conversation-based exercises, has attracted the attention of multiple researchers and educators <ref type="bibr" target="#b20">(Moodley, 2010;</ref><ref type="bibr" target="#b18">Macaro, 2005)</ref>, yielding promis- ing results in an elementary school study in South- Africa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Computational Approaches to</head><p>Code-switching</p><p>Additionally, there has been a limited number of studies of the computational approaches to code-switching, and in particular code-switched text generation. <ref type="bibr" target="#b26">Solorio and Liu (2008)</ref>, record and transcribe a corpus of Spanish-English code- mixed conversation to train a generative model (Naive Bayes) for the task of predicting code- switch points in conversation. Additionally they test their trained model in its ability to generate code-switched text with convincing results. Build- ing on their work, ( <ref type="bibr" target="#b0">Adel et al., 2012</ref>) employ ad- ditional features and a recurrent network language model for modeling code-switching in conversa- tional speech. Adel and collegues (2011) propose a statistical machine translation-based approach for generating code-switched text. We note, how- ever, that the primary goal of these methods is in the faithful modeling of the natural phenomenon of code-switching in bilingual populations, and not as a tool for language teaching. While useful in generating coherent, syntactically constrained code-switched texts in its own right, none of these methods explicitly consider code-switching as a vehicle for teaching language, and thus do not take on an optimization-based view with an ob- jective of improving lexical acquisition through the reading of the generated text. More recently, and concurrently with our work, Google's Lan- guage Immersion app employs the principle of code-switching for language pedagogy, by gener- ating code-switched web content, and allowing its users to tune it to their skill level. It does not, how- ever, seem to model the user explicitly, nor is it clear if it performs any optimization in generating the text, as no studies have been published to date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Computational Approaches to Sentence Simplification</head><p>Although not explicitly for teaching language, computational approaches that facilitate accessi- bility to texts that might otherwise be too difficult for its readers, either due to physical or learning disabilities, or language barriers, are relevant. In the recent work of <ref type="bibr" target="#b14">(Kauchak, 2013)</ref>, for example demonstrates an approach to increasing readability of texts by learning from unsimplified texts. Ap- proaches in this area span methods for simplify- ing lexis <ref type="bibr" target="#b27">(Yatskar et al., 2010;</ref><ref type="bibr" target="#b5">Biran et al., 2011</ref>), syntax <ref type="bibr" target="#b25">(Siddharthan, 2006;</ref><ref type="bibr" target="#b24">Siddharthan et al., 2004</ref>), discourse properties <ref type="bibr" target="#b13">(Hutchinson, 2005)</ref>, and making technical terminology more accessible to non-experts <ref type="bibr" target="#b9">(Elhadad and Sutaria, 2007)</ref>. While the resulting texts are of great potential aid to lan- guage learners and may implicitly improve upon a reader's language proficiency, they do not explic- itly attempt to promote learning as an objective in generating the simplified text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Recent Neurophysiological findings</head><p>Evidence for the potential effectiveness of code- switching for language acquisition, stem from the recent findings of ( <ref type="bibr" target="#b7">Borovsky et al., 2012</ref>), who have shown that even a single exposure to a novel word in a constrained context, results in the inte- gration of the word within your existing semantic base, as indicated by a change in the N400 elec- trophysiological response recorded from the sub- jects' scalps. N400 ERP marker has been found to correlate with the semantic "expectedness" of a word ( <ref type="bibr" target="#b15">Kutas and Hillyard, 1984)</ref>, and is believed to be an early indicator of word learning. Further- more, recent work of ( <ref type="bibr" target="#b10">Frank et al., 2013)</ref>, show that word surprisal predicts N400, providing con- crete motivation for artificial manipulation of text to explicitly elicit word learning through natural reading, directly motivating our approach. Prior to the above findings, it was widely believed that for evoking "incidental" word learning through read- ing alone, the word must appear with sufficiently high frequency within the text, such as to elicit the "noticing" effect -a prerequisite to lexical acqui- sition ( <ref type="bibr" target="#b22">Schmidt and Schmidt, 1995;</ref><ref type="bibr" target="#b8">Cobb, 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The formulation of our model is primarily moti- vated by two hypotheses that have been validated experimentally in the cognitive science literature. We re-state these hypotheses in the language of "surprisal":</p><p>1. Inserting a target word into a low surprisal context increases the rate of that word's inte- gration into a learner's lexicon.</p><p>2. Multiple exposures to the word in low sur- prisal contexts increases rate of that word's integration.</p><p>Hypothesis 1 is supported by evidence from ( <ref type="bibr" target="#b7">Borovsky et al., 2012;</ref><ref type="bibr" target="#b10">Frank et al., 2013)</ref>, and hy- pothesis 2 is supported by evidence from <ref type="bibr" target="#b22">(Schmidt and Schmidt, 1995)</ref>. We adopt the term "low- surprisal" context to identify contexts (e.g. n- grams) that are highly predictive of the target word (e.g. trailing word in the n-gram). The motiva- tion stems from the recent evidence ( <ref type="bibr" target="#b10">Frank et al., 2013</ref>) that low-surprisal contexts affect the N400 response and thus correlate with word acquisi- tion. To realize a "code-switched" mixture that adheres maximally to the above postulates, it is self-evident that a non-trivial optimization prob- lem must be solved. For example, naively select- ing a few words that appear in low-surprisal con- texts may facilitate their acquisition, but at the ex- pense of other words within the same context that may appear in a larger number of low-surprisal contexts further in the text.</p><p>To address this problem, we approach it with a formulation of a factor graph that takes global structure of the text into account. Factor graph for- malism allows us to capture local features of indi- vidual contexts, such as lexical and syntactic sur- prisal, while inducing dependencies between con- sequent "switching decisions" in the text. Max- imizing likelihood of the joint probability under the factorization of this graph yields an optimal sequence of these "switching decisions" in the en- tirety of the text. Maximizing joint likelihood, as we will show in the next section, is a surrogate to maximizing the probability of the learner acquir- ing novel words through the process of reading the generated text. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixed-Language Content</head><p>Figure 1: Overview of the approach. Probabilistic learner model (PLM) provides the current value of the belief in the learner's knowledge of any given word. Local contextual model provides the value of the belief in learning the word from the context alone. Upon exposure of the learner to the word in the given context, PLM is updated with the posterior belief in the user's knowledge of the word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Language Learner Model</head><p>A simplified model of the learner, that we shall term a Probabilistic Learner Model (PLM) serves as a basis for our approach. PLM is a model of a learner's lexical knowledge at any given time. PLM models the learner as a vector of indepen- dent Bernoulli distributions, where each compo- nent represents a probability of the learner know- ing the corresponding word. We motivate a proba- bilistic approach by taking the perspective of mea- suring our belief in the learner's knowledge of any given word, rather than the learner's uncertainty in own knowledge. Formally, we can fully specify this model for learner i as follows:</p><formula xml:id="formula_0">U i = (π i 0 , π i 1 , . . . , π i |V | )<label>(1)</label></formula><p>where V is the vocabulary set -identical across all users, and π i j is our degree of belief in the learner i's knowledge of a target word w j ∈ V . Statistical estimation techniques exist for estimat- ing an individual's vocabulary size, such as <ref type="bibr" target="#b3">(Bhat and Sproat, 2009;</ref><ref type="bibr" target="#b1">Beglar, 2010)</ref>, and can be di-rectly employed for estimating the parameters of this model as our prior belief about user i's knowl- edge.</p><p>The primary motivation behind a probabilistic user model, is to provide a mechanism for up- dating these probabilities as the user progresses through her reading. Maximizing the parameters of the PLM under a given finite span of code- switched text, thus, provides a handle for generat- ing optimal code-switched content. Additionally, a probabilistic approach allows for a natural inte- gration of the user model with the uncertainty in other components of the system, such as uncer- tainty in determining the degree of constraint im- posed by the context, and in bitext alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model overview</head><p>At the high level, as illustrated in <ref type="figure">Figure 1</ref>, our ap- proach integrates the model of the learner (PLM) with the local contextual features to update the PLM parameters incrementally as the learner pro- gresses through the text. The fundamental as- sumption behind our approach is that the learner's knowledge of a given word after observing it in a sentence is a function of 1) the learner's previ- ous knowledge of the word, prior to observing it in a given sentence and 2) a degree of constraint that a given context imposes on the meaning of the novel word, and is directly related to the surprisal of novel word in that context. Broadly, as the learner progresses from one sentence to the next, exposing herself to more novel words, the updated parameters of the language model in turn guide the selection of new "switch-points" for replac- ing source words with the target foreign words. In practice, however, this process is carried out im- plicitly and off-line by optimizing the estimated progress of the learner's PLM, without dynamic feedback. Next, we describe the model in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Switching Factor Graph Model</head><p>To aid in the specification of the factor graph struc- ture, we introduce new terminology. Because the PLM is updated progressively, we will refer to the parameters of the PLM for a given word w i after observing its k th appearance (instance) in the text, as the learner's state of knowledge of that word, and denote it as a binary random variable z i k .</p><formula xml:id="formula_1">P (z i k = 1) =   </formula><p>Probability that word w i ∈ V is understood on k th exposure Without explicit testing of the user, this variable is hidden. We can view the prior learning model as the parameters of the vector of random variables</p><formula xml:id="formula_2">(z 0 0 , z 1 0 , . . . z |V | 0 ).</formula><p>The key to our approach is in how the param- eters of these hidden variables are updated from repeated exposures to words in various contexts. Intuitively, an update to the parameter of z i k from z i k−1 occurs after the learner observes word w i in a context (this may be an n-gram, an entire sen- tence or paragraph containing w i , but we will re- strict our attention to fixed-length n-grams). In- tuitively an update to the parameter of z i k−1 will depend on how "constrained" the meaning of w i is in the given context. We will refer to it as the "learnability", denoted by L k i , of word w i on its k th appearance, given its context. Formally, we will define "learnability" as follows:</p><formula xml:id="formula_3">P (L i k = 1|w i , w \i , z \i k ) = P (constrained(w i ) = 1|w) i =j P (z j k = 1)<label>(2)</label></formula><p>where w \i represents the set of words that com- prise the context window of w i , not including w i , and z \i k are the states corresponding to each of the words in w \i . P (constrained(w i ) = 1|w) is a real value (scaled between 0 and 1) that represents the degree of constraint imposed on the meaning of word w i by its context. This value comes from a binary prediction model trained to predict the "predictability" of a word in its context, and is based on the dataset that we collected (described later in the paper). Generally, this value may come directly from the surprisal quantity given by a language model, or may incorporate additional features that are found informative in predicting the constraint on the word. Finally, the quantity is weighted by the parameters of the state vari- ables corresponding to the words other than w i contained in the context. This encodes an intu- ition that a degree of predictability of a given word given its context is related to the learner's knowl- edge of the other words in that context. If, for ex- ample, in the sentence "pass me the salt and pep- per, please", both "salt" and "pepper" are substi- tuted with their foreign translations that the learner is unlikely to know, it's equally unlikely that she will learn them after being exposed to this con- text, as the context itself will not offer sufficient information for both words to be inferred simulta- neously. On the other hand, substituting "salt" and "pepper" individually, is likely to make it much easier to infer the meaning of the other. </p><formula xml:id="formula_4">z i k1 z i k L i k</formula><formula xml:id="formula_5">P (z i k = 1|z i k−1 , L i k ) = 1 − [1 − P (L i k = 1)][1 − P (z k−1 = 1)]</formula><p>A noisy-OR-based CPD provides a convenient and tractable approximation in capturing the in- tended intuition: updated state of knowledge of a given word will increase if the word is observed in a "good" context, or if the learner already knows the word.</p><p>Combining Equation 2 for each word in the con- text using the noisy-OR, the updated state for word w i will now be conditioned on z i k−1 , z \i k , w k . Be- cause of the dependence of each z in the context on all other hidden variables in that context, we can capture the dependence using a single factor per context, with all of the z variables taking part in a clique, whose dimension is the size of the con- text.</p><p>We will now introduce a dual interpretation of the z variables: as "switching" variables that de- cide whether a given word will be replaced with its translation in the foreign language. If, for exam- ple, all of the words have high probability of be- ing known by a learner, than maximizing the joint likelihood of the model will result in most of the words "switched-out" -a desired result. For an arbitrary prior PLM and the input text, maximiz- ing joint likelihood will result in the selection of "switched-out" words that have the highest final probability of being "known" by the learner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Inference</head><p>The problem of selecting "switch-points" reduces to the problem of inference in the resulting factor graph. Unfortunately, without a fairly strong con- straint on the collocation of switched words, the resulting graph will contain loops, requiring tech- niques of approximate inference. To find the opti- mal settings of the z variables, we apply the loopy max-sum algorithm. While variants of loopy be- lief propagation, in general, are not guaranteed to converge, we found that the convergence does in- deed occur in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Predicting "predictable" words</head><p>We carried out experiments to determine which words are likely to be inferred from their context. The collected data-set is then used to train a logis- tic regression classifier to predict which words are likely to be easily inferred from their context. We believe that this dataset may also be useful to re- searchers in studying related phenomena, and thus make it publicly available.</p><p>For this task, we focus only on the following context features for predicting the "predictability" of words: n-gram probability, vector-space simi- larity score, coreferring mentions. N-gram prob- ability and vector-space similarity 2 score are all computed within a fixed-size window of the word (trigrams using Microsoft N-gram service). Coref- erence feature is a binary feature which indicates whether the word has a co-referring mention in a 3-sentence window preceding a given context (ob- tained using Stanford's CoreNLP package). We train L2-regularized logistic regression to predict a binary label L ∈ {Constrained, Unconstrained} using a crowd-sourced corpus described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Corpus Construction</head><p>For collecting data about which words are likely to be "predicted" given their content, we devel- oped an Amazon Mechanical Turk task that pre- sented turkers with excerpts of a short story (En- glish translation of "The Man who Repented" by</p><formula xml:id="formula_6">w i w j w i w i w j w j w k w i S 1 S 2 S 3 S 4 S 5 S 6</formula><p>Original Text Factor Graph</p><formula xml:id="formula_7">Mapping f 1 f 2 f 3 f 4 f 5 z i 0 z i 1 z i 2 z i 3 z j 0 z j 1 z j 2 z k 0</formula><p>Figure 3: Sequence of sentences in the text (left) is mapped into a factor graph, whose nodes correspond to specific occurences of individual words, connected in a clique corresponding to a context in which the word occurs.</p><p>Ana Maria Matute), with some sentences contain- ing a blank in place of a word. Only content words were considered for the task. Turkers were re- quired to type in their best guess, and the num- ber of semantically similar guesses were counted by an average number of 6 other turkers. A ra- tio of the median of semantically similar guesses to the total number of guesses was then taken as the score representing "predictability" of the word being guessed in the given context. All words cor- responding to blanks whose scores were equal to and above 0.6 were than taken as a positive la- bel (Constrained) and scores below 0.6 were taken as a negative label (Unconstrained). Turkers that judged the semantic similarity of the guesses of other turkers achieved an average Cohen's kappa agreement of 0.44, indicating fair to poor agree- ment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We carried out experiments on the effectiveness of our approach using the Amazon Mechanical Turk platform. Our experimental procedure was as follows: 162 turkers were partitioned into four groups, each corresponding to a treatment con- dition: OP T (N=34), HF (N=41), RAN DOM (N=43), M AN (N=44). Each condition corre- <ref type="figure">Figure 4</ref>: Visualization of the most "predictable" words in an excerpt from the "The Man who Re- pented" by Ana Maria Matute (English transla- tion). Font-size correlates with the score given by judge turkers in evaluating guesses of other turk- ers that were presented with the same text, but the word replaced with a blank. Snippet of the dataset that we release publicly. sponded to a model used to generate the presented code-switched text. For all experiments, the text used was a short story "Lottery" by Shirley Jack- son, and a total number of replaced words was controlled (34). Target vocabulary consisted of words from an artificial language, generated stat- ically by a mix of words from several languages. Below we describe the individual treatment condi- tions:</p><p>RANDOM (Baseline): words for switching are selected at random from content only words. HF (High Frequency) Baseline: words for switching are selected at random from a ranked list of words that occur most frequently in the pre- sented text.</p><p>MAN (Manual) Baseline: words for switch- ing are selected manually by the author, based on the intuition of which words are most likely to be guessed in context.</p><p>OPT (Optimization-based): factor graph-based model proposed in this paper is used for generat- ing code-switched content. The total number of switched words generated by this method is used as a constant for all baselines.</p><p>Turkers were solicited to participate in a study that involved "reading a short story with a twist" (title of HIT). Not the title, nor the description gave away the purpose of the study, nor that it would be followed by a quiz. Time was not con- trolled for this study, but on average turkers took 27 minutes to complete the reading. Upon com- pleting the reading portion of the task, turkers were presented with novel sentences that featured the words observed during reading, where only one of the sentences used the word in a semanti- cally correct way. Turkers were asked to select the sentence that "made the most sense". An example of the sentences presented during the test:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1</head><p>My edzino loves to go shopping every weekend.</p><p>The edzino was too big to explore on our own, so went with a group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English word: wife</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 2</head><p>His unpreadvers were utterly confus- ing and useless.</p><p>The unpreadvers was so strong, that he had to go to a hospital.</p><p>English word: directions A "recall" metric was computed for each turker, defined as the ratio of correctly selected sentences to the total number of sentences presented. The "grand-average recall" across all turkers was then computed and reported here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We perform a one-way ANOVA across the four groups listed above, with the resulting F = 11.38 and p = 9.7e−7. Consequently, multiple pairwise comparison of the models was performed with the Bonferroni-corrected pairwise t-test, yielding the only significantly different recall means between HF − M AN (p = 0.00018), RAN DOM − M AN (p = 2.8e − 6), RAN DOM − OP T (p = 0.00587). The results indicate that, while none of the automated methods (RAN DOM , HF , OP T ) outperform manually generated code- switched text, OP T outperforms the RAN DOM baseline (no decisive conclusion can be drawn with respect to the HF − RAN DOM pair). Additionally, we note, that for words with fre- quency less than 4, OP T produces recall that is on average higher than the HF baseline (p=0.043, Welch's t-test), but at the expense of higher fre- quency words. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We observe from our experiments that the optimization-based approach does not in general outperform the HF baseline. The strength of the frequency-based baseline is attributed to a well- known phenomenon that item frequency promotes the "noticing" effect during reading, critical for triggering incidental lexical acquisition. Gener- ating code-switched text by replacing high fre- quency content words, thus, in general is a sim- ple and viable approach for generating effective reading-based L2 curriculum aids. However, this method is fundamentally less flexible than the optimization-based method proposed in this paper, for several reasons:</p><p>• The optimization-based method explicitly models the learner and thus generates code- switched text progressively more fit for a given individual, even across a sequence of multiple texts. A frequency-based baseline alone would generate content at approxi- mately the same level of difficulty consis- tently, with the pattern that words that tend to have high frequency in the natural language in general to be the ones that are "switched- out" most often.</p><p>• An optimization-based approach is able to elicit higher recall in low frequency words, as the mechanism for their selection is driven by the context in which these words appear, rather than frequency alone, favoring those that are learned more readily through context.</p><p>Moreover, the proposed method in this pa- per is extensible to more sophisticated learner models, with a potential to surpass the results presented here. Another worthwhile applica- tion of this method is as a nested component within a larger optimization-based tool, that in addition to generating code-switched text as demonstrated here, aids in selecting con- tent (such as popular books) as units in the code-switched curriculum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future Work</head><p>In this work we demonstrated a pilot implemen- tation of a model-based, optimization-based ap- proach to content generation for assisting in the reading-based L2 language acquisition. Our ap- proach is based on static optimization, and while it would, in theory progress in difficulty with more reading, its open-loop nature precludes it from maintaining an accurate model of the learner in the long-term. For generating effecting L2 con- tent, it is important that the user be kept in a "zone of proximal development" -a tight region where the level of the taught content is at just the right difficulty. Maintaining an accurate internal model of the learner is the single most important require- ment for achieving this functionality. Closed-loop learning, with active user feedback is, thus, going to be functionally critical component of any sys- tem of this type that is designed to function in the long-term. Additionally, our approach is currently a proof- of-concept of an automated method for generat- ing content for assisted L2 acquisition, and is lim- ited to artificial language and only isolated lexi- cal items. The next step would be to integrate bitext alignment across texts in two natural lan- guages, inevitably introducing another stochas- tic component into the pipeline. Extending this method to larger units, like chunks and simple grammar is another important avenue along which we are taking this work. Early results from concur- rent research indicate that "code-switched based" method proposed here is also effective in eliciting acquisition of multi-word chunks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A noisy-OR combination of the learner's previous state of knowledge of the word z i k−1 and the word's "learnability" in the observed context L i k The updated parameter of z i k is obtained from a noisy-OR combination of the parameters of z i k−1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results presented for 4 groups, subjected to 4 treatment conditions: RAN DOM , HF , M AN , OP T. Recall performance for each group corresponds to the average ratio of selected sentences that correctly utilize codeswitched words in novel contexts, across all turkers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Subset of the results for 2 of the 4 treatment conditions: HF and OP T that correspond to recall only for words with item frequency in the presented text below 4.</figDesc></figure>

			<note place="foot" n="2"> we employ C&amp;W word embeddings from http:// metaoptimize.com/projects/wordreprs/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Recurrent neural network language modeling for code switching conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Schlippe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A rasch-based validation of the vocabulary size test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Beglar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Testing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="118" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Code switching and x-bar theory: The functional head constraint. Linguistic inquiry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Almeida Jacqueline</forename><surname>Edward J Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toribio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="221" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowing the unseen: estimating vocabulary size over unseen samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suma</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Code-switching, constraints, and optimal grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rakesh Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lingua</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="223" to="251" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Putting it simply: a context-aware approach to lexical simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">SMT-based Text Generation for Code-Switching Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Blaicher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<pubPlace>Singapore</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Nanyang Technological University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Once is enough: N400 indexes semantic integration of novel word meanings from a single exposure in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arielle</forename><surname>Borovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Jeffrey L Elman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kutas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Learning and Development</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="278" to="302" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computing the vocabulary demands of l2 reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Cobb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Learning &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="38" to="63" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mining a lexicon of technical terms and lay equivalents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Komal</forename><surname>Sutaria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</title>
		<meeting>the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Word surprisal predicts n400 amplitude during reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stefan L Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Otten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Galli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vigliocco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="878" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bilingual first language acquisition: Exploring the limits of the language faculty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Genesee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Applied Linguistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="153" to="168" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graded readers in english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ELT journal</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="204" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modelling the substitutability of discourse connectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving text simplification language modeling using unsimplified text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Brain potentials during reading reflect word expectancy and semantic association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Kutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hillyard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Vygotskian approaches to second language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Lantolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Appel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Greenwood Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Code-switching or borrowing? no sé so no puedo decir, you know</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Selected Proceedings of the Second Workshop on Spanish Sociolinguistics</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Codeswitching in the l2 classroom: A communication and learning strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernesto</forename><surname>Macaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Non-native language teachers</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="63" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">One speaker, two languages: Cross-disciplinary perspectives on code-switching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lesley</forename><surname>Milroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Muysken</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Code-switching and communicative competence in the language classroom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visvaganthie</forename><surname>Moodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal for Language Teaching</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="22" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning vocabulary in another language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nation</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Ernst Klett Sprachen</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention and awareness in foreign language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard W</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natl Foreign Lg Resource Ctr</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Developing and exploring the behaviour of two new versions of the vocabulary levels test. Language testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norbert</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Clapham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="55" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Syntactic simplification for improving content selection in multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Advaith</forename><surname>Siddharthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">896</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Syntactic simplification and text cohesion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Advaith</forename><surname>Siddharthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research on Language and Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="109" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to predict code-switching points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="973" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">For the sake of simplicity: Unsupervised extraction of lexical simplifications from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Danescu-Niculescumizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="365" to="368" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
