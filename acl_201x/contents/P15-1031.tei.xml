<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Effective Neural Network Model for Graph-based Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<addrLine>No.5 Yiheyuan Road</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<orgName type="institution">Haidian District</orgName>
								<address>
									<postCode>100871, 221009</postCode>
									<settlement>Beijing, Xuzhou</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Effective Neural Network Model for Graph-based Dependency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="313" to="322"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most existing graph-based parsing models rely on millions of hand-crafted features, which limits their generalization ability and slows down the parsing speed. In this paper, we propose a general and effective Neural Network model for graph-based dependency parsing. Our model can automatically learn high-order feature combinations using only atomic features by exploiting a novel activation function tanh-cube. Moreover, we propose a simple yet effective way to utilize phrase-level information that is expensive to use in conventional graph-based parsers. Experiments on the English Penn Treebank show that parsers based on our model perform better than conventional graph-based parsers.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dependency parsing is essential for computers to understand natural languages, whose performance may have a direct effect on many NLP applica- tion. Due to its importance, dependency parsing, has been studied for tens of years. Among a vari- ety of dependency parsing approaches <ref type="bibr" target="#b13">(McDonald et al., 2005;</ref><ref type="bibr" target="#b12">McDonald and Pereira, 2006;</ref><ref type="bibr" target="#b1">Carreras, 2007;</ref><ref type="bibr" target="#b10">Koo and Collins, 2010;</ref><ref type="bibr" target="#b20">Zhang and Nivre, 2011</ref>), graph-based models seem to be one of the most successful solutions to the challenge due to its ability of scoring the parsing decisions on whole-tree basis. Typical graph-based models factor the dependency tree into subgraphs, rang- ing from the smallest edge (first-order) to a con- trollable bigger subgraph consisting of more than one single edge (second-order and third order), and score the whole tree by summing scores of the subgraphs. In these models, subgraphs are usually represented as a high-dimensional feature vectors * Corresponding author which are fed into a linear model to learn the fea- ture weight for scoring the subgraphs.</p><p>In spite of their advantages, conventional graph- based models rely heavily on an enormous num- ber of hand-crafted features, which brings about serious problems. First, a mass of features could put the models in the risk of overfitting and slow down the parsing speed, especially in the high- order models where combinational features cap- turing interactions between head, modifier, sib- lings and (or) grandparent could easily explode the feature space. In addition, feature design re- quires domain expertise, which means useful fea- tures are likely to be neglected due to a lack of domain knowledge. As a matter of fact, these two problems exist in most graph-based models, which have stuck the development of dependency parsing for a few years.</p><p>To ease the problem of feature engineering, we propose a general and effective Neural Network model for graph-based dependency parsing in this paper. The main advantages of our model are as follows:</p><p>• Instead of using large number of hand-crafted features, our model only uses atomic fea- tures ( ) such as word uni- grams and POS-tag unigrams. Feature com- binations and high-order features are auto- matically learned with our novel activation function tanh-cube, thus alleviating the heavy burden of feature engineering in conven- tional graph-based models <ref type="bibr" target="#b13">(McDonald et al., 2005;</ref><ref type="bibr" target="#b12">McDonald and Pereira, 2006</ref>; <ref type="bibr" target="#b10">Koo and Collins, 2010)</ref>. Not only does it avoid the risk of overfitting but also it discovers useful new features that have never been used in conven- tional parsers.</p><p>• We propose to exploit phrase-level informa- tion through distributed representation for phrases (phrase embeddings). It not only en- ables our model to exploit richer context in- formation that previous work did not consider due to the curse of dimension but also cap- tures inherent correlations between phrases.</p><p>• Unlike other neural network based models <ref type="bibr" target="#b11">Le and Zuidema, 2014)</ref> where an additional parser is needed for ei- ther extracting features ) or generating k-best list for reranking <ref type="bibr" target="#b11">(Le and Zuidema, 2014)</ref>, both training and decoding in our model are performed based on our neu- ral network architecture in an effective way.</p><p>• Our model does not impose any change to the decoding process of conventional graph- based parsing model. First-order, second- order and higher order models can be easily implemented using our model.</p><p>We implement three effective models with in- creasing expressive capabilities. The first model is a simple first-order model that uses only atomic features and does not use any combinational fea- tures. Despite its simpleness, it outperforms conventional first-order model <ref type="bibr" target="#b13">(McDonald et al., 2005</ref>) and has a faster parsing speed. To fur- ther strengthen our parsing model, we incorpo- rate phrase embeddings into the model, which significantly improves the parsing accuracy. Fi- nally, we extend our first-order model to a second- order model that exploits interactions between two adjacent dependency edges as in <ref type="bibr" target="#b12">McDonald and Pereira (2006)</ref> thus further improves the model performance.</p><p>We evaluate our models on the English Penn Treebank. Experiment results show that both our first-order and second-order models outperform the corresponding conventional models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Network Model</head><p>A dependency tree is a rooted, directed tree span- ning the whole sentence. Given a sentence x, graph-based models formulates the parsing pro- cess as a searching problem:</p><formula xml:id="formula_0">y * (x) = arg maxˆy∈Y maxˆ maxˆy∈Y (x) Score(x, ˆ y(x); θ)<label>(1)</label></formula><p>where y * (x) is tree with highest score, Y (x) is the set of all trees compatible with x, θ are model parameters and Score(x, ˆ y(x); θ) represents how likely that a particular treê y(x) is the correct anal- ysis for x. However, the size of Y (x) is expo- nential large, which makes it impractical to solve equation (1) directly. Previous work <ref type="bibr" target="#b13">(McDonald et al., 2005;</ref><ref type="bibr" target="#b12">McDonald and Pereira, 2006;</ref><ref type="bibr" target="#b10">Koo and Collins, 2010)</ref> assumes that the score ofˆyofˆ ofˆy(x) fac- tors through the scores of subgraphs c ofˆyofˆ ofˆy(x) so that efficient algorithms can be designed for de- coding:</p><formula xml:id="formula_1">Score(x, ˆ y(x); θ) = c∈ˆyc∈ˆy(x)</formula><p>ScoreF (x, c; θ) (2) <ref type="figure" target="#fig_0">Figure 1</ref> gives two examples of commonly used factorization strategy proposed by <ref type="bibr">Mcdonald et.al (2005)</ref> and <ref type="bibr" target="#b12">Mcdonald and Pereira (2006)</ref>. The simplest subgraph uses a first-order factorization <ref type="bibr" target="#b13">(McDonald et al., 2005</ref>) which decomposes a de- pendency tree into single dependency arcs ( </p><formula xml:id="formula_2">ScoreF (x, c; θ) = w · f (x, c)<label>(3)</label></formula><p>where f (x, c) is the feature representation of sub- graph c and w is the corresponding weight vector. However, the effectiveness of this function relies heavily on the design of feature vector f (x, c). In previous work <ref type="bibr" target="#b13">(McDonald et al., 2005;</ref><ref type="bibr" target="#b12">McDonald and Pereira, 2006</ref>), millions of hand-crafted fea- tures were used to capture context and structure information in the subgraph which not only lim- its the model's ability to generalize well but only slows down the parsing speed. In our work, we propose a neural network model for scoring subgraph c in the tree:</p><formula xml:id="formula_3">ScoreF (x, c; θ) = N N (x, c)<label>(4)</label></formula><p>where N N is our scoring function based on neu- ral network ( <ref type="figure" target="#fig_2">Figure 2</ref>). As we will show in the fol- lowing sections, it alleviates the heavy burden of feature engineering in conventional graph-based models and achieves better performance by auto- matically learning useful information in the data. The effectiveness of our neural network de- pends on five key components: Feature Em- beddings, Phrase Embeddings, Direction-specific transformation, Learning Feature Combinations and Max-Margin Training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Embeddings</head><p>As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, part of the input to the neu- ral network is feature representation of the sub- graph. Instead of using millions of features as in conventional models, we only use use atomic fea- tures ( ) such as word unigrams and POS-tag unigrams, which are less likely to be sparse. The detailed atomic features we use will be described in Section 3. Unlike conventional models, the atomic features in our model are trans- formed into their corresponding distributed repre- sentations (feature embeddings).</p><p>The idea of distributed representation for sym- bolic data is one of the most important reasons why neural network works in NLP tasks. It is shown that similar features will have similar em- beddings which capture the syntactic and seman- tic information behind features (Bengio et al., <ref type="figure">Figure 3</ref>: Illustration for phrase embeddings. h, m and x 0 to x 6 are words in the sentence. <ref type="bibr" target="#b4">Collobert et al., 2011;</ref><ref type="bibr" target="#b16">Schwenk et al., 2012;</ref><ref type="bibr" target="#b14">Mikolov et al., 2013;</ref><ref type="bibr" target="#b17">Socher et al., 2013;</ref><ref type="bibr" target="#b15">Pei et al., 2014</ref>).</p><p>Formally, we have a feature dictionary D of size |D|. Each feature f ∈ D is represented as a real- valued vector (feature embedding) Embed(f ) ∈ R d where d is the dimensionality of the vector space. All feature embeddings stacking together forms the embedding matrix M ∈ R d×|D| . The embedding matrix M is initialized randomly and trained by our model (Section 2.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Phrase Embeddings</head><p>Context information of word pairs 1 such as the de- pendency pair (h, m) has been widely believed to be useful in graph-based models <ref type="bibr" target="#b13">(McDonald et al., 2005;</ref><ref type="bibr" target="#b12">McDonald and Pereira, 2006</ref>). Given a sen- tence x, the context for h and m includes three context parts: prefix, infix and suffix, as illustrated in <ref type="figure">Figure 3</ref>. We call these parts phrases in our work.</p><p>Context representation in conventional mod- els are limited: First, phrases cannot be used as features directly because of the data sparseness problem. Therefore, phrases are backed off to low-order representation such as bigrams and tri- grams. For example, Mcdonald et.al (2005) used tri-gram features of infix between head-modifier pair (h, m). Sometimes even tri-grams are expen- sive to use, which is the reason why Mcdonald and Pereira (2006) chose to ignore features over triples of words in their second-order model to prevent from exploding the size of the feature space. Sec-ond, bigrams or tri-grams are lexical features thus cannot capture syntactic and semantic information behind phrases. For instance, "hit the ball" and "kick the football" should have similar represen- tations because they share similar syntactic struc- tures, but lexical tri-grams will fail to capture their similarity.</p><p>Unlike previous work, we propose to use distributed representation (phrase embedding) of phrases to capture phrase-level information. We use a simple yet effective way to calculate phrase embeddings from word (POS-tag) embeddings. As shown in <ref type="figure">Figure 3</ref>, we average the word em- beddings in prefix, infix and suffix respectively and get three global word-phrase embeddings. For pairs where no prefix or suffix exists, the corre- sponding embedding is set to zero. We also get three global POS-phrase embeddings which are calculated in the same way as words. These em- beddings are then concatenated with feature em- beddings and fed to the following hidden layer.</p><p>Phrase embeddings provide panorama represen- tation of the context, allowing our model to cap- ture richer context information compared with the back-off tri-gram representation. Moreover, as a distributed representation, phrase embeddings perform generalization over specific phrases, thus better capture the syntactic and semantic informa- tion than back-off tri-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Direction-specific Transformation</head><p>In dependency representation of sentence, the edge direction indicates which one of the words is the head h and which one is the modifier m. Un- like previous work <ref type="bibr" target="#b13">(McDonald et al., 2005;</ref><ref type="bibr" target="#b12">McDonald and Pereira, 2006</ref>) that models the edge direction as feature to be conjoined with other fea- tures, we model the edge direction with direction- specific transformation.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, the parameters in hidden</p><formula xml:id="formula_4">layer (W d h , b d h ) and the output layer (W d o , b d o )</formula><p>are bound with index d ∈ {0, 1} which indicates the direction between head and modifier (0 for left arc and 1 for right arc). In this way, the model can learn direction-specific parameters and automati- cally capture the interactions between edge direc- tion and other features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Learning Feature Combination</head><p>The key to the success of graph-based dependency parsing is the design of features, especially com- binational features. Effective as these features are, as we have said in Section 1, they are prone to overfitting and hard to design. In our work, we introduce a new activation function that can auto- matically learn these feature combinations.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, we first concatenate the embeddings into a single vector a. Then a is fed into the next layer which performs linear trans- formation followed by an element-wise activation function g:</p><formula xml:id="formula_5">h = g(W d h a + b d h )<label>(5)</label></formula><p>Our new activation function g is defined as fol- lows:</p><formula xml:id="formula_6">g(l) = tanh(l 3 + l)<label>(6)</label></formula><p>where l is the result of linear transformation and tanh is the hyperbolic tangent activation function widely used in neural networks. We call this new activation function tanh-cube.</p><p>As we can see, without the cube term, tanh-cube would be just the same as the conventional non- linear transformation in most neural networks. The cube extension is added to enhance the abil- ity to capture complex interactions between input features. Intuitively, the cube term in each hid- den unit directly models feature combinations in a multiplicative way:</p><formula xml:id="formula_7">(w 1 a 1 + w 2 a 2 + ... + w n a n + b) 3 = i,j,k (w i w j w k )a i a j a k + i,j b(w i w j )a i a j ...</formula><p>These feature combinations are hand-designed in conventional graph-based models but our model learns these combinations automatically and en- codes them in the model parameters.</p><p>Similar ideas were also proposed in previous works <ref type="bibr" target="#b17">(Socher et al., 2013;</ref><ref type="bibr" target="#b15">Pei et al., 2014;</ref><ref type="bibr" target="#b2">Chen and Manning, 2014</ref>). Socher et.al (2013) and Pei et.al (2014) used a tensor-based activation function to learn feature combinations. However, tensor-based transformation is quite slow even with tensor factorization ( <ref type="bibr" target="#b15">Pei et al., 2014</ref>). <ref type="bibr" target="#b2">Chen and Manning (2014)</ref> proposed to use cube func- tion g(l) = l 3 which inspires our tanh-cube func- tion. Compared with cube function, tanh-cube has three advantages:</p><p>• The cube function is unbounded, making the activation output either too small or too big if the norm of input l is not properly controlled, especially in deep neural network. On the contrary, tanh-cube is bounded by the tanh function thus safer to use in deep neural net- work.</p><p>• Intuitively, the behavior of cube function re- sembles the "polynomial kernel" in SVM.</p><p>In fact, SVM can be seen as a special one- hidden-layer neural network where the ker- nel function that performs non-linear trans- formation is seen as a hidden layer and sup- port vectors as hidden units. Compared with cube function, tanh-cube combines the power of "kernel function" with the tanh non-linear transformation in neural network.</p><p>• Last but not least, as we will show in Section 4, tanh-cube converges faster than the cube function although the rigorous proof is still open to investigate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Model Output</head><p>After the non-linear transformation of hidden layer, the score of the subgraph c is calculated in the output layer using a simple linear function:</p><formula xml:id="formula_8">ScoreF (x, c) = W d o h + b d o<label>(7)</label></formula><p>The output score ScoreF (x, c) ∈ R |L| is a score vector where |L| is the number of dependency types and each dimension of ScoreF (x, c) is the score for each kind of dependency type of head- modifier pair (i.e. (h, m) in <ref type="figure" target="#fig_0">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Max-Margin Training</head><p>The parameters of our model are θ</p><formula xml:id="formula_9">= {W d h , b d h , W d o , b d o , M }.</formula><p>All parameters are initialized with uniform distribution within (-0.01, 0.01).</p><p>For model training, we use the Max-Margin cri- terion. Given a training instance (x, y), we search for the dependency tree with the highest score computed as equation <ref type="formula" target="#formula_0">(1)</ref> in Section 2. The object of Max-Margin training is that the highest scor- ing tree is the correct one: y * = y and its score will be larger up to a margin to other possible treêtreê y ∈ Y (x):</p><p>Score(x, y; θ) ≥ Score(x, ˆ y; θ) + (y, ˆ y)</p><p>The structured margin loss (y, ˆ y) is defined as:</p><formula xml:id="formula_10">(y, ˆ y) = n j κ1{h(y, x j ) = h(ˆ y, x j )} 1-order-atomic</formula><p>h−2.w, h−1.w, h.w, h1.w, h2.w h−2.p, h−1.p, h.p, h1.p, h2.p m−2.w, m−1.w, m.w, m1.w, m2.w m−2.p, m−1.p, m.p, m1.p, m2.p dis(h, m) where n is the length of sentence x, h(y, x j ) is the head (with type) for the j-th word of x in tree y and κ is a discount parameter. The loss is proportional to the number of word with an incorrect head and edge type in the proposed tree. This leads to the regularized objective function for m training ex- amples:</p><formula xml:id="formula_11">J(θ) = 1 m m i=1 l i (θ) + λ 2 ||θ|| 2 l i (θ) = maxˆy∈Y maxˆ maxˆy∈Y (x i ) (Score(x i , ˆ y; θ) + (y i , ˆ y)) −Score(x i , y i ; θ))<label>(8)</label></formula><p>We use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs (batch size = 20) to minimize the object function. We also apply dropout ( <ref type="bibr" target="#b9">Hinton et al., 2012</ref>) with 0.5 rate to the hidden layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Implementation</head><p>Base on our Neural Network model, we present three model implementations with increasing ex- pressive capabilities in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">First-order models</head><p>We first implement two first-order models: 1- order-atomic and 1-order-phrase. We use the Eisner <ref type="formula">(2000)</ref>   head word and its local neighbor words that are within the distance of 2 are selected as the head's word unigram features. The modifier's word un- igram features is extracted in the same way. We also use the POS-tags of the corresponding word features and the distance between head and modi- fier as additional atomic features.</p><p>We then improved 1-order-atomic to 1-order- phrase by incorporating additional phrase embed- dings. The three phrase embeddings of head- modifier pair (h, m): hm prefix, hm infix and hm suffix are calculated as in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Second-order model</head><p>Our model can be easily extended to a second- order model using the second-order decoding al- gorithm <ref type="bibr" target="#b6">(Eisner, 1996;</ref><ref type="bibr" target="#b12">McDonald and Pereira, 2006</ref>). The third row of <ref type="table">Table 1</ref> shows the addi- tional features we use in our second-order model.</p><p>Sibling node and its local context are used as additional atomic features. We also used the in- fix embedding for the infix between sibling pair (s, m), which we call sm infix. It is calculated in the same way as infix between head-modifier pair (h, m) (i.e., hm infix) in Section 2.2 except that the word pair is now s and m. For cases where no sibling information is available, the corresponding sibling-related embeddings are set to zero vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>We use the English Penn Treebank (PTB) to eval- uate our model implementations and <ref type="bibr" target="#b19">Yamada and Matsumoto (2003)</ref> head rules are used to extract dependency trees. We follow the standard splits of PTB3, using section 2-21 for training, section 22 as development set and 23 as test set. The Stanford POS Tagger ( <ref type="bibr" target="#b18">Toutanova et al., 2003</ref>) with ten-way jackknifing of the training data is used for assign- ing POS tags (accuracy ≈ 97.2%).</p><p>Hyper-parameters of our models are tuned on the development set and their final settings are as follows: embedding size d = 50, hidden layer (Layer 2) size = 200, regularization parameter λ = 10 −4 , discount parameter for margin loss κ = 0.3, initial learning rate of AdaGrad alpha = 0.1. <ref type="table" target="#tab_2">Table 2</ref> compares our models with several conven- tional graph-based parsers. We use MSTParser 2 for conventional first-order model <ref type="bibr" target="#b13">(McDonald et al., 2005)</ref> and second-order model <ref type="bibr" target="#b12">(McDonald and Pereira, 2006</ref>). We also include the result of a third-order model of <ref type="bibr" target="#b10">Koo and Collins (2010)</ref> for comparison <ref type="bibr">3</ref> . For our models, we report the results with and without unsupervised pre-training. Pre- training only trains the word-based feature embed- dings on Gigaword corpus ( <ref type="bibr" target="#b8">Graff et al., 2003</ref>) us- ing word2vec 4 and all other parameters are still initialized randomly. In all experiments, we re- port unlabeled attachment scores (UAS) and la- beled attachment scores (LAS) and punctuation <ref type="bibr">5</ref> is excluded in all evaluation metrics. The parsing speeds are measured on a workstation with Intel Xeon 3.4GHz CPU and 32GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Results</head><p>As we can see, even with random initialization, 1-order-atomic-rand performs as well as conven- tional first-order model and both 1-order-phrase- rand and 2-order-phrase-rand perform better than conventional models in MSTParser. Pre- training further improves the performance of all three models, which is consistent with the conclu- sion of previous work <ref type="bibr" target="#b15">(Pei et al., 2014;</ref><ref type="bibr" target="#b2">Chen and Manning, 2014</ref>). Moreover, 1-order-phrase per- forms better than 1-order-atomic, which shows that phrase embeddings do improve the model. 2- order-phrase further improves the performance because of the more expressive second-order fac- torization. All three models perform significantly better than their counterparts in MSTParser where millions of features are used and 1-order-phrase works surprisingly well that it even beats the con- ventional second-order model. With regard to parsing speed, 1-order-atomic is the fastest while other two models have similar speeds as MSTParser. Further speed up could be achieved by using pre-computing strategy as men- tioned in <ref type="bibr" target="#b2">Chen and Manning (2014)</ref>. We did not try this strategy since parsing speed is not the main focus of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>tanh-cube cube tanh 1-order-atomic 92.19 91.97 91.73 1-order-phrase 92.82 92.25 92.13 2-order-phrase 93.57 92.95 92.91 <ref type="table">Table 3</ref>: Model Performance of different activa- tion functions.</p><p>We also investigated the effect of different acti- vation functions. We trained our models with the same configuration except for the activation func- tion. <ref type="table">Table 3</ref> lists the UAS of three models on de- velopment set.   As we can see, tanh-cube function outperforms cube function because of advantages we men- tioned in Section 2.4. Moreover, both tanh-cube function and cube function performs better than tanh function. The reason is that the cube term can capture more interactions between input features.</p><p>We also plot the UAS of 2-order-phrase dur- ing each iteration of training. As shown in <ref type="figure" target="#fig_3">Figure  4</ref>, tanh-cube function converges faster than cube function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Analysis</head><p>In order to see why our models work, we made qualitative analysis on different aspects of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ability of Feature Abstraction</head><p>Feature embeddings give our model the ability of feature abstraction. They capture the inherent cor- relations between features so that syntactic similar features will have similar representations, which makes our model generalizes well on unseen data. <ref type="table" target="#tab_4">Table 4</ref> shows the effect of different feature embeddings which are obtained from 2-order- phrase after training. For each kind of feature type, we list several features as well as top 5 fea- tures that are nearest (measured by Euclidean dis- tance) to the corresponding feature according to their embeddings.</p><p>We first analysis the effect of word embeddings after training. For comparison, we also list the initial word embeddings in word2vec. As we can see, in word2vec word embeddings, words that are similar to in and which tends to be those  co-occuring with them and for word his, similar words are morphologies of he. On the contrary, similar words measured by our embeddings have similar syntactic functions. This is helpful for de- pendency parsing since parsing models care more about the syntactic functions of words rather than their collocations or morphologies.</p><p>POS-tag embeddings also show similar behav- ior with word embeddings. As shown in <ref type="table" target="#tab_4">Table 4</ref>, our model captures similarities between POS-tags even though their embeddings are initialized ran- domly.</p><p>We also investigated the effect of phrase embed- dings in the same way as feature embeddings. Ta- ble 5 lists the examples of similar phrases. Our phrase embeddings work pretty well given that only a simple averaging strategy is used. Phrases that are close to each other tend to share simi- lar syntactic and semantic information. By using phrase embeddings, our model sees panorama of the context rather than limited word tri-grams and thus captures richer context information, which is the reason why phrase embeddings significantly improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ability of Feature Learning</head><p>Finally, we try to unveil the mysterious hidden layer and investigate what features it learns. For each hidden unit of 2-order-phrase, we get its connections with embeddings (i.e., W d h in <ref type="figure" target="#fig_2">Figure  2</ref>) and pick the connections whose weights have absolute value &gt; 0.1. We sampled several hidden units and invenstigated which features their highly weighted connections belong to:</p><p>• Hidden 1: h.w, m.w, h−1.w, m1.w The samples above give qualitative results of what features the hidden layer learns:</p><p>• Hidden unit 1 and 2 show that atomic features of head, modifier, sibling and their local con- text words are useful in our model, which is consistent with our expectations since these features are also very important features in conventional graph-based models <ref type="bibr" target="#b12">(McDonald and Pereira, 2006</ref>).</p><p>• Features in the same hidden unit will "com- bine" with each other through our tanh-cube activation function. As we can see, feature combination in hidden unit 2 were also used in <ref type="bibr" target="#b12">Mcdonald and Pereira (2006)</ref>. However, these feature combinations are automatically captured by our model without the labor- intensive feature engineering.</p><p>• Hidden unit 3 to 5 show that phrase-level information like hm prefix, hm suffix and sm infix are effective in our model. These features are not used in conventional second- order model <ref type="bibr" target="#b12">(McDonald and Pereira, 2006</ref>) because they could explode the feature space. Through our tanh-cube activation function, our model further captures the interactions between phrases and other features without the concern of overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Models for dependency parsing have been stud- ied with considerable effort in the NLP commu- nity. Among them, we only focus on the graph- based models here. Most previous systems ad- dress this task by using linear statistical models with carefully designed context and structure fea- tures. The types of features available rely on tree factorization and decoding algorithm. Mcdonald et.al <ref type="bibr">(2005)</ref> proposed the first-order model which is also know as arc-factored model. Efficient de- coding can be performed with Eisner (2000) algo- rithm in O(n 3 ) time and O(n 2 ) space. <ref type="bibr" target="#b12">Mcdonald and Pereira (2006)</ref> further extend the first-order model to second-order model where sibling infor- mation is available during decoding. Eisner <ref type="formula">(2000)</ref> algorithm can be modified trivially for second- order decoding. <ref type="bibr" target="#b1">Carreras (2007)</ref> proposed a more powerful second-order model that can score both sibling and grandchild parts with the cost of O(n 4 ) time and O(n 3 ) space. To exploit more struc- ture information, <ref type="bibr" target="#b10">Koo and Collins (2010)</ref> pro- posed three third-order models with computational requirements of O(n 4 ) time and O(n 3 ) space.</p><p>Recently, neural network models have been in- creasingly focused on for their ability to minimize the effort in feature engineering. <ref type="bibr">Chen et.al (2014)</ref> proposed an approach to automatically learning feature embeddings for graph-based dependency parsing. The learned feature embeddings are used as additional features in conventional graph-based model. <ref type="bibr" target="#b11">Le and Zuidema (2014)</ref> proprosed an infinite-order model based on recursive neural net- work. However, their model can only be used as an reranking model since decoding is intractable.</p><p>Compared with these work, our model is a general and standalone neural network model. Both training and decoding in our model are per- formed based on our neural network architecture in an effective way. Although only first-order and second-order models are implemented in our work, higher-order graph-based models can be easily implemented using our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a general and effec- tive neural network model that can automatically learn feature combinations with our novel acti- vation function. Moreover, we introduce a sim- ple yet effect way to utilize phrase-level informa- tion, which greatly improves the model perfor- mance. Experiments on the benchmark dataset show that our model achieves better results than conventional models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: First-order and Second-order factorization strategy. Here h stands for head word, m stands for modifier word and s stands for the sibling of m.</figDesc><graphic url="image-1.png" coords="2,74.01,62.81,214.24,56.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 1 gives two examples of commonly used factorization strategy proposed by Mcdonald et.al (2005) and Mcdonald and Pereira (2006). The simplest subgraph uses a first-order factorization (McDonald et al., 2005) which decomposes a dependency tree into single dependency arcs (Figure 1(a)). Based on the first-order model, secondorder factorization (McDonald and Pereira, 2006) (Figure 1(b)) brings sibling information into decoding. Specifically, a sibling part consists of a triple of indices (h, m, s) where (h, m) and (h, s) are dependencies and s and m are successive modifiers to the same side of h. The most common choice for ScoreF (x, c; θ), which is the score function for subgraph c in the tree, is a simple linear function:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of the Neural Network</figDesc><graphic url="image-2.png" coords="3,95.28,62.81,171.72,184.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Convergence curve for tanh-cube and cube activation function.</figDesc><graphic url="image-4.png" coords="7,72.00,62.81,226.77,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Feature</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>algorithm for decoding. The first two rows of Table 1 list the features we use in these two models. 1-order-atomic only uses atomic features as shown in the first row of Table 1. Specifically, the</figDesc><table>Models 

Dev 
Test 
Speed (sent/s) 
UAS LAS UAS LAS 

First-order 

MSTParser-1-order 
92.01 90.77 91.60 90.39 
20 
1-order-atomic-rand 
92.00 90.71 91.62 90.41 
55 
1-order-atomic 
92.19 90.94 92.14 90.92 
55 
1-order-phrase-rand 
92.47 91.19 92.25 91.05 
26 
1-order-phrase 
92.82 91.48 92.59 91.37 
26 

Second-order 
MSTParser-2-order 
92.70 91.48 92.30 91.06 
14 
2-order-phrase-rand 
93.39 92.10 92.99 91.79 
10 
2-order-phrase 
93.57 92.29 93.29 92.13 
10 
Third-order (Koo and Collins, 2010) 93.49 N/A 93.04 N/A 
N/A 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Comparison with conventional graph-based models.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Examples of similar words and POS-tags 
according to feature embeddings. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Examples of similar phrases according to phrase embeddings.</figDesc><table></table></figure>

			<note place="foot" n="1"> A word pair is not limited to the dependency pair (h, m). It could be any pair with particular relation (e.g., sibling pair (s, m) in Figure 1). Figure 3 only uses (h, m) as an example.</note>

			<note place="foot" n="2"> http://sourceforge.net/projects/ mstparser 3 Note that Koo and Collins (2010)&apos;s third-order model and our models are not strict comparable since their model is an unlabeled model. 4 https://code.google.com/p/word2vec/ 5 Following previous work, a token is a punctuation if its POS tag is {&quot; &quot; : , .}</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by National Natural Science Foundation of China under Grant No. 61273318 and National Key Basic Research Pro-gram of China 2014CB340504. We want to thank Miaohong Chen and Pingping Huang for their valuable comments on the initial idea and helping pre-process the data.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Experiments with a higherorder projective dependency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLPCoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="957" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar, October</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feature embedding for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="816" to="826" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">999999</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Three new probabilistic models for dependency parsing: An exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th conference on Computational linguistics</title>
		<meeting>the 16th conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="340" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bilexical grammars and their cubic-time parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in probabilistic and other parsing technologies</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="29" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>English gigaword. Linguistic Data Consortium</publisher>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient thirdorder dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The insideoutside recursive neural network model for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="729" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Online learning of approximate dependency parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
		<editor>EACL. Citeseer</editor>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Maxmargin tensor neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large, pruned or continuous space language models on a gpu for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Attik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</title>
		<meeting>the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Statistical dependency analysis with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyasu</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWPT</title>
		<meeting>IWPT</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
