<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Easy Things First: Installments Improve Referring Expression Generation for Objects in Photographs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Zarrieß</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dialogue Systems Group // CITEC // Faculty of Linguistics and Literary Studies</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schlangen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dialogue Systems Group // CITEC // Faculty of Linguistics and Literary Studies</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Easy Things First: Installments Improve Referring Expression Generation for Objects in Photographs</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="610" to="620"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Research on generating referring expressions has so far mostly focussed on &quot;one-shot reference&quot;, where the aim is to generate a single, discriminating expression. In interactive settings, however, it is not uncommon for reference to be established in &quot;installments&quot;, where referring information is offered piecewise until success has been confirmed. We show that this strategy can also be advantageous in technical systems that only have uncertain access to object attributes and categories. We train a recently introduced model of grounded word meaning on a data set of REs for objects in images and learn to predict semantically appropriate expressions. In a human evaluation, we observe that users are sensitive to inadequate object names-which unfortunately are not unlikely to be generated from low-level visual input. We propose a solution inspired from human task-oriented interaction and implement strategies for avoiding and repairing semantically inaccurate words. We enhance a word-based REG with context-aware, referential installments and find that they substantially improve the refer-ential success of the system.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A speaker who wants to refer to an object in a vi- sual scene will try to produce a referring expres- sion (RE) that (i) is semantically adequate, i.e. ac- curately describes the visual properties of the tar- get referent, and (ii) is pragmatically and contextu- ally appropriate, i.e. distinguishes the target from girl in front man on right anywhere brown <ref type="figure">Figure 1</ref>: Example images and REs from the ReferIt corpus ( <ref type="bibr" target="#b16">Kazemzadeh et al., 2014)</ref> other objects in the scene but does not overload the listener with unnecessary information. <ref type="figure">Figure 1</ref> il- lustrates this with two examples from a corpus of REs collected from human subjects for objects in images ( <ref type="bibr" target="#b16">Kazemzadeh et al., 2014</ref>).</p><p>Research on referring expression generation (REG) has mostly focussed on (ii), modeling prag- matic adequacy in attribute selection tasks, using as input a fully specified, symbolic representation of the visual attributes of an object and its distrac- tors in a scene ( <ref type="bibr" target="#b4">Dale and Reiter, 1995;</ref><ref type="bibr" target="#b21">Krahmer and Van Deemter, 2012</ref>).</p><p>In this paper, we follow a more recent trend ( <ref type="bibr" target="#b16">Kazemzadeh et al., 2014;</ref><ref type="bibr" target="#b13">Gkatzia et al., 2015)</ref> and investigate REG on real-world images. In this setting, a low-level visual representation of an im- age (a scene) segmented into regions (objects), in- cluding the region of the target referent, consti- tutes the input. This task is closely related to the recently very active field of image-to-text gener- ation, where deep learning approaches have been used to directly map low-level visual input to nat- ural language sentences, e.g. ( <ref type="bibr" target="#b36">Vinyals et al., 2015;</ref><ref type="bibr" target="#b1">Chen and Lawrence Zitnick, 2015;</ref><ref type="bibr" target="#b8">Devlin et al., 2015)</ref>. Similarly, we propose to cast REG on im- ages as a word selection task. Thus, we base this work on a model of perceptually grounded word meaning, which associates words with classifiers that predict their semantic appropriateness given the low-level visual features of an object <ref type="bibr" target="#b18">(Kennington and Schlangen, 2015)</ref>. As our first con- tribution, we train this model on the ReferIt cor- pus ( <ref type="bibr" target="#b16">Kazemzadeh et al., 2014</ref>) and define decod- ing mechanisms tailored to REG.</p><p>Large-scale recognition of objects and their at- tributes in images is still a non-trivial task. Con- sequently, REG systems now face the challenge of dealing with semantically inadequate expressions. For instance, in <ref type="figure">Figure 1</ref>, the system might not precisely distinguish between man or woman and generate an inadequate, confusing RE like man in the middle. Therefore, we focus on evaluating our system in an object identification task with users, in contrast to previous approaches to REG on im- ages ( <ref type="bibr" target="#b23">Mao et al., 2015)</ref>. In order to assess pos- sible sources of misunderstanding more precisely, our set-up also introduces a restricted form of in- teraction: instead of measuring "one-shot" perfor- mance only, users have three trials for identifying a referent. In this set-up, we find that different pa- rameter settings of the systems (e.g. their visual in- puts) have a clear effect on the referential success rates, while automatic evaluation measures reflect the interactive effectiveness rather poorly.</p><p>Research on reference in human interaction has noticed that conversation partners try to minimize their joint effort and often prefer to present simple expressions that can be expanded on or repaired, if necessary <ref type="bibr" target="#b2">(Clark and Wilkes-Gibbs, 1986)</ref>. This strategy, called "referring in installments" is very effective for achieving common ground in task- oriented interaction <ref type="bibr" target="#b10">(Fang et al., 2014</ref>) and is at- tested in dialogue data ( <ref type="bibr" target="#b31">Striegnitz et al., 2012</ref>). The connection between reference in installments on the one and the status of distractors and dis- tinguishing expressions on the other hand is rela- tively unexplored, though it seems natural to com- bine the two perspectives <ref type="bibr" target="#b7">(DeVault et al., 2005</ref>). <ref type="figure">Figure 1</ref> shows an example for very a simple but highly effective expression -it mentions color as a salient and distinguishing property while avoiding a potentially unclear object name.</p><p>As our second contribution, we extend our prob- abilistic word selection model to work in a sim- ple interactive installment component that tries to avoid semantically inadequate words as much as possible and only expands the expression in case of misunderstanding. We present an algorithm that generates these installments depending on the con- text, based on ideas from traditional REG algo- rithms like <ref type="bibr" target="#b4">(Dale and Reiter, 1995)</ref>. We find that a context-aware installment strategy greatly im- proves referential success as it helps to avoid and repair misunderstandings and offers a combined treatment of semantic and pragmatic adequacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>2.1 Approaches to REG "One-shot REG" Foundational work in REG has investigated attribute selection algorithms <ref type="bibr" target="#b4">(Dale and Reiter, 1995</ref>) that compute a dis- tinguishing referring expression for an object in a visual scene, which is defined as a tar- get object r, set of distractor objects D = {d 1 , d 2 , d 3 , ...} and a set of attributes A = {type, position, size, color, ...}.</p><p>A manually specified database typically associates the target and distractors in D with atomic values for each attribute, cf. ( <ref type="bibr" target="#b21">Krahmer and Van Deemter, 2012</ref>). In this setting, an attribute a 1 ∈ A is said to rule out a distractor object from D, if the target and dis- tractor have different values. This is mostly based on the assumption that we have objects of partic- ular types (e.g. people, furniture, etc.) and that the system has perfect knowledge about these ob- ject types and, consequently, about potential dis- tractors of the target. This does not apply to REG on real-world images which, as we will show in this paper, triggers some new challenges and re- search questions for this field. Subsequent work has shown that human speakers do not necessar- ily produce minimally distinguishing expressions <ref type="bibr" target="#b34">(van Deemter et al., 2006;</ref><ref type="bibr" target="#b35">Viethen and Dale, 2008;</ref><ref type="bibr" target="#b20">Koolen et al., 2011)</ref>, and has tried to account for the wide range of factors -such as different speak- ers, modalities, object categories -that are related to attribute selection, cf. ( <ref type="bibr" target="#b24">Mitchell et al., 2010;</ref><ref type="bibr" target="#b19">Koolen and Krahmer, 2010;</ref><ref type="bibr" target="#b3">Clarke et al., 2013;</ref><ref type="bibr" target="#b33">Tarenskeen et al., 2015</ref>).</p><p>Task-oriented REG has looked at reference as a collaborative process where a speaker and a listener try to reach a common goal <ref type="bibr" target="#b2">(Clark and Wilkes-Gibbs, 1986;</ref><ref type="bibr" target="#b15">Heeman and Hirst, 1995;</ref><ref type="bibr" target="#b7">DeVault et al., 2005</ref>). Given the real-time con- straints of situated interaction, a speaker often has to start uttering before she has found the optimal expression, but at the same time, she can tailor, extend, adapt, revise or correct her referring ex- pressions in case the listener signals that he did not understand. Thus, human speakers can flex-ibly split and adapt their REs over several utter- ances during an interaction, a phenomenon called "reference in installments". In a corpus analysis of the S-GIVE domain, ( <ref type="bibr" target="#b31">Striegnitz et al., 2012)</ref> showed that installments are pervasive in human- human interaction in a task-oriented environment. However, while there has been research on goal- oriented and situated REG ( <ref type="bibr" target="#b29">Stoia et al., 2006;</ref><ref type="bibr" target="#b17">Kelleher and Kruijff, 2006;</ref><ref type="bibr" target="#b30">Striegnitz et al., 2011;</ref><ref type="bibr" target="#b11">Garoufi and Koller, 2013)</ref>, installments have been rarely implemented and empirically tested in in- teractive systems. A noticeable exception is the work by <ref type="bibr" target="#b10">Fang et al. (2014)</ref> who use reinforcement learning to induce an installment strategy that is targeted at robots that have uncertain knowledge about the objects in their environment. Using rel- atively simple computer-generated scenes and a standard representations of objects as sets of at- tributes, they learn a strategy that first guides the user to objects that the system can recognize with high confidence. Our work is targeted at more complex scenes in real-world images and large domains where no a priori knowledge about ob- ject types and their attributes is given. <ref type="bibr" target="#b23">Mao et al. (2015)</ref> use a convolutional neural network and an LSTM to generate REs directly and on the same data sets as we do in this paper, but they only re- port automatic evaluation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The ReferIt corpus</head><p>We train and evaluate our system on the ReferIt data set collected by <ref type="bibr" target="#b16">Kazemzadeh et al. (2014)</ref>. The basis of the corpus is a collection of "20,000 still natural images taken from locations around the world" ( <ref type="bibr" target="#b14">Grubinger et al., 2006</ref>), which was augmented by <ref type="bibr" target="#b9">Escalante et al. (2010)</ref> with seg- mentation masks identifying objects in the images (an average of 5 objects per image). This dataset also provides manual annotations of region labels and a vector of visual features for each region (e.g. region area, width, height, and color-related fea- tures). There are 256 types of objects (i.e. labels), out of which 140 labels are used for more than 50 regions ( <ref type="bibr" target="#b9">Escalante et al., 2010)</ref>. <ref type="bibr" target="#b16">Kazemzadeh et al. (2014)</ref> collected a large number of expressions referring to objects (for which segmentations ex- ist) from these images (130k REs for 96k objects), using a game-based crowd-sourcing approach, and they have assembled an annotated test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The WAC model</head><p>Given a corpus of REs aligned with objects in images, we can train a model that predicts se- mantically appropriate words given the visual rep- resentation of an image region. We adopt the WAC ("words-as-classifiers") model ( <ref type="bibr" target="#b18">Kennington and Schlangen, 2015)</ref>, which was originally used for reference resolution in situated dialogue. How- ever, WAC is essentially a task-independent ap- proach to predicting semantic appropriateness of words in visual contexts and can be flexibly com- bined with task-dependent decoding procedures.</p><p>The WAC model pairs each word w in its vocab- ulary V with an individual classifier that maps the low-level, real-valued visual properties of an ob- ject o to a semantic appropriateness score. In or- der to learn the meaning of e.g. the word red, the visual properties of all objects described as red in a corpus of REs are given as positive instances to a supervised (logistic regression) learner. Negative instances are randomly samples from the comple- mentary set of utterances (e.g. not containing red).</p><p>We used this relatively simple model in our work, because first of all we wanted to test wether it scales from a controlled domain of typical ref- erence game scenes ( <ref type="bibr" target="#b18">Kennington and Schlangen, 2015)</ref> to real-world images. Second, as compared to standard object recognisers that predict abstract image labels annotated in e.g. ImageNet <ref type="bibr" target="#b5">(Deng et al., 2009)</ref>, this model directly captures the rela- tion between actual words used in REs and visual properties of the corresponding referents. Follow- ing ( <ref type="bibr" target="#b27">Schlangen et al., 2016</ref>), we can easily base our classifiers on such a high-performance con- volutional neural network ( <ref type="bibr" target="#b32">Szegedy et al., 2015)</ref>, by applying it on our images and extracting the fi- nal fully-connected layer before the classification layer (see Section 3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Basic Algorithm for REG with WAC</head><p>Given a visual representation of an object, we can apply all word classifiers from the vocabulary of our WAC model and obtain an appropriateness ranking over words. As these WAC scores do not reflect appropriateness in the linguistic context, i.e. the previously generated words, we combine them with simple language model (bigram) prob- abilities (LM) computed on our corpus. The com- bination of WAC and LM scores is used to rank our vocabulary with respect to appropriateness given the visual features of the target referent and lin- guistic context.</p><p>Algorithm 1 shows our implementation of the decoding step, a beam search that iteratively adds n words with the highest combined LM and WAC score to a its agenda and terminates after a pre- specified number of maximum steps.</p><p>The algorithm takes the number of iterations as input, so it searches for the optimal RE given a fixed length. Deciding how many words have to be generated is very related to deciding how many attributes to include in more traditional REG. As a first approach, we have trained an additional re- gression classifier that predicts the length of the RE, given the number of objects in the scene and the visual properties of the target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Baseline REG with WAC</head><p>1: function WORD-GEN(object, maxsteps, V ) 2:</p><p>Agenda ← {'start } 3:</p><p>for n ∈ 0..maxsteps do 4:</p><p>Beam ← ∅ 5:</p><p>for re ∈ Agenda do 6:</p><formula xml:id="formula_0">w−1 ← LAST(re) 7: for w ∈ BIGRAMS(w−1, V ) do 8: s = WAC(w, object) + LM(w, w−1) 9:</formula><p>renew ← APPEND(re, word) 10:</p><p>Beam ← Beam ∪ {(renew, s)} 11:</p><p>end for 12:</p><p>end for 13:</p><formula xml:id="formula_1">Agenda ← K-BEST(Beam, k) 14:</formula><p>end for 15:</p><formula xml:id="formula_2">return K-BEST(Agenda, 1) 16: end function 3.2 Experimental Set-up</formula><p>Data We use the same test set as <ref type="bibr" target="#b16">Kazemzadeh et al. (2014)</ref> that is divided into the 3 subsets, each containing 500 objects: "Set A contains objects randomly sampled from the entire dataset, Set B was sampled from the most frequently occurring object categories in the dataset, excluding the less interesting categories, Set C contains objects sam- pled from images that contain at least 2 objects of the same category, excluding the less interesting categories." 1 For each object, there are 3 human- generated reference REs. We train the WAC model on the set of images that are not contained in the test set, which amounts to 100384 REs.</p><p>The classifiers We use <ref type="bibr" target="#b27">Schlangen et al. (2016)</ref>'s WAC model that is a trained on the REFERIT data ( <ref type="bibr" target="#b16">Kazemzadeh et al., 2014</ref>) based on the SAIAPR collection ( <ref type="bibr" target="#b14">Grubinger et al., 2006</ref>). We train bi- nary logistic regression classifiers (with 1 regu- larisation) for the 400 most frequent words from the training set. <ref type="bibr">2</ref> During training, we only con- sider non-relational expressions, as words from re- lational expressions would introduce further noise. Each classifier is trained with the same balance of positive and negative examples, a fixed ratio of 1 positive to 7 negative. Additionally, we train a re- gression classifier that predicts the expected length of the RE given the visual features of the target ob- ject and the number of objects in the entire scene. We also train a simple bigram language model on the data.</p><p>Feature sets In this experiment, we manipulate the features sets of the underlying word classifiers. We train it on (i) a small set of 27 low-level vi- sual features extracted and provided by <ref type="bibr" target="#b9">Escalante et al. (2010)</ref>, called SAIAPR features below, and (ii) a larger set of features automatically learned by a state-of-the-art convolutional neural network, "GoogLeNet" ( <ref type="bibr" target="#b32">Szegedy et al., 2015)</ref>. We derive representations of our visual inputs with this CNN, that was trained on data from the ImageNet cor- pus ( <ref type="bibr" target="#b5">Deng et al., 2009)</ref>, and extract the final fully- connected layer before the classification layer, to give us a 1024 dimensional representation of the region. We augment this with 7 features that en- code information about the region relative to the image: the (relative) coordinates of two corners, its (relative) area, distance to the center, and orien- tation of the image. The full representation hence is a vector of 1031 features. The feature extraction for (ii) is described in more detail in ( <ref type="bibr" target="#b27">Schlangen et al., 2016)</ref>. Generally, the SAIAPR features repre- sent interpretable visual information on position, area, and color of an image region, they could be associated with particular visual attributes. This is not possible with the GoogLeNet features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Automatic Evaluation</head><p>To the best of our knowledge, end-to-end REG performance has not been reported on the ReferIt data set before. <ref type="table">Table 1</ref> shows corpus-based BLEU and NIST measures calculated on the test set (using 3 references for each RE). The results indicate a minor gain of the GoogLeNet features. We also evaluate a version of the GoogLeNet- based system that instantiates the beam search with the gold length of the RE from the corpus (GoogLeNet glen ). This leads to a small improve- ment in BLEU and NIST, indicating that the length prediction is not a critical factor. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">A Game-based Human Evaluation</head><p>Set-up In parallel to the reference game in ( <ref type="bibr" target="#b16">Kazemzadeh et al., 2014</ref>), we set up a game be- tween a computer that generates REs and a human player who clicks on the location of the described object that he identifies based on the RE. After each click, the GUI presents some canned feed- back and informs the player whether he clicked on the intended object. In case of an unsuccess- ful click, the player has two more trials. In the following, we report the success rates with respect to each trial and the different test sets. This set- up will trigger a certain amount of user guesses such that the success rates do not correspond per- fectly to semantic accuracies. But it accounts for the increased difficulty as well as the interactive nature of the task. See Section 4.4 for an analysis of learning effects in this set-up and ( <ref type="bibr" target="#b12">Gatt et al., 2009;</ref><ref type="bibr" target="#b0">Belz and Hastie, 2014</ref>) for general discus- sion on REG and NLG evaluation.  For each player, we randomly sampled the games from the entire test set, but balanced the items so that they were equally distributed across the 3 test subsets A, B, C (see above) and the three systems. We also included human REs from the corpus. In total, we collected 1201 games played by 8 participants.</p><p>Results In <ref type="table" target="#tab_2">Table 2</ref>, we report the cumulative success rates for the different systems across the different trials, i.e. the success rate in the 3rd trial corresponds to the overall proportion of success- fully identified referents. First of all, this sug- gests that the differences in performance between the systems is much bigger in terms of their com- municative effectiveness as in terms of the corpus- based measures <ref type="table">(Table 1)</ref>. Thus, on the one hand, the GoogLeNet features are clearly superior to SA- IAPR, whereas differences between GoogLeNet and GoogLeNet glen are minor. Interestingly, the GoogLeNet features improve 1st trial as well as overall success, leading to a much better error re- duction rate 3 in object identification between the first and third trial. This means that, here, humans are more likely to recover from misunderstandings and indicates that REs generated by the SAIAPR system are more semantically inadequate.  In <ref type="table" target="#tab_4">Table 3</ref>, we report the overall success rates for the different test sets. All systems have a clearly higher performance on the B Set which contains the most frequent object types. Surpris- ingly, all systems have a largely comparable per- formance on Set A and C whereas only C con- tains images with distractors in the sense of tra- ditional REG. This shows that describing objects which belong to an infrequent type in a semanti- cally adequate way, which is necessary in Set A, is equally challenging as reaching pragmatic ade- quacy which is called for in Set C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Error Analysis</head><p>When users click on a distractor object instead of the intended referent in our object identifica- tion task, there can be several sources of misun- derstanding. For instance, it is possible that the system produced REs that are interpretable but not specific and distinguishing enough to identify the target. It is also possible that the system selected words that are semantically inadequate such that the expression becomes completely misleading. We can get some insight into possible sources of misunderstanding by comparing the the clicked-on distractor objects to their intended target, using the object labels annotated for each image region (see Section 2.2).</p><p>The analysis of mismatches between the ex- pected label of the target and the label of the object actually clicked on by the user reveals that many errors are due to semantic inadequacies and appar- ently severe interpretation failures: Looking at the total number of clicks on distractor objects, 80% are clicks on a distractor with a different label than the target. <ref type="bibr">4</ref> , e.g. the user clicked on a 'tree' in- stead of a 'man'. This is clear evidence for seman- tic inadequacies, suggesting that the systems often generate an inadequate noun for the object type. An example for such a label mismatch is shown in <ref type="figure" target="#fig_0">Figure 2</ref> where the system generated "person" for referring to a "sign", such that the user first clicked on distractor objects that are persons.</p><p>Similarly, we can get some evidence about how users try to repair misunderstandings, by compar- ing a distractor clicked on in the first trial to an- other distractor clicked on in the subsequent sec- ond, or third trial. Interestingly, we find that users do not seem to be aware of the fact that the system does not always generate the correct noun and do not generally try to click on objects with a differ- ent label. Only in 39% of the unsuccessful second trials, users decided for a distractor object with a different label, even though the first click had been unsuccessful. For instance, in <ref type="figure" target="#fig_0">Figure 2</ref>, the user clicked on the other person in the image in the second trial, although this referent is clearly not on the right. This suggests that users do not easily revise their RE interpretation with respect to the intended type of referent.</p><p>Moreover, we can compare the different dis- tractor clicks with respect to their spatial distance re generated : "person on the right" re human : "sign on the blue shelf in the back" to the target. We find that after an unsuccess- ful first trial, users click on an object that has a greater distance to the target in 70% of the cases (as e.g. in <ref type="figure" target="#fig_0">Figure 2</ref>). This means that users of- ten try to repair the misunderstanding with respect to the intended location, rather than with respect to the intended object type. Intuitively, this be- haviour makes sense: a human speaker is more likely to confuse e.g. left and right than e.g. man and tree. From the perspective of the system this is a problematic situation: words like left and right are much easier to generate (based on simple po- sitional features) than nouns like man and tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Towards interactive, contextual REG</head><p>In this Section, we extend our word-based REG to deal with semantic inadequacies. We take a first step towards interactive REG and implement installments, a pervasive strategy in human task- oriented interaction. The main idea is that the sys- tem should try to avoid semantically inadequate expressions wherever possible and, if misunder- standing occurs, try to react appropriately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Procedure</head><p>When a speaker or system refers in installments, they do not need to generate an RE in one shot, but can start with an initial, simple RE that is ex- tended or reformulated if this becomes necessary in the interaction, i.e. if the listener cannot iden- tify the referent. This setting is a straightforward extension of our game-based evaluation in Section 3.4, where users had 3 trials for identifying a refer- ent: instead of generating a single RE for the target and presenting it in every trial, we now produce a triple (re 1 , re 2 , re 3 ), where re 1 will be used in the first trial, re 2 in the second trial, etc.</p><p>In this set-up, we want to investigate whether installments and reformulations help to avoid se- mantic inadequacies and improve referential suc- cess, i.e. whether a dynamic approach to REG compares favourably to the non-dynamic version of our system (see Section 3). This question is, however, closely linked to another, more intricate question: what is the best strategy to realize in- stallments that, on the one hand, provide enough information so that a user can eventually identify the referent and, on the other hand, avoid mislead- ing words? To date, even highly interactive sys- tems do not generally treat installments, or if they do, only realise them via templates, e.g. ( <ref type="bibr" target="#b29">Stoia et al., 2006;</ref><ref type="bibr" target="#b28">Staudte et al., 2012;</ref><ref type="bibr" target="#b11">Garoufi and Koller, 2013;</ref><ref type="bibr" target="#b6">Dethlefs and Cuayáhuitl, 2015</ref>). As pointed out by <ref type="bibr" target="#b22">Liu et al. (2012)</ref>, data-driven approaches are not straightforward to set-up, due to the "mis- matched perceptual basis" between a human lis- tener and an REG system.</p><p>Based on the insights of our error analysis in Section 3.4, we will rely on a general installment strategy that is mostly targeted at avoiding seman- tically inadequate object names, and emphasizing the fact that location words generated by the sys- tem convey more reliable information. We have implemented two versions of this general strategy: (i) pattern-based installments that always avoid object names in their initial expression and dy- namically extend this if necessary, (ii) context- dependent installments that condition the initial expression on the complexity of the scene and ex- tend the initial expression accordingly, inspired by standard approaches to attribute selection in REG ( <ref type="bibr" target="#b21">Krahmer and Van Deemter, 2012</ref>). Thus, we do not test initial or reformulated expressions in iso- lation, but the strategy as a whole, which is similar to <ref type="figure" target="#fig_0">(Fang et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pattern-based Installments</head><p>This system generates a triple of REs for each im- age, corresponding to the respective trials in the object identification task. The triple for pattern- based installments is defined as follows:</p><p>• re1: a short RE that only contains location words, e.g. bottom left</p><p>• re2: a longer RE that contains location words and an object name, e.g. the car on the left</p><p>• re3: a reformulation of re2 that hedges the object name and suggests an alternative object name, e.g. vehicle or bottle on the left <ref type="figure" target="#fig_1">Figure 3</ref>(a) illustrates a case where this pattern is useful: the target is a horse, the biggest and most salient object in the image, which can be easily identified with a simple locative expression. As horses are not frequent in the training data, the sys- tem unfortunately generates hat guy as the most likely object name. This RE would be very mis- leading indeed if presented to a listener, as one of the distractors actually is a person with a hat.</p><p>Generation Procedure In order to generate the above installment triples with our REG system, we simply restrict the vocabulary of the underlying WAC-model. Thus, we divided the 400 word clas- sifiers into the following subsets:</p><p>• V1: 20 location words (manually defined) • V2: V1 + 183 object names (extracted from annotated section of the ReferIt corpus) • V3: entire vocabulary This basic installment-based system does not use V 3 (but see below). For generating the hedge of the object name in the third trial (re 3 ) we use the top-second and top-third word from the rank- ing that WAC produces over all object type words given the visual features of the target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Context-dependent Installments</head><p>Our context-dependent installment strategy deter- mines the initial RE (re 1 ) based on the surround- ing scene and generates subsequent reformulations (re 2 ,re 3 ) accordingly.</p><p>Initial REs and Distractors As we do not have a symbolic representation of the distractor objects and their properties, we use the word-based REG system to decide whether an RE can be expected to be distinguishing for the target in the scene. This is similar to <ref type="bibr" target="#b26">(Roy, 2002</ref>). Algorithm 2 shows the pro- cedure for determining the initial RE (re 1 ). Same as before, we restrict the vocabulary of the un- derlying WAC model, e.g. to contain only location words. But now, we apply the word generation function to the target object and to all the other ob- jects in the set of distractors (D). If the algorithm generates an identical chunk for the target and one of its distractors, it continues with a less restricted vocabulary and a longer expression. It terminates when it has found an RE that is optimal only for the target. This algorithm proceeds on the level of chunks, instead of single words, as e.g. location is often described by several words (e.g. bottom left).</p><formula xml:id="formula_3">Algorithm 2 A Context-aware REG Algorithm 1: function INC-GEN(object, maxsteps, D, V ) 2:</formula><p>for n ∈ 2..maxsteps do 3:</p><p>Vn ← RESTRICT(V, n) 4:</p><p>re ← WORD-GEN(object, Vn) 5:</p><formula xml:id="formula_4">for d ∈ D do 6: re d ← WORD-GEN(d, Vn) 7:</formula><p>if re d = re then 8:</p><formula xml:id="formula_5">break 9:</formula><p>end if 10:</p><p>end for 11:</p><p>return re 12:</p><p>end for 13: end function</p><p>As we found that the linguistic quality degrades for longer REs, we limit the maximal RE length to 6 words. We obtain 3 types of initial REs predicted to be distinguishing for a target by Algorithm 2:</p><p>• ref loc : 2 word RE, only location words (V1), <ref type="figure" target="#fig_1">Figure  3</ref>(a)</p><p>• ref object : 4 word RE, location words and object names (V2), <ref type="figure" target="#fig_1">Figure 3</ref>(b)</p><p>• ref att : 6 word RE, all attributes from the entire vocab- ulary (V3), <ref type="figure" target="#fig_1">Figure 3</ref>(c)</p><p>On our test set, this produces distinguishing REs for all targets, except 4 cases for which we use an initial 6 word RE as well.</p><p>Reformulations We have several options for generating the reformulation REs (re 2 ,re 3 ) -e.g. hedging the object name, extending the RE with more words, removing potentially misleading words, etc. -which are more or less appropriate, depending on the initial RE predicted by Algo- rithm 2. Therefore, we implemented the follow- ing types of installment triples that dynamically extend or reduce the initial RE: <ref type="figure" target="#fig_1">Figure 3</ref> shows examples for each triple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Human Evaluation</head><p>Set-up We use the task-oriented setup from Sec- tion 3.4 with 3 trials per image. But instead of pre- senting the same RE in each trial, the system now updates the phrases according to the RE triples described above. We have recruited 5 players and collected 1200 games, split equally between (a) Start with Location: re1: "in front" re2: "hat guy in front" re3: "hat or mountain in front"</p><p>(b) Start with Location, Object Type:</p><p>re1: "building on left side" re2: "house or bus on left side" re3: "yellow house or bus on top left side"</p><p>(c) Start with Location, Object Type,Other:</p><p>re1: "green plants on far right side" re2: "shrub or stand on right side" re3: "on right" Results <ref type="table" target="#tab_6">Table 4</ref> shows that even the simple, pattern-based installment system improves the 1st trial success rate compared to the non-interactive baseline (the GoogLeNet-based system from Sec- tion 3) and is clearly superior with respect to its overall success and error reduction rate over tri- als. This suggests that a fair amount of target ob- jects can be identified by users based on very sim- ple, locative REs as semantically inadequate ob- ject names are avoided. Another important find- ing here is the high rate of error reduction during the 2nd and 3rd trial achieved by the installment- based system. In the non-interactive system, users did not have additional cues for repairing their misunderstanding and probably guessed other pos- sible targets in individual, more or less system- atic ways. Apparently, even simple strategies for extending and hedging the initially presented RE provide very helpful cues for repairing initial mis- understandings.</p><p>As we expected, the pattern-based install- ment system is clearly improved by our context- dependent approach to generating installments. This systems seems to strike a much better balance between generating simple expressions that avoid  The finding that installment strategies should be combined with insights from traditional distractor- oriented REG is further corroborated when we compare the success rates on the different sub- sets of our test set, see <ref type="table" target="#tab_8">Table 5</ref>. Thus, the perfor- mance of the context-dependent installment sys- tem is much more stable on the different subsets than the pattern-based system which has a clear dip in success rate on Set C, which contains target referents with distractors of the same object type. This result suggests that our approach to determine distinguishing REs based purely on predictions of word-based REG (Section 4.3) presents a viable solution for REG on images, where information on distractors is not directly assessable in the low- level representation of the scene.  Finally, the graph in <ref type="figure">Figure 4</ref> shows the aver- age success rates over time and provides more evi- dence for the effectiveness of installments. We ob- serve a clear learning effect in the non-interactive system, meaning that users faced unexpected in- terpretation problems due to inaccurate expres- sions, but adapted to the situation to some extent. In contrast, both installment systems have stable performance over time, which indicates that sys- tem behaviour is immediately understandable and predictable for human users. 1.0 1.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">% Accuracy</head><p>Context install. Pattern install. No install. <ref type="figure">Figure 4</ref>: Participants' success rates in object identification over time</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>We have presented an REG system that ap- proaches the task as a word selection problem and circumvents manual specification of attributes in symbolic scene representations as required in tra- ditional REG <ref type="bibr" target="#b21">(Krahmer and Van Deemter, 2012</ref>), or manual specification of attribute-specific func- tions that map particular low-level visual fea- tures to attributes or words as in <ref type="bibr" target="#b26">(Roy, 2002;</ref><ref type="bibr" target="#b16">Kazemzadeh et al., 2014</ref>). This knowledge-lean approach allows us to use automatically learned ConvNet features and obtain a promising baseline that predicts semantically appropriate words based on visual object representations.</p><p>We have argued and demonstrated that REG in more realistic settings greatly benefits from a task- oriented, interactive account and should explore principled strategies for repairing and avoiding misunderstandings due to semantically inaccurate REs. In order to achieves this, we have augmented our approach with some manually designed in- stallment strategies. An obvious direction for fu- ture work is to automatically induce such a strat- egy, based on confidence measures that automati- cally predict the trust-worthiness of a word for an object.</p><p>Another extension that we have planned for fu- ture work is to implement relational expressions, similar to ( <ref type="bibr" target="#b18">Kennington and Schlangen, 2015)</ref>. Based on relational expressions, we will be able to generate reformulations and installments tailored to the interaction with the user. For instance, a very natural option for installments is to relate the wrong target object clicked on by the user to the intended target, e.g. something like to the left of that one, the bigger object.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example for an unsuccessful trial in object identification; first click: , second click: , third click: , target: )</figDesc><graphic url="image-3.png" coords="6,307.61,63.14,197.96,148.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples for context-dependent installments the pattern-based installment (Section 4.2) and the context-dependent installment strategy (Section 4.3). In this evaluation, we only use word classifiers trained on GoogLeNet features.</figDesc><graphic url="image-9.png" coords="8,307.49,244.80,101.96,76.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Human success and error reduction rates 
in object identification task, for different sets of 
visual features 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Human success rates for baseline REG systems trained on different visual feature sets</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Human evaluation for installment-based 
REG systems 

inadequate object names on the one and contextu-
ally appropriate expressions on the other hand. It 
improves the pattern-based installments in terms 
of 1st trial success rate and overall success and er-
ror reduction rate. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Human evaluation on different test sets 
for installment-based REG systems 

</table></figure>

			<note place="foot" n="3"> Word-based REG for Image Objects We describe a word selection model for REG on images, which reverses the decoding procedure of our reference resolution model (Kennington and Schlangen, 2015; Schlangen et al., 2016). The main question we pursue here is whether we can predict semantically adequate words for visually represented target objects in real-world images and achieve communicative success in a taskoriented evaluation.</note>

			<note place="foot" n="1"> Where objects mostly located in the background like &apos;sky&apos;, &apos;mountain&apos; are considered to be less interesting. 2 We used scikit learn (Pedregosa et al., 2011).</note>

			<note place="foot" n="3"> Calculated as (#error1st − #error 3rd )/#error1st</note>

			<note place="foot" n="4"> The percentage varies between saiapr (86%), GoogLeNet (71%)</note>

			<note place="foot" n="1">. (ref loc , ref object , ref object,hedge ), this corresponds to the pattern in Section 4.2 2. (ref object , ref object,hedge ,ref att ) 3. (ref att , ref att,hedge ,ref loc )</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We acknowledge support by the Cluster of Excellence "Cognitive Interaction Technology" (CITEC; EXC 277) at Bielefeld University, which is funded by the German Research Foundation (DFG).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Comparative evaluation and shared tasks for nlg in interactive systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Belz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Generation in Interactive Systems</title>
		<editor>Amanda Stent and Srinivas Bangalore</editor>
		<imprint>
			<publisher>Cambridge University Press. Cambridge Books Online</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="302" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mind&apos;s eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2422" to="2431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Referring as a collaborative process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deanna</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilkes-Gibbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Where&apos;s wally: the influence of visual salience on referring expression generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Alasdair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Frontiers in psychology, 4.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Computational interpretations of the gricean maxims in the generation of referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="263" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR09</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical reinforcement learning for situated natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Dethlefs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heriberto</forename><surname>Cuayáhuitl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="391" to="435" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An information-state approach to collaborative reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Devault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Kariaeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anubha</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Oved</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2005 on Interactive poster and demonstration sessions</title>
		<meeting>the ACL 2005 on Interactive poster and demonstration sessions</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language models for image captioning: The quirks and what works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="100" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The segmented and annotated IAPR TC-12 benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo Jair</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><forename type="middle">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>López-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">F</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Enrique</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Villaseñor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grubinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="419" to="428" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Collaborative Models for Referring Expression Generation in Situated Dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Doering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generation of effective referring expressions in situated context. Language and Cognitive Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantina</forename><surname>Garoufi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The tuna-reg challenge 2009: Overview and evaluation results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Belz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th European Workshop on Natural Language Generation</title>
		<meeting>the 12th European Workshop on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="174" to="182" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From the virtual to the real world: Referring to objects in real-world spatial scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitra</forename><surname>Gkatzia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Bartie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Mackaness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2015</title>
		<meeting>EMNLP 2015</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The IAPR TC-12 benchmark: a new evaluation resource for visual information systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation (LREC 2006)</title>
		<meeting>the International Conference on Language Resources and Evaluation (LREC 2006)<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collaborating on referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Heeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="351" to="382" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ReferItGame: Referring to Objects in Photographs of Natural Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Incremental generation of spatial referring expressions in situated dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert-Jan M</forename><surname>Kelleher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kruijff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1041" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simple learning and compositional application of perceptually grounded word meanings for incremental reference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Kennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schlangen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="292" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The d-tuna corpus: A dutch dataset for the evaluation of referring expression generation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruud</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Factors causing overspecification in definite descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruud</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martijn</forename><surname>Goudbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pragmatics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3231" to="3250" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Computational generation of referring expressions: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kees</forename><surname>Van Deemter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="173" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards mediating shared perceptual basis in situated dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno>abs/1511.02283</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Natural reference to objects in a visual domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Kees Van Deemter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th international natural language generation conference</title>
		<meeting>the 6th international natural language generation conference</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning visually grounded words and syntax for a scene description task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deb K Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="353" to="385" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Resolving references to objects in photographs using the words-as-classifiers model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schlangen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Zarriess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Kennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54rd Annual Meeting of the Association for Computational Linguistics (ACL</title>
		<meeting>the 54rd Annual Meeting of the Association for Computational Linguistics (ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using listener gaze to augment speech generation in a virtual 3d environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Staudte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantina</forename><surname>Garoufi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew W</forename><surname>Crocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual Conference of the Cognitive Science Society</title>
		<meeting>the 34th Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Noun phrase generation for situated dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Stoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darla</forename><forename type="middle">Magdalene</forename><surname>Shockley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><forename type="middle">K</forename><surname>Byron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Fosler-Lussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth international natural language generation conference</title>
		<meeting>the fourth international natural language generation conference</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Report on the second second challenge on generating instructions in virtual environments (give-2.5)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Striegnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gargett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantina</forename><surname>Garoufi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariët</forename><surname>Theune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Workshop on Natural Language Generation</title>
		<meeting>the 13th European Workshop on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Referring in installments: a corpus study of spoken object references in an interactive virtual environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Striegnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Buschmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Natural Language Generation Conference</title>
		<meeting>the Seventh International Natural Language Generation Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2015</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">hand me the yellow stapler or hand me the yellow dress: Colour overspecification depends on object category</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sammie</forename><surname>Tarenskeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirjam</forename><surname>Broersma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Geurts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>page 140</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Building a semantically transparent corpus for the generation of referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ielka</forename><surname>Kees Van Deemter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Van Der Sluis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Natural Language Generation Conference</title>
		<meeting>the Fourth International Natural Language Generation Conference</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="130" to="132" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The use of spatial relations in referring expression generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jette</forename><surname>Viethen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Natural Language Generation Conference</title>
		<meeting>the Fifth International Natural Language Generation Conference</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="59" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
