<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning in Semantic Kernel Spaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Filice</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Castellucci</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
						</author>
						<title level="a" type="main">Deep Learning in Semantic Kernel Spaces</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="345" to="354"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1032</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Kernel methods enable the direct usage of structured representations of textual data during language learning and inference tasks. Expressive kernels, such as Tree Kernels, achieve excellent performance in NLP. On the other side, deep neural networks have been demonstrated effective in automatically learning feature representations during training. However, their input is tensor data, i.e., they cannot manage rich structured information. In this paper , we show that expressive kernels and deep neural networks can be combined in a common framework in order to (i) explicitly model structured information and (ii) learn non-linear decision functions. We show that the input layer of a deep architecture can be pre-trained through the application of the Nyström low-rank approximation of kernel spaces. The resulting &quot;kernelized&quot; neural network achieves state-of-the-art accuracy in three different tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning for Natural Language Processing (NLP) requires to more or less explicitly account for trees or graphs to express syntactic and semantic in- formation. A straightforward modeling of such information has been obtained in statistical lan- guage learning with Tree Kernels (TKs) <ref type="bibr" target="#b8">(Collins and Duffy, 2001</ref>), or by means of structured neu- ral models <ref type="bibr" target="#b17">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b34">Socher et al., 2013</ref>). In particular, kernel-based methods <ref type="bibr" target="#b33">(Shawe-Taylor and Cristianini, 2004)</ref> have been largely applied in language processing for alleviating the need of complex activities of manual feature engineering (e.g., <ref type="bibr" target="#b28">(Moschitti et al., 2008)</ref>). Although ad-hoc features are adopted by many successful approaches to language learning (e.g., <ref type="bibr" target="#b15">(Gildea and Jurafsky, 2002)</ref>), kernels pro- vide a natural way to capture textual generaliza- tions directly operating over (possibly complex) linguistic structures. Sequence ( <ref type="bibr" target="#b4">Cancedda et al., 2003)</ref> or tree kernels <ref type="bibr" target="#b8">(Collins and Duffy, 2001</ref>) are of particular interest as the feature space they im- plicitly generate reflects linguistic patterns. On the other hand, Recursive Neural Networks ( <ref type="bibr" target="#b34">Socher et al., 2013</ref>) have been shown to learn dense feature representations of the nodes in a struc- ture, thus exploiting similarities between nodes and sub-trees. Also, Long-Short Term Mem- ory <ref type="bibr" target="#b17">(Hochreiter and Schmidhuber, 1997</ref>) networks build intermediate representations of sequences, resulting in similarity estimates over sequences and their inner sub-sequences.</p><p>While such methods are highly effective and reach state-of-the-art results in many tasks, their adoption can be problematic. In kernel-based Support Vector Machine (SVM) the classification model corresponds to the set of support vectors (SVs) and weights justifying the maximal margin hyperplane: the classification cost crucially de- pends on their number, as classifying a new in- stance requires a kernel computation against all SVs, making their adoption in large data settings prohibitive. This scalability issue is evident in many NLP and Information Retrieval applications, such as in answer re-ranking in question answer- ing ( <ref type="bibr" target="#b32">Severyn et al., 2013;</ref><ref type="bibr" target="#b12">Filice et al., 2016)</ref>, where the number of SVs is typically very large. Improving the efficiency of kernel-based methods is a largely studied topic. The reduction of com- putational costs has been early designed by impos- ing a budget <ref type="bibr" target="#b11">(Dekel and Singer, 2006</ref>; <ref type="bibr" target="#b37">Wang and Vucetic, 2010)</ref>, that is limiting the maximum num- ber of SVs in a model. However, in complex tasks, such methods still require large budgets to reach adequate accuracies. On the other hand, train- ing complex neural networks is also difficult as no common design practice is established against complex data structures. In <ref type="bibr" target="#b23">Levy et al. (2015)</ref>, a careful analysis of neural word embedding models is carried out and the role of the hyper-parameter estimation is outlined. Different neural architec- tures result in the same performances, whenever optimal hyper-parameter tuning is applied. In this latter case, no significant difference is observed across different architectures, making the choice between different neural architectures a complex and empirical task.</p><p>A general approach to the large scale modeling of complex structures is a critical and open prob- lem. A viable and general solution to this scal- ability issue is provided by the Nyström method ( <ref type="bibr" target="#b38">Williams and Seeger, 2001)</ref>; it allows to ap- proximate the Gram matrix of a kernel function and support the embedding of future input exam- ples into a low-dimensional space. For example, if used over TKs, the Nyström projection corre- sponds to the embedding of any tree into a low- dimensional vector.</p><p>In this paper, we show that the Nyström based low-rank embedding of input examples can be used as the early layer of a deep feed-forward neural network. A standard NN back-propagation training can thus be applied to induce non-linear functions in the kernel space. The resulting deep architecture, called Kernel-based Deep Architec- ture (KDA), is a mathematically justified integra- tion of expressive kernel functions and deep neu- ral architectures, with several advantages: it (i) di- rectly operates over complex non-tensor struc- tures, e.g., trees, without any manual feature or architectural engineering, (ii) achieves a drastic re- duction of the computational cost w.r.t. pure ker- nel methods, and (iii) exploits the non-linearity of NNs to produce accurate models. The experimen- tal evaluation shows that the proposed approach achieves state-of-the-art results in three semantic inference tasks: Semantic Parsing, Question Clas- sification and Community Question Answering.</p><p>In the rest of the paper, Section 2 surveys some of the investigated kernels. In Section 3 the Nyström methodology and KDA are presented. Experimental evaluations are described in Section 4. Finally, Section 5 derives the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Kernel-based Semantic Inference</head><p>In almost all NLP tasks, explicit models of com- plex syntactic and semantic structures are re- quired, such as in Paraphrase Detection: deciding whether two sentences are valid paraphrases in- volves learning grammatical rewriting rules, such as semantics preserving mappings among sub- trees. Also in Question Answering, the syntac- tic information about input questions is crucial. While manual feature engineering is always possi- ble, kernel methods on structured representations of data objects, e.g., sentences, have been largely applied. Since <ref type="bibr" target="#b8">Collins and Duffy (2001)</ref>, sen- tences can be modeled through their correspond- ing parse tree, and Tree Kernels (TKs) result in similarity metrics directly operating over tree frag- ments. Such kernels corresponds to dot products in the (implicit) feature space made of all possi- ble tree fragments <ref type="bibr" target="#b16">(Haussler, 1999)</ref>. Notice that the number of tree fragments in a tree bank is combinatorial with the number of tree nodes and gives rise to billions of features, i.e., dimensions. In this high-dimensional space, kernel-based algo- rithms, such as SVMs, can implicitly learn robust prediction models <ref type="bibr" target="#b33">(Shawe-Taylor and Cristianini, 2004</ref>), resulting in state-of-the-art approaches in several NLP tasks, e.g., Semantic Role Labeling ( <ref type="bibr" target="#b28">Moschitti et al., 2008)</ref>, Question Classification (Croce et al., 2011) or Paraphrase Identification ( <ref type="bibr" target="#b13">Filice et al., 2015)</ref>. As the feature space gener- ated by the structural kernels depends on the in- put structures, different tree representations can be adopted to reflect more or less expressive syntac- tic/semantic feature spaces. While constituency parse trees have been early used (e.g., <ref type="bibr" target="#b8">(Collins and Duffy, 2001)</ref>), dependency parse trees correspond to graph structures. TKs usually rely on their tree conversions, where grammatical edge labels corre- sponds to nodes. An expressive tree representation of dependency graphs is the Grammatical Relation Centered Tree (GRCT). As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, PoS-Tags and grammatical functions correspond to nodes, dominating their associated lexicals. Types of tree kernels. While a variety of TK functions have been studied, e.g., the Partial Tree Kernel (PTK) <ref type="bibr" target="#b27">(Moschitti, 2006</ref>), the kernels used in this work model grammatical and semantic in- formation, as triggered respectively by the depen- dency edge labels and lexical nodes. The lat- ter is exploited through recent results in distribu- tional models of lexical semantics, as proposed in word embedding methods (e.g., <ref type="bibr" target="#b26">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b31">Sahlgren, 2006</ref>). In particular, we adopt the Smoothed Partial Tree Kernel (SPTK) described in <ref type="bibr" target="#b10">Croce et al. (2011)</ref>: it extends the PTK formu- lation with a similarity function between lexical nodes in a GRCT, i.e., the cosine similarity be- tween word vector representations based on word embeddings. We also use a further extension of the SPTK, called Compositionally Smoothed Partial Tree Kernel (CSPTK) (as in <ref type="bibr" target="#b0">Annesi et al. (2014)</ref>). In CSPTK, the lexical information provided by the sentence words is propagated along the non- terminal nodes representing head-modifier depen- dencies. <ref type="figure" target="#fig_1">Figure 2</ref> shows a compositionally-labeled tree, where the similarity function at the nodes can model lexical composition, i.e., capturing contex- tual information. For example, in the sentence, "What instrument does Hendrix play?", the role of the word instrument can be fully captured only if its composition with the verb play is consid- ered. The CSPTK applies a composition func- tion between nodes: while several algebraic func- tions can be adopted to compose two word vectors representing a head/modifier pair, here we refer to a simple additive function that assigns to each (h, m) pair the linear combination of the involved vectors, i.e., (h, m) = Ah + Bm: although sim- ple and efficient, it actually produces very effec- tive CSPTK functions.</p><p>Complexity. The training phase of an optimal maximum margin algorithm (such as SVM) re- quires a number of kernel operations that is more than linear (almost O(n 2 )) with respect to the number of training examples n, as discussed in <ref type="bibr" target="#b5">Chang and Lin (2011)</ref>. Also the classification phase depends on the size of the input dataset and the intrinsic complexity of the targeted task: clas- sifying a new instance requires to evaluate the ker- nel function with respect to each support vector. For complex tasks, the number of selected sup- port vectors tends to be very large, and using the resulting model can be impractical. This cost is also problematic as single kernel operations can be very expensive: the cost of evaluating the PTK on a single tree pair is almost linear in the number of nodes in the input trees, as shown in <ref type="bibr" target="#b27">Moschitti (2006)</ref>. When lexical semantics is considered, as in SPTKs and CSPTKs, it is more than linear in the number of nodes (Croce et al., 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Learning in Kernel Spaces</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Nyström method</head><p>Given an input training dataset D, a kernel</p><formula xml:id="formula_0">K(o i , o j ) is a similarity function over D 2 that corresponds to a dot product in the implicit ker- nel space, i.e., K(o i , o j ) = Φ(o i ) · Φ(o j ).</formula><p>The advantage of kernels is that the projection func- tion Φ(o) = x ∈ R n is never explicitly com- puted <ref type="bibr" target="#b33">(Shawe-Taylor and Cristianini, 2004</ref>). In fact, this operation may be prohibitive when the dimensionality n of the underlying kernel space is extremely large, as for Tree Kernels <ref type="bibr" target="#b8">(Collins and Duffy, 2001</ref>). Kernel functions are used by learn- ing algorithms, such as SVM, to operate only im- plicitly on instances in the kernel space, by never accessing their explicit definition. Let us apply the projection function Φ over all examples from D to derive representations, x denoting the rows of the matrix X. The Gram matrix can always be computed as G = XX , with each single element corresponding to</p><formula xml:id="formula_1">G ij = Φ(o i )Φ(o j ) = K(o i , o j ).</formula><p>The aim of the Nyström method is to derive a new low-dimensional embedding˜xembedding˜ embedding˜x in a l-dimensional space, with l n so that˜Gthat˜ that˜G = ˜ X ˜ X and˜Gand˜ and˜G ≈ G. This is obtained by generating an approximatioñapproximatioñ G of G using a subset of l columns of the matrix, i.e., a selection of a subset L ⊂ D of the avail- able examples, called landmarks. Suppose we ran- domly sample l columns of G, and let C ∈ R |D|×l be the matrix of these sampled columns. Then, we can rearrange the columns and rows of G and de- fine X = [X 1 X 2 ] such that:</p><formula xml:id="formula_2">G = XX = W X 1 X 2 X 2 X 1 X 2 X 2 and C = W X 2 X 1<label>(1)</label></formula><p>where W = X 1 X 1 , i.e., the subset of G that con- tains only landmarks. The Nyström approxima- tion can be defined as:  </p><formula xml:id="formula_3">G ≈ ˜ G = CW † C<label>(2)</label></formula><formula xml:id="formula_4">W † = U S −1 U = U S − 1 2 S − 1 2</formula><p>U and the Equa- tion 2 can be rewritten as</p><formula xml:id="formula_5">G ≈ ˜ G = CU S − 1 2 S − 1 2 U C = (CU S − 1 2 )(CU S − 1 2 ) = ˜ X ˜ X (3) Given an input example o ∈ D, a new low- dimensional representatioñ</formula><p>x can be thus deter- mined by considering the corresponding item of</p><formula xml:id="formula_6">C as˜x as˜ as˜x = cU S − 1 2 (4)</formula><p>where c is the vector whose dimensions contain the evaluations of the kernel function between o and each landmark o j ∈ L. Therefore, the method produces l-dimensional vectors. If k is the average number of basic operations required during a sin- gle kernel computation, the overall cost of a sin- gle projection is O(kl + l 2 ), where the first term corresponds to the cost of generating the vector c, while the second term is needed for the ma- trix multiplications in Equation 4. Typically, the number of landmarks l ranges from hundreds to few thousands and, for complex kernels (such as Tree Kernels), the projection cost can be reduced to O(kl). Several policies have been defined to determine the best selection of landmarks to re- duce the Gram Matrix approximation error. In this work the uniform sampling without replacement is adopted, as suggested by <ref type="bibr" target="#b21">Kumar et al. (2012)</ref>, where this policy has been theoretically and em- pirically shown to achieve results comparable with other (more complex) selection policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Kernel-based Deep Architecture</head><p>The above introduced Nyström representatioñ x of any input example o is linear and can be adopted to feed a neural network architecture. We assume a labeled dataset L = {(o, y) | o ∈ D, y ∈ Y } being available, where o refers to a generic in- stance and y is its associated class. In this Sec- tion, we define a Multi-Layer Perceptron (MLP) architecture, with a specific Nyström layer based on the Nyström embeddings of Eq. 4. We will refer to this architecture as Kernel-based Deep Architecture (KDA). KDA has an input layer, a Nyström layer, a possibly empty sequence of non-linear hidden layers and a final classification layer, which produces the output.</p><p>The input layer corresponds to the input vector c, i.e., the row of the C matrix associated to an example o. Notice that, for adopting the KDA, the values of the matrix C should be all avail- able. In the training stage, these values are in gen- eral cached. During the classification stage, the c vector corresponding to an example o is directly computed by l kernel computations between o and each one of the l landmarks.</p><p>The input layer is mapped to the Nyström layer, through the projection in Equation 4. No- tice that the embedding provides also the proper weights, defined by U S − 1 2 , so that the mapping can be expressed through the Nyström matrix H N y = U S − 1 2 : it corresponds to a pre-trained stage derived through SVD, as discussed in Sec- tion 3.1. Equation 4 provides a static definition for H N y whose weights can be left invariant dur- ing the neural network training. However, the val- ues of H N y can be made available for the standard back-propagation adjustments applied for train- ing <ref type="bibr">1</ref> . Formally, the low-dimensional embedding of an input example o, is˜xis˜ is˜x = c H N y = c U S − 1 2 . The resulting outcome˜xoutcome˜ outcome˜x is the input to one or more non-linear hidden layers. Each t-th hidden layer is realized through a matrix H t ∈ R h t−1 ×ht and a bias vector b t ∈ R 1×ht , whereas h t denotes the desired hidden layer dimensionality. Clearly, given that H N y ∈ R l×l , h 0 = l. The first hid- den layer in fact receives in input˜xinput˜ input˜x = cH N y , that corresponds to t = 0 layer input x 0 = ˜ x and its computation is formally expressed by</p><formula xml:id="formula_7">x 1 = f (x 0 H 1 + b 1 ),</formula><p>where f is a non-linear acti- vation function. In general, the generic t-th layer is modeled as:</p><formula xml:id="formula_8">x t = f (x t−1 H t + b t )<label>(5)</label></formula><p>The</p><note type="other">final layer of KDA is the classification layer, realized through the output matrix H O and the output bias vector b O . Their dimensionality depends on the dimensionality of the last hidden layer (called O −1 ) and the number |Y | of different classes, i.e., H O ∈ R h O −1 ×|Y | and b O ∈ R 1×|Y | , respectively. In particular, this layer computes a linear classification function with a softmax oper- ator so thatˆythatˆ thatˆy = sof tmax(x O</note><formula xml:id="formula_9">−1 H O + b O ).</formula><p>In order to avoid over-fitting, two different reg- ularization schemes are applied. First, the dropout is applied to the input x t of each hidden layer (t ≥ 1) and to the input x O −1 of the final clas- sifier. Second, a L 2 regularization is applied to the norm of each layer 2 H t and H O .</p><p>Finally, the KDA is trained by optimizing a loss function made of the sum of two factors: first, the cross-entropy function between the gold classes and the predicted ones; second the L 2 regulariza- tion, whose importance is regulated by a meta- parameter λ. The final loss function is thus</p><formula xml:id="formula_10">L(y, ˆ y) = (o,y)∈L</formula><p>y log(ˆ y)+λ</p><formula xml:id="formula_11">H∈{Ht}∪{H O } ||H|| 2</formula><p>wherê y are the softmax values computed by the network and y are the true one-hot encoding val- ues associated with the example from the labeled training dataset L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Investigation</head><p>The proposed KDA has been applied adopting the same architecture but with different kernels to three NLP tasks, i.e., Question Classification, Community Question Answering, and Automatic Boundary Detection in Semantic Role Labeling. The Nyström projector has been implemented in the KeLP framework <ref type="bibr">3</ref> . The neural network has been implemented in Tensorflow <ref type="bibr">4</ref> , with 2 hidden layers whose dimensionality corresponds to the number of involved Nyström landmarks. The rec- tified linear unit is the non-linear activation func- tion in each layer. The dropout has been applied in each hidden layer and in the final classification layer. The values of the dropout parameter and the λ parameter of the L 2 -regularization have been se- lected from a set of values via grid-search. The Adam optimizer with a learning rate of 0.001 has been applied to minimize the loss function, with a multi-epoch (500) training, each fed with batches of size 256. We adopted an early stop strategy, where the best model was selected according to the performance over the development set. Every performance measure is obtained against a specific sampling of the Nyström landmarks. Results aver- aged against 5 such samplings are always hereafter reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Question Classification</head><p>Question Classification (QC) is the task of map- ping a question into a closed set of answer types in a Question Answering system. We used the UIUC dataset ( <ref type="bibr" target="#b24">Li and Roth, 2006</ref>), including a training and test set of 5, 452 and 500 questions, respectively, organized in 6 classes (like ENTITY or HUMAN). TKs resulted very effective, as shown in Croce et al. (2011); <ref type="bibr" target="#b0">Annesi et al. (2014)</ref>. In <ref type="bibr" target="#b0">Annesi et al. (2014)</ref>, QC is mapped into a One-vs- All multi-classification schema, where the CSPTK achieves state-of-the-art results of 95%: it acts di- rectly over compositionally labeled trees without relying on any manually designed feature.</p><p>In order to proof the benefits of the KDA ar- chitecture, we generated Nyström representation of the CSPTK kernel function 5 with default pa- rameters (i.e., µ = λ = 0.4). The SVM for- mulation by <ref type="bibr" target="#b5">Chang and Lin (2011)</ref>, fed with the CSPTK (hereafter KSVM), is here adopted to de- termine the reachable upper bound in classifica- tion quality, i.e., a 95% of accuracy, at higher com- putational costs. It establishes the state-of-the-art over the UIUC dataset. The resulting model in- cludes 3,873 support vectors: this corresponds to the number of kernel operations required to clas- sify any input test question. The Nyström method based on a number of landmarks ranging from 100 to 1,000 is adopted for modeling input vectors in the CSPTK kernel space. Results are reported in <ref type="table" target="#tab_1">Table 1</ref>: computational saving refers to the per- centage of avoided kernel computations with re- spect to the application of the KSVM to each test instance. To justify the need of the Neural Net- work, we compared the proposed KDA to an effi- cient linear SVM that is directly trained over the Nyström embeddings. This SVM implements the Dual Coordinate Descent method ( <ref type="bibr" target="#b18">Hsieh et al., 2008</ref>) and will be referred as SVM DCD . We also measured the state-of-the-art Convolutional Neu- ral Network 6 (CNN) of <ref type="bibr" target="#b20">Kim (2014)</ref>, achieving the remarkable accuracy of 93.6%. Notice that the linear classifier SVM DCD operating over the ap- proximated kernel space achieves the same classi- fication quality of the CNN when just 1,000 land- marks are considered. KDA improves this results, achieving 94.3% accuracy even with fewer land- marks (only 600), showing the effectiveness of non-linear learning over the Nyström input. Al- though KSVM improves to 95%, KDA provides a saving of more than 84% kernel computations at classification time. This result is straightfor- ward as it confirms that linguistic information encoded in a tree is important in the analysis of questions and can be used as a pre-training strat- egy. <ref type="figure" target="#fig_2">Figure 3</ref> shows the accuracy curves accord- ing to various approximations of the kernel space, i.e., number of landmarks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Community Question-Answering</head><p>In the SemEval-2016 task 3, participants were asked to automatically provide good answers in a community question answering setting ( <ref type="bibr" target="#b29">Nakov et al., 2016)</ref>. We focused on the subtask A: given a question and a large collection of question- comment threads created by a user community, <ref type="bibr">6</ref> The deep architecture presented in <ref type="bibr" target="#b20">Kim (2014)</ref> outper- forms several NN models, including the Recursive Neural Tensor Network or Tree-LSTM presented in <ref type="bibr" target="#b34">(Socher et al., 2013;</ref><ref type="bibr" target="#b35">Tai et al., 2015</ref>) which presents a semantic composi- tionality model that exploits parse trees.  <ref type="formula" target="#formula_2">2016)</ref>, a Kernel-based SVM clas- sifier (KSVM) achieved state-of-the-art results by adopting a kernel combination that exploited (i) feature vectors containing linguistic similarities between the texts in a pair; (ii) shallow syntactic trees that encode the lexical and morpho-syntactic information shared between text pairs; (iii) feature vectors capturing task-specific information. Such model includes 11,322 support vectors. We investigated the KDA architecture, trained by maximizing the F 1 measure, based on a Nyström layer initialized using the same kernel functions as KSVM. We varied the Nyström dimensions from 100 to 1,000 landmarks, i.e., a much lower number than the support vectors of KSVM. <ref type="table" target="#tab_2">Table 2</ref> reports the results: very high F 1 scores are observed with impressive savings in terms of kernel computations (between 91.2% and 99%). Also on the cQA task, the F 1 obtained by the SVM DCD is significantly lower than the KDA one. Moreover, with 800 landmarks KDA achieves the remarkable results of 0.68 of F 1 , that is the state-of-the-art against other convolutional sys- tems, e.g., <ref type="bibr">ConvKN (Barrón-Cedeño et al., 2016)</ref>: this latter combines convolutional tree kernels with kernels operating on sentence embeddings generated by a convolutional neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Argument Boundary Detection</head><p>Semantic Role Labeling (SRL) consists of the detection of the semantic arguments associated with the predicate of a sentence (called Lexi- cal Unit) and their classification into their spe- cific roles <ref type="bibr" target="#b14">(Fillmore, 1985)</ref>. For example, given the sentence "Bootleggers then copy the film onto hundreds of tapes" the task would be to recog- nize the verb copy as representing the DUPLICA- TION frame with roles, CREATOR for Bootleggers, ORIGINAL for the film and GOAL for hundreds of tapes. Argument Boundary Detection (ABD) corre- sponds to the SRL subtask of detecting the sen- tence fragments spanning individual roles. In the previous example the phrase "the film" represents a role (i.e., ORIGINAL), while "of tapes" or "film onto hundreds" do not, as they just partially cover one or multiple roles, respectively. The ABD task has been successfully tackled using TKs since <ref type="bibr" target="#b28">Moschitti et al. (2008)</ref>. It can be modeled as a bi- nary classification task over each parse tree node n, where the argument span reflects words covered by the sub-tree rooted at n. In our experiments, Grammatical Relation Centered Tree (GRCT) de- rived from dependency grammar <ref type="figure">(Fig. 4)</ref> are em- ployed, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. Each node is consid- ered as a candidate in covering a possible argu- ment. In particular, the structure in <ref type="figure" target="#fig_3">Fig. 5a</ref> is a positive example. On the contrary, in <ref type="figure" target="#fig_3">Fig. 5b</ref> the NMOD node only covers the phrase "of tapes", i.e., a subset of the correct role, and it represents a negative example <ref type="bibr">7</ref> .</p><p>We selected all the sentences whose predi- cate word (lexical unit) is a verb (they are about 60,000), from the 1.3 version of the Framenet dataset ( <ref type="bibr" target="#b1">Baker et al., 1998)</ref>. This gives rise to about 1,400,000 sub-trees, i.e., the positive and negative instances. The dataset is split in train and test according to the 90/10 proportion (as in <ref type="bibr" target="#b19">(Johansson and Nugues, 2008)</ref>). This size makes the application of a traditional kernel-based method unfeasible, unless a significant instance sub-sampling is performed. We firstly experimented standard SVM learning over a sampled training set of 10,000 examples, a typical size for annotated datasets in computa- tional linguistics tasks. We adopted the Smoothed Partial Tree <ref type="bibr">Kernel (Croce et al., 2011</ref>) with stan- dard parameters (i.e., µ = λ = 0.4) and lexical nodes expressed through 250-dimensional vectors obtained by applying <ref type="bibr">Word2Vec (Mikolov et al., 2013</ref>) to the entire Wikipedia. When trained over this 10k instances dataset, the kernel-based SVM (KSVM) achieves an F 1 of 70.2%, over the same test set used in  that in- cludes 146,399 examples. The KSVM learning produces a model including 2, 994 support vec- tors, i.e., the number of kernel operations required to classify each new test instance. We then ap- ply the Nyström linearization to a larger dataset made of 100k examples, and trained a classifier using both the Dual Coordinate Descent method ( <ref type="bibr" target="#b18">Hsieh et al., 2008)</ref>, SVM DCD , and the KDA pro- posed in this work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusions</head><p>In this work, we promoted a methodology to em- bed structured linguistic information within NNs, according to mathematically rich semantic simi- larity models, based on kernel functions. Struc- tured data, such as trees, are transformed into dense vectors according to the Nyström method- ology, and the NN is effective in capturing non- linearities in these representations, but still im- proving generalization at a reasonable complexity. At the best our knowledge, this work is one of the few attempts to systematically integrate lin- guistic kernels within a deep neural network archi- tecture. The problem of combining such method- ologies has been studied in specific works, such as ( <ref type="bibr" target="#b2">Baldi et al., 2011;</ref><ref type="bibr" target="#b7">Cho and Saul, 2009;</ref><ref type="bibr" target="#b39">Yu et al., 2009)</ref>. In Baldi et al. (2011) the authors pro- pose a hybrid classifier, for bridging kernel meth- ods and neural networks. In particular, they use the output of a kernelized k-nearest neighbors al- gorithm as input to a neural network. <ref type="bibr" target="#b7">Cho and Saul (2009)</ref> introduced a family of kernel functions that mimic the computation of large multilayer neu- ral networks. However, such kernels can be ap- plied only on vector inputs. In <ref type="bibr" target="#b39">Yu et al. (2009)</ref>, deep neural networks for rapid visual recognition are trained with a novel regularization method tak- ing advantage of kernels as an oracle represent- ing prior knowledge. The authors transform the kernel regularizer into a loss function and carry out the neural network training by gradient de- scent. In Zhuang et al. (2011) a different ap- proach has been promoted: a multiple (two) layer architecture of kernel functions, inspired by neural networks, is studied to find the best kernel com- bination in a Multiple Kernel Learning setting. In <ref type="bibr" target="#b25">Mairal et al. (2014)</ref> the invariance properties of convolutional neural networks ( <ref type="bibr" target="#b22">LeCun et al., 1998</ref>) are modeled through kernel functions, re- sulting in a Convolutional Kernel Network. Other effort for combining NNs and kernel methods is described in , where a SVM adopts a tree kernels combinations with em- beddings learned through a CNN.</p><p>The approach here discussed departs from pre- vious approaches in different aspects. First, a gen- eral framework is promoted: it is largely applica- ble to any complex kernel, e.g., structural kernels or combinations of them. The efficiency of the Nyström methodology encourages its adoption, especially when complex kernel computations are required. Notice that other low-dimensional ap- proximations of kernel functions have been stud- ied, as for example the randomized feature map- pings proposed in <ref type="bibr" target="#b30">Rahimi and Recht (2008)</ref>. How- ever, these assume that (i) instances have vectorial form and (ii) shift-invariant kernels are adopted. The Nyström method adopted here does not suffer of such limitations: as our target is the application to structured (linguistic) data, more general ker- nels, i.e., non-shift-invariant convolution kernels are needed.</p><p>Given the Nyström approximation, the learning setting corresponds to a general well-known neu- ral network architecture, i.e., a multilayer percep- tron, and does not require any manual feature en- gineering or the design of ad-hoc network archi- tectures. The success in three different tasks con- firms its large applicability without major changes or adaptations. Second, we propose a novel learn- ing strategy, as the capability of kernel methods to represent complex search spaces is combined with the ability of neural networks to find non-linear so-lutions to complex tasks. Last, the suggested KDA framework is fully scalable, as (i) the network can be parallelized on multiple machines, and (ii) the computation of the Nyström reconstruction vector c can be easily parallelized on multiple processing units, ideally l, as each unit can compute one c i value. Future work will address experimentations with larger scale datasets; moreover, it is interest- ing to experiment with more landmarks in order to better understand the trade-off between the repre- sentation capacity of the Nyström approximation of the kernel functions and the over-fitting that can be introduced in a neural network architecture. Fi- nally, the optimization of the KDA methodology through the suitable parallelization on multicore architectures, as well as the exploration of mech- anisms for the dynamic reconstruction of kernel spaces (e.g., operating over H N y ) also constitute interesting future research directions on this topic.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Grammatical Relation Centered Tree (GRCT) of "What is the width of a football field?"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Compositional Grammatical Relation Centered Tree (CGRCT) of "What instrument does Hendrix play?"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: QC task-accuracy curves w.r.t. the number of landmarks. the task consists in (re-)ranking the comments w.r.t. their utility in answering the question. Subtask A can be modeled as a binary classification problem, where instances are (question, comment) pairs. Each pair generates an example for a binary SVM, where the positive label is associated to a good comment and the negative label refers to potentially useful and bad comments. The classification score achieved over different (question, comment) pairs is used to sort instances and produce the final ranking over comments. The above setting results in a train and test dataset made of 20,340 and 3,270 examples, respectively. In Filice et al. (2016), a Kernel-based SVM classifier (KSVM) achieved state-of-the-art results by adopting a kernel combination that exploited (i) feature vectors containing linguistic similarities between the texts in a pair; (ii) shallow syntactic trees that encode the lexical and morpho-syntactic information shared between text pairs; (iii) feature vectors capturing task-specific information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 4: Example of dependency parse tree</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 : Results in terms of Accuracy and saving in the Question Classification task</head><label>1</label><figDesc></figDesc><table>Model 
#Land. 
Accuracy 
Saving 
CNN 
-
93.6% 
-
KSVM 
-
95.0% 
0.0% 
100 
88.5% (84.1%) 
97.4% 
200 
92.2% (88.7%) 
94.8% 
KDA 
400 
93.7% (91.6%) 
89.7% 
(SVMDCD) 
600 
94.3% (92.8%) 84.5% 
800 
94.3% (93.0%) 
79.3% 
1,000 
94.2% (93.6%) 
74.2% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results in terms of F 1 and savings in the 
Community Question Answering task 

Model 
#Land. 
F1 
Saving 
KSVM 
-
0.644 
0.0% 
ConvKN 
-
0.662 
-
100 
0.638 (0.596) 
99.1% 
200 
0.635 (0.627) 
98.2% 
KDA 
400 
0.657 (0.637) 
96.5% 
(SVMDCD) 
600 
0.669 (0.645) 
94.7% 
800 
0.680 (0.653) 92.9% 
1,000 
0.674 (0.644) 
91.2% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 presents</head><label>3</label><figDesc></figDesc><table>the results in 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results in terms of F 1 and saving in the 
Argument Boundary Detection task. 

Model 
Land. Tr.Size 
F1 
Saving 
KSVM 
-
10k 
0.702 
0.0% 
100 
100k 
0.711 (0.618) 
96.7% 
KDA 
200 
100k 
0.737 (0.661) 
93.3% 
(SVMDCD) 
300 
100k 
0.753 (0.686) 
90.0% 
400 
100k 
0.760 (0.704) 86.6% 
500 
100k 
0.754 (0.713) 
83.3% 

</table></figure>

			<note place="foot" n="1"> In our preliminary experiments, adjustments to the HNy matrix have been tested, but no significant effect was observed. Therefore, no adjustment has been used in any reported experiment, although more in depth exploration is needed on this aspect.</note>

			<note place="foot" n="2"> The input layer and the Nyström layer are not modified during the learning process, and they are not regularized. 3 http://www.kelp-ml.org</note>

			<note place="foot" n="4"> https://www.tensorflow.org/ 5 The lexical vectors used in the CSPTK are generated again using the Word2vec tool with a Skip-gram model.</note>

			<note place="foot" n="7"> The nodes of the subtree covering the words to be verified as possible argument are marked with a FE tag. The word evoking the frame and its ancestor nodes are also marked with the LU tag. The other nodes are pruned out, except the ones connecting the LU nodes to the FE ones.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic compositionality in tree kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Annesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Berkeley FrameNet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING-ACL</title>
		<meeting>of COLING-ACL<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bridging the gap between neural network and kernel methods: Applications to drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Azencott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Joshua</forename><surname>Swamidass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Italian Workshop on Neural Nets</title>
		<meeting>the 20th Italian Workshop on Neural Nets</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ConvKN at SemEval-2016 task 3: Answer and question selection for question answering on arabic and english fora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Alobaidli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kateryna</forename><surname>Romeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Tymoshenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2016</title>
		<meeting>SemEval-2016</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Word-sequence kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Cancedda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Renders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1059" to="1082" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Libsvm: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Chih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<idno type="doi">10.1145/1961189.1961199</idno>
		<idno>27:1-27:27</idno>
		<ptr target="https://doi.org/10.1145/1961189.1961199" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kernel methods for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngmin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="342" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolution kernels for natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Duffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS&apos;2001)</title>
		<meeting>Neural Information Processing Systems (NIPS&apos;2001)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale kernel-based language learning through the ensemble nystrom methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECIR</title>
		<meeting>ECIR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structured lexical similarity via convolution kernels on dependency trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP &apos;11</title>
		<meeting>EMNLP &apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1034" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Support vector machines on a budget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="345" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">KeLP at SemEval-2016 task 3: Learning semantic relations between questions and comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval &apos;16</title>
		<meeting>SemEval &apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structural representations for learning relations between pairs of texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moschitti</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1097" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2015</title>
		<meeting>ACL 2015<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1003" to="1013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Frames and the semantics of understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quaderni di Semantica</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="222" to="254" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic Labeling of Semantic Roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="288" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Convolution kernels on discrete structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Haussler</surname></persName>
		</author>
		<idno>UCS-CRL-99-10</idno>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>Santa Cruz</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A dual coordinate descent method for large-scale linear svm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sathiya Keerthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML</title>
		<meeting>the ICML</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="408" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The effect of syntactic representation on semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Nugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings EMNLP 2014</title>
		<meeting>EMNLP 2014<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sampling methods for the nyström method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="981" to="1006" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/570" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning question classifiers: the role of semantic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="249" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<ptr target="http://arxiv.org/abs/1301.3781" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient convolution kernels for dependency and constituent syntactic trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tree kernels for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Abed Alhakim Freihat, Jim Glass, and Bilal Randeree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Magdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamdy</forename><surname>Mubarak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2016</title>
		<meeting>SemEval-2016</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>SemEval-2016 task 3: Community question answering</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The Word-Space Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Stockholm University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Building structures from classifiers for passage reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ACM</publisher>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="969" to="978" />
			<pubPlace>New York, NY, USA, CIKM</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Kernel Methods for Pattern Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Taylor</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP &apos;13</title>
		<meeting>EMNLP &apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P/P15/P15-1150.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional neural networks vs. convolution kernels: Feature engineering for answer sentence reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kateryna</forename><surname>Tymoshenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Bonadiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Online passive-aggressive algorithms on a budget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Vucetic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research-Proceedings Track</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="908" to="915" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Using the nyström method to speed up kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning with kernel regularization for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1889" to="1896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Two-layer multiple kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS. JMLR.org</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="909" to="917" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
