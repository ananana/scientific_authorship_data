<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Bayesian Mixed Effects Model of Literary Character</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
							<email>dbamman@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of English</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Underwood</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Illinois Urbana</orgName>
								<address>
									<postCode>61801</postCode>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<email>nasmith@cs.cmu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Bayesian Mixed Effects Model of Literary Character</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="370" to="379"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We consider the problem of automatically inferring latent character types in a collection of 15,099 English novels published between 1700 and 1899. Unlike prior work in which character types are assumed responsible for probabilistically generating all text associated with a character, we introduce a model that employs multiple effects to account for the influence of extra-linguistic information (such as author). In an empirical evaluation, we find that this method leads to improved agreement with the preregistered judgments of a literary scholar, complementing the results of alternative models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent work in NLP has begun to exploit the potential of entity-centric modeling for a vari- ety of tasks: Chambers (2013) places entities at the center of probabilistic frame induction, show- ing gains over a comparable event-centric model <ref type="bibr" target="#b8">(Cheung et al., 2013)</ref>; <ref type="bibr" target="#b1">Bamman et al. (2013)</ref> ex- plicitly learn character types (or "personas") in a dataset of Wikipedia movie plot summaries; and entity-centric models form one dominant approach in coreference resolution ( <ref type="bibr" target="#b11">Durrett et al., 2013;</ref><ref type="bibr" target="#b18">Haghighi and Klein, 2010)</ref>.</p><p>One commonality among all of these very dif- ferent probabilistic approaches is that each learns statistical regularities about how entities are de- picted in text (whether for the sake of learning a set of semantic roles, character types, or link- ing anaphora to the entities to which they refer). In each case, the text we observe associated with an entity in a document is directly dependent on the class of entity-and only that class. This re- lationship between entity and text is a theoreti- cal assumption, with important consequences for learning: entity types learned in this way will be increasingly similar the more similar the do- main, author, and other extra-linguistic effects are between them. 1 While in many cases the topi- cally similar types learned under this assumption may be desirable, we explore here the alterna- tive, in which entity types are learned in a way that controls for such effects. In introducing a model based on different assumptions, we provide a method that complements past work and pro- vides researchers with more flexible tools to infer different kinds of character types.</p><p>We focus here on the literary domain, exploring a large collection of 15,099 English novels pub- lished in the 18th and 19th centuries. By account- ing for the influence of individual authors while in- ferring latent character types, we are able to learn personas that cut across different authors more ef- fectively than if we learned types conditioned on the text alone. Modeling the language used to de- scribe a character as the joint result of that charac- ter's latent type and of other formal variables al- lows us to test multiple models of character and assess their value for different interpretive prob- lems. As a test case, we focus on separating char- acter from authorial diction, but this approach can readily be generalized to produce models that pro- visionally distinguish character from other factors (such as period, genre, or point of view) as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Literary Background</head><p>Inferring character is challenging from a liter- ary perspective partly because scholars have not reached consensus about the meaning of the term. It may seem obvious that a "character" is a repre- sentation of a (real or imagined) person, and many humanists do use the term that way. But there is an equally strong critical tradition that treats char- acter as a formal dimension of narrative. To de- scribe a character as a "blocking figure" or "first- person narrator," for instance, is a statement less about the attributes of an imagined person than about a narrative function <ref type="bibr">(Keen, 2003)</ref>. Charac- ters are in one sense collections of psychological or moral attributes, but in another sense "word- masses" <ref type="bibr" target="#b15">(Forster, 1927)</ref>. This tension between "referential" and "formalist" models of character has been a centrally "divisive question in . . . liter- ary theory" <ref type="bibr">(Woloch, 2003)</ref>.</p><p>Considering primary source texts (as distinct from plot summaries) forces us to confront new theoretical questions about character. In a plot summary (such as those explored by <ref type="bibr" target="#b1">Bamman et al., 2013</ref>), a human reader may already have used implicit models of character to extract high-level features. To infer character types from raw narra- tive text, researchers need to explicitly model the relationship of character to narrative form. This is not a solved problem, even for human readers.</p><p>For instance, it has frequently been remarked that the characters of Charles Dickens share certain similarities-including a reliance on tag phrases and recurring tics. A referential model of character might try to distinguish this common stylistic element from underlying "personalities." A strictly formalist model might refuse to separate authorial diction from character at all. In prac- tice, human readers can adopt either perspective: we recognize that characters have a "Dickensian" quality but also recognize that a Dickens villain is (in one sense) more like villains in other authors than like a Dickensian philanthropist. Our goal is to show that computational methods can support the same range of perspectives-allowing a provi- sional, flexible separation between the referential and formal dimensions of narrative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>The dataset for this work consists of 15,099 dis- tinct narratives drawn from HathiTrust Digital Li- brary. <ref type="bibr">2</ref> From an initial collection of 469,200 vol- umes written in English and published between 1700 and 1899 (including poetry, drama, and non- fiction as well as prose narrative), we extract 32,209 volumes of prose fiction, remove dupli- cates and fuse multi-volume works to create the fi- nal dataset. Since the original texts were produced by scanning and running OCR on physical books, we automatically correct common OCR errors and trim front and back matter from the volumes using the page-level classifiers and HMM of <ref type="bibr">Underwood et al. (2013)</ref> Many aspects of this process would be sim- pler if we used manually-corrected texts, such as those drawn from Project Gutenberg. But we hope to produce research that has historical as well as computational significance, and doing so depends on the provenance of a collection. Gutenberg's decentralized selection process tends to produce exceptionally good coverage of currently-popular genres like science fiction, whereas HathiTrust ag- gregates university libraries. Library collections are not guaranteed to represent the past perfectly, but they are larger, and less strongly shaped by contemporary preferences.</p><p>The goal of this work is to provide a method to infer a set of character types in an unsupervised fashion from the data. As with prior work <ref type="bibr" target="#b1">(Bamman et al., 2013)</ref>, we define this target, a character persona, as a distribution over several categories of typed dependency relations: <ref type="bibr">3</ref> 1. agent: the actions of which a character is the agent (i.e., verbs for which the character holds an nsubj or agent relation). 2. patient: the actions of which a character is the patient (i.e., verbs for which the character holds a dobj or nsubjpass relation). 3. possessive: the objects that a character pos- sesses (i.e., all words for which the character holds a poss relation). 4. predicative: attributes predicated of a char- acter (i.e., adjectives or nouns holding an nsubj relation to the character, with an inflec- tion of be as a child).</p><p>This set captures the constellation of what a character does and has done to them, what they possess, and what they are described as being.</p><p>While previous work uses the Stanford CoreNLP toolkit to identify characters and extract typed dependencies for them, we found this approach to be too slow for the scale of our data (a total of 1.8 billion tokens); in particular, syntactic parsing, with cubic complexity in sentence length, and out-of-the-box coreference resolution (with thousands of potential antecedents) prove to be the biggest bottlenecks.</p><p>Before addressing character inference, we present here a prerequisite NLP pipeline that scales well to book-length documents. <ref type="bibr">4</ref> This pipeline uses the Stanford POS tagger ( <ref type="bibr">Toutanova et al., 2003)</ref>, the linear-time MaltParser <ref type="bibr">(Nivre et al., 2007</ref>) for dependency parsing (trained on Stan- ford typed dependencies), and the Stanford named entity recognizer ( <ref type="bibr" target="#b14">Finkel et al., 2005</ref>). It includes the following components for clustering charac- ter name mentions, resolving pronominal corefer- ence, and reducing vocabulary dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Character Clustering</head><p>First, let us terminologically distinguish between a character mention in a text (e.g., the token Tom on page 141 of The Adventures of Tom Sawyer) and a character entity (e.g., TOM SAWYER the character, to which that token refers). To resolve the former to the latter, we largely follow <ref type="bibr" target="#b9">Davis et al. (2003)</ref> and <ref type="bibr" target="#b13">Elson et al. (2010)</ref>: we define a set of initial characters corresponding to each unique charac- ter name that is not a subset of another (e.g., Mr. Tom Sawyer) and deterministically create a set of allowable variants for each one (Mr. Tom Sawyer → Tom, Sawyer, Tom Sawyer, Mr. Sawyer, and Mr. Tom); then, from the beginning of the book to the end, we greedily assign each mention to the most recently linked entity for whom it is a vari- ant. The result constitutes our set of characters, with all mentions partitioned among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pronominal Coreference Resolution</head><p>While the character clustering stage is essentially performing proper noun coreference resolution, approximately 74% of references to characters in books come in the form of pronouns. <ref type="bibr">5</ref> To resolve this more difficult class at the scale of an entire book, we train a log-linear discriminative classifier only on the task of resolving pronominal anaphora (i.e., ignoring generic noun phrases such as the paint or the rascal).</p><p>For this task, we annotated a set of 832 coref- erence links in 3 books (Pride and Prejudice, The Turn of the Screw, and Heart of Darkness) and fea- turized coreference/antecedent pairs with:</p><p>1. The syntactic dependency path from a pronoun to its potential antecedent (e.g., dobj↑pred→↓pred↓nsubj (where → de- notes movement across sentence boundaries). 2. The salience of the antecedent character (de- fined as the count of that character's named mentions in the previous 500 words). 3. The antecedent part of speech. 4. Whether or not the pronoun and antecedent appear in the same quotation scope (false if one appears in a quotation and one outside). 5. Whether or not the two agree for gender. 6. The syntactic tree distance between the two. 7. The linear (word) distance between the two.</p><p>With this featurization and training data, we train a binary logistic regression classifier with 1 regu- larization (where negative examples are comprised of all character entities in the previous 100 words not labeled as the true antecedent). In a 10-fold cross-validation on predicting the true nearest an- tecedent for a pronominal anaphor, this method achieves an average accuracy of 82.7%.</p><p>With this trained model, we then select the highest-scoring antecedent within 100 words for each pronominal anaphor in our data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dimensionality Reduction</head><p>To manage the degrees of freedom in the model described in §4, we perform dimensionality reduc- tion on the vocabulary by learning word embed- dings with a log-linear continuous skip-gram lan- guage model <ref type="bibr">(Mikolov et al., 2013</ref>) on the entire collection of 15,099 books. This method learns a low-dimensional real-valued vector representation of each word to predict all of the words in a win- dow around it; empirically, we find that with a suf- ficient window size (we use n = 10), these word embeddings capture semantic similarity (placing topically similar words near each other in vector space). <ref type="bibr">6</ref> We learn a 100-dimensional embedding for each of the 512,344 words in our vocabulary.</p><p>To create a partition over the vocabulary, we use hard K-means clustering (with Euclidean dis- tance) to group the 512,344 word types into 1,000 clusters. We then agglomeratively cluster those 1,000 groups to assign bitstring representations to each one, forming a balanced binary tree by only merging existing clusters at equal levels in the hi- 0111001110: hat coat cap cloak handkerchief 0111001111: pair boots shoes gloves leather 0111001100: dressed costume uniform clad clothed 0111001101: dress clothes wore worn wear 01110011 → erarchy. We use Euclidean distance as a funda- mental metric and a group-average similarity func- tion for calculating the distance between groups. <ref type="figure" target="#fig_1">Fig. 1</ref> illustrates four of the 1,000 learned clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>In order to separate out the effects that a charac- ter's persona has on the words that are associated with them (as opposed to other factors, such as time period, genre, or author), we adopt a hierar- chical Bayesian approach in which the words we observe are generated conditional on a combina- tion of different effects captured in a log-linear (or "maximum entropy") distribution.</p><p>Maximum entropy approaches to language modeling have been used since <ref type="bibr">Rosenfeld (1996)</ref> to incorporate long-distance information, such as previously-mentioned trigger words, into n-gram language models. This work has since been ex- tended to a Bayesian setting by applying both a Gaussian prior <ref type="bibr" target="#b7">(Chen and Rosenfeld, 2000)</ref>, which dampens the impact of any individual fea- ture, and sparsity-inducing priors <ref type="bibr" target="#b19">(Kazama and Tsujii, 2003;</ref><ref type="bibr" target="#b17">Goodman, 2004</ref>), which can drive many feature weights to 0. The latter have been applied specifically to the problem of estimating word probabilities with sparse additive generative (SAGE) models <ref type="bibr" target="#b12">(Eisenstein et al., 2011</ref>), where sparse extra-linguistic effects can influence a word probability in a larger generative setting.</p><p>In contrast to previous work in which the prob- ability of a word linked to a character is depen- dent entirely on the character's latent persona, in our model, we see the probability of a word as dependent on: (i) the background likelihood of the word, (ii) the author, so that a word becomes more probable if a particular author tends to use it more, and (iii) the character's persona, so that a word is more probable if appearing with a partic- ular persona. Intuitively, if the author Jane Austen is associated with a high weight for the word man- ners, and all personas have little effect for this word, then manners will have little impact on de- ciding which persona a particular Austen character embodies, since its presence is explained largely by Austen having penned the word. While we ad- dress only the author as an observed effect, this model is easily extended to other features as well, including period, genre, point of view, and others.</p><p>The generative story runs as follows ( <ref type="figure" target="#fig_4">Figure 2</ref> depicts the full graphical model): Let there be M unique authors in the data, P latent personas (a hyperparameter to be set), and V words in the vocabulary (in the general setting these may be word types; in our data the vocabulary is the set of 1,000 unique cluster IDs). Each role type r ∈ {agent, patient, possessive, predicative} and vocabulary word v (here, a cluster ID) is associated with a real-valued vector η r,v = [η meta r,v , η pers r,v , η 0 r,v ] of length M + P + 1. The first M + P elements are drawn from a Laplace prior with mean µ = 0 and scale λ = 1; the last el- ement η 0 r,v is an unregularized bias term account- ing for the background. Each element in this vec- tor captures the log-additive effect of each author, persona, and the background distribution on the word's probability (Eq. 1, below).</p><p>Much like latent Dirichlet allocation ( <ref type="bibr" target="#b3">Blei et al., 2003)</ref>, each document d in our dataset draws a multinomial distribution θ d over personas from a shared Dirichlet prior α, which captures the pro- portion of each character type in that particular document. Every character c in the document draws its persona p from this document-specific multinomial. Given document metadata m (here, one of a set of M authors) and persona p, each tu- ple of a role r with word w is assumed to be drawn from Eq. 1 in <ref type="figure" target="#fig_2">Fig. 3</ref>. This SAGE model can be understood as a log-linear distribution with three kinds of features (metadata, persona, and back-</p><formula xml:id="formula_0">P (w | m, p, r, η) = exp η meta r,w [m] + η pers r,w [p] + η 0 r,w V v=1 exp η meta r,v [m] + η pers r,v [p] + η 0 r,v<label>(1)</label></formula><p>P <ref type="formula" target="#formula_2">(</ref> ground bias).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hierarchical Softmax</head><p>The partition function in Eq. 1 can lead to slow inference for any reasonably-sized vocabulary. To address this, we reparameterize the model by ex- ploiting the structure of the agglomerative clus- tering in §3.3 to perform a hierarchical softmax, following Goodman <ref type="formula" target="#formula_0">(2001)</ref> The bitstring representations by which we en- code each word in the vocabulary serve as natural, and inherently meaningful, intermediate classes that correspond to semantically related subsets of the vocabulary, with each bitstring prefix denoting one such class. Longer bitstrings correspond to more fine-grained classes. In the example shown in <ref type="figure" target="#fig_1">Figure 1</ref>, 011100111 is one such intermediate class, containing the union of pair, boots, shoes, gloves leather and hat, coat, cap cloak, handker- chief. Because these classes recursively partition the vocabulary, they offer a convenient way to reparameterize the model through the chain rule of probability.</p><p>Consider, for example, a word represented as the bitstring c = 01011; calculating P (c = 01011)-we suppress conditioning variables for clarity-involves the product:</p><formula xml:id="formula_1">P (c 1 = 0) × P (c 2 = 1 | c 1 = 0) × P (c 3 = 0 | c 1:2 = 01) × P (c 4 = 1 | c 1:3 = 010) × P (c 5 = 1 | c 1:4 = 0101).</formula><p>Since each multiplicand involves a binary pre- diction, we can avoid partition functions and use the classic binary logistic regression. <ref type="bibr">7</ref> We have converted the V -way multiclass logistic regression problem of Eq. 1 into a sequence of log V evalua- tions (assuming a perfectly balanced tree). Given <ref type="bibr">7</ref> Recall that logistic regression lets PLR(y = 1 | x, β) = logit −1 (x β) = 1/(1 + exp −x β) for binary dependent variable y, independent variables x, and coefficients β. m, p, and r (as above) we let b = b 1 b 2 · · · b n de- note the bitstring representation of a word cluster, and the distribution is given by Eq. 2 in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>In this paramaterization, rather than one η- vector for each role and vocabulary term, we have one η-vector for each role and conditional binary decision in the tree (each bitstring prefix). Since the tree is binary with V leaves, this yields the same total number of parameters. As Goodman (2001) points out, while this reparameterization is exact for true probabilities, it remains an approx- imation for estimated models (with generalization behavior dependent on how well the class hierar- chy is supported by the data). In addition to en- abling faster inference, one advantage of the bit- string representation and the hierarchical softmax parameterization is that we can easily calculate probabilities of clusters at different granularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Inference</head><p>Our primary quantities of interest in this model are p (the personas for each character) and η, the effects that each author and persona have on the probability of a word. Rather than adopting a fully Bayesian approach (e.g., sampling all variables), we infer these values using stochastic EM, alter- nating between collapsed Gibbs sampling for each p and maximizing with respect to η.</p><p>Collapsed Gibbs for personas. 8 At each step, the required quantity is the probability that char- acter c in document d has persona z, given ev- erything else. This is proportional to the number of other characters in document d who also (cur- rently) have that persona (plus the Dirichlet hy- perparameter which acts as a smoother) times the probability (under p d,c = z) of all of the words observed in each role r for that character:</p><formula xml:id="formula_2">(count(z; p d,−c ) + α z )× R r=1 j:r j =r P (b j | m, p, r, η)<label>(</label></formula><p>3) The metadata features (like author, etc.) influence this probability by being constant for all choices of z; e.g., if the coefficient learned for Austen for vocabulary term manners is high and all coeffi- cients for all z are close to zero, then the proba- bility of manners will change little under different choices of z. Eq. 3 contains one multiplicand for every word associated with a character, and only one term reflecting the influence of the shared doc- ument multinomial. The implication is that, for major characters with many observed words, the words will dominate the choice of persona; where the document influence would have a bigger effect is with characters for whom we don't have much data. In that case, it can act as a kind of informed background; given what little data we have for that character, it would nudge us toward the character types that the other characters in the book embody.</p><p>Given an assignment of all p, we choose η to maximize the conditional log-likelihood of the words, as represented by their bitstring cluster IDs, given the observed author and background effects and the sampled personas. This equates to solving 4V 1 -regularized logistic regressions (see Eq. 2 in <ref type="figure" target="#fig_2">Figure 3</ref>), one for each role type and bitstring prefix, each with M + P + 1 parameters. We ap- ply OWL-QN ( <ref type="bibr" target="#b0">Andrew and Gao, 2007)</ref> to mini- mize the 1 -regularized objective with an absolute convergence threshold of 10 −5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>While standard NLP and machine learning prac- tice is to evaluate the performance of an algorithm on a held-out gold standard, articulating what a true "persona" might be for a character is inher- ently problematic. Rather, we evaluate the perfor- mance and output of our model by preregistering a set of 29 hypotheses of varying scope and diffi- culty and comparing the performance of different models in either confirming, or failing to confirm, those hypotheses. This kind of evaluation was pre- viously applied to a subjective text measurement problem by <ref type="bibr">Sim et al. (2013)</ref>. All hypotheses were created by a literary scholar with specialization in the period to not only give an empirical measure of the strengths and weaknesses of different models, but also to help explore exactly what the different models may, or may not, be learning. All preregistered hy- potheses establish the degrees of similarity among three characters, taking the form: "character X is more similar to character Y than either X or Y is to a distractor character Z"; for a given model and definition of distance under that model, each hy- pothesis yields two yes/no decisions that we can evaluate:</p><formula xml:id="formula_3">• distance(X, Y ) &lt; distance(X, Z) • distance(X, Y ) &lt; distance(Y, Z)</formula><p>To tease apart the different kinds of similarities we hope to explore, we divide the hypotheses into four classes:</p><p>A. This class constitutes sanity checks: charac- ter X and Y are more similar to each other in every way than to character Z. E.g.: Eliz- abeth Bennet in Pride and Prejudice resem- bles Elinor Dashwood in Sense and Sensibil- ity (Jane Austen) more than either character resembles Allen Quatermain in Allen Quater- main (H. Rider Haggard). (Austenian protag- onists should resemble each other more than they resemble a grizzled hunter.) B. This class captures our ability to identify two characters in the same author as being more similar to each other than to a closely re- lated character in a different author. E.g.:</p><p>Wickham in Pride and Prejudice resembles Willoughby in Sense and Sensibility (Jane Austen) more than either character resem- bles Mr. Rochester in Jane Eyre (Charlotte Brontë). C. This class captures our ability to discrimi- nate among similar characters in the same au- thor. In these hypotheses, two characters X and Y from the same author are more simi- lar to each other than to a third character Z from that same author. E.g.: Wickham in Pride and Prejudice (Jane Austen) resembles Willoughby in Sense and Sensibility more than either character resembles Mr. Darcy in Pride and Prejudice. D. This class constitutes more difficult, ex- ploratory hypotheses, including differences among point of view. E.g.: Montoni in Mysteries of Udolpho (Radcliffe) resem- bles Heathcliff in Wuthering Heights (Emily Brontë) more than either resembles Mr. Ben- net in Pride and Prejudice. (Testing our model's ability to discern similarities in spite of elapsed time.)</p><p>All 29 hypotheses can be found in a supplemen- tary technical report <ref type="bibr" target="#b2">(Bamman et al., 2014</ref>). We emphasize that the full set of hypotheses was locked before the model was estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Part of the motivation of our mixed effects model is to be able to tackle hypothesis class C-by fac- toring out the influence of a particular author on the learning of personas, we would like to be able to discriminate between characters that all have a common authorial voice. In contrast, the Per- sona Regression model of <ref type="bibr" target="#b1">Bamman et al. (2013)</ref>, which uses metadata variables (like authorship) to encourage entities with similar covariates to have similar personas, reflects an assumption that makes it likely to perform well at class B.</p><p>To judge their respective strengths on different hypothesis classes, we evaluate three models:</p><p>1. The mixed-effects Author/Persona model (described above), which includes author in- formation as a metadata effect; here, each η-vector (of length M + P + 1) contains a parameter for each of the distinct authors in our data, a parameter for each persona, and a background parameter. 2. A Basic persona model, which ablates au- thor information but retains the same log- linear architecture; here, the η-vector is of size P + 1 and does not model author effects. 3. The Persona Regression model of <ref type="bibr" target="#b1">Bamman et al. (2013)</ref>.</p><p>All models are run with P ∈ {10, 25, 50, 100, 250} personas; Persona Regression addition- ally uses K = 25 latent topics. All configura- tions use the full dataset of 15,099 novels, and all characters with at least 25 total roles (a total of 257,298 entities). All experiments are run with 50 iterations of Gibbs sampling to collect samples for the personas p, alternating with maximization steps for η. The value of α is optimized using slice sampling (with a non-informative prior) every 5 iterations. The value of λ is held constant at 1. At the end of inference, we calculate the posterior distributions over personas for all characters as the sampling probability of the final iteration. To formally evaluate "similarity" between two characters, we measure the Jensen-Shannon diver- gence between personas (calculated as the average JS distance over the cluster distributions for each role type), marginalizing over the characters' pos- terior distributions over personas; two characters with a lower JS divergence are judged to be more similar than two characters with a higher one.</p><p>As a Baseline, we also evaluate all hypotheses on a model with no latent variables whatsoever, which instead measures similarity as the average JS divergence between the empirical word distri- butions over each role type. <ref type="table" target="#tab_0">Table 1</ref> presents the results of this compari- son; for all models with latent variables, we re- port the average of 5 sampling runs with different random initializations. <ref type="figure" target="#fig_6">Figure 4</ref> provides a syn-P</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Hypothesis Class A B C D</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>250</head><p>Author/Persona 1.00 0.58 0.75 0.42 Basic Persona 1.00 0.73 0.58 0.53 Persona Reg.   Persona regression is best able to judge characters in one author to be more similar to each other than to characters in another (B), while our mixed-effects Author/Persona model outperforms other models at discriminating characters in the same author (C). opsis of this table by illustrating the average ac- curacy across all choice of P . All models, in- cluding the baseline, perform well on the sanity checks (A). As expected, the Persona Regres- sion model performs best at hypothesis class B (correctly judging two characters from the same author to be more similar to each other than to a character from a different author); this behavior is encouraged in this model by allowing an author (as an external metadata variable) to directly influence the persona choice, which has the effect of push- ing characters from the same author to embody the same character type. Our mixed effects Au- thor/Persona model, in contrast, outperforms the other models at hypothesis class C (correctly dis- criminating different character types present in the same author). By discounting author-specific lexi- cal effects during persona inference, we are better able to detect variation among the characters of a single author that we are not able to capture oth- erwise. While these different models complement each other in this manner, we note that there is no absolute separation among them, which may be suggestive of the degree to which the formal and referential dimensions are fused in novels. Nev- ertheless, the strengths of these different models on these different hypothesis classes gives us flex- ible alternatives to use depending on the kinds of character types we are looking to infer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis</head><p>The latent personas inferred from this model will support further exploratory analysis of literary his- tory. <ref type="figure" target="#fig_4">Figure 2</ref> illustrates this with a selection of three character types learned, displaying charac- teristic clusters for all role types, along with the distribution of that persona's use across time and the gender distribution of characters embodying that persona. In general, the personas learned so far do not align neatly with character types known to literary historians. But they do have legible as- sociations both with literary genres and with social categories. Even though gender is not an observ- able variable known to the model during inference, personas tend to be clearly gendered. This is not in itself surprising (since literary scholars know that assumptions about character are strongly gen- dered), but it does suggest that diachronic analysis of latent character types might cast new light on the history of gender in fiction. This is especially true since the distribution of personas across the time axis similarly reveals coherent trends. <ref type="table" target="#tab_3">Table 3</ref> likewise illustrates what our model learns by presenting a sample of the fixed effects learned for a set of five major 19th-century au- thors. These are clusters that are conditionally more likely to appear associated with a character in a work by the given author than they are in the overall data; by factoring this information out of the inference process for learning character types (by attributing its presence in a text to the author  <ref type="table">Table 2</ref>: Snapshots of three personas learned from the P = 50, Author/Persona model. Gender and time proportions are calculated by summing and normalizing the posterior distributions over all characters with that feature. We truncate time series at 1800 due to data sparsity before that date; the y-axis illustrates the frequency of its use in a given year, relative to its lifetime.  rather than the persona), we are able to learn per- sonas that cut across different topics more effec- tively than if a character type is responsible for explaining the presence of these terms as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Our method establishes the possibility of repre- senting the relationship between character and nar- rative form in a hierarchical Bayesian model. Pos- tulating an interaction between authorial diction and character allows models that consider the ef- fect of the author to more closely reproduce a hu- man reader's judgments, especially by learning to distinguish different character types within a sin- gle author's oeuvre. This opens the door to con- sidering other structural and formal dimensions of narration. For instance, representation of charac- ter is notoriously complicated by narrative point of view <ref type="bibr" target="#b4">(Booth, 1961)</ref>; and indeed, comparisons be- tween first-person narrators and other characters are a primary source of error for all models tested above. The strategy we have demonstrated sug- gests that it might be productive to address this by modeling the interaction of character and point of view as a separate effect analogous to authorship. It is also worth noting that the models tested above diverge from many structuralist theories of narrative <ref type="bibr">(Propp, 1998</ref>) by allowing multiple in- stances of the same persona in a single work. Learning structural limitations on the number of "protagonists" likely to coexist in a single story, for example, may be another fruitful area to ex- plore. In all cases, the machinery of hierarchical models gives us the flexibility to incorporate such effects at will, while also being explicit about the theoretical assumptions that attend them.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Bitstring representations of neural agglomerative clusters, illustrating the leaf nodes in a binary tree rooted in the prefix 01110011. Bitstring encodings of intermediate nodes and terminal leaves result by following the left (0) and right (1) branches of the merge tree created through agglomerative clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Parameterizations of the SAGE word distribution. Eq. 1 is a "flat" multinomial logistic regression with one η-vector per role and word. Eq. 2 uses the hierarchical softmax formulation, with one η-vector per role and node in the binary tree of word clusters, giving a distribution over bit strings (b) with the same number of parameters as Eq. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, Morin and Bengio (2005) and Mikolov et al. (2013).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Above: Probabilistic graphical model. Observed variables are shaded, latent variables are clear, and collapsed variables are dotted. Below: Definition of variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Synopsis of table 1: average accuracy across all P. Persona regression is best able to judge characters in one author to be more similar to each other than to characters in another (B), while our mixed-effects Author/Persona model outperforms other models at discriminating characters in the same author (C).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Agreement rates with preregistered hypotheses, av-
eraged over 5 sampling runs with different initializations. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Characteristic possessive clusters in a sample of 
major 19th-century authors. 

</table></figure>

			<note place="foot" n="1"> For example, many entities in Early Modern English texts may be judged to be more similar to each other than to entities from later texts simply by virtue of using hath and other archaic verb forms.</note>

			<note place="foot" n="2"> http://www.hathitrust.org</note>

			<note place="foot" n="3"> All categories are described using the Stanford typed dependencies (de Marneffe and Manning, 2008), but any syntactic formalism is equally applicable.</note>

			<note place="foot" n="4"> All code is available at http://www.ark.cs.cmu. edu/literaryCharacter 5 Over all 15,099 narratives, the average number of character proper name mentions is 1,673; the average number of gendered singular pronouns (he, she, him, his, her) is 4,641.</note>

			<note place="foot" n="6"> In comparison, Brown et al. (1992) clusters learned from the same data capture syntactic similarity (placing functionally similar words in the same cluster).</note>

			<note place="foot" n="8"> We assume the reader is familiar with collapsed Gibbs sampling as used in latent-variable NLP models.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgments</head><p>We thank the reviewers for their helpful com-ments. The research reported here was supported by a National Endowment for the Humanities start-up grant to T.U., U.S. National Science Foun-dation grant CAREER IIS-1054319 to N.A.S., and an ARCS scholarship to D.B. This work was made possible through the use of computing resources made available by the Pittsburgh Supercomputing Center. Eleanor Courtemanche provided advice about the history of narrative theory.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scalable training of l 1-regularized log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning latent personas of film characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Appendix to &apos;A Bayesian mixed effects model of literary character</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Underwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University, University of IllinoisUrbana Champaign</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The Rhetoric of Fiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Booth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1961" />
			<publisher>University of Chicago Press</publisher>
			<pubPlace>Chicago</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Event schema induction with a probabilistic entity-driven model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey of smoothing techniques for me models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roni</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="50" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic frame induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Methods for precise named entity matching in digital collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><forename type="middle">L</forename><surname>Klavans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of JCDL</title>
		<meeting>of JCDL<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Stanford typed dependencies manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Decentralized entity-level modeling for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sparse additive generative models of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Extracting social networks from literary fiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Dames</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Aspects of the Novel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Forster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1927" />
			<publisher>Harcourt, Brace &amp; Co</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Classes for fast maximum entropy training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exponential priors for maximum entropy models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coreference resolution in a modular, entity-centered model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluation and extension of maximum entropy models with inequality constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Jun&amp;apos;ichi Kazama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
