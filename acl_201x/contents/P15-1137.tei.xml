<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Anaphoricity and Antecedent Ranking Features for Coreference Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Engineering and Applied Sciences</orgName>
								<orgName type="department" key="dep2">Facebook AI Research New York</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA, NY</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Engineering and Applied Sciences</orgName>
								<orgName type="department" key="dep2">Facebook AI Research New York</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA, NY</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Engineering and Applied Sciences</orgName>
								<orgName type="department" key="dep2">Facebook AI Research New York</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA, NY</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Engineering and Applied Sciences</orgName>
								<orgName type="department" key="dep2">Facebook AI Research New York</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA, NY</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Anaphoricity and Antecedent Ranking Features for Coreference Resolution</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1416" to="1426"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a simple, non-linear mention-ranking model for coreference resolution that attempts to learn distinct feature representations for anaphoricity detection and antecedent ranking, which we encourage by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representations , and we report the best overall score on the CoNLL 2012 English test set to date.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the major challenges associated with re- solving coreference is that in typical documents the number of mentions (syntactic units capable of referring or being referred to) that are non- anaphoric -that is, that are not coreferent with any previous mention -far exceeds the number of mentions that are anaphoric <ref type="bibr" target="#b20">(Kummerfeld and Klein, 2013;</ref><ref type="bibr" target="#b12">Durrett and Klein, 2013)</ref>.</p><p>This preponderance of non-anaphoric mentions makes coreference resolution challenging, partly because many basic coreference features, such as those looking at head, number, or gender match fail to distinguish between truly coreferent pairs and the large number of matching but nonethe- less non-coreferent pairs. Indeed, several au- thors have noted that it is difficult to obtain good performance on the coreference task using sim- ple features ( <ref type="bibr" target="#b22">Lee et al., 2011;</ref><ref type="bibr" target="#b16">Fernandes et al., 2012;</ref><ref type="bibr" target="#b12">Durrett and Klein, 2013;</ref><ref type="bibr" target="#b20">Kummerfeld and Klein, 2013;</ref><ref type="bibr" target="#b4">Björkelund and Kuhn, 2014</ref>) and, as a result, state-of-the-art systems tend to use lin- ear models with complicated feature conjunction schemes in order to capture more fine-grained in- teractions. While this approach has shown suc- cess, it is not obvious which additional feature conjunctions will lead to improved performance, which is problematic as systems attempt to scale with new data and features.</p><p>In this work, we propose a data-driven model for coreference that does not require pre- specifying any feature relationships. Inspired by recent work in learning representations for nat- ural language tasks <ref type="bibr" target="#b7">(Collobert et al., 2011</ref>), we explore neural network models which take only raw, unconjoined features as input, and attempt to learn intermediate representations automatically.</p><p>In particular, the model we describe attempts to create independent feature representations useful for both detecting the anaphoricity of a mention (that is, whether or not a mention is anaphoric) and ranking the potential antecedents of an anaphoric mention. Adequately capturing anaphoricity in- formation has long been thought to be an impor- tant aspect of the coreference task (see <ref type="bibr" target="#b31">Ng (2004)</ref> and Section 7), since a strong non-anaphoric sig- nal might, for instance, discourage the erroneous prediction of an antecedent for a non-anaphoric mention even in the presence of a misleading head match.</p><p>We furthermore attempt to encourage the learn- ing of the desired feature representations by pre- training the model's weights on two correspond- ing subtasks, namely, anaphoricity detection and antecedent ranking of known anaphoric mentions.</p><p>Overall our best model has an absolute gain of almost 2 points in CoNLL score over a similar but linear mention-ranking model on the CoNLL 2012 English test set ( <ref type="bibr" target="#b32">Pradhan et al., 2012</ref>), and of over 1.5 points over the state-of-the-art coref- erence system. Moreover, unlike current state-of- the-art systems, our model does only local infer- ence, and is therefore significantly simpler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Problem Setting</head><p>We consider here the mention-ranking (or "mention-synchronous") approach to coreference resolution <ref type="bibr" target="#b9">(Denis and Baldridge, 2008;</ref><ref type="bibr" target="#b2">Bengtson and Roth, 2008;</ref><ref type="bibr" target="#b34">Rahman and Ng, 2009)</ref>, which has been adopted by several recent coreference systems <ref type="bibr" target="#b12">(Durrett and Klein, 2013;</ref>. Such systems aim to identify whether a mention is coreferent with an antecedent mention, or whether it is instead non-anaphoric (the first mention in the document referring to a particular entity). This is accomplished by assigning a score to the mention's potential antecedents as well as to the possibility that it is non-anaphoric, and then predicting the greatest scoring option. We furthermore assume the more realistic "system mention" setting, where it is not known a priori which mentions in a document participate in coreference clusters, and so (all) mentions must be automatically extracted, typically with the aid of automatically detected parse trees.</p><p>Formally, we denote the set of automatically de- tected mentions in a document by X . For a men- tion x ∈ X , let A(x) denote the set of mentions appearing before x; we refer to this set as x's po- tential antecedents. Additionally let the symbol denote the empty antecedent, to which we will view x as referring when x is non-anaphoric. <ref type="bibr">1</ref> De- noting the set A(x) ∪ {} by Y(x), a mention- ranking model defines a scoring function s(x, y) : X × Y → R, and predicts the antecedent of x to be y * = arg max y∈Y(x) s(x, y).</p><p>It is common to be quite liberal when extracting mentions, taking, essentially, every noun phrase or pronoun to be a candidate mention, so as not to prematurely discard those that might be coreferent <ref type="bibr" target="#b22">(Lee et al., 2011;</ref><ref type="bibr" target="#b16">Fernandes et al., 2012;</ref><ref type="bibr" target="#b5">Chang et al., 2012;</ref><ref type="bibr" target="#b12">Durrett and Klein, 2013)</ref>. For in- stance, the Berkeley Coreference System (herein BCS) <ref type="bibr" target="#b12">(Durrett and Klein, 2013)</ref>, which we use for mention extraction in our experiments, recov- ers approximately 96.4% of the truly anaphoric mentions in the CoNLL 2012 training set, with an almost 3.5:1 ratio of non-anaphoric mentions to anaphoric mentions among the extracted men- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Mention Ranking Models</head><p>The structural simplicity of the mention-ranking framework puts much of the burden on the scor- ing function s(x, y). We begin by consider- ing mention-ranking systems using linear scoring functions. In the next section, we will extend these models to operate over learned non-linear repre- sentations.</p><p>Linear mention-ranking models generally uti- lize the following scoring function</p><formula xml:id="formula_0">s lin (x, y) w T φ(x, y) ,</formula><p>where φ : X × Y → R d is a pairwise feature func- tion defined on a mention and a potential an- tecedent, and w is a learned parameter vector. To add additional flexibility to the model, lin- ear mention ranking models may duplicate indi- vidual features in φ, with one version being used when predicting an antecedent for x, and another when predicting that x is non-anaphoric <ref type="bibr" target="#b12">(Durrett and Klein, 2013)</ref>. Such a scheme effectively gives rise to the following piecewise scoring function</p><formula xml:id="formula_1">s lin+ (x, y) u T φ a (x) φ p (x,y) if y = v T φ a (x) if y = ,</formula><p>where φ a : X → R da is a feature function defined on a mention and its context, φ p : X × Y → R dp is a pairwise feature function defined on a mention and a potential antecedent, and parameters u and v replace w. Above, we have made an explicit dis- tinction between pairwise features (φ p ) and those strictly on x and its context (φ a ), and moreover as- sumed that our features need not examine potential antecedents when predicting y = .</p><p>We refer to the basic, unconjoined features used for φ a and φ p as raw features. <ref type="figure">Figure 2</ref> shows two versions of these features, a base set BASIC and an extended set BASIC+. The BASIC set are the raw features used in BCS, and BASIC+ in- cludes additional raw features used in other recent coreference sytems. For instance, BASIC+ addi- tionally includes features suggested by <ref type="bibr" target="#b35">Recasens et al. (2013)</ref> to be useful for anaphoricity, such as the number of a mention, its named entity sta- tus, and its animacy, as well as number and gen- der information. We additionally include bilexi- cal head features, which are used in many well- performing systems (for instance, that of Fernan- des et al. <ref type="formula">(2012)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problems with Raw Features</head><p>Many authors have observed that, taken individu- ally, raw features tend to not be particularly pre- dictive for the coreference task. We examine this phenomenon empirically in <ref type="figure" target="#fig_0">Figure 1</ref>. These graphs show that the vast majority of individual features do not give a strong positive signal either of anaphoricity or for an antecedent match.</p><p>To address this issue, state-of-the-art mention- ranking systems often rely on manual or otherwise induced conjunction schemes to capture specific feature interactions. <ref type="bibr" target="#b12">Durrett and Klein (2013)</ref>, for instance, conjoin all raw features in φ a with the type of the mention x, and all raw features in φ p with the types of the current mention and an- tecedent. For these purposes, the type of a mention is either "nominal", "proper", or a canonicaliza- tion of the pronoun if it is a pronominal mention. <ref type="bibr" target="#b16">Fernandes et al. (2012)</ref> and <ref type="bibr" target="#b4">Björkelund and Kuhn (2014)</ref> use an automatic but complicated scheme to induce conjunctions by first extracting feature templates from a separately trained decision tree, and then doing greedy forward selection among the templates. These conjunctions add some non- linearity to the scoring function while still main- taining a tractable, though large, feature set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Features for Ranking</head><p>As an alternative to the aforementioned feature conjunction schemes, we consider learning feature representations in order to better capture relevant aspects of the task. Representation learning af- fords the model more flexibility in exploiting fea- ture interactions, although it can make the under- lying training problem more difficult.  <ref type="figure">Figure 2</ref>: Features used for φ a (x) and φ p (x, y). The '+' indicates a feature is in BASIC+ feature set. V denotes the training vocabulary, and T denotes the set of mention types, viz., {nominal,proper} ∪ {canonical pronouns}, as defined in BCS. Conv. and Art. abbreviate conversation and article (resp.). Lexicalized features occurring fewer than 20 times in the training set back off to part-of-speech; bilexical heads occurring fewer than 10 times back off to an indicator feature. Animacy information is taken from a list and rules used in the Stanford Coreference system (Lee et al., 2013).</p><formula xml:id="formula_2">Mention Features (φ a ) Feature Value Set Mention Head V Mention First Word V Mention Last Word V Word Preceding Mention V Word Following Mention V # Words in Mention {1, 2, . . .} Mention Synt. Ancestry see BCS (2013) Mention Type T + Mention Governor V + Mention Sentence Index {1, 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>We use a neural network to define our model as an extension to the mention-ranking model intro- duced in Section 2. We consider in particular the scoring function:</p><formula xml:id="formula_3">s(x, y) u T g( ha(x) hp(x,y) ) + u 0 if y = v T h a (x) + v 0 if y = ,</formula><p>where h a and h p are feature representations, non- linear functions of the features φ a and φ p (respec- tively), and g is a function of these representa- tions. In particular, we define</p><formula xml:id="formula_4">h a (x) tanh(W a φ a (x) + b a ) h p (x, y) tanh(W p φ p (x, y) + b p ) ,</formula><p>and we take g to either be the identity func- tion, in which case the above model is analo- gous to s lin+ but defined over non-linear fea- ture representations, or to be an additional hidden layer: g(</p><formula xml:id="formula_5">ha(x) hp(x,y) ) = tanh(W ha(x) hp(x,y) + b).</formula><p>For ease of exposition, we will refer to these two settings of g as g 1 and g 2 (respectively) in what follows. As we will see below, both settings lead to comparable performance, but to a different error distribution.</p><p>In either case, by defining the functions h a and h p , we allow the model to learn representations of the input features φ a and φ p . The benefit of the added non-linearities is that, in theory, it is no longer necessary to explicitly specify feature con- junctions, since the model may learn them auto- matically if necessary. Accordingly, for this model we use only φ a and φ p consisting of the raw fea- tures in <ref type="figure">Figure 2</ref> without conjunctions. Any inter- action between these features must be learned by the feature representations h p and h a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>We can directly train our model using back- propagation. To specify the training problem, we first define notation for the training objective.</p><p>Define the set C(x) to contain just the mentions in A(x) that are coreferent with x. We then define</p><formula xml:id="formula_6">C (x) = C(x) if x is anaphoric {} otherwise .</formula><p>Finally, let y n = arg max y∈C (xn) s(x n , y) be the highest scoring correct antecedent of x n , which may be . (Thus, following recent work ( <ref type="bibr" target="#b43">Yu and Joachims, 2009;</ref><ref type="bibr" target="#b16">Fernandes et al., 2012;</ref><ref type="bibr" target="#b12">Durrett and Klein, 2013)</ref>, we view each mention as having a "latent antecedent". 2 ) We train to minimize the regularized, slack-rescaled, latent-variable loss <ref type="bibr">3</ref> given by:</p><formula xml:id="formula_7">L(θ) = N n=1 maxˆy∈Y maxˆ maxˆy∈Y(xn) ∆(xn, ˆ y)(1 + s(xn, ˆ y)−s(x n , y n )) + λ||θ|| 1 ,</formula><p>where ∆ is a mistake-specific cost function, which is 0 whenˆywhenˆ whenˆy ∈ C (x n ). Above, we use θ to refer to the full set of parameters</p><formula xml:id="formula_8">{W , u, v, W a , W p , b a , b p }.</formula><p>For experiments, we define ∆ to take on differ- ent costs for the three kinds of mistakes possible in a coreference task, as follows:</p><formula xml:id="formula_9">∆(x, ˆ y) = α1 ifˆyifˆ ifˆy = ∧ ∈ C (x) α2 ifˆyifˆ ifˆy = ∧ ∈ C (x) α3 ifˆyifˆ ifˆy = ∧ ˆ y ∈ C (x) .</formula><p>The α i determine the trade-off between these mis- takes (and thus precision and recall). Adopting the terminology of BCS, we refer to these mistakes as "false link" (FL), "false new" (FN), and "wrong link" (WL), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Representations from Subtasks</head><p>While we could train our full model directly, it is known to be difficult to train high performing non- convex neural-network models from a random ini- tialization ( <ref type="bibr" target="#b14">Erhan et al., 2010)</ref>. In order to over- come the problems associated with training from this setting, and to learn feature representations useful for the full coreference task, we pretrain subparts of the model on the subtasks targeting the desired feature representations. We then train the entire model on the full coreference task (from the pre-trained initializations). As we will see, the pre-training scheme outlined below helps the model achieve improved performance. The proposed pre-training scheme involves learning the parameters associated with h a and h p using two natural subtasks: anaphoricity detection and antecedent ranking. In particular, we (1) train h a on the task of predicting whether a particular mention is anaphoric or not, and (2) train h p on the task of predicting the antecedent of mentions known to be anaphoric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Anaphoricity Detection</head><p>For the first subtask we attempt to predict whether a mention is anaphoric or not based only on its  local context. <ref type="bibr">4</ref> Anaphoricity detection in vari- ous forms has been used as an initial step in sev- eral coreference systems ( <ref type="bibr" target="#b30">Ng and Cardie, 2002;</ref><ref type="bibr" target="#b2">Bengtson and Roth, 2008;</ref><ref type="bibr" target="#b34">Rahman and Ng, 2009;</ref><ref type="bibr" target="#b3">Björkelund and Farkas, 2012)</ref>, and the related question of whether a mention can be determined to be a singleton or not has been explored recently by <ref type="bibr" target="#b35">Recasens et al. (2013)</ref>, <ref type="bibr" target="#b27">Ma et al. (2014)</ref>, and others. <ref type="bibr">5</ref> Formally, let t n ∈ {−1, 1} indicate whether ∈ C (x n ) or not (respectively). That is, t n = 1 if and only if x n is anaphoric. Define the subtask scoring function s a : X → R as</p><formula xml:id="formula_10">s a (x) v a T h a (x) + ν 0 ,</formula><p>where the vector v a and the bias ν 0 are specific to this subtask and are discarded after pre-training. We train this model to minimize the following slack-rescaled objective</p><formula xml:id="formula_11">L a (θ a ) = N n=1 ∆ a (t n )[1 − t n s a (x n )] + + λ||θ a || 1 ,</formula><p>where ∆ a is a class-specific cost used to help en- courage anaphoric decisions given the imbalanced data set, and θ a = {v a , W a , b a } are the parame- ters of the subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Antecedent Ranking</head><p>For the second subtask, antecedent ranking, we predict the antecedent for mentions known a pri- ori to be anaphoric. This subtask is inspired by the "gold mention" version of the coreference task. Systems designed for this task are forced to handle many fewer non-anaphoric mentions and can often successfully utilize richer feature representations.</p><p>The setup for this task is similar to the full coreference problem, except that we discard any mention x n such that ∈ C (x n ). Thus, we define the pairwise scoring function s p : X × Y → R as</p><formula xml:id="formula_12">s p (x, y) u p T h p (x, y) + υ 0 .</formula><p>As before, u p and υ 0 are discarded after train- ing for this subtask, but we keep the rest of the parameters. For training, we use an analogous latent-variable loss function to that used for the full coreference task, except we replace C with C, and the cost ∆(x, ˆ y) is always 1 (when it is nonzero).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Subtask Performance</head><p>As a preliminary experiment, we train models for these two subtasks using both the BASIC and BA- SIC+ raw features. <ref type="table">Table 1</ref> shows the results. For the first subtask, experiments look at the preci- sion, recall, and F 1 score of predicting anaphoric mentions on the CoNLL 2012 development set. As a baseline we use an L1-regularized SVM implemented using LibLinear <ref type="bibr" target="#b15">(Fan et al., 2008)</ref>, both using raw features and using features con- joined according to the BCS scheme. For the sec- ond subtask, experiments look at the accuracy of the model in predicting the correct antecedent on known anaphoric mentions. As a baseline we use a linear mention ranking model, with and without conjunctions, trained using the same margin-based loss.</p><p>In both subtasks, the neural network model performs quite well, significantly better than the unconjoined baselines and better than the model trained with manually conjoined features. We pro- vide a visual representation of the antecedent rank- ing features learned in <ref type="figure" target="#fig_2">Figure 3</ref>. While the im- proved subtask performance does not imply better performance on the full coreference task, it shows that model can learn useful feature representations with only raw input features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Coreference Experiments</head><p>Our experiments examine performance as com- pared with other coreference systems, as well as the effect of features, pre-training, and model ar- chitecture. We also perform a qualitative compar- ison of our model with the analogous linear model on some challenging non-anaphoric cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methods</head><p>All experiments use the CoNLL 2012 English dataset ( <ref type="bibr" target="#b32">Pradhan et al., 2012)</ref>, which is based on the OntoNotes corpus ( <ref type="bibr" target="#b18">Hovy et al., 2006</ref>). The data set contains 3,493 documents consisting of 1.6 million words. We use the standard experi- mental split with the training set containing 2,802 documents and 156K annotated mentions, the de- velopment set containing 343 documents and 19K annotated mentions, and the test set containing 348 documents and 20K annotated mentions. For all experiments, we use BCS ( <ref type="bibr" target="#b12">Durrett and Klein, 2013)</ref> to extract system mentions and to compute some of the features.</p><p>For training, we minimize the loss described above using the composite mirror descent Ada- Grad update <ref type="bibr" target="#b11">(Duchi et al., 2011</ref>) with docu- ment sized mini-batches. <ref type="bibr">6</ref> We tuned the Ada- Grad learning rate and regularization parameters using a grid search over possible learning rates η ∈ {0.001, 0.002, 0.01, 0.02, 0.1, 0.2} and over regularization parameters λ ∈{10 −6 , . . . , 10 −1 }. For the full coreference task, we use a differ- ent learning rate for the pre-trained weights and for the second-layer weights, using η 1 = 0.1 and η 2 = 0.001, respectively, and λ = 10 −6 . When ini- tializing weight-matrices that were not pre-trained we used the sparse initialization technique pro- posed by <ref type="bibr" target="#b40">Sutskever et al. (2013)</ref>. For all experi- ments we use the cost-weights α = 0.5, 1.2, 1 in defining ∆.</p><p>For the anaphoricity representations the ma- trix dimensions used are W a ∈ R 128×da , and for the pairwise representations the matrix dimensions used are W p ∈ R 700×dp . In the g 2 model, the outer matrix dimensions are W ∈ R 128×(dp+da) . With the BASIC+ features, d p and d a come out to be slightly less than 10 6 and 10 4 , respectively, with bilexical head features accounting for the vast majority of d p . <ref type="bibr">7</ref> We tuned all hyper-parameters (as well as those of baseline systems) on the develop- ment set.</p><p>We use the CoNLL 2012 scoring script v8.01 8 ( , which scores based on 3 metrics, including MUC ( <ref type="bibr">Vilain et al., 1995)</ref>, CEAF e ( <ref type="bibr" target="#b26">Luo, 2005)</ref>, and B 3 (Bagga and Baldwin, 1998), as well as the CoNLL score, which is the arithmetic mean of the 3 metrics.</p><p>Code implementing our models is available at https://github.com/swiseman/nn_ coref. The system trains in time comparable to that of linear systems, mainly because we use only raw features and sparse margin-based gradient up- dates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Our main results are shown in <ref type="table" target="#tab_3">Table 2</ref>. This table compares the performance of our system with the performance reported by several other state-of-the art systems on the CoNLL 2012 English corefer- ence test set. Our full models achieve the best F 1 score across two of the three metrics and have the best aggregate (CoNLL) score, with an improve- ment of over 1.5 points over the best reported re- sult, and of almost 2 points over the best mention- ranking system. Our F 1 improvements on all three metrics are significant (p &lt; 0.05 under the boot- strap resample test <ref type="bibr" target="#b19">(Koehn, 2004)</ref>) as compared with both <ref type="bibr" target="#b4">Björkelund and Kuhn (2014)</ref>, and Dur- rett and <ref type="bibr" target="#b13">Klein (2014)</ref>, the two most recent, state- of-the-art systems.</p><p>Since our full models use some additional raw features (although an order of magnitude fewer total features than the comparable conjunction-    based linear model), we are interested in what part of the improvement in performance comes from features rather than modeling power. <ref type="table" target="#tab_4">Table 3</ref> com- pares the full model to BCS, a system effectively using the s lin+ scoring function together with a manual conjunction scheme, on both BASIC and BASIC+ features. While our models outperform BCS in both cases, we see that as we add more features (as in the BASIC+ set), the performance gap between our model and the linear system be- comes even more pronounced. We may also wonder whether the architecture represented by our scoring function, where the in- termediate representations h a and h p are sepa- rated in the first layer, is necessary for these re- sults. We accordingly compare with the fully connected versions of these two models (which are equivalent to 1 and 2 layer multi-layer per- ceptrons) using the BASIC+ features in <ref type="table">Table 4</ref>. <ref type="bibr">9</ref> There, we also evaluate the effect of pre-training on these models by comparing with the results of training from a random initialization. We see that while even randomly initialized models are capa- ble of excellent performance, pre-training is bene- ficial, especially for g 1 . <ref type="bibr">9</ref> We also experimented with bilinear models both with and without non-linearities; these were also inferior.  <ref type="table">Table 4</ref>: Comparison of performance (in F1 score) of vari- ous models on CoNLL 2012 development set using BASIC+ features. "PT" and "RI" refer to pretraining and random ini- tialization respectively. "Fully Conn." refers to baseline fully connected networks. See text for further model descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We attempt to gain insight into our model's er- rors using using two different error breakdowns. In <ref type="table">Table 5</ref> we show the errors as reported by the analysis tool of <ref type="bibr" target="#b20">Kummerfeld and Klein (2013)</ref>. In <ref type="table" target="#tab_7">Table 6</ref> we show a more fine-grained breakdown inspired by a similar analysis in <ref type="bibr" target="#b12">Durrett and Klein (2013)</ref>. In the latter table, we categorize the er- rors made by our system on the CoNLL 2012 de- velopment data in terms of <ref type="formula">(1)</ref> whether or not the mention has a head match with a previously oc- curring mention in the document, unless it is a pronominal mention, which we treat separately, (2) in terms of the status of the mention in the gold clustering, namely, singleton, first-in-cluster, or anaphoric, and (3) in terms of the type of error made (which, as discussed in Section 3, are one of FL, FN, and WL). We note that the two models have slightly dif- ferent error profiles, with g 1 being slightly better at recall and g 2 being slightly better at precision. Indeed, we see from <ref type="table" target="#tab_7">Table 6</ref> that the two mod- els make a comparable number of total errors (g 1 makes only 17 fewer errors overall). The increased precision of the g 2 model is presumably due to the second layer around h a and h p in g 2 allowing for antecedent evidence to interact with anaphoricity <ref type="table" target="#tab_3">Mention  0651  0568  0529  Extra Entity  0655  0623  0561   Divided Entity  1989  1837  1835  Missing Mention  1004  0997  1005  Missing Entity  1070  1026  1114   Table 5</ref>: Absolute error counts from the coreference analysis tool of <ref type="bibr" target="#b20">Kummerfeld and Klein (2013)</ref>. The upper set roughly corresponds to the precision and the lower to the recall of the coreference clusters produced by the model.   <ref type="formula">(1)</ref> mentions with a (previous) head match (HM), that is, mentions x such that A(x) contains another mention with the same head word, (2) with no previous head match (no HM), and (3) to pronominal mentions, respectively. The 3 column groups correspond to singleton, first-in-cluster, and anaphoric mentions (resp.), as determined by the gold clustering, with the number and type of errors on the left and the total number of mentions in the category (#) on the right.</p><note type="other">Error Type BCS NN (g 1 ) NN (g 2 ) Conflated Entities 1603 1434 1371 Extra</note><p>evidence in a more complicated way. Ultimately, however, coreference systems operating over sys- tem mentions are already biased toward precision, and so the increased precision of g 2 is not as help- ful as the increased recall of g 1 in the final CoNLL score.</p><p>In further analysis we found that many of the correct predictions made by the g 2 model not made by g 1 and the linear model involve predict- ing non-anaphoric even in the presence of highly misleading antecedent features like head-match. <ref type="figure">Figure 4</ref> shows some examples of mentions with previous head matches that the linear system pre- dicted as anaphoric and that our system correctly identifies as non-anaphoric.</p><p>We illustrate how the features in <ref type="figure">Figure 2</ref> might be useful in such cases by considering the first example in <ref type="figure">Figure 4</ref>. There, a comma follows "the Nika TV company" in the text (and is picked up by the "word following" feature), perhaps in- dicating an appositive, which makes anaphoric- ity unlikely. The model can also learn that the <ref type="bibr">Non-Anaphoric (x)</ref> Spurious Antecedent (y) the Nika TV company an independent company Lexus sales GM 's domestic car sales The storage area the harbor area the Budapest location Radio Free Europe 's new location the synagogue the synagogue too or something the equity market</p><p>The junk market their silver coin one silver coin the international school The Hong Kong elementary school the 1970s the early 1970s the 2003 season the 2001 season <ref type="figure">Figure 4</ref>: Example mentions x that were correctly marked non-anaphoric by g 2 , but incorrectly marked anaphoric with y as an antecedent by the BASIC+ linear model. These ex- amples highlight the difficult case where there is a spurious head-match between non-coreferent pairs. See text for fur- ther details.</p><p>"company-company" head match is often mislead- ing, and, in general, distance features may also rule out head matches. Note that while these fea- tures on their own may be more or less correlated with a mention being non-anaphoric, the model learns to combine them in a predictive way. <ref type="table" target="#tab_7">Table 6</ref> also gives a sense of where coreference systems such as ours need to improve. It is first important to note that the case of resolving an anaphoric mention that has no previous head matches (e.g., identifying that "the team" and "the New York Giants" are coreferent), which is of- ten taken to be one of the major challenges fac- ing coreference systems because it presumably requires semantic information, is not the largest source of errors. In fact, we see from <ref type="table" target="#tab_7">Table 6</ref> (second row, third column in both sub-tables) that while these cases do indeed account for a substan- tial percentage of errors, we make hundreds more errors predicting singleton pronominal mentions to be anaphoric (in the case of g 1 ) and on incor- rectly linking anaphoric pronominal mentions (in the case of g 2 ). Further analysis indicates that these errors are almost entirely related to incor- rectly linking pleonastic pronouns, such as "it" or "you," and that moreover the incorrectly predicted antecedent for these pleonastic pronouns is almost always (another instance of) the same pronoun. That these pleonastic cases are so problematic is interesting when considered against the back- drop of the inference strategies typically employed by coreference systems, which we briefly men- tion here but discuss more fully in the next sec- tion. Currently, coreference systems divide be-tween those using "local" models, which choose antecedents for potentially anaphoric mentions in- dependently of each other, and "non-local" mod- els, which make predictions that take into ac- count predictions made for previous mentions, and perhaps even attempt to jointly predict all men- tions in a document. While our model is en- tirely local, other recent high performing sys- tems, such as that of <ref type="bibr" target="#b4">Björkelund and Kuhn (2014)</ref>, are not. One might suspect, then, that "non- local" inference might allow us to capture the fact that, for instance, a cluster of coreferent mentions should generally not consist solely of pronouns, and thereby avoid predicting (identical) pronomi- nal antecedents for pleonastic pronouns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Further Improving Coreference Systems</head><p>As it turns out, however, almost 30% of the anaphoric pronominal mentions in the CoNLL de- velopment data participate in pronoun-only clus- ters (primarily in the context of broadcast or tele- phone conversations), which suggests that such a "non-local" rule may not be particularly useful, though further experiments are required. It is also worth noting that a suitably modified loss function may also be able to prevent excessive pronoun- pronoun linking, even in a local model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>There is a voluminous literature on machine learn- ing approaches to coreference resolution, effec- tively beginning with <ref type="bibr">Soon et al. (2001)</ref>. The re- cent introduction of the CoNLL datasets ( <ref type="bibr" target="#b32">Pradhan et al., 2012</ref>) has spurred research that takes ad- vantage of more fine-grained features and richer models <ref type="bibr" target="#b3">(Björkelund and Farkas, 2012;</ref><ref type="bibr" target="#b5">Chang et al., 2012;</ref><ref type="bibr" target="#b12">Durrett and Klein, 2013;</ref><ref type="bibr" target="#b4">Björkelund and Kuhn, 2014;</ref><ref type="bibr" target="#b27">Ma et al., 2014)</ref>. Of these approaches, our model is related to the mention-ranking approaches <ref type="bibr" target="#b2">(Bengtson and Roth, 2008;</ref><ref type="bibr" target="#b9">Denis and Baldridge, 2008;</ref><ref type="bibr" target="#b34">Rahman and Ng, 2009;</ref><ref type="bibr" target="#b12">Durrett and Klein, 2013;</ref>, as opposed to those that focus on non-local, structured prediction <ref type="bibr" target="#b28">(McCallum and Wellner, 2003;</ref><ref type="bibr" target="#b8">Culotta et al., 2006;</ref><ref type="bibr" target="#b17">Haghighi and Klein, 2010;</ref><ref type="bibr" target="#b16">Fernandes et al., 2012;</ref><ref type="bibr" target="#b39">Stoyanov and Eisner, 2012;</ref><ref type="bibr" target="#b3">Björkelund and Farkas, 2012;</ref><ref type="bibr" target="#b42">Wick et al., 2012;</ref><ref type="bibr" target="#b4">Björkelund and Kuhn, 2014;</ref><ref type="bibr" target="#b13">Durrett and Klein, 2014)</ref>.</p><p>In motivation, our work is most similar to that of Ng (2004), who notes that anaphoricity informa- tion is useful within the broader coreference task, and who accordingly attempts to "globally" opti- mize performance based on this information, as well as that of <ref type="bibr" target="#b10">Denis et al. (2007)</ref>, who do joint decoding of anaphoricity and coreference predic- tions using ILP. Both of these works are taken to contrast with the more popular approach of do- ing an initial non-anaphoric pruning step <ref type="bibr" target="#b30">(Ng and Cardie, 2002;</ref><ref type="bibr" target="#b34">Rahman and Ng, 2009;</ref><ref type="bibr" target="#b35">Recasens et al., 2013;</ref><ref type="bibr" target="#b24">Lee et al., 2013)</ref>. In contrast, we jointly learn non-linear functions of anaphoricity and an- tecedent features, rather than tune a threshold, or jointly decode based on independently trained classifiers (as in <ref type="bibr" target="#b10">Denis et al. (2007)</ref>). In a simi- lar vein, several authors have also proposed using the output of an anaphoricity classifier as a feature in a downstream coreference system <ref type="bibr" target="#b31">(Ng, 2004;</ref><ref type="bibr" target="#b2">Bengtson and Roth, 2008)</ref>. In our framework we (re)learn features jointly with the full task, after a pre-training scheme that targets anaphoricity as well antecedent representations.</p><p>There has also been some work on automat- ically inducing feature conjunctions for use in coreference systems <ref type="bibr" target="#b16">(Fernandes et al., 2012;</ref><ref type="bibr" target="#b21">Lassalle and Denis, 2013)</ref>, though the approach we present here is somewhat simpler, and unlike that of <ref type="bibr" target="#b21">Lassalle and Denis (2013)</ref> is designed for use on system rather than gold mentions.</p><p>There has been much interest recently in us- ing neural networks for classic natural language tasks such as tagging and semantic role labeling <ref type="bibr" target="#b7">Collobert et al. (2011)</ref>, sentiment analysis <ref type="bibr" target="#b36">(Socher et al., 2011;</ref><ref type="bibr" target="#b37">Socher et al., 2012</ref>), prepositional phrase attachment ( <ref type="bibr" target="#b1">Belinkov et al., 2014</ref>) among others. These systems often use some form of pre- training for initialization, often word-embeddings learned from external tasks. However, there has been little work of this form for coreference reso- lution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have presented a simple, local model ca- pable of learning feature representations useful for coreference-related subtasks, and of thereby achieving state-of-the-art performance. Because our approach automatically learns intermediate representations given raw features, directions for further research might alternately explore includ- ing additional (perhaps semantic) raw features, as well as developing loss functions that further discourage learning representations that allow for common errors (such as those involving pleonastic pronouns).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Two histograms illustrating the predictive ability of raw (unconjoined) features per feature occurrence: (top) mention-context features from φ a as independent predictors of anaphoricity (y = ), and (bottom) antecedent-mention features from φ p as independent predictors of coreferent mentions. Very few raw features are strong indicators of either anaphoricity or an antecedent match. Data taken from the CoNLL development set.</figDesc><graphic url="image-2.png" coords="3,72.00,141.77,227.01,90.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Feat</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of the representation matrix W p. A subset of the raw features were manually grouped into five classes indicating: full lexical match [F], head match [H], mention/sentence distance [D] (near versus far), gender/number match [G], and type [P] (pronoun versus other). The heat map illustrates 10-columns of W p as a weighted combination of these classes, roughly illustrating the combination of raw features required for this dimension of the representation.</figDesc><graphic url="image-3.png" coords="5,318.19,62.81,196.44,130.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>. (Conj.) Model</head><label></label><figDesc></figDesc><table>Anaphoric 
Ante 
P 
R 
F1 
Acc. 

BASIC (N) 
Lin. 
74.15 74.20 74.18 69.10 
BASIC (Y) 
Lin. 
73.98 75.04 74.51 79.76 
BASIC (N) 
NN 
75.30 75.36 75.33 81.65 

BASIC+ (N) 
Lin. 
74.14 74.71 74.43 74.02 
BASIC+ (Y) 
Lin. 
74.24 75.39 74.81 80.44 
BASIC+ (N) 
NN 
75.84 76.02 75.93 82.86 

Table 1: Performance of the two subtasks on the CoNLL 2012 
development set by feature set and model type. "Conj." indi-
cates whether conjunctions are used. The linear anaphoric 
system is an SVM (LibLinear implementation (Fan et al., 
2008)), and the linear antecedent system is a linear model 
with the margin-based objective. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results on CoNLL 2012 English test set. We compare against recent state-of-the-art systems, including (in order) 
Durrett and Klein (2013), Ma et al. (2014), Björkelund and Kuhn (2014), and Durrett and Klein (2014) (rescored with the v8.01 
scorer). F1 gains are significant (p &lt; 0.05 under the bootstrap resample test (Koehn, 2004)) compared with both B&amp;K and 
D&amp;K for all metrics. 

Model 
Features MUC 
B 3 
CEAFe CoNLL 

Lin. 
BASIC 
70.44 59.10 
55.57 
61.71 
NN (g 2 ) 
71.59 60.56 
57.45 
63.20 
NN (g 1 ) 
71.86 60.90 
57.90 
63.55 

Lin. 
BASIC+ 
70.92 60.05 
56.39 
62.45 
NN (g 2 ) 
72.68 61.70 
58.32 
64.23 
NN (g 1 ) 
72.74 61.77 
58.63 
64.38 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>F1 performance comparison between state-of-the-art 
linear mention-ranking model (Durrett and Klein, 2013) and 
our full models on CoNLL 2012 development set for different 
feature sets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Errors made by NN (g 1 ) (top) and NN (g 2 ) (bottom) 
on CoNLL 2012 English development data. Rows correspond 
to </table></figure>

			<note place="foot" n="1"> We make this stipulation for modeling convenience; it is not intended to reflect any linguistic fact.</note>

			<note place="foot" n="2"> Note that this renders the objectives of even models with a linear scoring function non-convex.</note>

			<note place="foot" n="3"> Previous work divides between log-loss and margin loss. We use the latter because gradient updates (within backprop) for the non-probabilistic objectives only involve terms relating tô y and y n , and are therefore faster.</note>

			<note place="foot" n="4"> While performance on this subtask can in fact be improved further by looking at previous mentions, features learned in this way led to inferior performance on the full task. 5 Note that singleton detection is slightly different from anaphoricity detection, since a mention can be non-anaphoric but not a singleton if it is the first mention in a cluster.</note>

			<note place="foot" n="6"> In preliminary experiments we also used Nesterov&apos;s accelerated gradient (Nesterov, 1983), but found AdaGrad to perform better.</note>

			<note place="foot" n="7"> Note that the BCS conjunction scheme, for instance, applied to our raw features gives a dp and da that are over an order of magnitude larger. 8 http://conll.github.io/ reference-coreference-scorers/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Algorithms for Scoring Coreference Chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breck</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The first international conference on language resources and evaluation workshop on linguistics coreference</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="563" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploring Compositional Architectures and Word Vector Representations for Prepositional Phrase Attachment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="561" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding the Value of Features for Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bengtson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Datadriven Multilingual Coreference Resolution using Resolver Stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="49" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning structured perceptrons for coreference Resolution with Latent Antecedents and Non-local Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Illinoiscoref: The UI System in the CoNLL-2012 Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajhans</forename><surname>Samdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLLShared Task</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="113" to="117" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Constrained Latent Variable Model for Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajhans</forename><surname>Samdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="601" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural Language Processing (almost) from Scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">First-order Probabilistic Models for Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>NAACL-HLT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Specialized Models and Ranking for Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="660" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint Determination of Anaphoricity and Coreference Resolution using Integer Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="236" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Easy Victories and Uphill Battles in Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1971" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<title level="m">A Joint Model for Entity Analysis: Coreference, Typing, and Linking. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="477" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Dumitru Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Liblinear: A Library for Large Linear Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Latent Structure Perceptron with Feature Induction for Unrestricted Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero Nogueira Dos</forename><surname>Eraldo Rezende Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruy Luiz</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milidiú</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coreference Resolution in a Modular, Entity-centered Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ontonotes: the 90% Solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the human language technology conference of the NAACL</title>
		<meeting>the human language technology conference of the NAACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Statistical Significance Tests for Machine Translation Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Errordriven Analysis of Challenges in Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving Pairwise Coreference Models through Feature Space Hierarchy Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Lassalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2013-Annual meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Stanford&apos;s Multi-pass Sieve Coreference Resolution System at the CoNLL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Peirsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Task</surname></persName>
		</author>
		<title level="m">Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Fifteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="28" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deterministic Coreference Resolution based on Entity-centric, Precision-ranked Rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Peirsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="885" to="916" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An Extension of BLANC to System Mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On Coreference Resolution Performance Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Prune-and-score: Learning for Greedy Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janardhan</forename><surname>Rao Doppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">, J Walker</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashanth</forename><surname>Mannem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Toward Conditional Models of Identity Uncertainty with Application to Proper Noun Coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wellner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 17</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Method of Solving a Convex Programming Problem with Convergence Rate O (1/k2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurii</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet Mathematics Doklady</title>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="372" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Identifying Anaphoric and Non-anaphoric Noun Phrases to Improve Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on Computational linguistics</title>
		<meeting>the 19th international conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning Noun Phrase Anaphoricity to Improve Coreference Resolution: Issues in Representation and Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 151. Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics, page 151. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conll2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scoring Coreference Partitions of Predicted Mentions: A Reference Implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Supervised Models for Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Altaf</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="968" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The Life and Death of Discourse Entities: Identifying Singleton Mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="627" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semi-supervised Recursive Autoencoders for Predicting Sentiment Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semantic Compositionality through Recursive Matrix-vector Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Machine Learning Approach to Coreference Resolution of Noun Phrases</title>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<editor>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim</editor>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="521" to="544" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Easy-first Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2519" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the Importance of Initialization and Momentum in Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dennis Connolly, and Lynette Hirschman. 1995. A Modeltheoretic Coreference Scoring Scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Vilain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Aberdeen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th conference on Message Understanding</title>
		<meeting>the 6th conference on Message Understanding</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A Discriminative Hierarchical Model for Fast Coreference at Large Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="379" to="388" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning Structural SVMs with Latent Variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Nam John</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1169" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
