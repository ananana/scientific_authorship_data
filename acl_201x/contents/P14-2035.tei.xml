<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Resolving Lexical Ambiguity in Tensor Regression Models of Meaning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
							<email>dimitri.kartsaklis@cs.ox.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
							<email>mehrnoosh.sadrzadeh@qmul.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Wolfson Bldg</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<addrLine>Parks Road Oxford</addrLine>
									<postCode>OX1 3QD</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Queen Mary Univ. of London School of Electronic Engineering and Computer Science Mile End Road London</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<addrLine>Wolfson Bldg, Parks Road Oxford</addrLine>
									<postCode>OX1 3QD, E1 4NS</postCode>
									<country>UK, UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Resolving Lexical Ambiguity in Tensor Regression Models of Meaning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="212" to="217"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper provides a method for improving tensor-based compositional distribu-tional models of meaning by the addition of an explicit disambiguation step prior to composition. In contrast with previous research where this hypothesis has been successfully tested against relatively simple compositional models, in our work we use a robust model trained with linear regression. The results we get in two experiments show the superiority of the prior dis-ambiguation method and suggest that the effectiveness of this approach is model-independent.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The provision of compositionality in distributional models of meaning, where a word is represented as a vector of co-occurrence counts with every other word in the vocabulary, offers a solution to the fact that no text corpus, regardless of its size, is capable of providing reliable co-occurrence statis- tics for anything but very short text constituents. By composing the vectors for the words within a sentence, we are still able to create a vectorial representation for that sentence that is very useful in a variety of natural language processing tasks, such as paraphrase detection, sentiment analysis or machine translation. Hence, given a sentence w 1 w 2 . . . w n , a compositional distributional model provides a function f such that:</p><formula xml:id="formula_0">− → s = f ( − → w 1 , − → w 2 , . . . , − → w n )<label>(1)</label></formula><p>where − → w i is the distributional vector of the ith word in the sentence and − → s the resulting compos- ite sentential vector. An interesting question that has attracted the at- tention of researchers lately refers to the way in which these models affect ambiguous words; in other words, given a sentence such as "a man was waiting by the bank", we are interested to know to what extent a composite vector can appropriately reflect the intended use of word 'bank' in that con- text, and how such a vector would differ, for exam- ple, from the vector of the sentence "a fisherman was waiting by the bank".</p><p>Recent experimental evidence <ref type="bibr" target="#b12">(Reddy et al., 2011;</ref> suggests that for a number of compositional models the introduction of a dis- ambiguation step prior to the actual composi- tional process results in better composite represen- tations. In other words, the suggestion is that Eq. 1 should be replaced by:</p><formula xml:id="formula_1">− → s = f (φ( − → w 1 ), φ( − → w 2 ), . . . , φ( − → w n ))<label>(2)</label></formula><p>where the purpose of function φ is to return a dis- ambiguated version of each word vector given the rest of the context (e.g. all the other words in the sentence). The composition operation, whatever that could be, is then applied on these unambigu- ous representations of the words, instead of the original distributional vectors.</p><p>Until now this idea has been verified on rela- tively simple compositional functions, usually in- volving some form of element-wise operation be- tween the word vectors, such as addition or mul- tiplication. An exception to this is the work of , who apply Eq. 2 on partial tensor-based compositional models. In a tensor-based model, relational words such as verbs and adjectives are represented by multi- linear maps; composition takes place as the ap- plication of those maps on vectors representing the arguments (usually nouns). What makes the models of the above work 'partial' is that the au- thors used simplified versions of the linear maps, projected onto spaces of order lower than that re- quired by the theoretical framework. As a result, a certain amount of transformational power was traded off for efficiency.</p><p>A potential explanation then for the effective- ness of the proposed prior disambiguation method can be sought on the limitations imposed by the compositional models under test. After all, the idea of having disambiguation emerge as a direct consequence of the compositional process, with- out the introduction of any explicit step, seems more natural and closer to the way the human mind resolves lexical ambiguities.</p><p>The purpose of this paper is to investigate the hypothesis whether prior disambiguation is important in a pure tensor-based compositional model, where no simplifying assumptions have been made. We create such a model by using lin- ear regression, and we explain how an explicit dis- ambiguation step can be introduced to this model prior to composition. We then proceed by com- paring the composite vectors produced by this ap- proach with those produced by the model alone in a number of experiments. The results show a clear superiority of the priorly disambiguated models following Eq. 2, confirming previous research and suggesting that the reasons behind the success of this approach are more fundamental than the form of the compositional function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Composition in distributional models</head><p>Compositional distributional models of meaning vary in sophistication, from simple element-wise operations between vectors such as addition and multiplication <ref type="bibr" target="#b10">(Mitchell and Lapata, 2008)</ref> to deep learning techniques based on neural networks <ref type="bibr" target="#b18">(Socher et al., 2011;</ref><ref type="bibr" target="#b19">Socher et al., 2012;</ref><ref type="bibr" target="#b5">Kalchbrenner and Blunsom, 2013a)</ref>. Tensor-based mod- els, formalized by <ref type="bibr" target="#b2">Coecke et al. (2010)</ref>, comprise a third class of models lying somewhere in be- tween these two extremes. Under this setting rela- tional words such as verbs and adjectives are rep- resented by multi-linear maps (tensors of various orders) acting on a number of arguments. An ad- jective for example is a linear map f : N → N (where N is our basic vector space for nouns), which takes as input a noun and returns a mod- ified version of it. Since every map of this sort can be represented by a matrix living in the ten- sor product space N ⊗ N , we now see that the meaning of a phrase such as 'red car' is given by red × − → car, where red is an adjective matrix and × indicates matrix multiplication. The same con- cept applies for functions of higher order, such as a transitive verb (a function of two arguments, so a tensor of order 3). For these cases, matrix mul- tiplication generalizes to the more generic notion of tensor contraction. The meaning of a sentence such as 'kids play games' is computed as:</p><formula xml:id="formula_2">− − → kids T × play × − −−− → games (3)</formula><p>where play here is an order-3 tensor (a "cube") and × now represents tensor contraction. A con- cise introduction to compositional distributional models can be found in <ref type="bibr" target="#b9">(Kartsaklis, 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Disambiguation and composition</head><p>The idea of separating disambiguation from com- position first appears in a work of <ref type="bibr" target="#b12">Reddy et al. (2011)</ref>, where the authors show that the intro- duction of an explicit disambiguation step prior to simple element-wise composition is beneficial for noun-noun compounds. Subsequent work by  reports very similar find- ings for verb-object structures, again on additive and multiplicative models. Finally, in ) these experiments were ex- tended to include tensor-based models following the categorical framework of <ref type="bibr" target="#b2">Coecke et al. (2010)</ref>, where again all "unambiguous" models present superior performance compared to their "ambigu- ous" versions. However, in this last work one of the dimen- sions of the tensors was kept empty (filled in with zeros). This simplified the calculations but also weakened the effectiveness of the multi-linear maps. If, for example, instead of using an order-3 tensor for a transitive verb, one uses some of the matrix instantiations of Kartsaklis and Sadrzadeh, Eq. 3 is reduced to one of the following forms:</p><formula xml:id="formula_3">play ( − − → kids ⊗ − −−− → games) , − − → kids (play × − −−− → games) ( − − → kids T × play) − −−− → games (4)</formula><p>where symbol denotes element-wise multipli- cation and play is a matrix. Here, the model does not fully exploit the space provided by the theo- retical framework (i.e. an order-3 tensor), which has two disadvantages: firstly, we lose space that could hold valuable information about the verb in this case and relational words in general; secondly, the generally non-commutative tensor contraction operation is now partly relying on element-wise multiplication, which is commutative, thus forgets (part of the) order of composition.</p><p>In the next section we will see how to apply lin- ear regression in order to create full tensors for verbs and use them for a compositional model that avoids these pitfalls.</p><p>In order to create a matrix for, say, the intransi- tive verb 'play', we first collect all instances of the verb occurring with some subject in the train- ing corpus, and then we create non-compositional holistic vectors for these elementary sentences fol- lowing exactly the same methodology as if they were words. We now have a dataset with instances of the form − −− → subj i , −−−−−−→ subj i play (e.g. the vector of 'kids' paired with the holistic vector of 'kids play', and so on), that can be used to train a linear regres- sion model in order to produce an appropriate ma- trix for verb 'play'. The premise of a model like this is that the multiplication of the verb matrix with the vector of a new subject will produce a re- sult that approximates the distributional behaviour of all these elementary two-word exemplars used in training.</p><p>We present examples and experiments based on this method, constructing ambiguous and dis- ambiguated tensors of order 2 (that is, matrices) for verbs taking one argument. In principle, our method is directly applicable to tensors of higher order, following a multi-step process similar to that of <ref type="bibr" target="#b4">Grefenstette et al. (2013)</ref> who create order- 3 tensors for transitive verbs using similar means. Instead of using subject-verb constructs as above we concentrate on elementary verb phrases of the form verb-object (e.g. 'play football', 'admit stu- dent'), since in general objects comprise stronger contexts for disambiguating the usage of a verb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental setting</head><p>Our basic vector space is trained from the ukWaC corpus <ref type="bibr" target="#b3">(Ferraresi et al., 2008)</ref>, originally using as a basis the 2,000 content words with the highest frequency (but excluding a list of stop words as well as the 50 most frequent content words since they exhibit low information content). We cre- ated vectors for all content words with at least 100 occurrences in the corpus. As context we considered a 5-word window from either side of the target word, while as our weighting scheme we used local mutual information (i.e. point-wise mutual information multiplied by raw counts). This initial semantic space achieved a score of 0.77 Spearman's ρ (and 0.71 Pearson's r) on the well-known benchmark dataset of <ref type="bibr" target="#b15">Rubenstein and Goodenough (1965)</ref>. In order to reduce the time of regression training, our vector space was normal- ized and projected onto a 300-dimensional space using singular value decomposition (SVD). The performance of the reduced space on the R&amp;G dataset was again very satisfying, specifically 0.73 Spearman's ρ and 0.72 Pearson's r.</p><p>In order to create the vector space of the holistic verb phrase vectors, we first collected all instances where a verb participating in the experiments ap- peared at least 100 times in a verb-object relation- ship with some noun in the corpus. As context of a verb phrase we considered any content word that falls into a 5-word window from either side of the verb or the object. For the 68 verbs participating in our experiments, this procedure resulted in 22k verb phrases, a vector space that again was pro- jected into 300 dimensions using SVD.</p><p>Linear regression For each verb we use simple linear regression with gradient descent directly ap- plied on matrices X and Y, where the rows of X correspond to vectors of the nouns that appear as objects for the given verb and the rows of Y to the holistic vectors of the corresponding verb phrases. Our objective function then becomes:</p><formula xml:id="formula_4">ˆ W = arg min W 1 2m WX T − Y T 2 + λW 2 (5)</formula><p>where m is the number of training examples and λ a regularization parameter. The matrix W is used as the tensor for the specific verb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Supervised disambiguation</head><p>In our first experiment we test the effectiveness of a prior disambiguation step for a tensor-based model in a "sandbox" using supervised learning. The goal is to create composite vectors for a num- ber of elementary verb phrases of the form verb- object with and without an explicit disambiguation step, and evaluate which model approximates bet- ter the holistic vectors of these verb phrases. The verb phrases of our dataset are based on the 5 ambiguous verbs of <ref type="table">Table 1</ref>. Each verb has been combined with two different sets of nouns that ap- pear in a verb-object relationship with that verb in the corpus (a total of 343 verb phrases). The nouns of each set have been manually selected in order to explicitly represent a different meaning of the verb. As an example, in the verb 'play' we im- pose the two distinct meanings of using a musical instrument and participating in a sport; so the first set of objects contains nouns such as 'oboe', 'pi- ano', 'guitar', and so on, while in the second set we see nouns such as 'football', 'baseball" etc. In more detail, the creation of the dataset was done in the following way: First, all verb entries with more than one definition in the Oxford Junior Dictionary ( <ref type="bibr" target="#b16">Sansome et al., 2000</ref>) were collected into a list. Next, a linguist (native speaker of En- glish) annotated the semantic difference between the definitions of each verb in a scale from 1 (sim- ilar) to 5 (distinct). Only verbs with definitions exhibiting completely distinct meanings (marked with 5) were kept for the next step. For each one of these verbs, a list was constructed with all the nouns that appear at least 50 times under a verb- object relationship in the corpus with the specific verb. Then, each object in the list was manually annotated as exclusively belonging to one of the two senses; so, an object could be selected only if it was related to a single sense, but not both. For example, 'attention' was a valid object for the at- tract sense of verb 'draw', since it is unrelated to the sketch sense of that verb. On the other hand, 'car' is not an appropriate object for either sense of 'draw', since it could actually appear under both of them in different contexts. The verbs of <ref type="table">Table  1</ref> were the ones with the highest numbers of ex- emplars per sense, creating a dataset of significant size for the intended task (each holistic vector is compared with 343 composite vectors).</p><p>We proceed as follows: We apply linear regres- sion in order to train verb matrices using jointly the object sets for both meanings of each verb, as well as separately-so in this latter case we get two matrices for each verb, one for each sense. For each verb phrase, we create a composite vector by matrix-multiplying the verb matrix with the vector of the specific object. Then we use 4-fold cross validation to evaluate which version of composite vectors (the one created by the ambiguous tensors or the one created by the unambiguous ones) ap- proximates better the holistic vectors of the verb phrases in our test set. This is done by comparing each holistic vector with all the composite ones, and then evaluating the rank of the correct com- posite vector within the list of results.</p><p>In order to get a proper mixing of objects from both senses of a verb in training and testing sets, we set the cross-validation process as follows: We first split both sets of objects in 4 parts. For each fold then, our training set is comprised by <ref type="bibr">3</ref> 4 of set #1 plus 3 4 of set #2, while the test set consists of the remaining 1 4 of set #1 plus 1 4 of set #2. The data points of the training set are presented in the  <ref type="table">Table 2</ref>: Results for the supervised task. 'Amb.' refers to models without the explicit disambigua- tion step, and 'Dis.' to models with that step.</p><p>learning algorithm in random order. We measure approximation in three different metrics. The first one, accuracy, is the strictest, and evaluates in how many cases the composite vector of a verb phrase is the closest one (the first one in the result list) to the corresponding holistic vector. A more relaxed and perhaps more repre- sentative method is to calculate the mean recipro- cal rank (MRR), which is given by:</p><formula xml:id="formula_5">MRR = 1 m m i=1 1 rank i (6)</formula><p>where m is the number of objects and rank i refers to the rank of the correct composite vector for the ith object. Finally, a third way to evaluate the efficiency of each model is to simply calculate the average co- sine similarity between every holistic vector and its corresponding composite vector. The results are presented in <ref type="table">Table 2</ref>, reflecting a clear supe- riority (p &lt; 0.001 for average cosine similarity) of the prior disambiguation method for every verb and every metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Unsupervised disambiguation</head><p>In Section 6 we used a controlled procedure to col- lect genuinely ambiguous verbs and we trained our models from manually annotated data. In this sec- tion we briefly outline how the process of creat- ing tensors for distinct senses of a verb can be au- tomated, and we test this idea on a generic verb phrase similarity task.</p><p>First, we use unsupervised learning in order to detect the latent senses of each verb in the corpus, following a procedure first described by <ref type="bibr" target="#b17">Schütze (1998)</ref>. For every occurrence of the verb, we cre- ate a vector representing the surrounding context by averaging the vectors of every other word in the same sentence. Then, we apply hierarchical agglomerative clustering (HAC) in order to cluster these context vectors, hoping that different groups of contexts will correspond to the different senses under which the word has been used in the corpus. The clustering algorithm uses Ward's method as inter-cluster measure, and Pearson correlation for measuring the distance of vectors within a clus- ter. Since HAC returns a dendrogram embedding all possible groupings, we measure the quality of each partitioning by using the variance ratio crite- rion <ref type="bibr" target="#b1">(Cali´nskiCali´nski and Harabasz, 1974)</ref> and we select the partitioning that achieves the best score (so the number of senses varies from verb to verb).</p><p>The next step is to classify every noun that has been used as an object with that verb to the most probable verb sense, and then use these sets of nouns as before for training tensors for the vari- ous verb senses. Being equipped with a number of sense clusters created as above for every verb, the classification of each object to a relevant sense is based on the cosine distance of the object vector from the centroids of the clusters. 1 Every sense with less than 3 training exemplars is merged to the dominant sense of the verb. The union of all object sets is used for training a single unambigu- ous tensor for the verb. As usual, data points are presented to learning algorithm in random order. No objects in our test set are used for training.</p><p>We test this system on a verb phase similarity task introduced in <ref type="bibr" target="#b11">(Mitchell and Lapata, 2010)</ref>. The goal is to assess the similarity between pairs of short verb phrases (verb-object constructs) and evaluate the results against human annotations. The dataset consists of 72 verb phrases, paired in three different ways to form groups of various degrees of phrase similarity-a total of 108 verb phrase pairs.</p><p>The experiment has the following form: For ev- ery pair of verb phrases, we construct composite vectors and then we evaluate their cosine similar- ity. For the ambiguous regression model, the com- position is done by matrix-multiplying the am- biguous verb matrix (learned by the union of all object sets) with the vector of the noun. For the disambiguated version, we first detect the most probable sense of the verb given the noun, again by comparing the vector of the noun with the centroids of the verb clusters; then, we matrix- multiply the corresponding unambiguous tensor created exclusively from objects that have been classified as closer to this specific sense of the verb with the noun. We also test a number of baselines: the 'verbs-only' model is a non- compositional baseline where only the two verbs are compared; 'additive' and 'multiplicative' com- pose the word vectors of each phrase by applying simple element-wise operations.  <ref type="table">Table 3</ref>: Results for the phrase similarity task. The difference between the ambiguous and the disam- biguated version is s.s. with p &lt; 0.001.</p><p>The results are presented in <ref type="table">Table 3</ref>, where again the version with the prior disambiguation step shows performance superior to that of the am- biguous version. There are two interesting obser- vations that can be made on the basis of <ref type="table">Table  3</ref>. First of all, the regression model is based on the assumption that the holistic vectors of the ex- emplar verb phrases follow an ideal distributional behaviour that the model aims to approximate as close as possible. The results of <ref type="table">Table 3</ref> confirm this: using just the holistic vectors of the corre- sponding verb phrases (no composition is involved here) returns the best correlation with human an- notations (0.403), providing a proof that the holis- tic vectors of the verb phrases are indeed reli- able representations of each verb phrase's mean- ing. Next, observe that the prior disambiguation model approximates this behaviour very closely (0.399) on unseen data, with a difference not sta- tistically significant. This is very important, since a regression model can only perform as well as its training dataset allows it; and in our case this is achieved to a very satisfactory level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and future work</head><p>This paper adds to existing evidence from previ- ous research that the introduction of an explicit disambiguation step before the composition im- proves the quality of the produced composed rep- resentations. The use of a robust regression model rejects the hypothesis that the proposed methodol- ogy is helpful only for relatively "weak" composi- tional approaches. As for future work, an interest- ing direction would be to see how a prior disam- biguation step can affect deep learning composi- tional settings similar to <ref type="bibr" target="#b19">(Socher et al., 2012)</ref> and <ref type="bibr" target="#b6">(Kalchbrenner and Blunsom, 2013b</ref>).</p></div>
			<note place="foot" n="4"> Creating tensors for verbs The essence of any tensor-based compositional model is the way we choose to create our sentenceproducing maps, i.e. the verbs. In this paper we adopt a method proposed by Baroni and Zamparelli (2010) for building adjective matrices, which can be generally applied to any relational word.</note>

			<note place="foot" n="1"> In general, our approach is quite close to the multiprototype models of Reisinger and Mooney (2010).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the three anonymous reviewers for their fruitful comments. Support by EPSRC grant EP/F042728/1 is gratefully ac-knowledged by D. Kartsaklis and M. Sadrzadeh.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nouns are Vectors, Adjectives are Matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Dendrite Method for Cluster Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cali´nskicali´nski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harabasz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in StatisticsTheory and Methods</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mathematical Foundations for Distributed Compositional Model of Meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lambek Festschrift. Linguistic Analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="345" to="384" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Introducing and evaluating ukWaC, a very large web-derived corpus of English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google</title>
		<meeting>the 4th Web as Corpus Workshop (WAC-4) Can we beat Google</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-step regression learning for compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Zhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Computational Semantics</title>
		<meeting>the 10th International Conference on Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>IWCS 2013</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for discourse compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 2013 Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Prior disambiguation of word tensors for constructing sentence vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Separating Disambiguation from Composition in Distributional Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 17th Conference on Computational Natural Language Learning (CoNLL-2013)</title>
		<meeting>17th Conference on Computational Natural Language Learning (CoNLL-2013)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Compositional operators in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<idno type="doi">DOI:10.1007/s40362-014-0017-z</idno>
		<imprint>
			<date type="published" when="2014-04" />
			<publisher>Springer Science Reviews</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vector-based Models of Semantic Composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1439" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic and static prototype vectors for semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Klapaftis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing</title>
		<meeting>5th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="705" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-prototype vector-space models of word meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Contextual Correlates of Synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The Oxford Junior Dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sansome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spooner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic Word Sense Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="97" to="123" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
