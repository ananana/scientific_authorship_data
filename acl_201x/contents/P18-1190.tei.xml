<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
							<email>dinghan.shen@duke.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paidamoyo</forename><surname>Chapfuwa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2041" to="2050"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2041</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Semantic hashing has become a powerful paradigm for fast similarity search in many information retrieval systems. While fairly successful, previous techniques generally require two-stage training , and the binary constraints are handled ad-hoc. In this paper, we present an end-to-end Neural Architecture for Semantic Hashing (NASH), where the binary hashing codes are treated as Bernoulli latent variables. A neural variational inference framework is proposed for training , where gradients are directly back-propagated through the discrete latent variable to optimize the hash function. We also draw connections between proposed method and rate-distortion theory , which provides a theoretical foundation for the effectiveness of the proposed framework. Experimental results on three public datasets demonstrate that our method significantly outperforms several state-of-the-art models on both unsuper-vised and supervised scenarios.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The problem of similarity search, also called nearest-neighbor search, consists of finding doc- uments from a large collection of documents, or corpus, which are most similar to a query doc- ument of interest. Fast and accurate similarity search is at the core of many information retrieval applications, such as plagiarism analysis <ref type="bibr" target="#b32">(Stein et al., 2007)</ref>, collaborative filtering <ref type="bibr" target="#b15">(Koren, 2008)</ref>, content-based multimedia retrieval ( <ref type="bibr" target="#b17">Lew et al., 2006</ref>) and caching ( <ref type="bibr" target="#b25">Pandey et al., 2009</ref>). Semantic hashing is an effective approach for fast similarity search <ref type="bibr" target="#b26">(Salakhutdinov and Hinton, 2009</ref>; Zhang * Equal contribution.</p><p>et al., <ref type="bibr">2010</ref>; <ref type="bibr" target="#b36">Wang et al., 2014</ref>). By represent- ing every document in the corpus as a similarity- preserving discrete (binary) hashing code, the similarity between two documents can be evalu- ated by simply calculating pairwise Hamming dis- tances between hashing codes, i.e., the number of bits that are different between two codes. Given that today, an ordinary PC is able to execute mil- lions of Hamming distance computations in just a few milliseconds ( <ref type="bibr" target="#b43">Zhang et al., 2010)</ref>, this seman- tic hashing strategy is very computationally attrac- tive.</p><p>While considerable research has been devoted to text (semantic) hashing, existing approaches typically require two-stage training procedures. These methods can be generally divided into two categories: (i) binary codes for documents are first learned in an unsupervised manner, then l binary classifiers are trained via supervised learning to predict the l-bit hashing code ( <ref type="bibr" target="#b43">Zhang et al., 2010;</ref><ref type="bibr" target="#b41">Xu et al., 2015)</ref>; (ii) continuous text representa- tions are first inferred, which are binarized as a second (separate) step during testing ( <ref type="bibr" target="#b38">Wang et al., 2013;</ref><ref type="bibr" target="#b5">Chaidaroon and Fang, 2017</ref>). Because the model parameters are not learned in an end-to-end manner, these two-stage training strategies may re- sult in suboptimal local optima. This happens be- cause different modules within the model are opti- mized separately, preventing the sharing of infor- mation between them. Further, in existing meth- ods, binary constraints are typically handled ad- hoc by truncation, i.e., the hashing codes are ob- tained via direct binarization from continuous rep- resentations after training. As a result, the in- formation contained in the continuous representa- tions is lost during the (separate) binarization pro- cess. Moreover, training different modules (map- ping and classifier/binarization) separately often requires additional hyperparameter tuning for each training stage, which can be laborious and time- consuming.</p><p>In this paper, we propose a simple and generic neural architecture for text hashing that learns bi- nary latent codes for documents in an end-to- end manner. Inspired by recent advances in neu- ral variational inference (NVI) for text processing ( <ref type="bibr" target="#b23">Miao et al., 2016;</ref><ref type="bibr" target="#b42">Yang et al., 2017;</ref><ref type="bibr" target="#b29">Shen et al., 2017b</ref>), we approach semantic hashing from a generative model perspective, where binary (hash- ing) codes are represented as either deterministic or stochastic Bernoulli latent variables. The infer- ence (encoder) and generative (decoder) networks are optimized jointly by maximizing a variational lower bound to the marginal distribution of input documents (corpus). By leveraging a simple and effective method to estimate the gradients with re- spect to discrete (binary) variables, the loss term from the generative (decoder) network can be di- rectly backpropagated into the inference (encoder) network to optimize the hash function.</p><p>Motivated by the rate-distortion theory <ref type="bibr" target="#b2">(Berger, 1971;</ref><ref type="bibr" target="#b34">Theis et al., 2017)</ref>, we propose to inject data-dependent noise into the latent codes during the decoding stage, which adaptively accounts for the tradeoff between minimizing rate (number of bits used, or effective code length) and distortion (reconstruction error) during training. The con- nection between the proposed method and rate- distortion theory is further elucidated, providing a theoretical foundation for the effectiveness of our framework.</p><p>Summarizing, the contributions of this paper are: (i) to the best of our knowledge, we present the first semantic hashing architecture that can be trained in an end-to-end manner; (ii) we pro- pose a neural variational inference framework to learn compact (regularized) binary codes for doc- uments, achieving promising results on both unsu- pervised and supervised text hashing; (iii) the con- nection between our method and rate-distortion theory is established, from which we demonstrate the advantage of injecting data-dependent noise into the latent variable during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Models with discrete random variables have at- tracted much attention in the deep learning com- munity ( <ref type="bibr" target="#b11">Jang et al., 2016;</ref><ref type="bibr" target="#b20">Maddison et al., 2016;</ref><ref type="bibr" target="#b35">van den Oord et al., 2017;</ref><ref type="bibr" target="#b18">Li et al., 2017;</ref><ref type="bibr" target="#b30">Shu and Nakayama, 2017)</ref>. Some of these structures are more natural choices for language or speech data, which are inherently discrete. More specifically,     For natural language processing (NLP), al- though significant research has been made to learn continuous deep representations for words or doc- uments ( <ref type="bibr" target="#b24">Mikolov et al., 2013;</ref><ref type="bibr" target="#b14">Kiros et al., 2015;</ref>, discrete neural representations have been mainly explored in learning word em- beddings ( <ref type="bibr" target="#b30">Shu and Nakayama, 2017;</ref><ref type="bibr" target="#b6">Chen et al., 2017)</ref>. In these recent works, words are repre- sented as a vector of discrete numbers, which are very efficient storage-wise, while showing compa- rable performance on several NLP tasks, relative to continuous word embeddings. However, dis- crete representations that are learned in an end- to-end manner at the sentence or document level have been rarely explored. Also there is a lack of strict evaluation regarding their effectiveness. Our work focuses on learning discrete (binary) repre- sentations for text documents. Further, we em- ploy semantic hashing (fast similarity search) as a mechanism to evaluate the quality of learned bi- nary latent codes.</p><formula xml:id="formula_0">g (x)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 g s o F B p B B A b m y f n 2 Z e N A 3 f T q K 6 U = " &gt; A A A B 7 3 i c b V B N T w I x E J 3 F L 8 Q v 1 K O X R m K C F 7 J r S N Q b 0 Y t H T F z B w I Z 0 S x c a 2 u 6 m 7 R r J h l / h x Y M a r / 4 d b / 4 b C + x B w Z d M 8 v L e T G b m h Q l n 2 r j u t 1 N Y W V 1 b 3 y h u l r a 2 d 3 b 3 y v s H 9 z p O F a E + i X m s 2 i H W l D N J f c M M p + 1 E U S x C T l v h 6 H r q t x 6 p 0 i y W d 2 a c 0 E D g g W Q R I 9 h Y 6 W H Q 6 y Z D V n 0 6 7 Z U r b s 2 d A S 0 T L y c V y N H s l b + 6 / Z i k g k p D O N a 6 4 7 m J C T K s D C O c T k r d V N M E k x E e 0 I 6 l E g u q g 2 x 2 8 A S d W K W P o l j Z k g b N 1 N 8 T G R Z a j 0 V o O w U 2 Q 7 3 o T c X / v E 5 q o o s g Y z J J D Z V k v i h K O T I x m n 6 P + k x R Y v j Y E k w U s 7 c i M s Q K E 2 M z K t k Q v M W X l 4 l / V r u s u b f 1 S u M q T 6 M I R 3 A M V f D g H B p w A 0 3 w g Y C A Z 3 i F N 0 c 5 L 8 6 7 8 z F v L T j 5 z C H 8 g f P 5 A 5 / Q j 9 M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 g s o F B p B B A b m y f n 2 Z e N A 3 f T q K 6 U = " &gt; A A A B 7 3 i c b V B N T w I x E J 3 F L 8 Q v 1 K O X R m K C F 7 J r S N Q b 0 Y t H T F z B w I Z 0 S x c a 2 u 6 m 7 R r J h l / h x Y M a r / 4 d b / 4 b C + x B w Z d M 8 v L e T G b m h Q l n 2 r j u t 1 N Y W V 1 b 3 y h u l r a 2 d 3 b 3 y v s H 9 z p O F a E + i X m s 2 i H W l D N J f c M M p + 1 E U S x C T l v h 6 H r q t x 6 p 0 i y W d 2 a c 0 E D g g W Q R I 9 h Y 6 W H Q 6 y Z D V n 0 6 7 Z U r b s 2 d A S 0 T L y c V y N H s l b + 6 / Z i k g k p D O N a 6 4 7 m J C T K s D C O c T k r d V N M E k x E e 0 I 6 l E g u q g 2 x 2 8 A S d W K W P o l j Z k g b N 1 N 8 T G R Z a j 0 V o O w U 2 Q 7 3 o T c X / v E 5 q o o s g Y z J J D Z V k v i h K O T I x m n 6 P + k x R Y v j Y E k w U s 7 c i M s Q K E 2 M z K t k Q v M W X l 4 l / V r u s u b f 1 S u M q T 6 M I R 3 A M V f D g H B p w A 0 3 w g Y C A Z 3 i F N 0 c 5 L 8 6 7 8 z F v L T j 5 z C H 8 g f P 5 A 5 / Q j 9 M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 g s o F B p B B A b m y f n 2 Z e N A 3 f T q K 6 U = " &gt; A A A B 7 3 i c b V B N T w I x E J 3 F L 8 Q v 1 K O X R m K C F 7 J r S N Q b 0 Y t H T F z B w I Z 0 S x c a 2 u 6 m 7 R r J h l / h x Y M a r / 4 d b / 4 b C + x B w Z d M 8 v L e T G b m h Q l n 2 r j u t 1 N Y W V 1 b 3 y h u l r a 2 d 3 b 3 y v s H 9 z p O F a E + i X m s 2 i H W l D N J f c M M p + 1 E U S x C T l v h 6 H r q t x 6 p 0 i y W d 2 a c 0 E D g g W Q R I 9 h Y 6 W H Q 6 y Z D V n 0 6 7 Z U r b s 2 d A S 0 T L y c V y N H s l b + 6 / Z i k g k p D O N a 6 4 7 m J C T K s D C O c T k r d V N M E k x E e 0 I 6 l E g u q g 2 x 2 8 A S d W K W P o l j Z k g b N 1 N 8 T G R Z a j 0 V o O w U 2 Q 7 3 o T c X / v E 5 q o o s g Y z J J D Z V k v i h K O T I x m n 6 P + k x R Y v j Y E k w U s 7 c i M s Q K E 2 M z K t k Q v M W X l 4 l / V r u s u b f 1 S u M q T 6 M I R 3 A M V f D g H B p w A 0 3 w g Y C A Z 3 i F N 0 c 5 L 8 6 7 8 z F v L T j 5 z C H 8 g f P 5 A 5 / Q j 9 M = &lt; / l a t e x i t &gt; z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W I l b T b B F L L c q O v t 8 1 z B c 0 3 G a g J U = " &gt; A A A B 5 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l F U G 9 F L x 5 b M L b Q h r L Z T t q 1 m 0 3 Y 3 Q g 1 9 B d 4 8 a D i 1 b / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W V 1 b X 1 j e J m a W t 7 Z 3 e v v H 9 w r + N U M f R Y L G L V D q h G w S V 6 h h u B 7 U Q h j Q K B r W B 0 M / V b j 6 g 0 j + W d G S f o R 3 Q g e c g Z N V Z q P v X K F b f q z k C W S S 0 n F c j R 6 J W / u v 2 Y p R F K w w T V u l N z E + N n V B n O B E 5 K 3 V R j Q t m I D r B j q a Q R a j + b H T o h J 1 b p k z B W t q Q h M / X 3 R E Y j r c d R Y D s j a o Z 6 0 Z u K / 3 m d 1 I S X f s Z l k h q U b L 4 o T A U x M Z l + T f p c I T N i b A l l i t t b C R t S R Z m x 2 Z R s C L X F l 5 e J d 1 a 9 q r r N 8 0 r 9 O k + j C E d w D K d Q g w u o w y 0 0 w A M G C M / w C m / O g / P i v D s f 8 9 a C k 8 8 c w h 8 4 n z 9 X U 4 z R &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W I l b T b B F L L c q O v t 8 1 z B c 0 3 G a g J U = " &gt; A A A B 5 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l F U G 9 F L x 5 b M L b Q h r L Z T t q 1 m 0 3 Y 3 Q g 1 9 B d 4 8 a D i 1 b / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W V 1 b X 1 j e J m a W t 7 Z 3 e v v H 9 w r + N U M f R Y L G L V D q h G w S V 6 h h u B 7 U Q h j Q K B r W B 0 M / V b j 6 g 0 j + W d G S f o R 3 Q g e c g Z N V Z q P v X K F b f q z k C W S S 0 n F c j R 6 J W / u v 2 Y p R F K w w T V u l N z E + N n V B n O B E 5 K 3 V R j Q t m I D r B j q a Q R a j + b H T o h J 1 b p k z B W t q Q h M / X 3 R E Y j r c d R Y D s j a o Z 6 0 Z u K / 3 m d 1 I S X f s Z l k h q U b L 4 o T A U x M Z l + T f p c I T N i b A l l i t t b C R t S R Z m x 2 Z R s C L X F l 5 e J d 1 a 9 q r r N 8 0 r 9 O k + j C E d w D K d Q g w u o w y 0 0 w A M G C M / w C m / O g / P i v D s f 8 9 a C k 8 8 c w h 8 4 n z 9 X U 4 z R &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W I l b T b B F L L c q O v t 8 1 z B c 0 3 G a g J U = " &gt; A A A B 5 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l F U G 9 F L x 5 b M L b Q h r L Z T t q 1 m 0 3 Y 3 Q g 1 9 B d 4 8 a D i 1 b / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W V 1 b X 1 j e J m a W t 7 Z 3 e v v H 9 w r + N U M f R Y L G L V D q h G w S V 6 h h u B 7 U Q h j Q K B r W B 0 M / V b j 6 g 0 j + W d G S f o R 3 Q g e c g Z N V Z q P v X K F b f q z k C W S S 0 n F c j R 6 J W / u v 2 Y p R F K w w T V u l N z E + N n V B n O B E 5 K 3 V R j Q t m I D r B j q a Q R a j + b H T o h J 1 b p k z B W t q Q h M / X 3 R E Y j r c d R Y D s j a o Z 6 0 Z u K / 3 m d 1 I S X f s Z l k h q U b L 4 o T A U x M Z l + T f p c I T N i b A l l i t t b C R t S R Z m x 2 Z R s C L X F l 5 e J d 1 a 9 q r r N 8 0 r 9 O k + j C E d w D K d Q g w u o w y 0 0 w A M G C M / w C m / O g / P i v D s f 8 9 a C k 8 8 c w h 8 4 n z 9 X U 4 z R &lt; / l a t e x i t &gt;</head><formula xml:id="formula_1">+ N U M f R Y L G L V D q h G w S V 6 h h u B 7 U Q h j Q K B r W B 0 M / V b j 6 g 0 j + W d G S f o R 3 Q g e c g Z N V Z q P v X K F b f q z k C W S S 0 n F c j R 6 J W / u v 2 Y p R F K w w T V u l N z E + N n V B n O B E 5 K 3 V R j Q t m I D r B j q a Q R a j + b H T o h J 1 b p k z B W t q Q h M / X 3 R E Y j r c d R Y D s j a o Z 6 0 Z u K / 3 m d 1 I S X f s Z l k h q U b L 4 o T A U x M Z l + T f p c I T N i b A l l i t t b C R t S R Z m x 2 Z R s C L X F</formula><formula xml:id="formula_2">+ N U M f R Y L G L V D q h G w S V 6 h h u B 7 U Q h j Q K B r W B 0 M / V b j 6 g 0 j + W d G S f o R 3 Q g e c g Z N V Z q P v X K F b f q z k C W S S 0 n F c j R 6 J W / u v 2 Y p R F K w w T V u l N z E + N n V B n O B E 5 K 3 V R j Q t m I D r B j q a Q R a j + b H T o h J 1 b p k z B W t q Q h M / X 3 R E Y j r c d R Y D s j a o Z 6 0 Z u K / 3 m d 1 I S X f s Z l k h q U b L 4 o T A U x M Z l + T f p c I T N i b A l l i t t b C R t S R Z m x 2 Z R s C L X F</formula><formula xml:id="formula_3">+ N U M f R Y L G L V D q h G w S V 6 h h u B 7 U Q h j Q K B r W B 0 M / V b j 6 g 0 j + W d G S f o R 3 Q g e c g Z N V Z q P v X K F b f q z k C W S S 0 n F c j R 6 J W / u v 2 Y p R F K w w T V u l N z E + N n V B n O B E 5 K 3 V R j Q t m I D r B j q a Q R a j + b H T o h J 1 b p k z B W t q Q h M / X 3 R E Y j r c d R Y D s j a o Z 6 0 Z u K / 3 m d 1 I S X f s Z l k h q U b L 4 o T A U x M Z l + T f p c I T N i b A l l i t t b C R t S R Z m x 2 Z R s C L X F</formula><formula xml:id="formula_4">i t &gt; MLP z 0 ⇠ N (z, I)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o L / k S 6 0 C r A 7 r 8 c e u y O Q m k l S U P / Y = " &gt; A A A C D 3 i c b V B N S 8 N A E N 3 U r 1 q / o h 6 9 L B a x g p R E B P V W 9 K I X q W B s o Y l l s 9 2 2 S 3 e T s L s R 2 p C f 4 M W / 4 s W D i l e v 3 v w 3 b t o c t P X B w O O 9 G W b m + R G j U l n W t 1 G Y m 1 9 Y X C o u l 1 Z W 1 9 Y 3 z M 2 t O x n G A h M H h y w U T R 9 J w m h A H E U V I 8 1 I E M R 9 R h r + 4 C L z G w 9 E S B o G t 2 o Y E Y + j X k C 7 F C O l p b a 5 P 7 p P 3 E h Q T l J X U g 5 d j l Q f I 5 Z c p 5 X R I d R a j y N 4 d d A 2 y 1 b V G g P O E j s n Z Z C j 3 j a / 3 E 6 I Y 0 4 C h R m S s m V b k f I S J B T F j K Q l N 5 Y k Q n i A e q S l a Y A 4 k V 4 y f i i F e 1 r p w G 4 o d A U K j t X f E w n i U g 6 5 r z u z e + W 0 l 4 n / e a 1 Y d U + 9 h A Z R r E i A J 4 u 6 M Y M q h F k 6 s E M F w Y o N N U F Y U H 0 r x H 0 k E F Y 6 w 5 I O w Z 5 + e Z Y 4 R 9 W z q n V z X K 6 d 5 2 k U w Q 7 Y B R V g g x N Q A 5 e g D h y A w S N 4 B q / g z X g y X o x 3 4 2 P S W j D y m W 3 w B 8 b n D 5 4 + n H s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o L / k S 6 0 C r A 7 r 8 c e u y O Q m k l S U P / Y = " &gt; A A A C D 3 i c b V B N S 8 N A E N 3 U r 1 q / o h 6 9 L B a x g p R E B P V W 9 K I X q W B s o Y l l s 9 2 2 S 3 e T s L s R 2 p C f 4 M W / 4 s W D i l e v 3 v w 3 b t o c t P X B w O O 9 G W b m + R G j U l n W t 1 G Y m 1 9 Y X C o u l 1 Z W 1 9 Y 3 z M 2 t O x n G A h M H h y w U T R 9 J w m h A H E U V I 8 1 I E M R 9 R h r + 4 C L z G w 9 E S B o G t 2 o Y E Y + j X k C 7 F C O l p b a 5 P 7 p P 3 E h Q T l J X U g 5 d j l Q f I 5 Z c p 5 X R I d R a j y N 4 d d A 2 y 1 b V G g P O E j s n Z Z C j 3 j a / 3 E 6 I Y 0 4 C h R m S s m V b k f I S J B T F j K Q l N 5 Y k Q n i A e q S l a Y A 4 k V 4 y f i i F e 1 r p w G 4 o d A U K j t X f E w n i U g 6 5 r z u z e + W 0 l 4 n / e a 1 Y d U + 9 h A Z R r E i A J 4 u 6 M Y M q h F k 6 s E M F w Y o N N U F Y U H 0 r x H 0 k E F Y 6 w 5 I O w Z 5 + e Z Y 4 R 9 W z q n V z X K 6 d 5 2 k U w Q 7 Y B R V g g x N Q A 5 e g D h y A w S N 4 B q / g z X g y X o x 3 4 2 P S W j D y m W 3 w B 8 b n D 5 4 + n H s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o L / k S 6 0 C r A 7 r 8 c e u y O Q m k l S U P / Y = " &gt; A A A C D 3 i c b V B N S 8 N A E N 3 U r 1 q / o h 6 9 L B a x g p R E B P V W 9 K I X q W B s o Y l l s 9 2 2 S 3 e T s L s R 2 p C f 4 M W / 4 s W D i l e v 3 v w 3 b t o c t P X B w O O 9 G W b m + R G j U l n W t 1 G Y m 1 9 Y X C o u l 1 Z W 1 9 Y 3 z M 2 t O x n G A h M H h y w U T R 9 J w m h A H E U V I 8 1 I E M R 9 R h r + 4 C L z G w 9 E S B o G t 2 o Y E Y + j X k C 7 F C O l p b a 5 P 7 p P 3 E h Q T l J X U g 5 d j l Q f I 5 Z c p 5 X R I d R a j y N 4 d d A 2 y 1 b V G g P O E j s n Z Z C j 3 j a / 3 E 6 I Y 0 4 C h R m S s m V b k f I S J B T F j K Q l N 5 Y k Q n i A e q S l a Y A 4 k V 4 y f i i F e 1 r p w G 4 o d A U K j t X f E w n i U g 6 5 r z u z e + W 0 l 4 n / e a 1 Y d U + 9 h A Z R r E i A J 4 u 6 M Y M q h F k 6 s E M F w Y o N N U F Y U H 0 r x H 0 k E F Y 6 w 5 I O w Z 5 + e Z Y 4 R 9 W z q n V z X K 6 d 5 2 k U w Q 7 Y B R V g g x N Q A 5 e g D h y A w S N 4 B q / g z X g y X o x 3 4 2 P S W j D y m W 3 w B 8 b n D 5 4 + n H s = &lt; / l a t e x i t &gt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hashing under the NVI Framework</head><p>Inspired by the recent success of variational au- toencoders for various NLP problems ( <ref type="bibr" target="#b23">Miao et al., 2016;</ref><ref type="bibr" target="#b3">Bowman et al., 2015;</ref><ref type="bibr" target="#b42">Yang et al., 2017;</ref><ref type="bibr" target="#b22">Miao et al., 2017;</ref><ref type="bibr" target="#b29">Shen et al., 2017b;</ref>, we approach the training of discrete (bi- nary) latent variables from a generative perspec-tive. Let x and z denote the input document and its corresponding binary hash code, respectively. Most of the previous text hashing methods focus on modeling the encoding distribution p(z|x), or hash function, so the local/global pairwise simi- larity structure of documents in the original space is preserved in latent space ( <ref type="bibr" target="#b43">Zhang et al., 2010;</ref><ref type="bibr" target="#b38">Wang et al., 2013;</ref><ref type="bibr" target="#b41">Xu et al., 2015;</ref><ref type="bibr" target="#b36">Wang et al., 2014</ref>). However, the generative (decoding) pro- cess of reconstructing x from binary latent code z, i.e., modeling distribution p(x|z), has been rarely considered. Intuitively, latent codes learned from a model that accounts for the generative term should naturally encapsulate key semantic information from x because the generation/reconstruction ob- jective is a function of p(x|z). In this regard, the generative term provides a natural training objec- tive for semantic hashing.</p><p>We define a generative model that simultane- ously accounts for both the encoding distribu- tion, p(z|x), and decoding distribution, p(x|z), by defining approximations q φ (z|x) and q θ (x|z), via inference and generative networks, g φ (x) and g θ (z), parameterized by φ and θ, respectively. Specifically, x ∈ Z |V | + is the bag-of-words (count) representation for the input document, where |V | is the vocabulary size. Notably, we can also em- ploy other count weighting schemes as input fea- tures x, e.g., the term frequency-inverse document frequency (TFIDF) ( <ref type="bibr" target="#b21">Manning et al., 2008</ref>). For the encoding distribution, a latent variable z is first inferred from the input text x, by construct- ing an inference network g φ (x) to approximate the true posterior distribution p(z|x) as q φ (z|x). Subsequently, the decoder network g θ (z) maps z back into input space to reconstruct the original sequence x asˆxasˆ asˆx, approximating p(x|z) as q θ (x|z) (as shown in <ref type="figure" target="#fig_4">Figure 1</ref>). This cyclic strategy, x → z → ˆ x ≈ x, provides the latent variable z with a better ability to generalize <ref type="bibr" target="#b23">(Miao et al., 2016</ref>).</p><p>To tailor the NVI framework for semantic hash- ing, we cast z as a binary latent variable and as- sume a multivariate Bernoulli prior on z:</p><formula xml:id="formula_5">p(z) ∼ Bernoulli(γ) = l i=1 γ z i i (1 − γ i ) 1−z i , where γ i ∈ [0, 1] is component i of vector γ.</formula><p>Thus, the encoding (approximate posterior) distribution q φ (z|x) is restricted to take the form q φ (z|x) = Bernoulli(h), where h = σ(g φ (x)), σ(·) is the sig- moid function, and g φ (·) is the (nonlinear) infer- ence network specified as a multilayer perceptron (MLP). As illustrated in <ref type="figure" target="#fig_4">Figure 1</ref>, we can obtain samples from the Bernoulli posterior either deter- ministically or stochastically. Suppose z is a l-bit hash code, for the deterministic binarization, we have, for i = 1, 2, ......, l:</p><formula xml:id="formula_6">z i = 1 σ(g i φ (x))&gt;0.5 = sign(σ(g i φ (x) − 0.5) + 1 2 ,<label>(1)</label></formula><p>where z is the binarized variable, and z i and g i φ (x) denote the i-th dimension of z and g φ (x), respec- tively. The standard Bernoulli sampling in (1) can be understood as setting a hard threshold at 0.5 for each representation dimension, therefore, the binary latent code is generated deterministically. Another strategy to obtain the discrete variable is to binarize h in a stochastic manner:</p><formula xml:id="formula_7">z i = 1 σ(g i φ (x))&gt;µ i = sign(σ(g i φ (x)) − µ i ) + 1 2 ,<label>(2)</label></formula><p>where µ i ∼ Uniform(0, 1). Because of this sam- pling process, we do not have to assume a pre- defined threshold value like in (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training with Binary Latent Variables</head><p>To estimate the parameters of the encoder and decoder networks, we would ideally maximize the marginal distribution p(x) = p(z)p(x|z)dz. However, computing this marginal is intractable in most cases of interest. Instead, we maximize a variational lower bound. This approach is typ- ically employed in the VAE framework <ref type="bibr" target="#b13">(Kingma and Welling, 2013)</ref>: </p><formula xml:id="formula_8">L vae = E q φ (z|x) log q θ (x|z)p(z) q φ (z|x) ,<label>(3)</label></formula><formula xml:id="formula_9">= E q φ (z|x) [log q θ (x|z)] − D KL (q φ (z|x)||p(z)),</formula><formula xml:id="formula_10">D KL = g φ (x) log g φ (x) γ + (1 − g φ (x)) log 1 − g φ (x) 1 − γ .<label>(4)</label></formula><p>Note that the gradient for the KL divergence term above can be evaluated easily.</p><p>For the first term in (3), we should in principle estimate the influence of µ i in (2) on q θ (x|z) by averaging over the entire (uniform) noise distribu- tion. However, a closed-form distribution does not exist since it is not possible to enumerate all possi- ble configurations of z, especially when the latent dimension is large. Moreover, discrete latent vari- ables are inherently incompatible with backpropa- gation, since the derivative of the sign function is zero for almost all input values. As a result, the exact gradients of L vae wrt the inputs before bina- rization would be essentially all zero.</p><p>To estimate the gradients for binary latent vari- ables, we utilize the straight-through (ST) estima- tor, which was first introduced by <ref type="bibr" target="#b9">Hinton (2012)</ref>. So motivated, the strategy here is to simply back- propagate through the hard threshold by approxi- mating the gradient ∂z/∂φ as 1. Thus, we have:</p><formula xml:id="formula_11">dE q φ (z|x) [log q θ (x|z)] ∂φ = dE q φ (z|x) [log q θ (x|z)] dz dz dσ(g i φ (x)) dσ(g i φ (x)) dφ ≈ dE q φ (z|x) [log q θ (x|z)] dz dσ(g i φ (x)) dφ<label>(5)</label></formula><p>Although this is clearly a biased estimator, it has been shown to be a fast and efficient method rela- tive to other gradient estimators for discrete vari- ables, especially for the Bernoulli case ( <ref type="bibr" target="#b1">Bengio et al., 2013;</ref><ref type="bibr" target="#b10">Hubara et al., 2016;</ref><ref type="bibr" target="#b34">Theis et al., 2017)</ref>. With the ST gradient estimator, the first loss term in (3) can be backpropagated into the encoder network to fine-tune the hash function g φ (x). For the approximate generator q θ (x|z) in <ref type="formula" target="#formula_8">(3)</ref>, let x i denote the one-hot representation of ith word within a document. Note that x = i x i is thus the bag-of-words representation for document x. To reconstruct the input x from z, we utilize a soft- max decoding function written as:</p><formula xml:id="formula_12">q(x i = w|z) = exp(z T Ex w + b w ) |V | j=1 exp(z T Ex j + b j ) ,<label>(6)</label></formula><p>where q(x i = w|z) is the probability that x i is word w ∈ V , q θ (x|z) = i q(x i = w|z) and θ = {E, b 1 , . . . , b |V | }. Note that E ∈ R d×|V | can be interpreted as a word embedding matrix to be learned, and {b i } |V | i=1 denote bias terms. Intuitively, the objective in (6) encourages the discrete vector z to be close to the embeddings for every word that appear in the input document x. As shown in Section 5.3.1, meaningful semantic structures can be learned and manifested in the word embedding matrix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Injecting Data-dependent Noise to z</head><p>To reconstruct text data x from sampled binary representation z, a deterministic decoder is typi- cally utilized ( <ref type="bibr" target="#b23">Miao et al., 2016;</ref><ref type="bibr" target="#b5">Chaidaroon and Fang, 2017</ref>). Inspired by the success of employing stochastic decoders in image hashing applications <ref type="bibr" target="#b7">(Dai et al., 2017;</ref><ref type="bibr" target="#b34">Theis et al., 2017)</ref>, in our exper- iments, we found that injecting random Gaussian noise into z makes the decoder a more favorable regularizer for the binary codes, which in practice leads to stronger retrieval performance. Below, we invoke the rate-distortion theory to perform some further analysis, which leads to interesting find- ings.</p><p>Learning binary latent codes z to represent a continuous distribution p(x) is a classical informa- tion theory concept known as lossy source coding. From this perspective, semantic hashing, which compresses an input document into compact bi- nary codes, can be casted as a conventional rate- distortion tradeoff problem ( <ref type="bibr" target="#b34">Theis et al., 2017;</ref><ref type="bibr" target="#b0">Ballé et al., 2016)</ref>:</p><formula xml:id="formula_13">min − log 2 R(z) Rate +β ·D(x, ˆ x) Distortion ,<label>(7)</label></formula><p>where rate and distortion denote the effective code length, i.e., the number of bits used, and the dis- tortion introduced by the encoding/decoding se- quence, respectively. Further, ˆ x is the recon- structed input and β is a hyperparameter that con- trols the tradeoff between the two terms.</p><p>Considering the case where we have a Bernoulli prior on z as p(z) ∼ Bernoulli(γ), and x conditionally drawn from a Gaussian distribution p(x|z) ∼ N (Ez, σ 2 I).</p><formula xml:id="formula_14">Here, E = {e i } |V | i=1</formula><p>, where e i ∈ R d can be interpreted as a codebook with |V | codewords. In our case, E corresponds to the word embedding matrix as in (6).</p><p>For the case of stochastic latent variable z, the objective function in (3) can be written in a form similar to the rate-distortion tradeoff:</p><formula xml:id="formula_15">min E q φ (z|x)     − log q φ (z|x) Rate + 1 2σ 2 β ||x − Ez|| 2 2 Distortion +C     , (8)</formula><p>where C is a constant that encapsulates the prior distribution p(z) and the Gaussian distribution normalization term. Notably, the trade-off hyper- parameter β = σ −2 /2 is closely related to the variance of the distribution p(x|z). In other words, by controlling the variance σ, the model can adap- tively explore different trade-offs between the rate and distortion objectives. However, the optimal trade-offs for distinct samples may be different.</p><p>Inspired by the observations above, we propose to inject data-dependent noise into latent variable z, rather than to setting the variance term σ 2 to a fixed value <ref type="bibr" target="#b7">(Dai et al., 2017;</ref><ref type="bibr" target="#b34">Theis et al., 2017)</ref>. Specifically, log σ 2 is obtained via a one-layer MLP transformation from g φ (x). Afterwards, we sample z from N (z, σ 2 I), which then replace z in (6) to infer the probability of generating individual words (as shown in <ref type="figure" target="#fig_4">Figure 1)</ref>. As a result, the vari- ances are different for every input document x, and thus the model is provided with additional flexibil- ity to explore various trade-offs between rate and distortion for different training observations. Al- though our decoder is not a strictly Gaussian dis- tribution, as in <ref type="formula" target="#formula_12">(6)</ref>, we found empirically that in- jecting data-dependent noise into z yields strong retrieval results, see Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Supervised Hashing</head><p>The proposed Neural Architecture for Semantic Hashing (NASH) can be extended to supervised hashing, where a mapping from latent variable z to labels y is learned, here parametrized by a two- layer MLP followed by a fully-connected softmax layer. To allow the model to explore and balance between maximizing the variational lower bound in (3) and minimizing the discriminative loss, the following joint training objective is employed:</p><formula xml:id="formula_16">L = −L vae (θ, φ; x) + αL dis (η; z, y). (9)</formula><p>where η refers to parameters of the MLP classi- fier and α controls the relative weight between the variational lower bound (L vae ) and discrimina- tive loss (L dis ), defined as the cross-entropy loss. The parameters {θ, φ, η} are learned end-to-end via Monte Carlo estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use the following three standard publicly available datasets for training and evaluation: (i) Reuters21578, containing 10,788 news docu- ments, which have been classified into 90 differ- ent categories. (ii) 20Newsgroups, a collection of 18,828 newsgroup documents, which are catego- rized into 20 different topics. (iii) TMC (stands for SIAM text mining competition), containing air traffic reports provided by NASA. TMC consists 21,519 training documents divided into 22 differ- ent categories. To make direct comparison with prior works, we employed the TFIDF features on these datasets supplied by <ref type="bibr" target="#b5">(Chaidaroon and Fang, 2017)</ref>, where the vocabulary sizes for the three datasets are set to 10,000, 7,164 and 20,000, re- spectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details</head><p>For the inference networks, we employ a feed- forward neural network with 2 hidden layers (both with 500 units) using the ReLU non-linearity ac- tivation function, which transform the input doc- uments, i.e., TFIDF features in our experiments, into a continuous representation. Empirically, we found that stochastic binarization as in <ref type="formula" target="#formula_7">(2)</ref> shows stronger performance than deterministic binariza- tion, and thus use the former in our experiments. However, we further conduct a systematic ablation study in Section 5.2 to compare the two binariza- tion strategies.</p><p>Our model is trained using Adam ( <ref type="bibr" target="#b12">Kingma and Ba, 2014)</ref>, with a learning rate of 1 × 10 −3 for all parameters. We decay the learning rate by a fac- tor of 0.96 for every 10,000 iterations. Dropout ( <ref type="bibr" target="#b31">Srivastava et al., 2014</ref>) is employed on the output of encoder networks, with the rate selected from {0.7, 0.8, 0.9} on the validation set. To facilitate comparisons with previous methods, we set the di- mension of z, i.e., the number of bits within the hashing code) as 8, 16, 32, 64, or 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We evaluate the effectiveness of our framework on both unsupervised and supervised semantic hash- ing tasks. We consider the following unsuper- vised baselines for comparisons: Locality Sensi- tive Hashing (LSH) ( <ref type="bibr" target="#b8">Datar et al., 2004</ref>), Stack Re- stricted Boltzmann Machines (S-RBM) <ref type="bibr" target="#b26">(Salakhutdinov and Hinton, 2009)</ref>, Spectral Hashing (SpH) ( <ref type="bibr" target="#b40">Weiss et al., 2009</ref>), Self-taught Hashing (STH) ( <ref type="bibr" target="#b43">Zhang et al., 2010)</ref> and Variational Deep Se- mantic Hashing (VDSH) <ref type="bibr" target="#b5">(Chaidaroon and Fang, 2017</ref>).  <ref type="table">Table 1</ref>: Precision of the top 100 retrieved docu- ments on Reuters dataset (Unsupervised hashing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method 8 bits 16 bits 32 bits 64 bits 128 bits</head><p>For supervised semantic hashing, we also com- pare NASH against a number of baselines: Su- pervised Hashing with Kernels (KSH) ( <ref type="bibr" target="#b19">Liu et al., 2012</ref>), Semantic Hashing using Tags and Topic Modeling (SHTTM) ( <ref type="bibr" target="#b38">Wang et al., 2013)</ref> and Su- pervised VDSH <ref type="bibr" target="#b5">(Chaidaroon and Fang, 2017)</ref>. It is worth noting that unlike all these baselines, our NASH model is trained end-to-end in one-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation Metrics</head><p>To evaluate the hashing codes for similarity search, we consider each document in the testing set as a query document. Similar documents to the query in the corresponding training set need to be retrieved based on the Hamming distance of their hashing codes, i.e. number of different bits. To facilitate comparison with prior work ( <ref type="bibr" target="#b38">Wang et al., 2013;</ref><ref type="bibr" target="#b5">Chaidaroon and Fang, 2017)</ref>, the per- formance is measured with precision. Specifically, during testing, for a query document, we first re- trieve the 100 nearest/closest documents accord- ing to the Hamming distances of the correspond- ing hash codes (i.e., the number of different bits). We then examine the percentage of documents among these 100 retrieved ones that belong to the same label (topic) with the query document (we consider documents having the same label as rel- evant pairs). The ratio of the number of relevant documents to the number of retrieved documents (fixed value of 100) is calculated as the precision score. The precision scores are further averaged over all test (query) documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We experimented with four variants for our NASH model: (i) NASH: with deterministic decoder; (ii) NASH-N: with fixed random noise injected to de- coder; (iii) NASH-DN: with data-dependent noise injected to decoder; (iv) NASH-DN-S: NASH-DN with supervised information during training.  <ref type="figure">Figure 2</ref>: Precision of the top 100 retrieved doc- uments on Reuters dataset (Supervised hashing), compared with other supervised baselines. <ref type="table">Table 1</ref> presents the results of all models on Reuters dataset. Regarding unsupervised seman- tic hashing, all the NASH variants consistently outperform the baseline methods by a substan- tial margin, indicating that our model makes the most effective use of unlabeled data and manage to assign similar hashing codes, i.e., with small Hamming distance to each other, to documents that belong to the same label. It can be also observed that the injection of noise into the de- coder networks has improved the robustness of learned binary representations, resulting in better retrieval performance. More importantly, by mak- ing the variances of noise adaptive to the specific input, our NASH-DN achieves even better results, compared with NASH-N, highlighting the impor- tance of exploring/learning the trade-off between rate and distortion objectives by the data itself. We observe the same trend and superiority of our NASH-DN models on the other two benchmarks, as shown in <ref type="table" target="#tab_2">Tables 3 and 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Semantic Hashing Evaluation</head><p>Another observation is that the retrieval results tend to drop a bit when we set the length of hash- ing codes to be 64 or larger, which also happens for some baseline models. This phenomenon has been reported previously in ; <ref type="bibr" target="#b19">Liu et al. (2012)</ref>; <ref type="bibr" target="#b38">Wang et al. (2013)</ref>; <ref type="bibr" target="#b5">Chaidaroon and Fang (2017)</ref>, and the reasons could be twofold: (i) for longer codes, the number of data points that are assigned to a certain binary code decreases exponentially. As a result, many queries may fail to return any neighbor documents ( ; (ii) considering the size of train- ing data, it is likely that the model may over- fit with long hash codes <ref type="bibr" target="#b5">(Chaidaroon and Fang, 2017)</ref>. However, even with longer hashing codes, <ref type="table">Word  weapons  medical  companies  define  israel  book   NASH   gun  treatment  company  definition  israeli  books  guns  disease  market  defined  arabs  english  weapon  drugs  afford  explained  arab  references  armed  health  products  discussion  jewish  learning  assault  medicine  money  knowledge  jews  reference   NVDM   guns  medicine  expensive  defined  israeli  books  weapon  health  industry  definition  arab  reference  gun  treatment  company  printf  arabs  guide  militia  disease  market  int  lebanon  writing  armed  patients  buy  sufficient  lebanese  pages   Table 2</ref>: The five nearest words in the semantic space learned by NASH, compared with the results from NVDM ( <ref type="bibr" target="#b23">Miao et al., 2016</ref>).    our NASH models perform stronger than the base- lines in most cases (except for the 20Newsgroups dataset), suggesting that NASH can effectively al- locate documents to informative/meaningful hash- ing codes even with limited training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method 8 bits 16 bits 32 bits 64 bits 128 bits</head><p>We also evaluate the effectiveness of NASH in a supervised scenario on the Reuters dataset, where the label or topic information is utilized dur- ing training. As shown in <ref type="figure">Figure 2</ref>, our NASH- DN-S model consistently outperforms several su- pervised semantic hashing baselines, with vari- ous choices of hashing bits. Notably, our model exhibits higher Top-100 retrieval precision than VDSH-S and VDSH-SP, proposed by <ref type="bibr" target="#b5">Chaidaroon and Fang (2017)</ref>. This may be attributed to the fact that in VDSH models, the continuous embeddings are not optimized with their future binarization in mind, and thus could hurt the relevance of learned binary codes. On the contrary, our model is opti- mized in an end-to-end manner, where the gradi- ents are directly backpropagated to the inference network (through the binary/discrete latent vari- able), and thus gives rise to a more robust hash function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">The effect of stochastic sampling</head><p>As described in Section 3, the binary latent vari- ables z in NASH can be either deterministically (via (1)) or stochastically (via (2)) sampled. We compare these two types of binarization functions in the case of unsupervised hashing. As illustrated in <ref type="figure" target="#fig_9">Figure 3</ref>, stochastic sampling shows stronger re- trieval results on all three datasets, indicating that endowing the sampling process of latent variables with more stochasticity improves the learned rep- resentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">The effect of encoder/decoder networks</head><p>Under the variational framework introduced here, the encoder network, i.e., hash function, and de- coder network are jointly optimized to abstract se- mantic features from documents. An interesting question concerns what types of network should be leveraged for each part of our NASH model. In this regard, we further investigate the effect of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head><p>Title/Subject 8-bit code 16-bit code   using an encoder or decoder with different non- linearity, ranging from a linear transformation to two-layer MLPs. We employ a base model with an encoder of two-layer MLPs and a linear de- coder (the setup described in Section 3), and the ablation study results are shown in <ref type="table">Table 6</ref>.  <ref type="table">Table 6</ref>: Ablation study with different en- coder/decoder networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseball</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dave Kingman for the hall of fame 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 Time of game 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 Game score report 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 Why is Barry Bonds not batting 4th? 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 Electronics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>It is observed that for the encoder networks, in- creasing the non-linearity by stacking MLP layers leads to better empirical results. In other words, endowing the hash function with more modeling capacity is advantageous to retrieval tasks. How- ever, when we employ a non-linear network for the decoder, the retrieval precision drops dramat- ically. It is worth noting that the only difference between linear transformation and one-layer MLP is whether a non-linear activation function is em- ployed or not.</p><p>This observation may be attributed the fact that the decoder networks can be considered as a sim- ilarity measure between latent variable z and the word embeddings E k for every word, and the probabilities for words that present in the docu- ment is maximized to ensure that z is informative. As a result, if we allow the decoder to be too ex- pressive (e.g., a one-layer MLP), it is likely that we will end up with a very flexible similarity mea- sure but relatively less meaningful binary repre- sentations. This finding is consistent with several image hashing methods, such as SGH ( <ref type="bibr" target="#b7">Dai et al., 2017)</ref> or binary autoencoder <ref type="bibr" target="#b4">(Carreira-Perpinán and Raziperchikolaei, 2015)</ref>, where a linear de- coder is typically adopted to obtain promising re- trieval results. However, our experiments may not speak for other choices of encoder-decoder archi- tectures, e.g., LSTM-based sequence-to-sequence models ) or DCNN-based autoencoder ( <ref type="bibr" target="#b44">Zhang et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Analysis of Semantic Information</head><p>To understand what information has been learned in our NASH model, we examine the matrix E ∈ R d×l in (6). Similar to ( <ref type="bibr" target="#b23">Miao et al., 2016;</ref><ref type="bibr" target="#b16">Larochelle and Lauly, 2012)</ref>, we select the 5 near- est words according to the word vectors learned from NASH and compare with the corresponding results from NVDM.</p><p>As shown in <ref type="table">Table 2</ref>, although our NASH model contains a binary latent variable, rather than a con- tinuous one as in NVDM, it also effectively group semantically-similar words together in the learned vector space. This further demonstrates that the proposed generative framework manages to by- pass the binary/discrete constraint and is able to abstract useful semantic information from docu- ments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Case Study</head><p>In <ref type="table" target="#tab_4">Table 5</ref>, we show some examples of the learned binary hashing codes on 20Newsgroups dataset. We observe that for both 8-bit and 16- bit cases, NASH typically compresses documents with shared topics into very similar binary codes. On the contrary, the hashing codes for documents with different topics exhibit much larger Ham- ming distance. As a result, relevant documents can be efficiently retrieved by simply computing their Hamming distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper presents a first step towards end-to-end semantic hashing, where the binary/discrete con- straints are carefully handled with an effective gra- dient estimator. A neural variational framework is introduced to train our model. Motivated by the connections between the proposed method and rate-distortion theory, we inject data-dependent noise into the Bernoulli latent variable at the train- ing stage. The effectiveness of our framework is demonstrated with extensive experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: NASH for end-to-end semantic hashing. The inference network maps x → z using an MLP and the generative network recovers x as z → ˆ x. van den Oord et al. (2017) combined VAEs with vector quantization to learn discrete latent representation, and demonstrated the utility of these learned representations on images, videos, and speech data. Li et al. (2017) leveraged both pairwise label and classification information to learn discrete hash codes, which exhibit state-of-the-art performance on image retrieval tasks. For natural language processing (NLP), although significant research has been made to learn continuous deep representations for words or documents (Mikolov et al., 2013; Kiros et al., 2015; Shen et al., 2018), discrete neural representations have been mainly explored in learning word embeddings (Shu and Nakayama, 2017; Chen et al., 2017). In these recent works, words are represented as a vector of discrete numbers, which are very efficient storage-wise, while showing comparable performance on several NLP tasks, relative to continuous word embeddings. However, discrete representations that are learned in an endto-end manner at the sentence or document level have been rarely explored. Also there is a lack of strict evaluation regarding their effectiveness. Our work focuses on learning discrete (binary) representations for text documents. Further, we employ semantic hashing (fast similarity search) as a mechanism to evaluate the quality of learned binary latent codes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>where the Kullback-Leibler (KL) divergence D KL (q φ (z|x)||p(z)) encourages the approximate posterior distribution q φ (z|x) to be close to the multivariate Bernoulli prior p(z). In this case, D KL (q φ (z|x)|p(z)) can be written in closed-form as a function of g φ (x):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The precisions of the top 100 retrieved documents for NASH-DN with stochastic or deterministic binary latent variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Precision of the top 100 retrieved docu- ments on 20Newsgroups dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Precision of the top 100 retrieved docu- ments on TMC dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Examples of learned compact hashing codes on 20Newsgroups dataset. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-end optimization of nonlinear transform codes for perceptual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valero</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Picture Coding Symposium (PCS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rate-distortion theory. Encyclopedia of Telecommunications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Berger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06349</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hashing with binary autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Miguel A Carreira-Perpinán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raziperchikolaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="557" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Variational deep semantic hashing for text documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suthee</forename><surname>Chaidaroon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 40th international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning k-way d-dimensional discrete code for compact embedding representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03067</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02815</idno>
		<title level="m">Stochastic generative hashing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Locality-sensitive hashing scheme based on p-stable distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayur</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Immorlica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vahab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twentieth annual symposium on Computational geometry</title>
		<meeting>the twentieth annual symposium on Computational geometry</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://coursera.org/course/neuralnets" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ran El-Yaniv, and Yoshua Bengio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4107" to="4115" />
		</imprint>
	</monogr>
	<note>Binarized neural networks</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A neural autoregressive topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2708" to="2716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Content-based multimedia information retrieval: State of the art and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Michael S Lew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chabane</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Djeraba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10999</idno>
		<title level="m">Deep supervised discrete hashing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Supervised hashing with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2074" to="2081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<title level="m">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00359</idno>
		<title level="m">Discovering discrete latent topics with neural variational inference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1727" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nearest-neighbor caching for contentmatch applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavio</forename><surname>Chierichetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanja</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergei</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th international conference on World wide web</title>
		<meeting>the 18th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="441" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="969" to="978" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adaptive convolutional filter generation for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.08294</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Baseline needs more love: On simple wordembedding-based models and associated pooling mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deconvolutional latent-variable model for text sequence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Nakayama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01068</idno>
		<title level="m">Compressing word embeddings via deep compositional code learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Strategies for retrieving plagiarized documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Meyer Zu Eissen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 30th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="825" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Lossy image compression with compressive autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6309" to="6318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Hashing for similarity search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Heng Tao Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.2927</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semi-supervised hashing for large-scale search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2393" to="2406" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantic hashing using tags and topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 36th international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="213" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Topic compositional neural language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spectral hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1753" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhua</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<title level="m">Convolutional neural networks for text hashing. In IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1369" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Improved variational autoencoders for text modeling using dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08139</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-taught hashing for fast similarity search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dell</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval. ACM</title>
		<meeting>the 33rd international ACM SIGIR conference on Research and development in information retrieval. ACM</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deconvolutional paragraph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4172" to="4182" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
