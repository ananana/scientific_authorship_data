<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Hashing for Language and Dialect Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
						</author>
						<title level="a" type="main">Feature Hashing for Language and Dialect Identification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="399" to="403"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2063</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We evaluate feature hashing for language identification (LID), a method not previously used for this task. Using a standard dataset, we first show that while feature performance is high, LID data is highly dimensional and mostly sparse (&gt;99.5%) as it includes large vocabularies for many languages; memory requirements grow as languages are added. Next we apply hash-ing using various hash sizes, demonstrating that there is no performance loss with dimensionality reductions of up to 86%. We also show that using an ensemble of low-dimension hash-based classifiers further boosts performance. Feature hashing is highly useful for LID and holds great promise for future work in this area.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language Identification (LID) is the task of deter- mining the language of a text, at the document, sub-document or even sentence level. LID is a fundamental preprocessing task in NLP and is also used in lexicography, machine translation and in- formation retrieval. It is widely used for filtering to select documents in a specific language; e.g. LID can filter webpages or tweets by language.</p><p>Although LID has been widely studied, several open issues remain ( <ref type="bibr" target="#b7">Hughes et al., 2006</ref>). Current goals include developing models that can iden- tify thousands of languages; extending the task to more fine-grained dialect identification; and mak- ing LID functionality more readily available to users/developers. A common challenge among these goals is dealing with high dimensional fea- ture spaces. LID differs from traditional text cate- gorization tasks in some important aspects. Stan- dard tasks, such as topic classification, are usually performed within a single language, and the max- imum feature space size is a function of the single language's vocabulary. However, LID must deal with vocabulary from many languages and the fea- ture space grows prodigiously.</p><p>This raises immediate concerns about memory requirements for such systems and portends im- plementation issues for applying the systems to dozens, hundreds or even thousands of languages. Recent LID work has reported results on datasets including over 1,300 languages <ref type="bibr" target="#b1">(Brown, 2014)</ref>, al- beit using small samples. Such models are going to include an extraordinarily large feature space, and individual vectors for each sample are going to be extremely sparse. LID is usually done using n-grams and as the number of languages and/or n gets larger, the feature space will become pro- hibitively large or impractical for real-world use.</p><p>For high dimensional input, traditional dimen- sionality reduction methods (e.g. PCA, LDA) can be computationally expensive. Feature selection methods, e.g. those using entropy, are simpler but still expensive. Recently, feature hashing has been shown to be a very effective dimensionality re- duction method <ref type="bibr" target="#b17">(Weinberger et al., 2009)</ref>. It has proven to be useful in numerous machine learning applications, particularly for handling extremely high dimensional data. It also provides numerous other benefits, which we describe in §2.1.</p><p>Although hashing could be tremendously use- ful for LID, to our knowledge no such experiments have been reported to date. It is unclear how colli- sions of features from different languages would affect its application for LID. Accordingly, the aims of the present work are to: (1) evaluate the effectiveness of hashing for LID; (2) compare its performance to the standard n-gram approach; (3) assess the role of hash size (and collision rate) on accuracy for different feature types; and (4) deter- mine if ensemble methods can boost performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Language and Dialect Identification</head><p>Work in language identification (LID) dates back to the seminal work of <ref type="bibr" target="#b0">Beesley (1988)</ref>, <ref type="bibr" target="#b6">Dunning (1994)</ref> and <ref type="bibr" target="#b3">Cavnar and Trenkle (1994)</ref>. Automatic LID methods have since been widely used in NLP research and applications. Recently, attention has turned to discriminating between close languages, such as Malay-Indonesian and Croatian-Serbian <ref type="bibr" target="#b9">(Ljubeši´Ljubeši´c et al., 2007)</ref>, or even dialects/varieties of one language, e.g. Arabic dialects ( . This has been the focus of the "Discrim- inating Similar Language" (DSL) shared task se- ries in recent years. In this work we use data from the 2016 task ( <ref type="bibr" target="#b14">Malmasi et al., 2016)</ref>.</p><p>The 2016 task used data from 12 different lan- guages/dialects. A training and development set consisting of 20,000 sentences from each lan- guage and an unlabelled test set of 1,000 sentences per language was used for evaluation. Most par- ticipants relied on multi-class discriminative clas- sifiers trained with word unigrams and character n-grams ( <ref type="bibr" target="#b14">Malmasi et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature Hashing</head><p>Feature hashing is a method for mapping a high- dimensional input to a low-dimensional space us- ing hashing <ref type="bibr" target="#b17">(Weinberger et al., 2009)</ref>. Hashing has proven to be simple, efficient and effective. It has been applied to various tasks including protein se- quence classification <ref type="bibr" target="#b2">(Caragea et al., 2012)</ref>, senti- ment analysis <ref type="bibr" target="#b5">(Da Silva et al., 2014)</ref>, and malware detection <ref type="bibr" target="#b8">(Jang et al., 2011)</ref>.</p><p>This method uses a hash function h(x) to arbi- trarily map input to a hash key of a specified size. The hash size, e.g. 2 18 , determines the size of the mapped feature space. Hash functions are many- to-one mappings. Collision occur when distinct inputs yield the same output, i.e. h(a) = h(b). The collision rate is affected by the hash size. From a learning perspective, collisions cause random clustering of features and introduce noise; unre- lated features map to the same vector index and may degrade the learner's accuracy. However, it has been shown that "the interference between in- dependently hashed subspaces is negligible with high probability" <ref type="bibr" target="#b17">(Weinberger et al., 2009)</ref>.</p><p>A positive by-product of hashing is that it elim- inates the need for a feature dictionary. In NLP, bag-of-words models require a full pass over the data to identify the vocabulary for each feature type (e.g. n-grams) and build a feature index.</p><p>Eliminating this has many benefits: it simpli- fies implementation of feature extraction meth- ods, reduces memory overhead, and facilitates dis- tributed computing. Global statistics, e.g. totals and per-class feature counts, are usually required for feature selection and dimensionality reduction methods. Feature hashing may eliminate the need for full processing of the data to calculate these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data and Experimental Setup</head><p>Our methodology is based on the results of 2016 DSL Shared Task ( <ref type="bibr" target="#b14">Malmasi et al., 2016</ref>) and we use their dataset. The DSL task is performed at the sentence level, making it more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>A key shortcoming in LID research has been the absence of a common dataset for evaluation <ref type="bibr" target="#b7">(Hughes et al., 2006</ref>), a need that has been met by the corpora released as part of the DSL shared task series. We use the DSLCC 3.0 corpus from the 2016 DSL task. 1 This allows us to compare our findings to that of the 17 participants. Using a standard, publicly available dataset also facilitates replicability of our results. The 2016 task used data from 12 different languages and varieties, 2 including training/development sets composed of 20,000 sentences per language. An unlabelled test set of 1,000 sentences per language was used for evaluation. The total sentences for training and testing are 240k and 12k, respectively. We report our results on the standard test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classifier and Evaluation</head><p>Participants applied various methods, but the task organizers note that linear classifiers, particularly SVMs, were the most successful ( <ref type="bibr" target="#b14">Malmasi et al., 2016)</ref>. This is unsurprising as SVMs have been very successful for text classification and we adopt this method. The data is balanced across classes, so accuracy is used as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Features</head><p>Most DSL entries use surface features, with words and high-order character n-grams being particu- larly successful. We apply character n-grams of order 1-6 (CH1-6) and word unigrams (WD1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Feature Performance &amp; Dimensionality</head><p>In our first experiment we examine the feature space in our dataset and establish the memory re- quirements and accuracy of the feature types we use (character 1-6 n-grams and word unigrams).</p><p>For each feature type we extract the training vectors and use it to train a linear SVM model which is used to classify the standard test set. We report the feature's accuracy on the test set along with some statistics about the data: the number of features in the training data, the number of out-of- vocabulary (OOV) features 3 in the test data, and the sparsity 4 of the training data matrix. These re- sults are shown in <ref type="figure" target="#fig_0">Figure 1</ref>.  These results reveal a number of interesting patterns. Character n-grams perform better than word unigrams. The winner of the 2016 DSL task combined various character n-grams into an SVM model to obtain 89.4% accuracy on the test set. We obtain the best result of 89.2% using charac- ter 6-grams alone. The character n-gram feature grow rapidly, from 61k trigrams to 3.7m 6-grams. While accuracy plateaus at 6-grams, there is only a 1% improvement from 4-to 6-grams, but a huge feature space increase. The number of OOV test features also increases, but is relatively small.</p><p>The sparsity analysis is also revealing, showing that for many features only around 0.1% of the training matrix contains non-zero values. We can expect that this sparsity will rapidly grow with the addition of more classes to the dataset. This poses a huge problem for practical applications of LID for discriminating large number of languages.</p><p>In this regard, we can also assess how the di- mensionality cumulatively increases as languages are added, shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Features increase even as similar languages are added; we expect this trend to continue if more classes are added. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Feature Hashing Performance</head><p>Having established baseline performance for our features using the standard approach, we now experiment with applying feature hashing to the same data in order to evaluate its effectiveness for this task and compare it to the standard approach. We also assess the effect of hash size on the fea- ture collision rate, and in turn, on classification accuracy. To do this we test each feature 5 using hash sizes in the range 2 10 (1024) to 2 22 (2.1m) features, which covers most of our feature types. Our hash function is implemented using the signed 32-bit version of MurmurHash3. <ref type="bibr">6</ref> We report each feature's accuracy at every hash size, with the smallest hash that yields maximal accuracy considered to be the best result. Each feature is compared against its performance using the full feature space (baseline). These results, along with the reduction in the feature space for the best results, are listed in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>Our first observation is that every feature matches baseline performance at a hash size smaller than its full feature space. This demon- strates that feature hashing is useful for LID.</p><p>We can also assess the effect of feature colli- sions using the results, which we plot in <ref type="figure" target="#fig_5">Figure 3</ref>. We note that at the same hash size, features with a larger space perform worse. That is, with a 2 12 hash size, CHAR4 outperforms 5-and 6-grams. This is evidence of performance degradation due to hash collision. However, we see that when us- ing an appropriately sized hash, feature collisions between languages do not degrade performance.     Hash Size Exponent We also analyze the memory reduction achieved via hashing by calculating the relative difference in dimensionality between the best result and the full feature set, listed in the last column of <ref type="table" target="#tab_2">Table 1</ref>. We see very significant reductions of up to 86% in dimensionality without any performance loss. Character 4-grams yield very competitive results (0.88) with a large feature space reduction of 82% using a hash size of 2 16 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Hashing-based Ensemble Classifier</head><p>Ensemble classifiers combine multiple learners with the aim of improving accuracy through en- hanced decision making. They have been ap- plied to many tasks and shown to achieve bet- ter results compared to single-classifier methods <ref type="bibr" target="#b16">(Oza and Tumer, 2008)</ref>. By aggregating the out- puts of multiple classifiers their outputs are gen- erally considered to be more robust. Ensembles have been successfully used for LID, e.g. winning the 2015 task <ref type="bibr" target="#b10">(Malmasi and Dras, 2015a</ref>). They also achieve state-of-the-art performance for Na- tive Language Identification ( <ref type="bibr" target="#b12">Malmasi and Dras, 2017)</ref>. Could an ensemble composed of low- dimension hash-based classifiers achieve compet- itive performance?</p><p>In order to assess this we created an ensemble of our features with a hash size of 2 16 . Eval- uating against the test set, a hard voting ensem- ble achieved 88.7% accuracy while a probability- based combination obtained 89.2%. Comparing to the winning shared task accuracy of 89.4%, this is an excellent result given that only 65,536 features were used by our system. Ensemble combination boosted our best single-model 2 16 hash size result by 1.1%. This highlights the utility of ensemble methods for hashing-based feature spaces. It also shows that model combination can compensate for small performance losses caused by hashing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Conclusion</head><p>We presented the first application of feature hash- ing for language identification. Results show that hashing is highly effective for LID and can amelio- rate the dimensionality issues that can impose pro- hibitive memory requirements for some LID tasks. We further showed that reduced feature spaces with as few as 65k features can be combined in en- semble classifiers to boost performance. We also demonstrated the effect of hash collision on accu- racy, and outlined the type of analysis needed to choose the correct hash size for a given feature.</p><p>Hashing provided dimensionality reductions of up to 86% without performance degradation. This is impressive considering that no feature selection or analysis was performed, making it highly effi-cient. This reduction facilitates model loading and training as we also showed that LID data is ex- tremely sparse, with over 99% of our training ma- trix cells containing zeros. This is particularly use- ful for limited memory scenarios (e.g. handheld or embedded devices). It may also enable the use of methods requiring dense data representations, something often infeasible for large datasets.</p><p>Another key advantage of hashing is that it eliminates the need for maintaining a feature dic- tionary, making it easy to develop feature extrac- tion modules. This greatly simplifies paralleliza- tion, lending itself to online learning and dis- tributed systems, which are important issues for LID systems in our experience.</p><p>Hashing also holds promise for facilitating the use of deep learning methods for LID. In the 2016 DSL task, such systems performed "poorly com- pared to traditional classifiers"; participants cited "memory requirements and long training times" spanning several days ( <ref type="bibr" target="#b14">Malmasi et al., 2016)</ref>. Fea- ture hashing has recently been used to compress neural networks <ref type="bibr" target="#b4">(Chen et al., 2015</ref>) and its appli- cation for deep learning-based text classification may provide insightful results.</p><p>There are also downsides to hashing, including the inability to interpret feature weights and model parameters, and some minor performance loss.</p><p>Future work in this area includes evaluation on larger datasets, as well as cross-corpus experi- ments, which may also be insightful. The ap- plication of these methods to other text classifi- cation tasks, particularly those dealing with lan- guage varieties such as Native Language Identifi- cation <ref type="bibr" target="#b11">(Malmasi and Dras, 2015b)</ref>, could also pro- vide a deeper understanding about how they work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Details of our feature spaces. Training set feature counts and test set OOV counts are shown as bars (left axis, logarithmic scale). Training data sparsity and test set accuracy are shown as lines (in %, right axis). These results reveal a number of interesting patterns. Character n-grams perform better than word unigrams. The winner of the 2016 DSL task combined various character n-grams into an SVM model to obtain 89.4% accuracy on the test set. We obtain the best result of 89.2% using character 6-grams alone. The character n-gram feature grow rapidly, from 61k trigrams to 3.7m 6-grams. While accuracy plateaus at 6-grams, there is only a 1% improvement from 4-to 6-grams, but a huge feature space increase. The number of OOV test features also increases, but is relatively small. The sparsity analysis is also revealing, showing that for many features only around 0.1% of the training matrix contains non-zero values. We can expect that this sparsity will rapidly grow with the addition of more classes to the dataset. This poses a huge problem for practical applications of LID for discriminating large number of languages. In this regard, we can also assess how the dimensionality cumulatively increases as languages</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Feature growth rate as classes are added. Words (bars, left axis) and character n-grams (lines, right axis) grow as languages are added.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance (left axis) of 3 features at different hash sizes. Higher order n-grams perform worse at lower hash sizes due to collisions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Test set accuracy for hashed features at each hash size. Baseline is accuracy without hashing. 
Best result (w/ smallest hash) per row in bold. Last column is the best result's reduction in dimensionality. 
We observe that every feature matches its baseline at a hash size smaller than its full feature space. 

0.6 
0.62 
0.64 
0.66 
0.68 
0.7 
0.72 
0.74 
0.76 
0.78 
0.8 
0.82 
0.84 
0.86 
0.88 
0.9 

10 11 12 13 14 15 16 17 18 19 20 21 22 

</table></figure>

			<note place="foot" n="1"> http://ttg.uni-saarland.de/resources/DSLCC/ 2 Bosnian (BS), Argentine Spanish (ES AR), Peninsular Spanish (ES ES), Mexican Spanish (ES MX), Canadian French (FR CA), Hexagonal French (FR FR), Croatian (HR), Indonesian (ID), Malay (MY), Brazilian Portuguese (PT BR), European Portuguese (PT PT) and Serbian (SR).</note>

			<note place="foot" n="3"> i.e. features present in the test set but not the training set. 4 Matrix sparsity is the proportion of zero-valued elements.</note>

			<note place="foot" n="5"> Except character unigrams which only have 272 features. 6 https://github.com/aappleby/smhasher</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language identifier: A computer program for automatic natural-language identification of on-line text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kenneth R Beesley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual Conference of the American Translators Association</title>
		<meeting>the 29th Annual Conference of the American Translators Association</meeting>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
	<note>page 54</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Non-linear Mapping for Improved Identification of 1300+ Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ralf D Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Protein sequence classification using feature hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Silvescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteome science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">NGram-Based Text Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Cavnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trenkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SDAIR-94</title>
		<meeting>SDAIR-94<address><addrLine>Las Vegas, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="161" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tweet sentiment analysis with classifier ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia Ff Da</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">R</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hruschka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="170" to="179" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Statistical identification of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Dunning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Computing Research Laboratory, New Mexico State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Reconsidering language identification for written language resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baden</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mackinlay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bitshred: feature hashing malware for scalable triage and semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyong</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Brumley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shobha</forename><surname>Venkataraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Computer and communications security</title>
		<meeting>the 18th ACM conference on Computer and communications security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="309" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language indentification: How to distinguish similar languages?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Ljubeši´ljubeši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nives</forename><surname>Mikeli´cmikeli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damir</forename><surname>Boras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Technology Interfaces</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="541" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language Identification using Classifier Ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LT4VarDial</title>
		<meeting>LT4VarDial</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multilingual Native Language Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Engineering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Native Language Identification using Stacked Generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06541</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Arabic Dialect Identification using a Parallel Multidialectal Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eshrag</forename><surname>Refaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PACLING 2015</title>
		<meeting>PACLING 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="209" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminating between Similar Languages and Arabic Dialect Identification: A Report on the Third DSL Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Zampieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Ljubeši´ljubeši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Language Technology for Closely Related Languages, Varieties and Dialects</title>
		<meeting>the 3rd Workshop on Language Technology for Closely Related Languages, Varieties and Dialects</meeting>
		<imprint>
			<publisher>VarDial</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Japan</forename><surname>Osaka</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Classifier ensembles: Select real-world applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nikunj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kagan</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="20" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature hashing for large scale multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Attenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1113" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
