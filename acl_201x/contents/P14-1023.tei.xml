<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Don&apos;t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Mind/Brain Sciences</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Mind/Brain Sciences</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Mind/Brain Sciences</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Don&apos;t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="238" to="247"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Context-predicting models (more commonly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the literature is still lacking a systematic comparison of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A long tradition in computational linguistics has shown that contextual information provides a good approximation to word meaning, since semanti- cally similar words tend to have similar contex- tual distributions <ref type="bibr" target="#b36">(Miller and Charles, 1991)</ref>. In concrete, distributional semantic models (DSMs) use vectors that keep track of the contexts (e.g., co-occurring words) in which target terms appear in a large corpus as proxies for meaning represen- tations, and apply geometric techniques to these vectors to measure the similarity in meaning of the corresponding words <ref type="bibr" target="#b13">(Clark, 2013;</ref><ref type="bibr" target="#b16">Erk, 2012;</ref><ref type="bibr" target="#b46">Turney and Pantel, 2010)</ref>.</p><p>It has been clear for decades now that raw co- occurrence counts don't work that well, and DSMs achieve much higher performance when various transformations are applied to the raw vectors, for example by reweighting the counts for con- text informativeness and smoothing them with di- mensionality reduction techniques. This vector optimization process is generally unsupervised, and based on independent considerations (for ex- ample, context reweighting is often justified by information-theoretic considerations, dimension- ality reduction optimizes the amount of preserved variance, etc.). Occasionally, some kind of indi- rect supervision is used: Several parameter set- tings are tried, and the best setting is chosen based on performance on a semantic task that has been selected for tuning.</p><p>The last few years have seen the development of a new generation of DSMs that frame the vec- tor estimation problem directly as a supervised task, where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus ( <ref type="bibr" target="#b6">Bengio et al., 2003;</ref><ref type="bibr" target="#b14">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b15">Collobert et al., 2011;</ref><ref type="bibr" target="#b25">Huang et al., 2012;</ref><ref type="bibr" target="#b32">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b45">Turian et al., 2010</ref>). The traditional con- struction of context vectors is turned on its head: Instead of first collecting context vectors and then reweighting these vectors based on various crite- ria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear. Since similar words occur in similar contexts, the system naturally learns to assign similar vectors to similar words.</p><p>This new way to train DSMs is attractive be- cause it replaces the essentially heuristic stacking of vector transforms in earlier models with a sin- gle, well-defined supervised learning step. At the same time, supervision comes at no manual anno- tation cost, given that the context windows used for training can be automatically extracted from an unannotated corpus (indeed, they are the very same data used to build traditional DSMs). More- over, at least some of the relevant methods can ef- ficiently scale up to process very large amounts of input data. <ref type="bibr">1</ref> We will refer to DSMs built in the traditional way as count models (since they initialize vectors with co-occurrence counts), and to their training- based alternative as predict(ive) models. <ref type="bibr">2</ref> Now, the most natural question to ask, of course, is which of the two approaches is best in empirical terms. Surprisingly, despite the long tradition of extensive evaluations of alternative count DSMs on standard benchmarks ( <ref type="bibr" target="#b0">Agirre et al., 2009;</ref><ref type="bibr" target="#b3">Baroni and Lenci, 2010;</ref><ref type="bibr" target="#b10">Bullinaria and Levy, 2007;</ref><ref type="bibr" target="#b11">Bullinaria and Levy, 2012;</ref><ref type="bibr" target="#b42">Sahlgren, 2006;</ref><ref type="bibr" target="#b37">Padó and Lapata, 2007)</ref>, the existing literature contains very little in terms of direct comparison of count vs. predictive DSMs. This is in part due to the fact that context-predicting vectors were first devel- oped as an approach to language modeling and/or as a way to initialize feature vectors in neural- network-based "deep learning" NLP architectures, so their effectiveness as semantic representations was initially seen as little more than an interest- ing side effect. Sociological reasons might also be partly responsible for the lack of systematic com- parisons: Context-predictive models were devel- oped within the neural-network community, with little or no awareness of recent DSM work in com- putational linguistics.</p><p>Whatever the reasons, we know of just three works reporting direct comparisons, all limited in their scope. <ref type="bibr" target="#b25">Huang et al. (2012)</ref> compare, in pass- ing, one count model and several predict DSMs on the standard WordSim353 benchmark <ref type="table">(Table  3</ref> of their paper). In this experiment, the count model actually outperforms the best predictive ap- proach. Instead, in a word-similarity-in-context task <ref type="table">(Table 5)</ref>, the best predict model outperforms the count model, albeit not by a large margin.</p><p>Blacoe and Lapata (2012) compare count and predict representations as input to composition functions. Count vectors make for better inputs in a phrase similarity task, whereas the two repre- sentations are comparable in a paraphrase classifi- cation experiment. 3 Allocation (LDA) models ( <ref type="bibr" target="#b8">Blei et al., 2003;</ref><ref type="bibr" target="#b21">Griffiths et al., 2007)</ref>, where parameters are set to optimize the joint prob- ability distribution of words and documents. However, the fully probabilistic LDA models have problems scaling up to large data sets. <ref type="bibr">2</ref> We owe the first term to Hinrich Schütze (p.c.). Predic- tive DSMs are also called neural language models, because their supervised context prediction training is performed with neural networks, or, more cryptically, "embeddings". <ref type="bibr">3</ref> We refer here to the updated results reported in the erratum at http://homepages.inf.ed.ac.uk/ s1066731/pdf/emnlp2012erratum.pdf Finally, <ref type="bibr" target="#b35">Mikolov et al. (2013d)</ref> compare their predict models to "Latent Semantic Analysis" (LSA) count vectors on syntactic and semantic analogy tasks, finding that the predict models are highly superior. However, they provide very little details about the LSA count vectors they use. <ref type="bibr">4</ref> In this paper, we overcome the comparison scarcity problem by providing a direct evaluation of count and predict DSMs across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks. Our title already gave away what we discovered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Distributional semantic models</head><p>Both count and predict models are extracted from a corpus of about 2.8 billion tokens constructed by concatenating ukWaC, 5 the English Wikipedia <ref type="bibr">6</ref> and the British National Corpus. <ref type="bibr">7</ref> For both model types, we consider the top 300K most frequent words in the corpus both as target and context ele- ments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Count models</head><p>We prepared the count models using the DISSECT toolkit. <ref type="bibr">8</ref> We extracted count vectors from sym- metric context windows of two and five words to either side of target. We considered two weight- ing schemes: positive Pointwise Mutual Informa- tion and Local Mutual Information (akin to the widely used Log-Likelihood Ratio scheme) <ref type="bibr" target="#b17">(Evert, 2005</ref>). We used both full and compressed vec- tors. The latter were obtained by applying the Sin- gular Value Decomposition ( <ref type="bibr" target="#b20">Golub and Van Loan, 1996</ref>) or Non-negative Matrix Factorization <ref type="bibr" target="#b29">(Lee and Seung, 2000</ref>), Lin (2007) algorithm, with re- duced sizes ranging from 200 to 500 in steps of 100. In total, 36 count models were evaluated.</p><p>Count models have such a long and rich his- tory that we can only explore a small subset of the counting, weighting and compressing meth- ods proposed in the literature. However, it is worth pointing out that the evaluated parameter subset encompasses settings (narrow context win- dow, positive PMI, SVD reduction) that have been found to be most effective in the systematic explo- rations of the parameter space conducted by <ref type="bibr" target="#b10">Bullinaria and Levy (2007;</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Predict models</head><p>We trained our predict models with the word2vec toolkit. <ref type="bibr">9</ref> The toolkit implements both the skip- gram and CBOW approaches of <ref type="bibr" target="#b32">Mikolov et al. (2013a;</ref><ref type="bibr" target="#b34">2013c)</ref>. We experimented only with the latter, which is also the more computationally- efficient model of the two, following <ref type="bibr" target="#b33">Mikolov et al. (2013b)</ref> which recommends CBOW as more suitable for larger datasets.</p><p>The CBOW model learns to predict the word in the middle of a symmetric window based on the sum of the vector representations of the words in the window. We considered context windows of 2 and 5 words to either side of the central ele- ment. We vary vector dimensionality within the 200 to 500 range in steps of 100. The word2vec toolkit implements two efficient alternatives to the standard computation of the output word proba- bility distributions by a softmax classifier. Hi- erarchical softmax is a computationally efficient way to estimate the overall probability distribu- tion using an output layer that is proportional to log(unigram.perplexity(W )) instead of W (for W the vocabulary size). As an alternative, nega- tive sampling estimates the probability of an out- put word by learning to distinguish it from draws from a noise distribution. The number of these draws (number of negative samples) is given by a parameter k. We test both hierarchical softmax and negative sampling with k values of 5 and 10. Very frequent words such as the or a are not very informative as context features. The word2vec toolkit implements a method to downsize their ef- fect (and simultaneously improve speed perfor- mance). More precisely, words in the training data are discarded with a probability that is pro- portional to their frequency (capturing the same intuition that motivates traditional count vector weighting measures such as PMI). This is con- trolled by a parameter t and words that occur with higher frequency than t are aggressively subsam- pled. We train models without subsampling and with subsampling at t = 1e −5 (the toolkit page suggests 1e −3 − 1e −5 as a useful range based on empirical observations).</p><p>In total, we evaluate 48 predict models, a num- 9 https://code.google.com/p/word2vec/ ber comparable to that of the count models we consider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Out-of-the-box models</head><p>Baroni and <ref type="bibr" target="#b3">Lenci (2010)</ref> make the vectors of their best-performing Distributional Memory (dm) model available. <ref type="bibr">10</ref> This model, based on the same input corpus we use, exemplifies a "linguistically rich" count-based DSM, that relies on lemmas instead or raw word forms, and has dimensions that encode the syntactic relations and/or lexico- syntactic patterns linking targets and contexts. Ba- roni and Lenci showed, in a large scale evaluation, that dm reaches near-state-of-the-art performance in a variety of semantic tasks. We also experiment with the popular predict vectors made available by Ronan Collobert. 11 Fol- lowing the earlier literature, with refer to them as Collobert and Weston (cw) vectors. These are 100-dimensional vectors trained for two months (!) on the Wikipedia. In particular, the vectors were trained to optimize the task of choosing the right word over a random alternative in the middle of an 11-word context window (Collobert et al., 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation materials</head><p>We test our models on a variety of benchmarks, most of them already widely used to test and com- pare DSMs. The following benchmark descrip- tions also explain the figures of merit and state- of-the-art results reported in <ref type="table" target="#tab_0">Table 2</ref>.</p><p>Semantic relatedness A first set of semantic benchmarks was constructed by asking human subjects to rate the degree of semantic similarity or relatedness between two words on a numeri- cal scale. The performance of a computational model is assessed in terms of correlation between the average scores that subjects assigned to the pairs and the cosines between the corresponding vectors in the model space (following the previ- ous art, we use Pearson correlation for rg, Spear- man in all other cases). The classic data set of <ref type="bibr" target="#b41">Rubenstein and Goodenough (1965)</ref> (rg) consists of 65 noun pairs. State of the art performance on this set has been reported by Hassan and Mi- halcea (2011) using a technique that exploits the Wikipedia linking structure and word sense dis- ambiguation techniques. <ref type="bibr" target="#b18">Finkelstein et al. (2002)</ref> introduced the widely used WordSim353 set (ws) that, as the name suggests, consists of 353 pairs. The current state of the art is reached by <ref type="bibr" target="#b22">Halawi et al. (2012)</ref> with a method that is in the spirit of the predict models, but lets synonymy infor- mation from WordNet constrain the learning pro- cess (by favoring solutions in which WordNet syn- onyms are near in semantic space). <ref type="bibr" target="#b0">Agirre et al. (2009)</ref> split the ws set into similarity (wss) and re- latedness (wsr) subsets. The first contains tighter taxonomic relations, such as synonymy and co- hyponymy (king/queen) whereas the second en- compasses broader, possibly topical or syntag- matic relations (family/planning). We report state- of-the-art performance on the two subsets from the work of Agirre and colleagues, who used different kinds of count vectors extracted from a very large corpus (orders of magnitude larger than ours). Fi- nally, we use (the test section of) MEN (men), that comprises 1,000 word pairs. <ref type="bibr" target="#b9">Bruni et al. (2013)</ref>, the developers of this benchmark, achieve state-of- the-art performance by extensive tuning on ad-hoc training data, and by using both textual and image- extracted features to represent word meaning.</p><p>Synonym detection The classic TOEFL (toefl) set was introduced by <ref type="bibr" target="#b28">Landauer and Dumais (1997)</ref>. It contains 80 multiple-choice questions that pair a target term with 4 synonym candidates. For example, for the target levied one must choose between imposed (correct), believed, requested and correlated. The DSMs compute cosines of each candidate vector with the target, and pick the candidate with largest cosine as their answer. Per- formance is evaluated in terms of correct-answer accuracy. Bullinaria and Levy (2012) achieved 100% accuracy by a very thorough exploration of the count model parameter space.</p><p>Concept categorization Given a set of nominal concepts, the task is to group them into natural cat- egories (e.g., helicopters and motorcycles should go to the vehicle class, dogs and elephants into the mammal class). Following previous art, we tackle categorization as an unsupervised clustering task. The vectors produced by a model are clustered into n groups (with n determined by the gold stan- dard partition) using the CLUTO toolkit <ref type="bibr" target="#b26">(Karypis, 2003)</ref>, with the repeated bisections with global op- timization method and CLUTO's default settings otherwise (these are standard choices in the liter- ature). Performance is evaluated in terms of pu- rity, a measure of the extent to which each cluster contains concepts from a single gold category. If the gold partition is reproduced perfectly, purity reaches 100%; it approaches 0 as cluster quality deteriorates. The Almuhareb-Poesio (ap) bench- mark contains 402 concepts organized into 21 cat- egories <ref type="bibr" target="#b2">(Almuhareb, 2006</ref>). State-of-the-art purity was reached by <ref type="bibr" target="#b40">Rothenhäusler and Schütze (2009</ref></p><note type="other">) with a count model based on carefully crafted syn- tactic links. The ESSLLI 2008 Distributional Se- mantic Workshop shared-task set (esslli) contains 44 concepts to be clustered into 6 categories (Ba- roni et al., 2008) (we ignore here the 3-and 2- way higher-level partitions coming with this set). Katrenko and Adriaans (2008) reached top per- formance on this set using the full Web as a cor- pus and manually crafted, linguistically motivated patterns. Finally, the Battig (battig) test set intro- duced by Baroni et al. (2010) includes 83 concepts from 10 categories. Current state of the art was reached by the window-based count model of Ba- roni and Lenci (2010).</note><p>Selectional preferences We experiment with two data sets that contain verb-noun pairs that were rated by subjects for the typicality of the noun as a subject or object of the verb (e.g., peo- ple received a high average score as subject of to eat, and a low score as object of the same verb). We follow the procedure proposed by <ref type="bibr" target="#b3">Baroni and Lenci (2010)</ref> to tackle this challenge: For each verb, we use the corpus-based tuples they make available to select the 20 nouns that are most strongly associated to the verb as subjects or ob- jects, and we average the vectors of these nouns to obtain a "prototype" vector for the relevant ar- gument slot. We then measure the cosine of the vector for a target noun with the relevant proto- type vector (e.g., the cosine of people with the eat- ing subject prototype vector). Systems are eval- uated by Spearman correlation of these cosines with the averaged human typicality ratings. Our first data set was introduced by Ulrike Padó (2007) and includes 211 pairs (up). Top-performance was reached by the supervised count vector system of Herda˘ gdelen and Baroni (2009) (supervised in the sense that they directly trained a classifier on gold data, as opposed to the 0-cost supervision of the context-learning methods). The mcrae set ( <ref type="bibr" target="#b31">McRae et al., 1998</ref>) consists of 100 noun-verb pairs, with top performance reached by the DepDM system of <ref type="bibr" target="#b3">Baroni and Lenci (2010)</ref>, a count DSM relying on syntactic information.</p><p>Analogy While all the previous data sets are rel- atively standard in the DSM field to test traditional count models, our last benchmark was introduced in <ref type="bibr" target="#b32">Mikolov et al. (2013a)</ref> specifically to test pre- dict models. The data-set contains about 9K se- mantic and 10.5K syntactic analogy questions. A semantic question gives an example pair (brother- sister), a test word (grandson) and asks to find another word that instantiates the relation illus- trated by the example with respect to the test word (granddaughter). A syntactic question is similar, but in this case the relationship is of a grammatical nature (work-works, speak. . . speaks). Mikolov and colleagues tackle the challenge by subtract- ing the second example term vector from the first, adding the test term, and looking for the nearest neighbour of the resulting vector (what is the near- est neighbour of brother − sister + grandson?). Systems are evaluated in terms of proportion of questions where the nearest neighbour from the whole semantic space is the correct answer (the given example and test vector triples are excluded from the nearest neighbour search). <ref type="bibr" target="#b32">Mikolov et al. (2013a)</ref> reach top accuracy on the syntactic subset (ansyn) with a CBOW predict model akin to ours (but trained on a corpus twice as large). Top ac- curacy on the entire data set (an) and on the se- mantic subset (ansem) was reached by Mikolov et al. (2013c) using a skip-gram predict model. Note however that, because of the way the task is framed, performance also depends on the size of the vocabulary to be searched: <ref type="bibr" target="#b32">Mikolov et al. (2013a)</ref> pick the nearest neighbour among vectors for 1M words, <ref type="bibr" target="#b34">Mikolov et al. (2013c)</ref> among 700K words, and we among 300K words.</p><p>Some characteristics of the benchmarks we use are summarized in <ref type="table">Table 1</ref>. <ref type="table" target="#tab_0">Table 2</ref> summarizes the evaluation results. The first block of the table reports the maximum per- task performance (across all considered parameter settings) for count and predict vectors. The latter emerge as clear winners, with a large margin over count vectors in most tasks. Indeed, the predic- tive models achieve an impressive overall perfor- mance, beating the current state of the art in sev- eral cases, and approaching it in many more. It is worth stressing that, as reviewed in Section 3, the state-of-the-art results were obtained in almost all cases using specialized approaches that rely on ex- ternal knowledge, manually-crafted rules, parsing, larger corpora and/or task-specific tuning. Our predict results were instead achieved by simply downloading the word2vec toolkit and running it with a range of parameter choices recommended by the toolkit developers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The success of the predict models cannot be blamed on poor performance of the count mod- els. Besides the fact that this would not explain the near-state-of-the-art performance of the pre- dict vectors, the count model results are actually quite good in absolute terms. Indeed, in several cases they are close, or even better than those at- tained by dm, a linguistically-sophisticated count- based approach that was shown to reach top per- formance across a variety of tasks by <ref type="bibr" target="#b3">Baroni and Lenci (2010)</ref>.</p><p>Interestingly, count vectors achieve perfor- mance comparable to that of predict vectors only on the selectional preference tasks. The up task in particular is also the only benchmark on which predict models are seriously lagging behind state- of-the-art and dm performance. Recall from Sec- tion 3 that we tackle selectional preference by cre- ating average vectors representing typical verb ar- guments. We conjecture that this averaging ap- proach, that worked well for dm vectors, might be problematic for prediction-trained vectors, and we plan to explore alternative methods to build the prototypes in future research.</p><p>Are our results robust to parameter choices, or are they due to very specific and brittle settings? The next few blocks of <ref type="table" target="#tab_0">Table 2</ref> address this ques- tion. The second block reports results obtained with single count and predict models that are best in terms of average performance rank across tasks (these are the models on the top rows of tables 3 and 4, respectively). We see that, for both ap- proaches, performance is not seriously affected by using the single best setup rather than task-specific settings, except for a considerable drop in perfor- mance for the best predict model on esslli (due to the small size of this data set?), and an even more dramatic drop of the count model on ansem. A more cogent and interesting evaluation is reported in the third block of  <ref type="table">Table 1</ref>: Benchmarks used in experiments, with type of task, figure of merit (measure), original reference (source) and reference to current state-of-the-art system (soa). rg ws wss wsr men toefl ap esslli battig up mcrae an ansyn ansem best setup on each task cnt 74 <ref type="bibr">62</ref>  Table 2: Performance of count (cnt), predict (pre), dm and cw models on all tasks. See Section 3 and <ref type="table">Table 1</ref> for figures of merit and state-of-the-art results (soa). Since dm has very low coverage of the an* data sets, we do not report its performance there.</p><note type="other">relatedness Spearman Agirre et al. (2009) Agirre et al. (2009) wsr relatedness Spearman Agirre et al. (2009) Agirre et al. (2009) men relatedness Spearman Bruni et al. (2013) Bruni et al. (2013) toefl synonyms accuracy Landauer and Dumais Bullinaria and Levy (2012) (1997) ap categorization purity Almuhareb (2006) Rothenhäusler and Schütze (2009) esslli categorization purity Baroni et al. (2008) Katrenko and Adriaans (2008) battig categorization purity Baroni et al. (2010) Baroni and Lenci (2010) up sel pref Spearman Padó (2007) Herda˘ gdelen and Baroni (2009) mcrae sel pref Spearman McRae et al. (1998) Baroni and Lenci (2010) an analogy accuracy Mikolov et al. (2013a) Mikolov et al. (2013c) ansyn analogy accuracy Mikolov et al. (2013a) Mikolov et al. (2013a) ansem analogy accuracy Mikolov et al. (2013a) Mikolov et al. (2013c)</note><p>experimenter might be tempted to choose without tuning). The count model performance is severely affected by this unlucky choice (2-word window, Local Mutual Information, NMF, 400 dimensions, mean performance rank: 83), whereas the predict approach is much more robust: To put its worst in- stantiation (2-word window, hierarchical softmax, no subsampling, 200 dimensions, mean rank: 51) into perspective, its performance is more than 10% below the best count model only for the an and ansem tasks, and actually higher than it in 3 cases (note how on esslli the worst predict models per- forms much better than the best one, confirming our suspicion about the brittleness of this small data set). The fourth block reports performance in what might be the most realistic scenario, namely by tuning the parameters on a development task. Specifically, we pick the models that work best on the small rg set, and report their performance on all tasks (we obtained similar results by pick- ing other tuning sets). The selected count model is the third best overall model of its class as re- ported in <ref type="table">Table 3</ref>. The selected predict model is the fourth best model in <ref type="table">Table 4</ref>. The overall count performance is not greatly affected by this choice.</p><p>Again, predict models confirm their robustness, in that their rg-tuned performance is always close (and in 3 cases better) than the one achieved by the best overall setup. <ref type="table">Tables 3 and 4</ref> let us take a closer look at the most important count and predict parame- ters, by reporting the characteristics of the best models (in terms of average performance-based ranking across tasks) from both classes. For the count models, PMI is clearly the better weight- ing scheme, and SVD outperforms NMF as a di- mensionality reduction technique. However, no compression at all (using all 300K original dimen- sions) works best. Compare this to the best over- all predict vectors, that have 400 dimensions only, making them much more practical to use. For the predict models, we observe in <ref type="table">Table 4</ref> that nega- tive sampling, where the task is to distinguish the target output word from samples drawn from the noise distribution, outperforms the more costly hi- erarchical softmax method. Subsampling frequent words, which downsizes the importance of these words similarly to PMI weighting in count mod- els, is also bringing significant improvements.</p><p>Finally, we go back to <ref type="table" target="#tab_0">Table 2</ref> to point out the poor performance of the out-of-the-box cw model. window weight compress dim <ref type="table" target="#tab_0">. mean  rank  2  PMI  no  300K 35  5  PMI  no  300K 38  2  PMI  SVD  500 42  2  PMI  SVD  400 46  5  PMI  SVD  500 47  2  PMI  SVD  300 50  5  PMI  SVD  400 51  2  PMI  NMF  300 52  2  PMI  NMF  400 53  5</ref> PMI SVD 300 53 <ref type="table">Table 3</ref>: Top count models in terms of mean performance-based model ranking across all tasks. The first row states that the window-2, PMI, 300K count model was the best count model, and, across all tasks, its average rank, when ALL models are decreasingly ordered by performance, was 35. See Section 2.1 for explanation of the parameters.</p><p>We must leave the investigation of the parameters that make our predict vectors so much better than cw (more varied training corpus? window size? objective function being used? subsampling? . . . ) to further work. Still, our results show that it's not just training by context prediction that ensures good performance. The cw approach is very popu- lar (for example both <ref type="bibr" target="#b25">Huang et al. (2012)</ref> and Bla- coe and Lapata (2012) used it in the studies we dis- cussed in Section 1). Had we also based our sys- tematic comparison of count and predict vectors on the cw model, we would have reached opposite conclusions from the ones we can draw from our word2vec-trained vectors!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper has presented the first systematic com- parative evaluation of count and predict vectors. As seasoned distributional semanticists with thor- ough experience in developing and using count vectors, we set out to conduct this study because we were annoyed by the triumphalist overtones of- ten surrounding predict models, despite the almost complete lack of a proper comparison to count vectors. <ref type="bibr">12</ref> Our secret wish was to discover that it is all hype, and count vectors are far superior to their predictive counterparts. A more realistic expec-win. hier.</p><p>neg. subsamp. dim mean softm. samp. <ref type="table" target="#tab_0">rank  5  no  10  yes  400 10  2  no  10  yes  300 13  5  no  5  yes  400 13  5  no  5  yes  300 13  5  no  10  yes  300 13  2  no  10  yes  400 13  2  no  5  yes  400 15  5  no  10  yes  200 15  2  no  10  yes  500 15  2  no  5  yes  300 16   Table 4</ref>: Top predict models in terms of mean performance-based model ranking across all tasks. See Section 2.2 for explanation of the parameters.</p><p>tation was that a complex picture would emerge, with predict and count vectors beating each other on different tasks. Instead, we found that the pre- dict models are so good that, while the triumphal- ist overtones still sound excessive, there are very good reasons to switch to the new architecture. However, due to space limitations we have only focused here on quantitative measures: It remains to be seen whether the two types of models are complementary in the errors they make, in which case combined models could be an interesting av- enue for further work. The space of possible parameters of count DSMs is very large, and it's entirely possible that some options we did not consider would have im- proved count vector performance somewhat. Still, given that the predict vectors also outperformed the syntax-based dm model, and often approxi- mated state-of-the-art performance, a more profic- uous way forward might be to focus on parameters and extensions of the predict models instead: Af- ter all, we obtained our already excellent results by just trying a few variations of the word2vec de- faults. Add to this that, beyond the standard lex- ical semantics challenges we tested here, predict models are currently been successfully applied in cutting-edge domains such as representing phrases <ref type="bibr" target="#b34">(Mikolov et al., 2013c;</ref>) or fus- ing language and vision in a common semantic space ( <ref type="bibr" target="#b19">Frome et al., 2013;</ref><ref type="bibr" target="#b44">Socher et al., 2013)</ref>.</p><p>Based on the results reported here and the con- siderations we just made, we would certainly rec- ommend anybody interested in using DSMs for theoretical or practical applications to go for the predict models, with the important caveat that they are not all created equal (cf. the big difference be- tween word2vec and cw models). At the same time, given the large amount of work that has been carried out on count DSMs, we would like to ex- plore, in the near future, how certain questions and methods that have been considered with re- spect to traditional DSMs will transfer to predict models. For example, the developers of Latent Semantic Analysis <ref type="bibr" target="#b28">(Landauer and Dumais, 1997</ref>), Topic Models ( <ref type="bibr" target="#b21">Griffiths et al., 2007)</ref> and related DSMs have shown that the dimensions of these models can be interpreted as general "latent" se- mantic domains, which gives the corresponding models some a priori cognitive plausibility while paving the way for interesting applications. An- other important line of DSM research concerns "context engineering": There has been for exam- ple much work on how to encode syntactic in- formation into context features <ref type="bibr" target="#b37">(Padó and Lapata, 2007)</ref>, and more recent studies construct and com- bine feature spaces expressing topical vs. func- tional information <ref type="bibr" target="#b47">(Turney, 2012)</ref>. To give just one last example, distributional semanticists have looked at whether certain properties of vectors re- flect semantic relations in the expected way: e.g., whether the vectors of hypernyms "distribution- ally include" the vectors of hyponyms in some mathematical precise sense.</p><p>Do the dimensions of predict models also en- code latent semantic domains? Do these models afford the same flexibility of count vectors in cap- turing linguistically rich contexts? Does the struc- ture of predict vectors mimic meaningful seman- tic relations? Does all of this even matter, or are we on the cusp of discovering radically new ways to tackle the same problems that have been ap- proached as we just sketched in traditional distri- butional semantics?</p><p>Either way, the results of the present investiga- tion indicate that these are important directions for future research in computational semantics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 ,</head><label>2</label><figDesc>where we see what happens if we use the single models with worst performance across tasks (recall from Section 2 above that, in any case, we are exploring a space of reasonable parameter settings, of the sort that an</figDesc><table>name task 

measure 
source 
soa 
rg 
relatedness 
Pearson 
Rubenstein and Goodenough Hassan and Mihalcea (2011) 
(1965) 
ws 
relatedness 
Spearman Finkelstein et al. (2002) 
Halawi et al. (2012) 
wss 
</table></figure>

			<note place="foot" n="1"> The idea to directly learn a parameter vector based on an objective optimum function is shared by Latent Dirichlet</note>

			<note place="foot" n="4"> Chen et al. (2013) present an extended empirical evaluation, that is however limited to alternative context-predictive models, and does not include the word2vec variant we use here. 5 http://wacky.sslmit.unibo.it 6 http://en.wikipedia.org 7 http://www.natcorp.ox.ac.uk 8 http://clic.cimec.unitn.it/composes/ toolkit/</note>

			<note place="foot" n="10"> http://clic.cimec.unitn.it/dm/ 11 http://ronan.collobert.com/senna/</note>

			<note place="foot" n="12"> Here is an example, where word2vec is called the crown jewel of natural language processing: http://bit.ly/ 1ipv72M</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We acknowledge ERC 2011 Starting Independent Research Grant n. 283554 (COMPOSES).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kravalova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Marius Pasça, and Aitor Soroa</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A study on similarity and relatedness using distributional and WordNet-based approaches</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL<address><addrLine>Boulder, CO</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Attributes in Lexical Acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulrahman</forename><surname>Almuhareb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>University of Essex</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Phd thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributional Memory: A general framework for corpus-based semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="673" to="721" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Evert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<title level="m">Bridging the Gap between Semantic Theory and Computational Simulations: Proceedings of the ESSLLI Workshop on Distributional Lexical Semantic. FOLLI</title>
		<meeting><address><addrLine>Hamburg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Strudel: A distributional semantic model based on properties and types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="222" to="254" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comparison of vector-based representations for semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Blacoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="546" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<ptr target="http://clic.cimec.unitn.it/marco/publications/mmds-jair.pdf" />
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>In press</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extracting semantic representations from word co-occurrence statistics: Stop-lists, stemming and SVD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="890" to="907" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The expressive power of word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<ptr target="https://sites.google.com/site/deeplearningicml2013/accepted_papers" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML Workshop on Deep Learning for Audio, Speech and Language Processing</title>
		<meeting>the ICML Workshop on Deep Learning for Audio, Speech and Language Processing<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Published online</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vector space models of lexical meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<ptr target="http://www.cl.cam.ac.uk/˜sc609/pubs/sem_handbook.pdf" />
	</analytic>
	<monogr>
		<title level="m">Handbook of Contemporary Semantics</title>
		<editor>Shalom Lappin and Chris Fox</editor>
		<meeting><address><addrLine>Malden, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>2nd ed. Blackwell. In press</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vector space models of word meaning and phrase meaning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Linguistics Compass</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="635" to="653" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The Statistics of Word Cooccurrences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><forename type="middle">Evert</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Stuttgart University</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Ph.D dissertation</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DeViSE: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS<address><addrLine>Lake Tahoe, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Van Loan</surname></persName>
		</author>
		<title level="m">Matrix Computations</title>
		<meeting><address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<publisher>JHU Press</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note>3rd ed.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Topics in semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="211" to="244" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale learning of word relatedness with constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Halawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1406" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic relatedness using salient semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samer</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="884" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BagPack: A general framework to represent semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaç</forename><surname>Herda˘ Gdelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of GEMS</title>
		<meeting>GEMS<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">CLUTO: A clustering toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<idno>02-017</idno>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>University of Minnesota Department of Computer Science</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Qualia structures and their impact on the concrete noun categorization task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Katrenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Adriaans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ESSLLI Workshop on Distributional Lexical Semantics</title>
		<meeting>the ESSLLI Workshop on Distributional Lexical Semantics<address><addrLine>Hamburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A solution to Plato&apos;s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="240" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Algorithms for Non-negative Matrix Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Projected gradient methods for Nonnegative Matrix Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2756" to="2779" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Mcrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Spivey-Knowlton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tanenhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="283" to="312" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1301.3781/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1309.4168" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS<address><addrLine>Lake Tahoe, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contextual correlates of semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Cognitive Processes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dependency-based construction of semantic space models</title>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The Integration of Syntax and Semantic Plausibility in a Wide-Coverage Model of Sentence Processing. Dissertation, Saarland University</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><surname>Padó</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Saarbrücken</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised classification with dependency based word spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rothenhäusler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of GEMS</title>
		<meeting>GEMS<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The Word-Space Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Stockholm University</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Ph.D dissertation</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS<address><addrLine>Lake Tahoe, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Domain and function: A dualspace model of semantic relations and compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="533" to="585" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
