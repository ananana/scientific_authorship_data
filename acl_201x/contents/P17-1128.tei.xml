<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transductive Non-linear Learning for Chinese Hypernym Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoying</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>He</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Trustworthy Computing</orgName>
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transductive Non-linear Learning for Chinese Hypernym Prediction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1394" to="1404"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1128</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Finding the correct hypernyms for entities is essential for taxonomy learning, fine-grained entity categorization, knowledge base construction, etc. Due to the flexibility of the Chinese language, it is challenging to identify hypernyms in Chinese accurately. Rather than extracting hyper-nyms from texts, in this paper, we present a transductive learning approach to establish mappings from entities to hyper-nyms in the embedding space directly. It combines linear and non-linear embedding projection models, with the capacity of encoding arbitrary language-specific rules. Experiments on real-world datasets illustrate that our approach outperforms previous methods for Chinese hypernym prediction .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A hypernym of an entity characterizes the type or the class of the entity. For example, the word country is the hypernym of the entity Canada. The accurate prediction of hypernyms benefits a variety of NLP tasks, such as taxonomy learn- ing ( <ref type="bibr" target="#b37">Wu et al., 2012;</ref><ref type="bibr" target="#b6">Fu et al., 2014</ref>), fine-grained entity categorization <ref type="bibr" target="#b25">(Ren et al., 2016)</ref>, knowledge base construction ( <ref type="bibr" target="#b32">Suchanek et al., 2007)</ref>, etc.</p><p>In previous work, the detection of hypernyms requires lexical, syntactic and/or semantic analy- sis of relations between entities and their respec- tive hypernyms from a language-specific knowl- edge source. For example, <ref type="bibr" target="#b10">Hearst (1992)</ref> is the pi- oneer work to extract is-a relations from a text cor- pus based on handcraft patterns. The following- up work mostly focuses on is-a relation extrac- tion using automatically generated patterns (Snow * Corresponding author.</p><p>et al., <ref type="bibr">2004</ref>; <ref type="bibr" target="#b26">Ritter et al., 2009;</ref><ref type="bibr" target="#b28">Sang and Hofmann, 2009;</ref><ref type="bibr" target="#b13">Kozareva and Hovy, 2010</ref>) and re- lation inference based on distributional similar- ity measures ( <ref type="bibr" target="#b12">Kotlerman et al., 2010;</ref><ref type="bibr" target="#b14">Lenci and Benotto, 2012;</ref><ref type="bibr" target="#b30">Shwartz et al., 2016)</ref>.</p><p>While these approaches have relatively high precision over English corpora, extracting hy- pernyms for entities is still challenging for Chi- nese. From the linguistic perspective, Chinese is a lower-resourced language with very flexible expressions and grammatical rules ( ). For instance, there are no word spaces, ex- plicit tenses and voices, and distinctions between singular and plural forms in Chinese. The order of words can be changed flexibly in sentences. Hence, as previous research indicates, hypernym extraction methods for English are not necessarily suitable for the Chinese language ( <ref type="bibr" target="#b6">Fu et al., 2014;</ref><ref type="bibr" target="#b35">Wang and He, 2016)</ref>.</p><p>Based on such conditions, several classification methods are proposed to distinguish is-a and not- is-a relations based on Chinese encyclopedias ( <ref type="bibr" target="#b18">Lu et al., 2015;</ref><ref type="bibr" target="#b16">Li et al., 2015)</ref>. Similar to Prince- ton WordNet, a few Chinese wordnets have also been developed ( <ref type="bibr" target="#b11">Huang et al., 2004;</ref><ref type="bibr" target="#b38">Xu et al., 2008;</ref><ref type="bibr" target="#b36">Wang and Bond, 2013)</ref>. The most recent ap- proaches for Chinese is-a relation extraction ( <ref type="bibr" target="#b6">Fu et al., 2014;</ref><ref type="bibr" target="#b35">Wang and He, 2016</ref>) use word em- bedding based linear projection models to map embeddings of hyponyms to those of their hyper- nyms, which outperform previous algorithms.</p><p>However, we argue that these projection-based methods may have three potential limitations: (i) Only positive is-a relations are used for projec- tion learning. The distinctions between is-a and not-is-a relations in the embedding space are not modeled. (ii) These methods lack the capacity to encode linguistic rules, which are designed by lin- guists and usually have high precision. (iii) It as- sumes that the linguistic regularities of is-a rela-tions can be solely captured by single or multiple linear projection models.</p><p>In this paper, we address these limitations by a two-stage transductive learning approach. It dis- tinguishes is-a and not-is-a relations given a Chi- nese word/phrase pair as input. In the initial stage, we train linear projection models on positive and negative training data separately and predict is- a relations jointly. In the transductive learning stage, the initial prediction results, linguistic rules and the non-linear mappings from entities to hy- pernyms are optimized simultaneously in a uni- fied framework. This optimization problem can be efficiently solved by blockwise gradient descent. We evaluate our method over two public datasets and show that it outperforms state-of-the-art ap- proaches for Chinese hypernym prediction.</p><p>The rest of this paper is organized as follows. We summarize the related work in Section 2. Our approach is introduced in Section 3. Experimental results are presented in Section 4. We conclude our paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we overview the related work on hypernym prediction and discuss the challenges of Chinese hypernym detection.</p><p>Pattern based methods identify is-a relations from texts by handcraft or automatically generated patterns. Hearst patterns <ref type="bibr" target="#b10">(Hearst, 1992)</ref> are lexical patterns in English that are employed to extract is- a relations for taxonomy construction ( <ref type="bibr" target="#b37">Wu et al., 2012)</ref>. Automatic approaches mostly use itera- tive learning paradigms such that the system learns new is-a relations and patterns simultaneously. A few relevant studies can be found in <ref type="bibr" target="#b2">(Caraballo, 1999;</ref><ref type="bibr" target="#b5">Etzioni et al., 2004;</ref><ref type="bibr" target="#b27">Sang, 2007;</ref><ref type="bibr" target="#b23">Pantel and Pennacchiotti, 2006;</ref><ref type="bibr" target="#b13">Kozareva and Hovy, 2010)</ref>. To avoid "semantic drift" in iterations, <ref type="bibr" target="#b31">Snow et al. (2004)</ref> train a hypernym classifier based on syn- tactic features based on parse trees. <ref type="bibr" target="#b3">Carlson et al. (2010)</ref> exploit multiple learners to extract relations via coupled learning. These approaches are not ef- fective for Chinese for two reasons: i) Chinese is-a relations are expressed in a highly flexible man- ner ( <ref type="bibr" target="#b6">Fu et al., 2014</ref>) and ii) the accuracy of basic NLP tasks such as dependency parsing still need improvement for Chinese ( <ref type="bibr" target="#b15">Li et al., 2013)</ref>.</p><p>Inference based methods take advantage of distributional similarity measures (DSM) to in- fer relations between words. They assume that a hypernym may appear in all contexts of the hy- ponyms and a hyponym can only appear in part of the contexts of its hypernyms. In previous work, <ref type="bibr" target="#b12">Kotlerman et al. (2010)</ref> design directional DSMs to model the asymmetric property of is-a relations. Other DSMs are introduced in <ref type="bibr" target="#b1">(Bhagat et al., 2007;</ref><ref type="bibr" target="#b33">Szpektor et al., 2007;</ref><ref type="bibr" target="#b14">Lenci and Benotto, 2012;</ref><ref type="bibr" target="#b29">Santus et al., 2014</ref>). <ref type="bibr" target="#b30">Shwartz et al. (2016)</ref> combine dependency parsing and DSM to improve the performance of hypernymy detection. The reason why DSM is not effective for Chinese is that the contexts of entities in Chinese are flexi- ble and sparse.</p><p>Encyclopedia based methods take encyclo- pedias as knowledge sources to construct tax- onomies. <ref type="bibr" target="#b24">Ponzetto and Strube (2007)</ref> design fea- tures from multiple aspects to predict is-a rela- tions between entities and categories in English Wikipedia. The taxonomy in <ref type="bibr">YAGO (Suchanek et al., 2007</ref>) is constructed by linking concep- tual categories in Wikipedia to WordNet synsets <ref type="bibr" target="#b21">(Miller, 1995)</ref>. For Chinese, <ref type="bibr" target="#b16">Li et al. (2015)</ref> pro- pose an SVM-based approach to build a large Chi- nese taxonomy from Wikipedia. Similar clas- sification based algorithms are presented in ( <ref type="bibr" target="#b7">Fu et al., 2013;</ref><ref type="bibr" target="#b18">Lu et al., 2015)</ref>. Due to the lack of Chinese version of WordNet, several Chinese se- mantic dictionaries have been conducted, such as Sinica BOW ( <ref type="bibr" target="#b11">Huang et al., 2004</ref>), SEW ( <ref type="bibr" target="#b38">Xu et al., 2008)</ref>, COW ( <ref type="bibr" target="#b36">Wang and Bond, 2013)</ref>, etc. These approaches have higher accuracy than mining hy- pernym relations from texts directly. However, they heavily rely on existing knowledge sources and are difficult to extend to different domains.</p><p>To tackle these challenges, word embedding based methods directly model the task of hyper- nym prediction as learning a mapping from en- tity vectors to their respective hypernym vectors in the embedding space. The vectors can be pre- trained by neural language models ( <ref type="bibr" target="#b20">Mikolov et al., 2013</ref>). For the Chinese language, <ref type="bibr" target="#b6">Fu et al. (2014)</ref> train piecewise linear projection models based on a Chinese thesaurus. The state-of-the-art method ( <ref type="bibr" target="#b35">Wang and He, 2016</ref>) combines an iterative learn- ing procedure and Chinese Hearst-style patterns to improve the performance of projection mod- els. They can reduce data noise by avoiding direct parsing of Chinese texts, but still capture the lin- guistic regularities of is-a relations based on word embeddings. Additionally, several work aims to study how to combine word embeddings for re-lation classification, such as ( <ref type="bibr" target="#b22">Mirza and Tonelli, 2016)</ref>. In our paper, we extend these approaches by modeling non-linear mappings from entities to hypernyms and adding linguistic rules via a uni- fied transductive learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>This section begins with a brief overview of our approach. After that, the detailed steps and the learning algorithm are introduced in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Given a word/phrase pair (x i , y i ), the goal of our task is to learn a classification model to predict whether y i is the hypernym of x i .</p><p>As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, our approach has two stages: initial stage and transductive learning stage. The input is a positive is-a set D + , a neg- ative is-a set D − and an unlabeled set D U , all of which are the collections of word/phrase pairs. Denote x i as the embedding vector of word x i , pre-trained and stored in a lookup table. In the ini- tial stage, we train a linear projection model over D + such that for each (x i , y i ) ∈ D + , a projection matrix maps the entity vector x i to its hypernym vector y i . A similar model is also trained over D − . Based on the two models, we estimate the prediction score and the confidence score for each (x i , y i ) ∈ D U . In the transductive learning stage, a joint optimization problem is formed to learn the final prediction score for each (x i , y i ) ∈ D U . It aims to minimize the prediction errors based on the human labeled data, the initial model predic- tion and linguistic rules. It also employs non- linear mappings to capture linguistic regularities of is-a relations other than linear projections.  <ref type="figure" target="#fig_0">Figure 1</ref>: General framework of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Initial Model Training</head><p>The initial stage models how entities are mapped to their hypernyms or non-hypernyms by projec- tion learning. We first train a Skip-gram model ( <ref type="bibr" target="#b20">Mikolov et al., 2013</ref>) to learn word embeddings over a large text corpus. Inspired by ( <ref type="bibr" target="#b6">Fu et al., 2014;</ref><ref type="bibr" target="#b35">Wang and He, 2016)</ref>, for each (x i , y i ) ∈ D + , we assume there is a positive projection model such that M + x i ≈ y i where M + is an |x i |×|x i | projection matrix <ref type="bibr">1</ref> . However, this model does not capture the semantics of not-is-a rela- tions. Thus, we learn a negative projection model</p><formula xml:id="formula_0">M − x i ≈ y i where (x i , y i ) ∈ D − .</formula><p>This approach is equivalent to learning two separate translation models within the same semantic space. For pa- rameter estimation, we minimize the two follow- ing objectives:</p><formula xml:id="formula_1">J(M + ) = 1 2 (x i ,y i )∈D + M + x i −y i 2 2 + λ 2 M + 2 F J(M − ) = 1 2 (x i ,y i )∈D − M − x i −y i 2 2 + λ 2 M − 2 F</formula><p>where λ &gt; 0 is a Tikhonov regularization param- eter ( <ref type="bibr" target="#b9">Golub et al., 1999</ref>).</p><p>In the testing phase, for each (</p><formula xml:id="formula_2">x i , y i ) ∈ D U , denote d + (x i , y i ) = M + x i − y i 2 and d − (x i , y i ) = M − x i −y i 2 .</formula><p>The prediction score is defined as:</p><formula xml:id="formula_3">score(x i , y i ) = tanh(d − (x i , y i ) − d + (x i , y i ))</formula><p>where score(x i , y i ) ∈ (−1, 1). Higher prediction score indicates there is a larger probability of an is-a relation between x i and y i . We choose the hy- perbolic tangent function rather than the sigmoid function to avoid the widespread saturation of sig- moid function ( <ref type="bibr" target="#b19">Menon et al., 1996)</ref>. Because the semantics of Chinese is-a and not-is-a relations are complicated and difficult to model ( <ref type="bibr" target="#b6">Fu et al., 2014</ref>), we do not impose explicit connections be- tween M + and M − and let the algorithm learn the parameters automantically.</p><p>The difference between d + (x i , y i ) and d − (x i , y i ) can be also used to indicate whether the models are confident enough to make a prediction.</p><p>In this paper, we calculate the confidence score as:</p><formula xml:id="formula_4">conf (x i , y i ) = |d + (x i , y i ) − d − (x i , y i )| max{d + (x i , y i ), d − (x i , y i )}</formula><p>where conf (x i , y i ) ∈ (0, 1). Higher confidence score means that there is a larger probability that the models can predict whether there is an is-a relation between x i and y i correctly. This score gives different data instances different weights in the transductive learning stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Transductive Non-linear Learning</head><p>Although linear projection methods are effective for Chinese hypernym prediction, it does not en- code non-linear transformation and only leverages the positive data. We present an optimization framework for non-linear mapping utilizing both labeled and unlabeled data and linguistic rules by transductive learning ( <ref type="bibr" target="#b8">Gammerman et al., 1998;</ref><ref type="bibr" target="#b4">Chapelle et al., 2006</ref>).</p><p>Let F i be the final prediction score of the word/phrase pair (x i , y i ). In the initialization stage of our algorithm, we set</p><formula xml:id="formula_5">F i = 1 if (x i , y i ) ∈ D + , F i = −1 if (x i , y i ) ∈ D − and set F i ran- domly in (−1, 1) if (x i , y i ) ∈ D U .</formula><p>In matrix rep- resentation, denote F as the m × 1 final prediction vector where m = |D + | + |D − | + |D U |. F i is the ith element in F. The three components in our transductive learning model are as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Initial Prediction</head><p>Denote S as an m × 1 initial prediction vector. We</p><formula xml:id="formula_6">set S i = 1 if (x i , y i ) ∈ D + , S i = −1 if (x i , y i ) ∈ D − and S i = score(x i , y i ) if (x i , y i ) ∈ D U .</formula><p>In order to encode the confidence of model predic- tion, we define W as an m × m diagonal weight matrix. The element in the ith row and the jth col- umn of W is set as follows:</p><formula xml:id="formula_7">W i,j =      conf (x i , y i ) i = j, (x i , y i ) ∈ D U 1 i = j, (x i , y i ) ∈ D + ∪ D − 0 Otherwise</formula><p>The objective function is defined as:</p><formula xml:id="formula_8">O s = W(F − S) 2</formula><p>2 , which encodes the hypothesis that the final prediction should be similar to the initial prediction for unlabeled data or human labeling for training data. The weight matrix W gives the largest weight (i.e., 1) to all the pairs in D + ∪ D − and a larger weight to the pair (x i , y i ) ∈ D U if the initial prediction is more confident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Linguistic Rules</head><p>Although linguistic rules can only cover a few cir- cumstances, they are effective to guide the learn- ing process. For Chinese hypernym prediction, <ref type="bibr" target="#b16">Li et al. (2015)</ref> study the word formation of con- ceptual categories in Chinese Wikipedia. In our model, let C be the collection of linguistic rules. γ i is the true positive (or negative) rate with re- spect to the respective positive (or negative) rule c i ∈ C, estimated over the training set. Consid- ering the word formation of Chinese entities and hypernyms, we design one positive rule (i.e., P1) and two negative rules (i.e., N1 and N2), shown in <ref type="table">Table 1</ref>.</p><p>Let R be an m × 1 linguistic rule vector and R i is the ith element in R. For training data, we</p><formula xml:id="formula_9">set R i = 1 if (x i , y i ) ∈ D + and R i = −1 if (x i , y i ) ∈ D − ,</formula><p>which follows the same settings as those in S. For unlabeled pairs that do not match any linguistic rules in C, we update R i = F i in each iteration of the learning process, meaning no loss for errors imposed in this part.</p><p>For other conditions, denote C (x i ,y i ) ⊆ C as the collection of rules that (x i , y i ) matches. If C (x i ,y i ) are positive rules, we set R i as follows:</p><formula xml:id="formula_10">R i = max{F i , max c j ∈C (x i ,y i ) γ j }</formula><p>Similarly, if C (x i ,y i ) are negative rules, we have:</p><formula xml:id="formula_11">R i = − max{−F i , max c j ∈C (x i ,y i ) γ j }</formula><p>which means F i receives a penalty only if F i &lt; max c j ∈C (x i ,y i ) γ j for pairs that match positive rules or F i &gt; − max c j ∈C (x i ,y i ) γ j for negative rules 2 . The objective function is: O r = F−R 2 2 . In this way, our model can integrate arbitrary "soft" constraints, making it robust to false posi- tives or negatives introduced by these rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Non-linear Learning</head><p>TransLP is a transductive label propagation frame- work ( <ref type="bibr" target="#b17">Liu and Yang, 2015</ref>) for link prediction, previously used for applications such as text clas- sification ( <ref type="bibr" target="#b39">Xu et al., 2016)</ref>. In our work, we extend their work for our task, modeling non-linear map- pings from entities to hypernyms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P1</head><p>The head word of the entity x matches that of the candidate hypernym y. For example, 动物 (Animal) is the correct hypernym of 哺乳动物 (Mammal). N1 The head word of the entity x matches the non-head word of the candidate hypernym y. For example, 动物学 (Zoology) is not a hypernym of 哺乳动物 (Mammal). N2 The head word of the candidate hypernym y matches an entry in a Chinese lexicon extended based on the lexicon used in <ref type="bibr" target="#b16">Li et al. (2015)</ref>. It consists of 184 non-taxonomic, thematic words such as 政治(Politics), 军事(Military), etc. <ref type="table">Table 1</ref>: Three linguistic rules used in our work for Chinese hypernym prediction.</p><p>For is-a relations, we find that if y is the hyper- nym of x, it is likely that y is the hypernym of enti- ties that are semantically close to x. For example, if we know United States is a country, we can infer country is the hypernym of similar en- tities such as Canada, Australia, etc. This in- tuition can be encoded in the similarity of the two pairs p i = (x i , y i ) and p j = (x j , y j ):</p><formula xml:id="formula_12">sim(p i , p j ) = cos(x i , x j ) y i = y j 0 otherwise (1)</formula><p>where x i is the embedding vector of x i 3 . This similarity indicates there exists a non- linear mapping from entities to hypernyms, which can not be encoded in linear projection based methods ( <ref type="bibr" target="#b6">Fu et al., 2014;</ref><ref type="bibr" target="#b35">Wang and He, 2016)</ref>. Based on TransLP ( <ref type="bibr" target="#b17">Liu and Yang, 2015)</ref>, this intuition can be model as propagating class la- bels (is-a or not-is-a) of labeled word/phrase pairs to similar unlabeled ones based on Eq. (1). For example, the score of is-a relations between United State and country will propagate to pairs such as (Canada, country) and (Australia, country) by random walks.</p><p>Denote F * as the optimal solution of the prob- lem min O s + O r . Inspired by ( <ref type="bibr" target="#b17">Liu and Yang, 2015;</ref><ref type="bibr" target="#b39">Xu et al., 2016)</ref>, we can add a Gaussian prior N (F * , Σ) to F where Σ is the covariance matrix and Σ i,j = sim(p i , p j ). Hence the opti- mization objective of this part is defined as: O n = F T Σ −1 F which is linearly proportional to the negative likelihood of the Gaussian random field prior. This means we minimize the training er- ror and encourage F to have a smooth propagation with respect to the similarities among pairs defined by Eq. (1) at the same time. <ref type="bibr">3</ref> We only consider the similarity between entities and not candidate hypernyms because the similar rule for candidate hypernyms is not true. For example, nouns close to country in our Skip-gram model are region, department, etc. They are not all correct hypernyms of United States, Canada, Australia, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Joint Optimization</head><p>By combining the three components together, we minimize the following function:</p><formula xml:id="formula_13">J(F) = O s + O r + µ 1 2 O n + µ 2 2 F 2 2 (2)</formula><p>where F 2 2 imposes an additional smooth l 2 - regularization on F. µ 1 and µ 2 are regularization parameters that can be tuned manually.</p><p>Based on the convexity of the optimization problem, we can learn the optimal values of F is via gradient descent. The derivative of F with re- spect to J(F) is:</p><formula xml:id="formula_14">dJ(F) dF = W 2 (F−S)+(F−R)+µ 1 Σ −1 F+µ 2 F</formula><p>which is computationally expensive when m is large. After W 2 , S, R and Σ −1 are pre-computed, the runtime complexity of the loop of gradient de- scent is O(tm 2 ) where t is the number of itera- tions.</p><p>To speed up the learning process, we introduce a blockwise gradient descent technique. From the definition of Eq. (2), we can see that the optimal values of F i and F j with respect to (x i , y i ) and (x j , y j ) are irrelevant if y i = y j . Therefore, the original optimization problem can be decomposed and solved separately according to different can- didate hypernyms.</p><p>Let H be the collection of candidate hypernyms in D U . For each h ∈ H, denote D h as the col- lection of word/phase pairs in D + ∪ D − ∪ D U that share the same candidate hypernym h. The original problem can be decomposed into |H| op- timization subproblems over D h for each h ∈ H. Denote W h , S h , R h , F h and Σ h as the weight matrix, the initial prediction vector, the rule pre- diction vector, the final prediction vector and the entity similarity covariance matrix with respect D h . The objective function can be rewritten as:</p><formula xml:id="formula_15">J(F) = h∈H˜J h∈H˜ h∈H˜J(F h ) where˜J where˜ where˜J(F h ) = W h (F h − S h ) 2 2 + F h − R h 2 2 + µ 1 2 F T h Σ −1 h F h + µ 2 2 F h 2 2</formula><p>We additionally use (n) to denote the values of matrices or vectors in the nth iteration. F (n) h is it- eratively updated based on the following equation:</p><formula xml:id="formula_16">F (n+1) h = F (n) h − η · d ˜ J(F (n) h ) dF (n) h</formula><p>where η is the learning rate. To this end, we present the learning algorithm in Algorithm 1. Estimate γ i over the training set; 7: end for</p><formula xml:id="formula_17">8: while F (n) h − F (n+1) h 2 &lt; 10 −3 do 9:</formula><p>Compute R </p><formula xml:id="formula_18">Calculate d ˜ J(F (n) h ) dF (n) h = W 2 h (F (n) h − S h ) + (F (n) h − R (n) h ) + µ 1 Σ −1 h F (n) h + µ 2 F (n) h ; 11:</formula><p>Compute F (n+1) h for the next iteration:</p><formula xml:id="formula_19">F (n+1) h = F (n) h − η · d ˜ J(F (n) h ) dF (n) h ; 12:</formula><p>Update counter n = n + 1; 13: end while</p><formula xml:id="formula_20">14: return Final prediction vector F (n+1) h ;</formula><p>The runtime complexity of this algorithm is O(</p><formula xml:id="formula_21">h∈D h t h |D h | 2 )</formula><p>where t h is the number of it- erations to solve the subproblem over D h . Al- though we do not know the upper bounds on the numbers of iterations of these two learning tech- niques, the runtime complexity can be reduced by blockwise gradient descent for two reasons: i) h∈D h |D h | ≤ m and ii) t h has a large probabil- ity to be smaller than t due to the smaller num- ber of data instances. This technique can be also viewed as optimizing Eq. (2) based on blockwise matrix computation.</p><p>Finally, for each (x i , y i ) ∈ D U , we predict that y i is a hypernym of x i if F i &gt; θ where θ ∈ (−1, 1) is a threshold tuned on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct experiments to eval- uate our method. Section 4.1 to Section 4.5 re- port the experimental steps on Chinese datasets. We present the performance on English datasets in Section 4.6 and a discussion in Section 4.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Data</head><p>We have two collections of Chinese word/phase pairs as ground truth datasets. Each pair is labeled with an is-a or not-is-a tag. The first one (denoted as FD) is from <ref type="bibr" target="#b6">Fu et al. (2014)</ref>, containing 1,391 is-a pairs and 4,294 not-is-a pairs, which is the first publicly available dataset to evaluate this task. The second one (denoted as BK) is larger in size and crawled from Baidu Baike by ourselves, con- sisting of &lt;entity, category&gt; pairs. For each pair in BK, we ask multiple human annotators to label the tag and discard the pair with inconsistent la- bels by different annotators. In total, it contains 3,870 is-a pairs and 3,582 not-is-a pairs <ref type="bibr">4</ref> .</p><p>The Chinese text corpus is extracted from the contents of 1.2M entity pages from Baidu Baike 5 , a Chinese online encyclopedia. It contains ap- proximately 1.1B words. We use the open source toolkit Ansj 6 for Chinese word segmentation. Chi- nese words/phrases in our test sets may consist of multiple Chinese characters. We treat such word/phrase as a whole to learn embeddings, in- stead of using character-level embeddings.</p><p>In the following experiments, we use 60% of the data for training, 20% for development and 20% for testing, partitioned randomly. By rotating the 5-fold subsets of the datasets, we report the per- formance of each method on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parameter Analysis</head><p>The word embeddings are pre-trained by ourselves on the Chinese corpus. In total, we obtain the 100- dimensional embedding vectors of 5.8M distinct words. The regularization parameters are set to λ = 10 −3 and µ 1 = µ 2 = 10 −4 , fine tuned on the development set.</p><p>The choice of θ reflects the precision-recall trade-off in our model. A larger value of θ means we pay more attention to precision rather than re- call. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates the precision-recall curves</p><formula xml:id="formula_22">Dataset FD BK Method P R F P R F Fu et al. (2014) (S)</formula><p>64.1 56.0 59.8 71.4 64.8 67.9 <ref type="bibr">Fu et al. (2014) (P)</ref> 66.4 59.3 62.6 72.7 67.5 70.0 <ref type="bibr" target="#b16">Li et al. (2015)</ref> 54.3 38.4 45.0 61.2 47.5 53.5 Mirza and Tonelli (2016) (C) 67.7 75.2 69.7 80.3 75.9 78.0 Mirza and Tonelli (2016) (A) 65.3 60.7 62.9 72.7 65.6 68.9 Mirza and Tonelli (2016) (S) 71.9 60.6 65.7 78.4 60.7 68.4 <ref type="bibr" target="#b35">Wang and He (2016)</ref> 69.3 64.5 66.9 73.9 69.8 71.8 Ours (Initial) 70.7 69.2 69.9 81.7 78.5 80.0 Ours 72.8 70.5 71.6 83.6 80.6 82.1 <ref type="table">Table 2</ref>: Performance comparison on test sets for Chinese hypernym prediction (%). on both datasets. It can be seen that the perfor- mance of our method is generally better in BK than FD. The most probable cause is that BK is a large dataset with more "balanced" numbers of positive and negative data. Finally, θ is set to 0.05 on FD and 0.1 on BK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance</head><p>In a series of previous work ( <ref type="bibr" target="#b7">Fu et al., 2013</ref><ref type="bibr" target="#b6">Fu et al., , 2014</ref><ref type="bibr" target="#b35">Wang and He, 2016)</ref>, several pattern-based, inference-based and encyclopedia-based is-a re- lation extraction methods for English have been implemented for the Chinese language. As their experiments show, these methods achieve the F- measure of lower than 60% in most cases, which are not suggested to be strong baselines for Chi- nese hypernym prediction. Interested readers may refer to their papers for the experimental results.</p><p>To make the convincing conclusion, we employ two recent state-of-the-art approaches for Chinese is-a relation identification ( <ref type="bibr" target="#b6">Fu et al., 2014;</ref><ref type="bibr" target="#b35">Wang and He, 2016)</ref> as baselines. We also take the word embedding based classification approach <ref type="bibr" target="#b22">(Mirza and Tonelli, 2016)</ref>  <ref type="bibr">7</ref> and Chinese Wikipedia based <ref type="bibr">7</ref> Although the experiments in their paper are mostly re- lated to temporal relations, the method can be applied to is-a SVM model ( <ref type="bibr" target="#b16">Li et al., 2015)</ref> as baselines to predict is-a relations between words 8 . The experimental results are illustrated in <ref type="table">Table 2</ref>.</p><p>For <ref type="bibr" target="#b6">Fu et al. (2014)</ref>, we test the performance using a linear projection model (denoted as S in <ref type="table">Table 2</ref>) and piecewise projection models (P). It shows that the semantics of is-a relations are bet- ter modeled by multiple projection models, with a slight improvement in F-measure. By combin- ing iterative projection models and pattern-based validation, the most recent approach ( <ref type="bibr" target="#b35">Wang and He, 2016)</ref> increases the F-measure by 4% and 2% in two datasets. In this method, the pattern- based statistics are calculated using the same cor- pus over which we train word embedding models. The main reason of the improvement may be that the projection models have a better generalization power by applying an iterative learning paradigm. <ref type="bibr" target="#b22">Mirza and Tonelli (2016)</ref> is implemented using three different strategies in combining the word vectors of a pair: i) concatenation x i ⊕ y i (de- relations without modification.</p><p>8 Previously, these methods used different knowledge sources to train models and thus the results in their papers are not directly comparable with ours. To make fair compar- ison, we take the training data as the same knowledge source to train models for all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidate Hypernym</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P T Candidate Hypernym P T Entity: 乙烯(Ethylene)</head><p>Entity:  <ref type="table">Table 4</ref>: TP/TN rates of three linguistic rules (%).</p><formula xml:id="formula_23">孙燕姿(Stefanie Sun) 化学品(Chemical) √ √ 歌手(Singer) √ √ 有机化学(Organic Chemistry) × × 明星(Star) √ √ 有机物(Organics) √ √ 人物(Person) √ √ 气体(Gas) √ √ 金曲奖 (Golden Melody Award) √ × 自然科学(Natural Science) × × 音乐人(Musician) √ √ Entity: 显卡(Graphics Card) Entity: 核反应堆(Nuclear Reactor) 硬件(Hardware) √ √ 建筑学(Architecture) × × 电子产品(Electronic Product) √ √ 核科学(Nuclear Science) × × 电脑硬件(Computer Hardware) √ √ 核能 (Nuclear Energy) √ × 数码(Digit) × × 自然科学(Natural Science) × ×</formula><p>noted as C), ii) addition x i + y i (A) and iii) sub- traction x i − y i (S). As seen, the classification models using addition and subtraction have sim- ilar performance in two datasets, while the con- catenation strategy outperforms previous two ap- proaches. Although <ref type="bibr" target="#b16">Li et al. (2015)</ref> achieve a high performance in their dataset, this method does not perform well in ours. The most likely cause is that the features in that work are designed specifically for the Chinese Wikipedia category system. Our initial model has a higher accuracy than all the baselines. By utilizing the transductive learning framework, we boost the F-measure by 1.7% and 2.1%, respectively. Therefore, our method is ef- fective to predict hypernyms of Chinese entities. We further conduct statistical tests which show our method significantly (p &lt; 0.01) improves the F- measure over the state-of-the-art method ( <ref type="bibr" target="#b35">Wang and He, 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effectiveness of Linguistic Rules</head><p>To illustrate the effectiveness of linguistic rules, we present the true positive (or negative) rate by using one positive (or negative) rule solely, shown in <ref type="table">Table 4</ref>. These values serve as γ i s in the trans- ductive learning stage. The results indicate that these rules have high precision (over 90%) over both datasets for our task.</p><p>We state that currently we only use a few hand- craft linguistic rules in our work. The proposed approach is a general framework that can encode arbitrary numbers of rules and in any language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Error Analysis and Case Studies</head><p>We analyze correct and error cases in the exper- iments. Some examples of prediction results are shown in <ref type="table" target="#tab_1">Table 3</ref>. We can see that our method is generally effective. However, some mistakes occur mostly because it is difficult to distinguish strict is-a and topic-of relations. For example, the entity Nuclear Reactor is semantically close to Nuclear Energy. The error statistics show that such kind of errors account for approximately 80.2% and 78.6% in two test sets, respectively.</p><p>Based on the literature study, we find that such problem has been also reported in ( <ref type="bibr" target="#b7">Fu et al., 2013;</ref><ref type="bibr" target="#b35">Wang and He, 2016)</ref>. To reduce such errors, we employ the Chinese thematic lexicon based on <ref type="bibr" target="#b16">Li et al. (2015)</ref> in the transductive learning stage but the coverage is still limited. Two possible solu- tions are: i) adding more negative training data of this kind; and ii) constructing a large-scale the- matic lexicon automatically from the Web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Experiments on English Datasets</head><p>To examine how our method can benefit hyper- nym prediction for the English language, we use two standard datasets in this paper. The first one is a benchmark dataset for distributional semantic evaluation, i.e., BLESS ( <ref type="bibr" target="#b0">Baroni and Lenci, 2011</ref>  <ref type="bibr" target="#b14">Lenci and Benotto (2012)</ref> 42.8 38.6 40.6 38.5 50.1 43.5 <ref type="bibr" target="#b29">Santus et al. (2014)</ref> 59.2 52.3 55.4 51.2 71.5 59.6 <ref type="bibr">Fu et al. (2014) (S)</ref> 65.3 62.4 63.8 65.6 66.1 65.8 <ref type="bibr">Fu et al. (2014) (P)</ref> 68.1 64.2 66.1 62.3 71.9 67.3 Mirza and Tonelli (2016) (C) 79.4 84.1 81.7 79.3 80.9 80.1 Mirza and Tonelli (2016) (A) 80.7 72.3 76.3 79.1 79.6 79.4 Mirza and Tonelli (2016) (S) 78.0 81.2 79.6 80.5 77.5 79.0 <ref type="bibr" target="#b35">Wang and He (2016)</ref> 76.2 75.4 75.8 75.1 76.3 75.6 Ours <ref type="table">(Initial)</ref> 79.3 76.3 77.7 77.2 76.8 77.0 Ours 84.4 79.5 81.9 79.1 77.5 78.3 <ref type="table">Table 5</ref>: Performance comparison on test sets for English hypernym prediction (%).</p><p>statistics, and the pre-trained embedding vectors of English words 9 . For comparison, we test all the baselines over English datasets except <ref type="bibr" target="#b16">Li et al. (2015)</ref>. This is because most features in <ref type="bibr" target="#b16">Li et al. (2015)</ref> can only be used in the Chinese environment. To imple- ment <ref type="bibr" target="#b35">Wang and He (2016)</ref> for English, we use the original Hearst patterns <ref type="bibr" target="#b10">(Hearst, 1992)</ref> to perform relation selection and do not consider not-is-a pat- terns. We also take two recent DSM based ap- proaches ( <ref type="bibr" target="#b14">Lenci and Benotto, 2012;</ref><ref type="bibr" target="#b29">Santus et al., 2014</ref>) as baselines. As for our own method, we do not use linguistic rules in <ref type="table">Table 1</ref> for English. The results are illustrated in <ref type="table">Table 5</ref>. As seen, our method is superior to all the baselines over BLESS, with an F-measure of 81.9%. In Shwartz, while the approach ( <ref type="bibr" target="#b22">Mirza and Tonelli, 2016)</ref> has the highest F-measure of 80.1%, our method is gen- erally comparable to theirs and outperforms oth- ers. The results suggest that although our method is not necessarily the state-of-the-art for English hypernym prediction, it has several potential ap- plications. Refer to Section 4.7 for discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Discussion</head><p>From the experiments, we can see that the pro- posed approach outperforms the state-of-the-art methods for Chinese hypernym prediction. Al- though the English language is not our focus, our approach still has relatively high performance. Additionally, our work has potential values for the following applications:</p><p>• Domain-specific or Context-sparse Rela- tion Extraction. If the task is to predict re- 9 http://nlp.stanford.edu/projects/glove/ lations between words when it is related to a specific domain or the contexts are sparse, even for English, traditional pattern-based methods are likely to fail. Our method can predict the existence of relations without ex- plicit textual patterns and requires a relatively small amount of pairs as training data.</p><p>• Under-resourced Language Learning. Our method can be adapted for relation ex- traction in languages with flexible expres- sions, few knowledge resources and/or low- performance NLP tools. Our method does not require deep NLP parsing of sentences in a text corpus and thus the performance is not affected by parsing errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In summary, this paper introduces a transuctive learning approach for Chinese hypernym predic- tion. By modeling linear projection models, lin- guistic rules and non-linear mappings, our method is able to identify Chinese hypernyms with high accuracy. Experiments show that the performance of our method outperforms previous approaches. We also discuss the potential applications of our method besides Chinese hypernym prediction. In our work, the candidate Chinese hyponyms and hypernyms are extracted from user generated cat- egories. In the future, we will study how to con- struct a taxonomy from texts in Chinese.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Learning Algorithm 1: Initialize W h and S h based on the initial pre- diction model; 2: Randomly initialize F (0) h ; 3: Compute Σ −1 h based on entity similarities; 4: Initialize counter n = 1; 5: for each linguistic rule c i ∈ C do 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Precision-recall curve with respect to the tuning of θ on development sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Examples of model prediction. (P: prediction result, T: ground truth, 
√ : positive, ×: negative) 

TP/TN Rate Rule P1 Rule N1 Rule N2 
Dataset FD 98.6 
92.3 
94.1 
Dataset BK 97.6 
96.8 
97.3 

</table></figure>

			<note place="foot" n="1"> We have also examined piecewise linear projection models proposed in (Fu et al., 2014; Wang and He, 2016) as the initial models for transductive learning. However, we found that this practice is less efficient and the performance does not improve significantly.</note>

			<note place="foot" n="2"> We do not consider the cases where a pair matches both positive and negative rules because such cases are very rare, and even non-existent in our datasets. However, our method can deal with these cases by using some simple heuristics. For example, we can update Ri using either of the following two ways: i) Ri = Fi and ii) Ri = Fi + c j ∈C (x i ,y i ) γj.</note>

			<note place="foot" n="4"> https://chywang.github.io/data/acl17.zip 5 https://baike.baidu.com/ 6 https://github.com/NLPchina/ansj seg/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the National Key Re-search and Development Program of China under Grant No. 2016YFB1000904.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How we blessed distributional semantic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics</title>
		<meeting>the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">LEDIR: an unsupervised algorithm for learning directionality of inference rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic construction of a hypernym-labeled noun hierarchy from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><forename type="middle">A</forename><surname>Caraballo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Coupled semi-supervised learning for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estevam</forename><forename type="middle">R</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Web Search and Web Data Mining</title>
		<meeting>the Third International Conference on Web Search and Web Data Mining</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Transductive Inference and SemiSupervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Web-scale information extraction in knowitall: (preliminary results)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana-Maria</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on World Wide Web</title>
		<meeting>the 13th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="100" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning semantic hierarchies via word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiji</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1199" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting multiple sources for open-domain hypernym discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiji</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1224" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning by transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katy</forename><forename type="middle">S</forename><surname>Azoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Fourteenth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="148" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tikhonov regularization and total least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianne</forename><forename type="middle">P</forename><surname>Per Christian Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>O&amp;apos;leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Analysis Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="194" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sinica BOW (bilingual ontological wordnet): Integration of bilingual wordnet and SUMO</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ru-Yng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hshiang-Pin</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation</title>
		<meeting>the Fourth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Directional distributional similarity for lexical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Kotlerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Zhitomirsky-Geffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="389" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning arguments and supertypes of semantic relations using recursive patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eduard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1482" to="1491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identifying hypernyms in distributional semantic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Benotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Semantic Evaluation</title>
		<meeting>the Sixth International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="543" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A relation extraction method of chinese named entities based on location and semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xindong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong-Qing</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">User generated content oriented chinese taxonomy construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Technologies and Applications-17th Asia-Pacific Web Conference</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="623" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bipartite edge prediction via transductive learning over product graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1880" to="1888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Taxonomy induction from chinese encyclopedias by combinatorial optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shansong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baogang</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th CCF Conference on Natural Language Processing and Chinese Computing</title>
		<meeting>the 4th CCF Conference on Natural Language Processing and Chinese Computing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="299" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Characterization of a class of sigmoid functions with applications to neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishan</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chilukuri</forename><forename type="middle">K</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ranka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="819" to="835" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the Acm</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the contribution of word embeddings to temporal relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paramita</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Tonelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics</title>
		<meeting>the 26th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Espresso: Leveraging generic patterns for automatically harvesting semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deriving a large-scale taxonomy from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1440" to="1445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">AFET: automatic finegrained entity typing by hierarchical partial-label embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Xiang Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1369" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">What is this, anyway: Automatic hypernym discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning by Reading and Learning to Read, the 2009 AAAI Spring Symposium</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="88" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Extracting hypernym pairs from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lexical patterns or dependency patterns: Which is better for hypernym extraction?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="174" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Chasing hypernyms in vector spaces with entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="38" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving hypernymy detection with an integrated path-based and distributional method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning syntactic patterns for automatic hypernym discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 17, NIPS 2004</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web</title>
		<meeting>the 16th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Instance-based evaluation of entailment rule acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Shnarch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="456" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Challenges in chinese knowledge graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st IEEE International Conference on Data Engineering Workshops</title>
		<meeting>the 31st IEEE International Conference on Data Engineering Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="59" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Chinese hypernym-hyponym extraction from user generated categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics</title>
		<meeting>the 26th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1350" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cbuilding the chinese open wordnet (cow): Starting from core synsets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Workshop on Asian Language Resources: ALR-2013 a Workshop of The 6th International Joint Conference on Natural Language Processing</title>
		<meeting>the 11th Workshop on Asian Language Resources: ALR-2013 a Workshop of The 6th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Probase: a probabilistic taxonomy for text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><forename type="middle">Qili</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="481" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An integrated approach for automatic construction of bilingual chinese-english wordnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingji</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web, Proceedings of the 3rd Asian Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="302" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cross-lingual text classification via model translation with limited dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
