<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Word Distributions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
							<email>andrew@cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Word Distributions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1645" to="1656"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/P17-1151</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word embeddings provide point representations of words containing useful semantic information. We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, en-tailment, and rich uncertainty information. To learn these distributions, we propose an energy-based max-margin objective. We show that the resulting approach captures uniquely expressive semantic information , and outperforms alternatives, such as word2vec skip-grams, and Gaus-sian embeddings, on benchmark datasets such as word similarity and entailment.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To model language, we must represent words. We can imagine representing every word with a binary one-hot vector corresponding to a dictio- nary position. But such a representation contains no valuable semantic information: distances be- tween word vectors represent only differences in alphabetic ordering. Modern approaches, by con- trast, learn to map words with similar meanings to nearby points in a vector space <ref type="bibr" target="#b25">(Mikolov et al., 2013a)</ref>, from large datasets such as Wikipedia. These learned word embeddings have become ubiquitous in predictive tasks. <ref type="bibr" target="#b42">Vilnis and McCallum (2014)</ref> recently proposed an alternative view, where words are represented by a whole probability distribution instead of a de- terministic point vector. Specifically, they model each word by a Gaussian distribution, and learn its mean and covariance matrix from data. This approach generalizes any deterministic point em- bedding, which can be fully captured by the mean vector of the Gaussian distribution. Moreover, the full distribution provides much richer information than point estimates for characterizing words, rep- resenting probability mass and uncertainty across a set of semantics.</p><p>However, since a Gaussian distribution can have only one mode, the learned uncertainty in this rep- resentation can be overly diffuse for words with multiple distinct meanings (polysemies), in or- der for the model to assign some density to any plausible semantics <ref type="bibr" target="#b42">(Vilnis and McCallum, 2014)</ref>. Moreover, the mean of the Gaussian can be pulled in many opposing directions, leading to a biased distribution that centers its mass mostly around one meaning while leaving the others not well rep- resented.</p><p>In this paper, we propose to represent each word with an expressive multimodal distribution, for multiple distinct meanings, entailment, heavy tailed uncertainty, and enhanced interpretability. For example, one mode of the word 'bank' could overlap with distributions for words such as 'fi- nance' and 'money', and another mode could overlap with the distributions for 'river' and 'creek'. It is our contention that such flexibility is critical for both qualitatively learning about the meanings of words, and for optimal performance on many predictive tasks.</p><p>In particular, we model each word with a mix- ture of Gaussians (Section 3.1). We learn all the parameters of this mixture model using a maximum margin energy-based ranking objective <ref type="bibr" target="#b18">(Joachims, 2002;</ref><ref type="bibr" target="#b42">Vilnis and McCallum, 2014</ref>) <ref type="bibr">(Section 3.3)</ref>, where the energy function describes the affinity between a pair of words. For analytic tractability with Gaussian mixtures, we use the in- ner product between probability distributions in a Hilbert space, known as the expected likelihood kernel ( <ref type="bibr" target="#b17">Jebara et al., 2004</ref>), as our energy func- tion (Section 3.4). Additionally, we propose trans- formations for numerical stability and initializa- tion A.2, resulting in a robust, straightforward, and scalable learning procedure, capable of training on a corpus with billions of words in days. We show that the model is able to automatically discover multiple meanings for words <ref type="bibr">(Section 4.3)</ref>, and significantly outperform other alternative meth- ods across several tasks such as word similarity and entailment <ref type="bibr">(Section 4.4, 4.5, 4.7)</ref>. We have made code available at http://github.com/ benathi/word2gm, where we implement our model in Tensorflow <ref type="bibr" target="#b9">(Abadi et. al, 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In the past decade, there has been an explo- sion of interest in word vector representations. word2vec, arguably the most popular word em- bedding, uses continuous bag of words and skip- gram models, in conjunction with negative sam- pling for efficient conditional probability estima- tion ( <ref type="bibr">Mikolov et al., 2013a,b)</ref>. Other popular ap- proaches use feedforward ( <ref type="bibr" target="#b3">Bengio et al., 2003)</ref> and recurrent neural network language models ( <ref type="bibr" target="#b27">Mikolov et al., 2010</ref><ref type="bibr" target="#b28">Mikolov et al., , 2011b</ref><ref type="bibr" target="#b7">Collobert and Weston, 2008</ref>) to predict missing words in sentences, producing hidden layers that can act as word em- beddings that encode semantic information. They employ conditional probability estimation tech- niques, including hierarchical softmax ( <ref type="bibr" target="#b26">Mikolov et al., 2011a;</ref><ref type="bibr" target="#b32">Mnih and Hinton, 2008;</ref><ref type="bibr" target="#b33">Morin and Bengio, 2005</ref>) and noise contrastive estimation ( <ref type="bibr" target="#b13">Gutmann and Hyvärinen, 2012)</ref>.</p><p>A different approach to learning word em- beddings is through factorization of word co- occurrence matrices such as GloVe embeddings ( <ref type="bibr" target="#b36">Pennington et al., 2014</ref>). The matrix factoriza- tion approach has been shown to have an implicit connection with skip-gram and negative sampling <ref type="bibr" target="#b22">Levy and Goldberg (2014)</ref>. Bayesian matrix fac- torization where row and columns are modeled as Gaussians has been explored in <ref type="bibr" target="#b39">Salakhutdinov and Mnih (2008)</ref> and provides a different probabilistic perspective of word embeddings.</p><p>In exciting recent work, <ref type="bibr" target="#b42">Vilnis and McCallum (2014)</ref> propose a Gaussian distribution to model each word. Their approach is significantly more expressive than typical point embeddings, with the ability to represent concepts such as entailment, by having the distribution for one word (e.g. 'mu- sic') encompass the distributions for sets of related words <ref type="bibr">('jazz' and 'pop')</ref>. However, with a uni- modal distribution, their approach cannot capture multiple distinct meanings, much like most deter- ministic approaches.</p><p>Recent work has also proposed deterministic embeddings that can capture polysemies, for ex- ample through a cluster centroid of context vec- tors ( <ref type="bibr" target="#b16">Huang et al., 2012)</ref>, or an adapted skip-gram model with an EM algorithm to learn multiple la- tent representations per word ( <ref type="bibr" target="#b41">Tian et al., 2014</ref>). <ref type="bibr" target="#b35">Neelakantan et al. (2014)</ref> also extends skip-gram with multiple prototype embeddings where the number of senses per word is determined by a non-parametric approach. <ref type="bibr" target="#b23">Liu et al. (2015)</ref> learns topical embeddings based on latent topic models where each word is associated with multiple top- ics. Another related work by <ref type="bibr" target="#b34">Nalisnick and Ravi (2015)</ref> models embeddings in infinite-dimensional space where each embedding can gradually repre- sent incremental word sense if complex meanings are observed.</p><p>Probabilistic word embeddings have only re- cently begun to be explored, and have so far shown great promise. In this paper, we propose, to the best of our knowledge, the first probabilistic word embedding that can capture multiple meanings. We use a Gaussian mixture model which allows for a highly expressive distributions over words. At the same time, we retain scalability and analytic tractability with an expected likelihood kernel en- ergy function for training. The model and train- ing procedure harmonize to learn descriptive rep- resentations of words, with superior performance on several benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we introduce our Gaussian mix- ture (GM) model for word representations, and present a training method to learn the parameters of the Gaussian mixture. This method uses an energy-based maximum margin objective, where we wish to maximize the similarity of distribu- tions of nearby words in sentences. We propose an energy function that compliments the GM model by retaining analytic tractability. We also pro- vide critical practical details for numerical stabil- ity and initialization. The code for model training and evaluation is available at http://github. com/benathi/word2gm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Representation</head><p>We represent each word w in a dictionary as a Gaussian mixture with K components. Specif- ically, the distribution of w, f w , is given by the density</p><formula xml:id="formula_0">f w ( x) = K i=1 p w,i N [ x; µ w,i , Σ w,i ] (1) = K i=1 p w,i 2π|Σ w,i | e − 1 2 ( x− µ w,i ) Σ −1 w,i ( x− µ w,i ) ,</formula><p>where K i=1 p w,i = 1. The mean vectors µ w,i represent the location of the i th component of word w, and are akin to the point embeddings provided by popular approaches like word2vec. p w,i represents the component probability (mix- ture weight), and Σ w,i is the component covari- ance matrix, containing uncertainty information. Our goal is to learn all of the model parameters µ w,i , p w,i , Σ w,i from a corpus of natural sentences to extract semantic information of words. Each Gaussian component's mean vector of word w can represent one of the word's distinct meanings. For instance, one component of a polysemous word such as 'rock' should represent the meaning re- lated to 'stone' or 'pebbles', whereas another com- ponent should represent the meaning related to music such as 'jazz' or 'pop'. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates our word embedding model, and the difference be- tween multimodal and unimodal representations, for words with multiple meanings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Skip-Gram</head><p>The training objective for learning θ = { µ w,i , p w,i , Σ w,i } draws inspiration from the continuous skip-gram model ( <ref type="bibr" target="#b25">Mikolov et al., 2013a)</ref>, where word embeddings are trained to maximize the probability of observing a word given another nearby word. This procedure follows the distributional hypothesis that words occurring in natural contexts tend to be semanti- cally related. For instance, the words 'jazz' and 'music' tend to occur near one another more often than 'jazz' and 'cat'; hence, 'jazz' and 'music' are more likely to be related. The learned word representation contains useful semantic informa- tion and can be used to perform a variety of NLP tasks such as word similarity analysis, sentiment classification, modelling word analogies, or as a preprocessed input for complex system such as statistical machine translation. Each Gaussian component is rep- resented by an ellipsoid, whose center is specified by the mean vector and contour surface specified by the covariance matrix, reflecting subtleties in meaning and uncertainty. On the left, we show ex- amples of Gaussian mixture distributions of words where Gaussian components are randomly initial- ized. After training, we see on the right that one component of the word 'rock' is closer to 'stone' and 'basalt', whereas the other component is closer to 'jazz' and 'pop'. We also demonstrate the entailment concept where the distribution of the more general word 'music' encapsulates words such as 'jazz', 'rock', 'pop'. Bottom: A Gaussian embedding model ( <ref type="bibr" target="#b42">Vilnis and McCallum, 2014</ref>). For words with multiple meanings, such as 'rock', the variance of the learned representation becomes unnecessarily large in order to assign some proba- bility to both meanings. Moreover, the mean vec- tor for such words can be pulled between two clus- ters, centering the mass of the distribution on a re- gion which is far from certain meanings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Energy-based Max-Margin Objective</head><p>Each sample in the objective consists of two pairs of words, (w, c) and (w, c ). w is sampled from a sentence in a corpus and c is a nearby word within a context window of length . For instance, a word w = 'jazz' which occurs in the sentence 'I listen to jazz music' has context words <ref type="bibr">('I', 'listen', 'to' , 'music')</ref>. c is a negative context word (e.g. 'air- plane') obtained from random sampling.</p><p>The objective is to maximize the energy be- tween words that occur near each other, w and c, and minimize the energy between w and its nega- tive context c . This approach is similar to neg-ative sampling <ref type="bibr">(Mikolov et al., 2013a,b)</ref>, which contrasts the dot product between positive context pairs with negative context pairs. The energy func- tion is a measure of similarity between distribu- tions and will be discussed in Section 3.4.</p><p>We use a max-margin ranking objective <ref type="bibr" target="#b18">(Joachims, 2002)</ref>, used for Gaussian embeddings in <ref type="bibr" target="#b42">Vilnis and McCallum (2014)</ref>, which pushes the similarity of a word and its positive context higher than that of its negative context by a margin m:</p><formula xml:id="formula_1">L θ (w, c, c ) = max(0, m − log E θ (w, c) + log E θ (w, c ))</formula><p>This objective can be minimized by mini-batch stochastic gradient descent with respect to the pa- rameters θ = { µ w,i , p w,i , Σ w,i } -the mean vec- tors, covariance matrices, and mixture weights - of our multimodal embedding in Eq. (1).</p><p>Word Sampling We use a word sampling scheme similar to the implementation in word2vec ( <ref type="bibr">Mikolov et al., 2013a,b)</ref> to bal- ance the importance of frequent words and rare words. Frequent words such as 'the', 'a', 'to' are not as meaningful as relatively less frequent words such as 'dog', 'love', 'rock', and we are often more interested in learning the semantics of the less frequently observed words. We use subsampling to improve the performance of learning word vectors ( <ref type="bibr" target="#b29">Mikolov et al., 2013b</ref>). This technique discards word w i with probability P (w i ) = 1 − t/f (w i ), where f (w i ) is the frequency of word w i in the training corpus and t is a frequency threshold.</p><p>To generate negative context words, each word type w i is sampled according to a distribution P n (w i ) ∝ U (w i ) 3/4 which is a distorted version of the unigram distribution U (w i ) that also serves to diminish the relative importance of frequent words. Both subsampling and the negative distri- bution choice are proven effective in word2vec training ( <ref type="bibr" target="#b29">Mikolov et al., 2013b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Energy Function</head><p>For vector representations of words, a usual choice for similarity measure (energy function) is a dot product between two vectors. Our word repre- sentations are distributions instead of point vec- tors and therefore need a measure that reflects not only the point similarity, but also the uncertainty. We propose to use the expected likelihood kernel, which is a generalization of an inner product be- tween vectors to an inner product between distri- butions ( <ref type="bibr" target="#b17">Jebara et al., 2004</ref>). That is,</p><formula xml:id="formula_2">E(f, g) = f (x)g(x) dx = f, g L 2</formula><p>where ·, ·· L 2 denotes the inner product in Hilbert space L 2 . We choose this form of energy since it can be evaluated in a closed form given our choice of probabilistic embedding in Eq. (1). For Gaussian mixtures f, g representing the</p><formula xml:id="formula_3">words w f , w g , f (x) = K i=1 p i N (x; µ f,i , Σ f,i ) and g(x) = K i=1 q i N (x; µ g,i , Σ g,i ), K i=1 p i = 1, and K i=1 q i = 1</formula><p>, we find (see Section A.1) the log energy is</p><formula xml:id="formula_4">log E θ (f, g) = log K j=1 K i=1 p i q j e ξ i,j<label>(2)</label></formula><p>where</p><formula xml:id="formula_5">ξ i,j ≡ log N (0; µ f,i − µ g,j , Σ f,i + Σ g,j ) = − 1 2 log det(Σ f,i + Σ g,j ) − D 2 log(2π) − 1 2 ( µ f,i − µ g,j ) (Σ f,i + Σ g,j ) −1 ( µ f,i − µ g,j )<label>(3)</label></formula><p>We call the term ξ i,j partial (log) energy. Observe that this term captures the similarity between the i th meaning of word w f and the j th meaning of word w g . The total energy in Equation 2 is the sum of possible pairs of partial energies, weighted accordingly by the mixture probabilities p i and q j . The term</p><formula xml:id="formula_6">−( µ f,i − µ g,j ) (Σ f,i +Σ g,j ) −1 ( µ f,i − µ g,j )</formula><p>in ξ i,j explains the difference in mean vectors of semantic pair (w f , i) and (w g , j). If the seman- tic uncertainty (covariance) for both pairs are low, this term has more importance relative to other terms due to the inverse covariance scaling. We observe that the loss function L θ in Section 3.3 at- tains a low value when E θ (w, c) is relatively high. High values of E θ (w, c) can be achieved when the component means across different words µ f,i and µ g,j are close together (e.g., similar point repre- sentations). High energy can also be achieved by large values of Σ f,i and Σ g,j , which washes out the importance of the mean vector difference. The term − log det(Σ f,i + Σ g,j ) serves as a regularizer that prevents the covariances from being pushed too high at the expense of learning a good mean embedding.</p><p>At the beginning of training, ξ i,j roughly are on the same scale among all pairs (i, j)'s. During this time, all components learn the signals from the word occurrences equally. As training progresses and the semantic representation of each mixture becomes more clear, there can be one term of ξ i,j 's that is predominantly higher than other terms, giv- ing rise to a semantic pair that is most related.</p><p>The negative KL divergence is another sensible choice of energy function, providing an asymmet- ric metric between word distributions. However, unlike the expected likelihood kernel, KL diver- gence does not have a closed form if the two dis- tributions are Gaussian mixtures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We have introduced a model for multi-prototype embeddings, which expressively captures word meanings with whole probability distributions. We show that our combination of energy and ob- jective functions, proposed in Section 3, enables one to learn interpretable multimodal distribu- tions through unsupervised training, for describing words with multiple distinct meanings. By rep- resenting multiple distinct meanings, our model also reduces the unnecessarily large variance of a Gaussian embedding model, and has improved re- sults on word entailment tasks.</p><p>To learn the parameters of the proposed mix- ture model, we train on a concatenation of two datasets: UKWAC (2.5 billion tokens) and Wackypedia (1 billion tokens) ( <ref type="bibr" target="#b2">Baroni et al., 2009</ref>). We discard words that occur fewer than 100 times in the corpus, which results in a vocab- ulary size of 314, 129 words. Our word sampling scheme, described at the end of Section 4.3, is sim- ilar to that of word2vec with one negative con- text word for each positive context word.</p><p>After training, we obtain learned parameters</p><formula xml:id="formula_7">{ µ w,i , Σ w,i , p i } K i=1</formula><p>for each word w. We treat the mean vector µ w,i as the embedding of the i th mix- ture component with the covariance matrix Σ w,i representing its subtlety and uncertainty. We per- form qualitative evaluation to show that our em- beddings learn meaningful multi-prototype repre- sentations and compare to existing models using a quantitative evaluation on word similarity datasets and word entailment.</p><p>We name our model as Word to Gaussian Mix- ture (w2gm) in constrast to Word to Gaussian (w2g) <ref type="bibr" target="#b42">(Vilnis and McCallum, 2014</ref>). Unless stated otherwise, w2g refers to our implementa- tion of w2gm model with one mixture component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hyperparameters</head><p>Unless stated otherwise, we experiment with K = 2 components for the w2gm model, but we have results and discussion of K = 3 at the end of sec- tion 4.3. We primarily consider the spherical case for computational efficiency. We note that for di- agonal or spherical covariances, the energy can be computed very efficiently since the matrix inver- sion would simply require O(d) computation in- stead of O(d 3 ) for a full matrix. Empirically, we have found diagonal covariance matrices become roughly spherical after training. Indeed, for these relatively high dimensional embeddings, there are sufficient degrees of freedom for the mean vec- tors to be learned such that the covariance matrices need not be asymmetric. Therefore, we perform all evaluations with spherical covariance models.</p><p>Models used for evaluation have dimension D = 50 and use context window = 10 unless stated otherwise. We provide additional hyperpa- rameters and training details in the supplementary material (A.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Similarity Measures</head><p>Since our word embeddings contain multiple vec- tors and uncertainty parameters per word, we use the following measures that generalizes similarity scores. These measures pick out the component pair with maximum similarity and therefore deter- mine the meanings that are most relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Expected Likelihood Kernel</head><p>A natural choice for a similarity score is the ex- pected likelihood kernel, an inner product between distributions, which we discussed in Section 3.4. This metric incorporates the uncertainty from the covariance matrices in addition to the similarity between the mean vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Maximum Cosine Similarity</head><p>This metric measures the maximum similarity of mean vectors among all pairs of mixture com- ponents between distributions f and g. That is,</p><formula xml:id="formula_8">d(f, g) = max i,j=1,...,K µ f,i , µ g,j ||µ f,i || · ||µ g,j ||</formula><p>, which corre- sponds to matching the meanings of f and g that are the most similar. For a Gaussian embedding, maximum similarity reduces to the usual cosine similarity.</p><p>Word Co.</p><p>Nearest Neighbors rock 0 basalt:1, boulder:1, boulders:0, stalagmites:0, stalactites:0, rocks:1, sand:0, quartzite:1, bedrock:0 rock 1 rock/:1, ska:0, funk:1, pop-rock:1, punk:1, indie-rock:0, band:0, indie:0, pop:1 bank 0 banks:1, mouth:1, river:1, River:0, confluence:0, waterway:1, downstream:1, upstream:0, dammed:0 bank 1 banks:0, banking:1, banker:0, Banks:1, bankas:1, Citibank:1, Interbank:1, Bankers:0, transactions:1 Apple 0 Strawberry:0, Tomato:1, Raspberry:1, Blackberry:1, Apples:0, Pineapple:1, Grape:1, Lemon:0 Apple 1 Macintosh:1, Mac:1, OS:1, Amiga:0, Compaq:0, Atari:1, PC:1, Windows:0, iMac:0 star 0 stars:0, Quaid:0, starlet:0, Dafoe:0, Stallone:0, Geena:0, Niro:0, Zeta-Jones:1, superstar:0 star 1 stars:1, brightest:0, Milky:0, constellation:1, stellar:0, nebula:1, galactic:1, supernova:1, Ophiuchus:1 cell 0 cellular:0, Nextel:0, 2-line:0, Sprint:0, phones.:1, pda:1, handset:0, handsets:1, pushbuttons:0 cell 1 cytoplasm:0, vesicle:0, cytoplasmic:1, macrophages:0, secreted:1, membrane:0, mitotic:0, endocytosis:1 left 0 After:1, back:0, finally:1, eventually:0, broke:0, joined:1, returned:1, after:1, soon:0 left 1 right-hand:0, hand:0, right:0, left-hand:0, lefthand:0, arrow:0, turn:0, righthand:0, Left:0  <ref type="table">Table 1</ref>: Nearest neighbors based on cosine similarity between the mean vectors of Gaussian components for Gaussian mixture embedding (top) (for K = 2) and Gaussian embedding (bottom). The notation w:i denotes the i th mixture component of the word w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Minimum Euclidean Distance</head><p>Cosine similarity is popular for evaluating em- beddings. However, our training objective di- rectly involves the Euclidean distance in Eq. (3), as opposed to dot product of vectors such as in word2vec. Therefore, we also consider the Eu- clidean metric: d(f, g) = min</p><p>i,j=1,...,K</p><p>[||µ f,i −µ g,j ||].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Evaluation</head><p>In <ref type="table">Table 1</ref>, we show examples of polysemous words and their nearest neighbors in the embed- ding space to demonstrate that our trained em- beddings capture multiple word senses. For in- stance, a word such as 'rock' that could mean ei- ther 'stone' or 'rock music' should have each of its meanings represented by a distinct Gaussian com- ponent. Our results for a mixture of two Gaussians model confirm this hypothesis, where we observe that the 0 th component of 'rock' being related to ('basalt', 'boulders') and the 1 st component being related to ('indie', 'funk', 'hip-hop'). Similarly, the word bank has its 0 th component representing the river bank and the 1 st component representing the financial bank. By contrast, in <ref type="table">Table 1</ref> (bottom), see that for Gaussian embeddings with one mixture compo- nent, nearest neighbors of polysemous words are predominantly related to a single meaning. For in- stance, 'rock' mostly has neighbors related to rock music and 'bank' mostly related to the financial bank. The alternative meanings of these polyse- mous words are not well represented in the embed- dings. As a numerical example, the cosine simi- larity between 'rock' and 'stone' for the Gaussian representation of <ref type="bibr" target="#b42">Vilnis and McCallum (2014)</ref> is only 0.029, much lower than the cosine similarity 0.586 between the 0 th component of 'rock' and 'stone' in our multimodal representation.</p><p>In cases where a word only has a single popu- lar meaning, the mixture components can be fairly close; for instance, one component of 'stone' is close to <ref type="bibr">('stones', 'stonework', 'slab')</ref> and the other to <ref type="bibr">('carving, 'relic', 'excavated')</ref>, which re- flects subtle variations in meanings. In general, the mixture can give properties such as heavy tails and more interesting unimodal characterizations of un- certainty than could be described by a single Gaus- sian.</p><p>Embedding Visualization We provide an interactive visualization as part of our code repos- itory:</p><p>https://github.com/benathi/ word2gm#visualization that allows real- time queries of words' nearest neighbors (in the embeddings tab) for K = 1, 2, 3 components. We use a notation similar to that of <ref type="table">Table 1</ref>, where a token w:i represents the component i of a word w. For instance, if in the K = 2 link we search for bank:0, we obtain the nearest neigh-bors such as river:1, confluence:0, waterway:1, which indicates that the 0 th component of 'bank' has the meaning 'river bank'. On the other hand, searching for bank:1 yields nearby words such as banking:1, banker:0, ATM:0, indicating that this com- ponent is close to the 'financial bank'. We also have a visualization of a unimodal (w2g) for comparison in the K = 1 link.</p><p>In addition, the embedding link for our Gaus- sian mixture model with K = 3 mixture compo- nents can learn three distinct meanings. For in- stance, each of the three components of 'cell' is close to ('keypad', 'digits'), ('incarcerated', 'in- mate') or ('tissue', 'antibody'), indicating that the distribution captures the concept of 'cellphone', 'jail cell', or 'biological cell', respectively. Due to the limited number of words with more than 2 meanings, our model with K = 3 does not gen- erally offer substantial performance differences to our model with K = 2; hence, we do not further display K = 3 results for compactness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Word Similarity</head><p>We evaluate our embeddings on several standard word similarity datasets, namely, <ref type="bibr">SimLex (Hill et al., 2014</ref>), WS or WordSim-353, WS-S (sim- ilarity), WS-R (relatedness) ( <ref type="bibr" target="#b10">Finkelstein et al., 2002</ref>), MEN ( <ref type="bibr" target="#b4">Bruni et al., 2014</ref>), MC <ref type="bibr" target="#b30">(Miller and Charles, 1991)</ref>, RG <ref type="bibr" target="#b38">(Rubenstein and Goodenough, 1965)</ref>, YP ( <ref type="bibr" target="#b45">Yang and Powers, 2006</ref>), MTurk(- 287,-771) <ref type="bibr" target="#b37">(Radinsky et al., 2011;</ref><ref type="bibr" target="#b14">Halawi et al., 2012)</ref>, and RW ( <ref type="bibr" target="#b24">Luong et al., 2013)</ref>. Each dataset contains a list of word pairs with a human score of how related or similar the two words are.</p><p>We calculate the Spearman correlation <ref type="bibr" target="#b40">(Spearman, 1904</ref>) between the labels and our scores gen- erated by the embeddings. The Spearman corre- lation is a rank-based correlation measure that as- sesses how well the scores describe the true labels.</p><p>The correlation results are shown in <ref type="table">Table 2</ref> us- ing the scores generated from the expected like- lihood kernel, maximum cosine similarity, and maximum Euclidean distance.</p><p>We show the results of our Gaussian mixture model and compare the performance with that of word2vec and the original Gaussian em- bedding by <ref type="bibr" target="#b42">Vilnis and McCallum (2014)</ref>. We note that our model of a unimodal Gaussian embedding w2g also outperforms the original model, which differs in model hyperparame- ters and initialization, for most datasets. Our multi-prototype model w2gm also performs better than skip-gram or Gaussian embedding methods on many datasets, namely, WS, WS-R, MEN, MC, RG, YP, MT-287, RW. The maximum cosine similarity yields the best performance on most datasets; however, the minimum Euclidean distance is a better metric for the datasets MC and RW. These results are consistent for both the single-prototype and the multi-prototype models.</p><p>We also compare out results on WordSim-353 with the multi-prototype embedding method by <ref type="bibr" target="#b16">Huang et al. (2012)</ref> and <ref type="bibr" target="#b35">Neelakantan et al. (2014)</ref>, shown in <ref type="table" target="#tab_3">Table 3</ref>. We observe that our single- prototype model <ref type="bibr">w2g</ref> is competitive compared to models by <ref type="bibr" target="#b16">Huang et al. (2012)</ref>, even without us- ing a corpus with stop words removed. This could be due to the auto-calibration of importance via the covariance learning which decrease the impor- tance of very frequent words such as 'the', 'to', 'a', etc. Moreover, our multi-prototype model sub- stantially outperforms the model of <ref type="bibr" target="#b16">Huang et al. (2012)</ref> and the MSSG model of <ref type="bibr" target="#b35">Neelakantan et al. (2014)</ref> on the WordSim-353 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Word Similarity for Polysemous Words</head><p>We use the dataset SCWS introduced by <ref type="bibr" target="#b16">Huang et al. (2012)</ref>, where word pairs are chosen to have variations in meanings of polysemous and homonymous words.</p><p>We compare our method with multiprototype models by <ref type="bibr" target="#b16">Huang (Huang et al., 2012</ref><ref type="bibr" target="#b41">), Tian (Tian et al., 2014</ref>), ), and MSSG model by <ref type="bibr" target="#b35">(Neelakantan et al., 2014</ref>). We note that Chen model uses an external lexical source WordNet that gives it an extra advantage.</p><p>We use many metrics to calculate the scores for the Spearman correlation. MaxSim refers to the maximum cosine similarity. AveSim is the aver- age of cosine similarities with respect to the com- ponent probabilities.</p><p>In <ref type="table" target="#tab_5">Table 4</ref>, the model w2g performs the best among all single-prototype models for either 50 or 200 vector dimensions. Our model <ref type="bibr">w2gm</ref> performs competitively compared to other multi- prototype models. In SCWS, the gain in flexibility in moving to a probability density approach ap- pears to dominate over the effects of using a multi- prototype. In most other examples, we see w2gm surpass <ref type="bibr">w2g</ref>, where the multi-prototype structure is just as important for good performance as the  <ref type="table">Table 2</ref>: Spearman correlation for word similarity datasets. The models sg, w2g, w2gm denote word2vec skip-gram, Gaussian embedding, and Gaussian mixture embedding (K=2). The measures mc, el, me denote maximum cosine similarity, expected likelihood kernel, and minimum Euclidean distance. For each of w2g and w2gm, we underline the similarity metric with the best score. For each dataset, we boldface the score with the best performance across all models. The correlation scores for sg * , w2g * are taken from <ref type="bibr" target="#b42">Vilnis and McCallum (2014)</ref> and correspond to cosine distance.   <ref type="bibr" target="#b35">Neelakantan et al. (2014)</ref>. Huang * is trained using data with all stop words removed. All models have dimension D = 50 except for MSSG 300D with D = 300 which is still outper- formed by our w2gm model. probabilistic representation.</p><note type="other">Dataset sg* w2g* w2g/mc w2g/el w2g/me w2gm/mc w2gm/el w2gm/</note><note type="other">70.01 71 73.30 72.29 72.12 74.51 71.55 73.52 YP 39.34 41.5 41.96 38.38 36.41 45.07 39.18 38.58 MT-287 - - 64.79 57.5 58.31 66.60 57.24 60.61 MT-771 - - 60.86 55.89 54.12 60.82 57.26 56.43 RW - - 28.78 32.34 33.16 28.62 31.64 35.27</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Reduction in Variance of Polysemous Words</head><p>One motivation for our Gaussian mixture embed- ding is to model word uncertainty more accurately than Gaussian embeddings, which can have overly large variances for polysemous words (in order to assign some mass to all of the distinct mean- ings). We see that our Gaussian mixture model does indeed reduce the variances of each compo- nent for such words. For instance, we observe that the word rock in w2g has much higher variance per dimension (e −1.8 ≈ 1.65) compared to that of Gaussian components of rock in w2gm (which has variance of roughly e −2.5 ≈ 0.82). We also  see, in the next section, that the Gaussian mixture model has desirable quantitative behavior for word entailment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Word Entailment</head><p>We evaluate our embeddings on the word entail- ment dataset from <ref type="bibr" target="#b1">Baroni et al. (2012)</ref>. The lexical entailment between words is denoted by w 1 |= w 2 which means that all instances of w 1 are w 2 . The entailment dataset contains positive pairs such as aircraft |= vehicle and negative pairs such as air- craft |= insect.</p><p>We generate entailment scores of word pairs and find the best threshold, measured by Average Precision (AP) or F1 score, which identifies neg- ative versus positive entailment. We use the max-  <ref type="table">Table 5</ref>: Entailment results for models w2g and w2gm with window size 5 and 10. The metrics used are the maximum cosine similarity, or the maximum negative KL divergence. We calculate the best average precision as well as the best F1 score. In most cases, w2gm outperforms w2g for describing entailment.</p><p>imum cosine similarity and the minimum KL di- vergence, d(f, g) = min</p><p>i,j=1,...,K KL(f ||g), for en- tailment scores. The minimum KL divergence is similar to the maximum cosine similarity, but also incorporates the embedding uncertainty. In addi- tion, KL divergence is an asymmetric measure, which is more suitable for certain tasks such as word entailment where a relationship is unidirec- tional. For instance, w 1 |= w 2 does not imply w 2 |= w 1 . Indeed, aircraft |= vehicle does not im- ply vehicle |= aircraft, since all aircraft are vehi- cles but not all vehicles are aircraft. The difference between KL(w 1 ||w 2 ) versus KL(w 2 ||w 1 ) distin- guishes which word distribution encompasses an- other distribution, as demonstrated in <ref type="figure" target="#fig_0">Figure 1</ref>. <ref type="table">Table 5</ref> shows the results of our w2gm model versus the Gaussian embedding model w2g. We observe a trend for both models with window size 5 and 10 that the KL metric yields improvement (both AP and F1) over cosine similarity. In ad- dition, w2gm has a better performance compared to <ref type="bibr">w2g</ref>. The multi-prototype model estimates the meaning uncertainty better since it is no longer constrained to be unimodal, leading to better char- acterizations of entailment. On the other hand, the Gaussian embedding model suffers from large variance problem for polysemous words, which results in less informative word distribution and inferior entailment scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We introduced a model that represents words with expressive multimodal distributions formed from Gaussian mixtures. To learn the properties of each mixture, we proposed an analytic energy function for combination with a maximum margin objec- tive. The resulting embeddings capture different semantics of polysemous words, uncertainty, and entailment, and also perform favorably on word similarity benchmarks.</p><p>Elsewhere, latent probabilistic representations are proving to be exceptionally valuable, able to capture nuances such as face angles with varia- tional autoencoders <ref type="bibr" target="#b20">(Kingma and Welling, 2013)</ref> or subtleties in painting strokes with the InfoGAN ( . Moreover, classically deter- ministic deep learning architectures are now being generalized to probabilistic deep models, for full predictive distributions instead of point estimates, and significantly more expressive representations ( <ref type="bibr">Wilson et al., 2016b,a;</ref><ref type="bibr" target="#b0">Al-Shedivat et al., 2016;</ref><ref type="bibr" target="#b12">Gan et al., 2016;</ref><ref type="bibr" target="#b11">Fortunato et al., 2017)</ref>.</p><p>Similarly, probabilistic word embeddings can capture a range of subtle meanings, and advance the state of the art in predictive tasks. Multimodal word distributions naturally represent our belief that words do not have single precise meanings: indeed, the shape of a word distribution can ex- press much more semantic information than any point representation.</p><p>In the future, multimodal word distributions could open the doors to a new suite of applica- tions in language modelling, where whole word distributions are used as inputs to new probabilis- tic LSTMs, or in decision functions where un- certainty matters. As part of this effort, we can explore different metrics between distributions, such as KL divergences, which would be a natu- ral choice for order embeddings that model entail- ment properties. It would also be informative to explore inference over the number of components in mixture models for word distributions. Such an approach could potentially discover an unbounded number of distinct meanings for words, but also distribute the support of each word distribution to express highly nuanced meanings. Alternatively, we could imagine a dependent mixture model where the distributions over words are evolving with time and other covariates. One could also build new types of supervised language models, constructed to more fully leverage the rich infor- mation provided by word distributions.</p><p>The expected likelihood kernel is given by</p><formula xml:id="formula_9">E θ (f, g) = K i=1 piN (x; µ f,i , Σ f,i ) · K j=1 qjN (x; µg,j, Σg,j) dx = K i=1 K j=1 piqj N (x; µ f,i , Σ f,i ) · N (x; µg,j, Σg,j) dx = K i=1 K j=1 piqjN (0; µ f,i − µg,j, Σ f,i + Σg,j) = K i=1 K j=1 piqje ξ i,j</formula><p>where we note that N (x; µi, Σi)N (x; µj, Σj) dx = N (0, µi − µj, Σi + Σj) <ref type="bibr" target="#b42">(Vilnis and McCallum, 2014</ref>) and ξi,j is the log partial energy, given by equation 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Implementation</head><p>In this section we discuss practical details for training the pro- posed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reduction to Diagonal Covariance</head><p>We use a diagonal Σ, in which case inverting the covariance matrix is trivial and computations are particularly efficient.</p><p>Let where • denotes element-wise multiplication. The spherical case which we use in all our experiments is similar since we simply replace a vector d with a single value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization Constraint and Stability</head><p>We optimize log d since each component of diagonal vector d is constrained to be positive. Similarly, we constrain the probability pi to be in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> and sum to 1 by optimizing over unconstrained scores si ∈ (−∞, ∞) and using a softmax function to convert the scores to probability pi = e s i K j=1 e s j . The loss computation can be numerically unstable if ele- ments of the diagonal covariances are very small, due to the term log(d f r + d g r ) and 1 d q +d p . Therefore, we add a small constant = 10 −4 so that d f r + d g r and d q + d p becomes d f r + d g r + and d q + d p + . In addition, we observe that ξi,j can be very small which would result in e ξ i,j ≈ 0 up to machine precision. In order to stabilize the computation in eq. 2, we compute its equivalent form log E(f, g) = ξ i ,j + log</p><formula xml:id="formula_10">K j=1 K i=1 piqje ξ i,j −ξ i ,j</formula><p>where ξ i ,j = maxi,j ξi,j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Hyperparameters and Training Details</head><p>In the loss function L θ , we use a margin m = 1 and a batch size of 128. We initialize the word embeddings with a uni- form distribution over <ref type="bibr">[− 3 D , 3 D</ref> ] so that the expectation of variance is 1 and the mean is zero ( <ref type="bibr" target="#b21">LeCun et al., 1998</ref>). We initialize each dimension of the diagonal matrix (or a sin- gle value for spherical case) with a constant value v = 0.05. We also initialize the mixture scores si to be 0 so that the initial probabilities are equal among all K components. We use the threshold t = 10 −5 for negative sampling, which is the recommended value for word2vec skip-gram on large datasets.</p><p>We also use a separate output embeddings in addition to input embeddings, similar to word2vec implementation ( <ref type="bibr">Mikolov et al., 2013a,b)</ref>. That is, each word has two sets of distributions qI and qO, each of which is a Gaussian mixture. For a given pair of word and context (w, c), we use the input distribution qI for w (input word) and the output distribution qO for context c (output word). We optimize the parameters of both qI and qO and use the trained input distributions qI as our final word representations.</p><p>We use mini-batch asynchronous gradient descent with Adagrad ( <ref type="bibr" target="#b8">Duchi et al., 2011</ref>) which performs adaptive learn- ing rate for each parameter. We also experiment with Adam ( <ref type="bibr" target="#b19">Kingma and Ba, 2014</ref>) which corrects the bias in adaptive gradient update of Adagrad and is proven very popular for most recent neural network models. However, we found that it is much slower than Adagrad (≈ 10 times). This is because the gradient computation of the model is relatively fast, so a complex gradient update algorithm such as Adam becomes the bottleneck in the optimization. Therefore, we choose to use Adagrad which allows us to better scale to large datasets. We use a linearly decreasing learning rate from 0.05 to 0.00001.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Top: A Gaussian Mixture embedding, where each component corresponds to a distinct meaning. Each Gaussian component is represented by an ellipsoid, whose center is specified by the mean vector and contour surface specified by the covariance matrix, reflecting subtleties in meaning and uncertainty. On the left, we show examples of Gaussian mixture distributions of words where Gaussian components are randomly initialized. After training, we see on the right that one component of the word 'rock' is closer to 'stone' and 'basalt', whereas the other component is closer to 'jazz' and 'pop'. We also demonstrate the entailment concept where the distribution of the more general word 'music' encapsulates words such as 'jazz', 'rock', 'pop'. Bottom: A Gaussian embedding model (Vilnis and McCallum, 2014). For words with multiple meanings, such as 'rock', the variance of the learned representation becomes unnecessarily large in order to assign some probability to both meanings. Moreover, the mean vector for such words can be pulled between two clusters, centering the mass of the distribution on a region which is far from certain meanings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>d f , d g denote the diagonal vectors of Σ f , Σg The ex- pression for ξi,j reduces to+ d q • (µp,i − µq,j)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Spearman's correlation (ρ) on WordSim-
353 datasets for our Word to Gaussian Mixture 
embeddings, as well as the multi-prototype em-
bedding by Huang et al. (2012) and the MSSG 
model by </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Spearman's correlation ρ on dataset 
SCWS. We show the results for single proto-
type (top) and multi-prototype (bottom) The suffix 
-(S,M) refers to single and multiple prototype 
models, respectively. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank NSF IIS-1563887 for support.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Derivation of Expected Likelihood Kernel</head><p>We derive the form of expected likelihood kernel for Gaussian mixtures. Let f, g be Gaussian mixture distributions representing the words w f , wg.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maruan</forename><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunus</forename><surname>Saatchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.08936</idno>
		<title level="m">Learning scalable deep kernels with recurrent structure</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Entailment above the word level in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Quynh</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chieh</forename><surname>Shan</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology-new/E/E12/E12-1004.pdf" />
	</analytic>
	<monogr>
		<title level="m">EACL 2012, 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-04-23" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The wacky wide web: a collection of very large linguistically processed web-crawled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
		<idno type="doi">10.1007/s10579-009-9081-4</idno>
		<ptr target="https://doi.org/10.1007/s10579-009-9081-4" />
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
	<note>December 5-10</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D14/D14-1110.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008)</title>
		<meeting><address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06-05" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Placing search in context: the concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02798</idno>
		<title level="m">Bayesian recurrent neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08034</idno>
		<title level="m">Scalable bayesian learning of recurrent neural networks for language modeling</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="307" to="361" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale learning of word relatedness with constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Halawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;12</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-08-12" />
			<biblScope unit="page" from="1406" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Simlex999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno>CoRR abs/1408.3456</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P12-1092" />
	</analytic>
	<monogr>
		<title level="m">The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference</title>
		<meeting><address><addrLine>Jeju Island</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2012-07-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probability product kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="819" to="844" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Edmonton, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07-23" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>CoRR abs/1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the trade</title>
		<editor>G. Orr and Muller K.</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Topical word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-01-25" />
			<biblScope unit="page" from="2418" to="2424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Strategies for training large scale neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukás</forename><surname>Burget</surname></persName>
		</author>
		<idno type="doi">10.1109/ASRU.2011.6163930</idno>
		<ptr target="https://doi.org/10.1109/ASRU.2011.6163930" />
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Workshop on Automatic Speech Recognition &amp; Understanding, ASRU 2011</title>
		<meeting><address><addrLine>Waikoloa, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-01" />
			<biblScope unit="page" from="196" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukás</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernock´ynock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Makuhari, Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09-26" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukás</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<idno type="doi">10.1109/ICASSP.2011.5947611</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2011.5947611" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech, and Signal Processing<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Prague Congress Center</publisher>
			<date type="published" when="2011-05-22" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><forename type="middle">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
		<title level="m">Contextual Correlates of Semantic Similarity. Language &amp; Cognitive Processes</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<idno type="doi">10.1080/01690969108406936</idno>
		<ptr target="https://doi.org/10.1080/01690969108406936" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21, Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12-08" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics</title>
		<meeting>the Tenth International Workshop on Artificial Intelligence and Statistics<address><addrLine>Bridgetown</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-01-06" />
		</imprint>
	</monogr>
	<note>AISTATS</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Infinite dimensional word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">T</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<idno>CoRR abs/1511.05392</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient non-parametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D14/D14-1113.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1059" to="1069" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D14/D14-1162.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A word at a time: Computing word relatedness using temporal semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on World Wide Web. WWW &apos;11</title>
		<meeting>the 20th International Conference on World Wide Web. WWW &apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bayesian probabilistic matrix factorization using markov chain monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="doi">10.1145/1390156.1390267</idno>
		<ptr target="https://doi.org/10.1145/1390156.1390267" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008)</title>
		<meeting><address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06-05" />
			<biblScope unit="page" from="880" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The proof and measurement of association between two things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spearman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="88" to="103" />
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A probabilistic model for learning multi-prototype word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C/C14/C14-1016.pdf" />
	</analytic>
	<monogr>
		<title level="m">COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08-23" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Word representations via gaussian embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>CoRR abs/1412.6623</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stochastic variational deep kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Andrew G Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2586" to="2594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Andrew Gordon Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 19th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Verb similarity on the taxonomy of wordnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Powers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In the 3rd International WordNet Conference (GWC-06)</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
