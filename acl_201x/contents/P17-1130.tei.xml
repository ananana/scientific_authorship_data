<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-lingual Distillation for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">Cross-lingual Distillation for Text Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1415" to="1425"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1130</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Cross-lingual text classification(CLTC) is the task of classifying documents written in different languages into the same tax-onomy of categories. This paper presents a novel approach to CLTC that builds on model distillation, which adapts and extends a framework originally proposed for model compression. Using soft proba-bilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents , we train classifiers successfully for new languages in which labeled training data are not available. An adversarial feature adaptation technique is also applied during the model training to reduce distribution mismatch. We conducted experiments on two benchmark CLTC datasets, treating English as the source language and German, French, Japan and Chinese as the unlabeled target languages. The proposed approach had the advantageous or comparable performance of the other state-of-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The availability of massive multilingual data on the Internet makes cross-lingual text classification (CLTC) increasingly important. The task is de- fined as to classify documents in different lan- guages using the same taxonomy of predefined categories.</p><p>CLTC systems build on supervised machine learning require a sufficiently amount of labeled training data for every domain of interest in each language. But in reality, labeled data are not evenly distributed among languages and across domains. English, for example, is a label-rich lan- guage in the domains of news stories, Wikipedia pages and reviews of hotels, products, etc. But many other languages do not necessarily have such rich amounts of labeled data. This leads to an open challenge in CLTC, i.e., how can we effec- tively leverage the trained classifiers in a label-rich source language to help the classification of docu- ments in other label-poor target languages?</p><p>Existing methods in CLTC use either a bilingual dictionary or a parallel corpus to bridge language barriers and to translate classification models ( <ref type="bibr" target="#b28">Xu et al., 2016</ref>) or text data( <ref type="bibr" target="#b33">Zhou et al., 2016a</ref>). There are limitations and challenges in using ei- ther type of resources. Dictionary-based meth- ods often ignore the dependency of word mean- ing and its context, and cannot leverage domain- specific disambiguation when the dictionary on hand is a general-purpose one. Parallel-corpus based methods, although more effective in de- ploying context (when combined with word em- bedding in particular), often have an issue of do- main mismatch or distribution mismatch if the available source-language training data, the paral- lel corpus (human-aligned or machine-translation induced one) and the target documents of in- terest are not in exactly the same domain and genre <ref type="bibr" target="#b9">(Duh et al., 2011</ref>). How to solve such do- main/distribution mismatch problems is an open question for research.</p><p>This paper proposes a new parallel-corpus based approach, focusing on the reduction of do- main/distribution matches in CLTC. We call this approach Cross-lingual Distillation with Feature Adaptation or CLDFA in short. It is inspired by the recent work in model compression <ref type="bibr">(Hinton et al., 2015)</ref> where a large ensemble model is transformed to a compact (small) model. The assumption of knowledge distillation for model compression is that the knowledge learned by the large model can be viewed as a mapping from in-put space to output (label) space. Then, by train- ing with the soft labels predicted by the large model, the small model can capture most of the knowledge from the large model. Extending this key idea to CLTC, if we see parallel documents as different instantiations of the same semantic concepts in different languages, a target-language classifier should gain the knowledge from a well- trained source classifier by training with the target- language part of the parallel corpus and the soft labels made by the source classifier on the source language side. More specifically, we propose to distillate knowledge from the source language to the target language in the following 2-step process:</p><p>• Firstly, we train a source-language classi- fier with both labeled training documents and adapt it to the unlabeled documents from the source-language side of the parallel corpus. The adaptation enforces our classifier to ex- tract features that are: 1) discriminative for the classification task and 2) invariant with regard to the distribution shift between train- ing and parallel data.</p><p>• Secondly, we use the trained source-language classifier to obtain the soft labels for a par- allel corpus, and the target-language part of the parallel corpus to train a target classifier, which yields a similar category distribution over target-language documents as that over source-language documents. We also use un- labeled testing documents in the target lan- guage to adapt the feature extractor in this training step.</p><p>Intuitively, the first step addresses the potential domain/distribution mismatch between the labeled data and the unlabeled data in the source language. The second step addresses the potential mismatch between the target-domain training data (in the parallel corpus) and the test data (not in the par- allel corpus). The soft-label based training of tar- get classifiers makes our approach unique among parallel-corpus based CLTC methods (Section 2.1. The feature adaptation step makes our framework particularly robust in addressing the distributional difference between in-domain documents and par- allel corpus, which is important for the success of CLTC with low-resource languages.</p><p>The main contributions in this paper are the fol- lowing:</p><p>• We propose a novel framework (CLDFA) for knowledge distillation in CLTC through a parallel corpus. It has the flexibility to be built on a large family of existing monolin- gual text classification methods and enables the use of a large amount of unlabeled data from both source and target language.</p><p>• CLDFA has the same computational com- plexity as the plug-in text classification method and hence is very efficient and scal- able with the proper choice of plug-in text classifier.</p><p>• Our evaluation on benchmark datasets shows that our method had a better or at least com- parable performance than that of other state- of-art CLTC methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Related work can be outlined with respect to the representative work in CLTC and the recent progress in deep learning for knowledge distilla- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CLTC Methods</head><p>One branch of CLTC methods is to use lexical level mappings to transfer the knowledge from the source language to the target language. The work by <ref type="bibr" target="#b2">Bel et al. (Bel et al., 2003</ref>) was the first ef- fort to solve CLTC problem. They translated the target-language documents to source language us- ing a bilingual dictionary. The classifier trained in the source language was then applied on those translated documents. Similarly, <ref type="bibr" target="#b16">Mihalcea et al. (Mihalcea et al., 2007</ref>) built cross-lingual classi- fier by translating subjectivity words and phrases in the source language into the target language. <ref type="bibr" target="#b22">Shi et al. (Shi et al., 2010</ref>) also utilized a bilingual dictionary. Instead of translating the documents, they tried to translate the classification model from source language to target language. <ref type="bibr" target="#b20">Prettenhofer and Stein. (Prettenhofer and Stein, 2010</ref>) also used the bilingual dictionary as a word translation ora- cle and built their CLTC system on structural cor- respondence learning, a theory for domain adap- tation. A more recent work by ( <ref type="bibr" target="#b28">Xu et al., 2016)</ref> extended seminal bilingual dictionaries with unla- beled corpora in low-resource languages. <ref type="bibr" target="#b5">Chen et al. (Chen et al., 2016</ref>) used bilingual word em- bedding to map documents in source and target language into the same semantic space, and adver- sarial training was applied to enforce the trained classifier to be language-invariant. Some recent efforts in CLTC focus on the use of automatic machine translation (MT) tech- nology. For example, Wan <ref type="bibr" target="#b26">(Wan, 2009)</ref> used machine translation systems to give each doc- ument a source-language and a target-language version, where one version is machine-translated from the another one. A co-training ( <ref type="bibr" target="#b3">Blum and Mitchell, 1998</ref>) algorithm was applied on two ver- sions of both source and target documents to it- erative train classifiers in both languages. MT- based CLTC also include the work on multi-view learning with different algorithms, such as ma- jority voting( <ref type="bibr" target="#b0">Amini et al., 2009)</ref>, matrix com- pletion( <ref type="bibr" target="#b27">Xiao and Guo, 2013)</ref> and multi-view co- regularization( <ref type="bibr" target="#b12">Guo and Xiao, 2012a)</ref>.</p><p>Another branch of CLTC methods focuses on representation learning or the mapping of the in- duced representations in cross-language settings <ref type="bibr" target="#b13">(Guo and Xiao, 2012b;</ref><ref type="bibr" target="#b33">Zhou et al., 2016a</ref><ref type="bibr" target="#b32">Zhou et al., , 2015</ref><ref type="bibr" target="#b34">Zhou et al., , 2016b</ref><ref type="bibr" target="#b27">Xiao and Guo, 2013;</ref><ref type="bibr">Jagarlamudi et al., 2011;</ref><ref type="bibr" target="#b8">De Smet et al., 2011;</ref><ref type="bibr" target="#b25">Vinokourov et al., 2002;</ref><ref type="bibr" target="#b19">Platt et al., 2010;</ref><ref type="bibr">Littman et al., 1998</ref>). For example, <ref type="bibr" target="#b15">Meng et al. (Meng et al., 2012</ref>) and <ref type="bibr">Lu et al. (Lu et al., 2011</ref>) used a parallel corpus to learn word alignment probabilities in a pre-processing step. Some other work attempts to find a language- invariant (or interlingua) representation for words or documents in different languages using vari- ous techniques, such as latent semantic indexing ( <ref type="bibr">Littman et al., 1998)</ref>, kernel canonical correlation analysis ( <ref type="bibr" target="#b25">Vinokourov et al., 2002</ref>), matrix comple- tion( <ref type="bibr" target="#b27">Xiao and Guo, 2013)</ref>, principal component analysis ( <ref type="bibr" target="#b19">Platt et al., 2010)</ref> and Bayesian graphi- cal models <ref type="bibr" target="#b8">(De Smet et al., 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Distillation</head><p>The idea of distilling knowledge in a neural net- work was proposed by Hinton et al ( <ref type="bibr">Hinton et al., 2015)</ref>, in which they introduced a student-teacher paradigm. Once the cumbersome teacher network was trained, the student network was trained ac- cording to soften predictions of the teacher net- work. In the field of computer vision, it has been empirically verified that student network trained by distillation performs better than the one trained with hard labels. ( <ref type="bibr">Hinton et al., 2015;</ref><ref type="bibr" target="#b21">Romero et al., 2014;</ref><ref type="bibr" target="#b1">Ba and Caruana, 2014</ref>). <ref type="bibr">Gupta et al.(Gupta et al., 2015</ref>) transfers supervision be- tween images from different modalities(e.g. from RGB image to depth image). There are also some recent works applied distillation in the field of nat- ural language. For example, <ref type="bibr" target="#b18">Lili et al. (Mou et al., 2015</ref>) distilled task specific knowledge from a set of high-dimensional embeddings to a low- dimensional space. Zhiting et al. used an iterative distillation method to transfer the structured infor- mation of logic rules into the weights of a neural network. <ref type="bibr">Kim et al. (Kim and Rush, 2016)</ref> applied knowledge distillation approaches in the field of machine translation to reduce the size of neural machine translation model. Our framework shares the same purpose of existing works that trans- fer knowledge between models of different prop- erties, such as model complexity, modality, and structured logic. However, our transfer happens between models working on different languages. To the best of knowledge, this is the first work us- ing knowledge distillation to bridge the language gap for NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task and Notation</head><p>CLTC aims to use the training data in the source language to build a model applicable in the target language. In our setting, we have labeled data in source language</p><formula xml:id="formula_0">L src = {x i , y i } L i=1</formula><p>, where x i is the labeled document in source language and y i is the label vector. We then have our test data in the target language, given by</p><formula xml:id="formula_1">T tgt = {x i } T i=1</formula><p>. Our framework can also use unlabeled documents from both languages in transductive learning set- tings. We use</p><formula xml:id="formula_2">U src = {x i } M i=1 to denote source- language unlabeled documents,U tgt = {x i } N i=1</formula><p>to denote target-language unlabeled documents, and</p><formula xml:id="formula_3">U parl = {(x i , x i )} P i=1</formula><p>to denote a unlabeled bilin- gual parallel corpus where x i and x i are paired document translations of each other. We assume that the unlabeled parallel corpus does not overlap with the source-language training documents and the target-language test documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolutional Neural Network (CNN) as a Plug-in Classifier</head><p>We use a state-of-the-art CNN-based neural net- work classifier <ref type="bibr">(Kim, 2014)</ref> as the plug-in classi- fier in our framework. Instead of using a bag-of- words representation for each document, the CNN model concatenates the word embeddings (verti- cal vectors) of each input document into a n × k matrix, where n is the length (number of word oc- currences) of the document, and k is the dimension of word embedding. Denoting by</p><formula xml:id="formula_4">x 1:n = x 1 ⊕ x 2 ⊕ ... ⊕ x n</formula><p>as the resulted matrix, with ⊕ the concatena- tion operator. One-dimensional convolutional fil- ter w ∈ R hk with window size h operates on ev- ery consecutive h words, with non-linear function f and bias b. For window of size h started at index i, the feature after convolutional filter is given by:</p><formula xml:id="formula_5">c i = f (w · x i:i+h−1 + b)</formula><p>A max-over-time pooling <ref type="bibr" target="#b6">(Collobert et al., 2011</ref>) is applied on c over all possible positions such that each filter extracts one feature. The model uses multiple filters with different window sizes. The concatenated outputs from filters consist the fea- ture of each document. We can see the convolu- tional filters and pooling layers as feature extractor</p><formula xml:id="formula_6">f = G f (x, θ f )</formula><p>, where θ f contains parameters for embedding layer and convolutional layer. Theses features are then passed to a fully connected soft- max layer to produce probability distributions over labels. We see the final fully connected softmax layer as a label classifier G y (f , θ y ) that takes the output f from the feature extractor. The final out- put of model is given by G y (G f (x, θ f ), θ y ), which is jointly parameterized by {θ f , θ y } We want to emphasize that our choice of the plug-in classifier here is mainly for its simplic- ity and scalability to demonstrate our framework. There is a large family of neural classifiers for monolingual text classification that could be used in our framework as well, including other convo- lutional neural networks by <ref type="bibr">(Johnson and Zhang, 2014)</ref>, the recurrent neural networks by <ref type="bibr">(Lai et al., 2015;</ref><ref type="bibr" target="#b30">Zhang et al., 2016;</ref><ref type="bibr">Johnson and Zhang, 2016;</ref><ref type="bibr" target="#b23">Sutskever et al., 2014;</ref><ref type="bibr" target="#b7">Dai and Le, 2015</ref>), the attention mechanism by , the deep dense network by <ref type="bibr">(Iyyer et al., 2015)</ref>, and more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Framework</head><p>Let us introduce two versions of our model for cross-language knowledge distillation, i.e., the vanilla version and the full version with feature adaptation. Both are supported by the proposed framework. We denote the former by CLD-KCNN and the latter by CLDFA-KCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Vanilla Distillation</head><p>Without loss of generality, assume we are learning a multi-class classifier for the target language. We have y ∈ 1, 2, ..., |v | where v is the set of all possi- ble classes. We assume the base classification net- work produces real number logits q j for each class. For example, for the case of CNN text classifier, the logits can be produced by a linear transforma- tion which takes features extracted max-pooling layer and outputs a vector of size |v |. The logits are converted into probabilities of classes through the softmax layer, by normalizing each q j with all other logits.</p><formula xml:id="formula_7">p j = exp(q j /T ) |v | k=1 exp(q k /T ) (1)</formula><p>where T is a temperature and is normally set to 1. Using a higher value of T produces a softer probability distribution over classes.</p><p>The first step of our framework is to train the source-language classifier on labeled source docu- ments L src . We use standard temperature T = 1 and cross-entropy loss as the objective to mini- mize. For each example and its label (x i , y i ) from the source training set, we have:</p><formula xml:id="formula_8">L(θ src ) = − (x i ,y i )∈Lsrc |v | k=1 1{y i = k} log p(y = k|x i ; θ src )<label>(2)</label></formula><p>where p(y = k|x; θ src ) is source model con- trolled by parameter θ src and 1{·} is the indicator function.</p><p>In the second step, the knowledge captured in θ src is transferred to the distilled model in the target language by training it on the parallel cor- pus. The intuition is that paired documents in parallel corpus should have the same distribution of class predicted by the source model and tar- get model. In the simplest version of our frame- work, for each source-language document in the parallel corpus, we predict a soft class distribution by source model with high temperature. Then we minimize the cross-entropy between soft distribu- tion produced by source model and the soft dis- tribution produced by target model on the paired documents in the target language. More formally, we optimize θ tgt according to the following loss function for each document pair (x i , x i ) in paral- lel corpus.</p><formula xml:id="formula_9">L(θ tgt ) = − (x i ,x i )∈U parl |v | k=1 p(y = k|x i ; θ src ) log p(y = k|x i ; θ tgt )<label>(3)</label></formula><p>During distillation, the same high temperature is used for training target model. After it has been trained, we set the temperature to 1 for testing.</p><p>We can show that under some assumptions, the two-step cross-lingual distillation is equivalent to distilling a target-language classifier in the target- language input space.</p><formula xml:id="formula_10">Lemma 1. Assume the parallel corpus {x i , x i } ∈ U parl is generated by x i ∼ p(X ; η) and x i = t(x i )</formula><p>, where η controls the marginal distribution of x i and t is a differentiable translation func- tion with integrable derivative. Let f θsrc (t(x )) be the function that outputs soft labels of p(y = k|t(x ); θ src ). The distillation given by equation 3 can be interpreted as distillation of a target lan- guage classifier f θsrc (t(x )) on target language documents sampled from p(X ; η). f θsrc (t(x )) is the classifier that takes input of target documents, translates them into source doc- uments through t and makes prediction using the source classifier. If we further assume the test- ing documents have the same marginal distribu- tion P (X ; η), then the distilled classifier should have similar generalization power as f θsrc (t(x )).</p><p>Theorem 2. Let source training data x i ∈ L src has marginal distribution p(X; λ). Un- der the assumptions of lemma 1, further as- sume p(t(x ); λ) = p(x ; η), p(y|t(x )) = p(y|x ) and t (x ) ≈ C, where C is a constant.</p><p>Then f θsrc (t(x )) actually mini- mizes the expected loss in target language data</p><formula xml:id="formula_11">E x ∼p(X;η),y∼p(Y |x ) [L y, f (t(x )) ].</formula><p>Proof. By definition of equation 2, f θsrc (x) minimizes the expected loss</p><formula xml:id="formula_12">E x∼p(X;λ),y∼p(Y |x) [L y, f (x) ]</formula><p>, where L is cross-entropy loss in our case. Then we can write Although vanilla distillation is intuitive and simple, it cannot handle distribution mismatch is- sues. For example, the marginal feature distribu- tions of source-language documents in L src and U parl could be different, so are the distributions of target-language documents in U parl and T tgt . Ac- cording to theorem 2, the vanilla distillation works for the best performance under unrealistic assump- tion: p(t(x )|λ) = p(x |η). To further illustrate our point, we trained a CNN classifier according to equation 2 and used the features extracted by G f to present the source-language documents in both L src and U parl . Then we projected the high- dimensional features onto a 2-dimensional space via t-Distributed Stochastic Neighbor Embedding (t-SNE) <ref type="bibr" target="#b14">(Maaten and Hinton, 2008)</ref>. This resulted the visualization of the project data in <ref type="figure" target="#fig_0">Figures 1  and 2</ref>.</p><formula xml:id="formula_13">E x∼p(X;λ),y∼p(Y |x) [L y, f (x) ] = p(x; λ) y p(y|x)L y, f (x) dx = p(t(x ); λ) y p(y|t(x ))L y, f (t(x )) t (x )dx ≈C p(x ; η) y p(y|x )L y, f (t(x )) dx =CE x ∼p(X;η),y∼p(Y |x ) [L y, f (t(x )) ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Distillation with Adversarial Feature Adaptation</head><p>It is quite obvious in <ref type="figure" target="#fig_0">Figure 1</ref> that the general- purpose parallel corpus has a very different fea- ture distribution from that of the labeled source training set. Even for machine-translated paral- lel data from the same domain, as shown in <ref type="figure" target="#fig_1">fig- ure 2</ref>, there is still a non-negligible distribution shift from the source language to the target lan- guage for the extracted features. Our interpreta- tion of this observation is that when the MT sys- tem (e.g. Google Translate) is a general-purpose one, it non-avoidably add translation ambiguities which would lead the distribution shift from the original domain. To address the distribution di- vergence brought by either a general-purpose par- allel corpus or an imperfect MT system, we seek to adapt the features extraction part of our neu- ral classifier such that the feature distributions on both sides should be close as possible in the newly induced feature space. We adapt the adversarial training method by <ref type="bibr" target="#b11">(Ganin and Lempitsky, 2014)</ref> to the cross-lingual settings in our problems.</p><p>Given a set of training set of L = {x i , y i } i=1,...,N and an unlabeled set U = {x i } i=1,...,M , our goal is to find a neural classifier G y (G f (x, θ f ), θ y ), which has good discriminative performance on L and also extracts features which have similar distributions on L and U . One way to maximize the similarity of two distributions is to maximize the loss of a discriminative classifier whose job is to discriminate the two feature dis- tributions. We denote this classifier by G d <ref type="figure">(·, θ d )</ref>, which is parameterized by θ d .</p><p>At training time, we seek θ f to minimize the loss of G y and maximize the loss of G d . Mean- while, θ y and θ d are also optimized to minimize their corresponding loss. The overall optimization could be summarized as follows:</p><formula xml:id="formula_14">E(θ f , θ y , θ d ) = x i ,y i ∈L L y (y i , G y (G f (x i , θ f ), θ y )) − α x i ∈L L d (0, G d (G f (x i , θ f ), θ d )) − α x j ∈U L d (1, G d (G f (x j , θ f ), θ d ))</formula><p>where L y is the loss function for true labels y, L d is loss function for binary labels indicat- ing the source of data and α is the hyperparam- eter that controls the relative importance of two losses. We optimize θ f , θ y for minimizing E and optimize θ d for maximizing E. We jointly optimize θ f , θ y , θ d through the gradient reversal layer( <ref type="bibr" target="#b11">Ganin and Lempitsky, 2014</ref>).</p><p>We use this feature adaptation technique to firstly adapt the source-language classifier to the source-language documents of the parallel cor- pus. When training the target-language classifier by matching soft labels on the parallel corpus, we also adapt the classifier to the target testing docu- ments. We use cross-entropy loss functions as L y and L d for both feature adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>Our experiments used two benchmark datasets, as described below.  We used the multilingual multi-domain Ama- zon review dataset created by <ref type="bibr" target="#b20">Prettenhofer and Stein (Prettenhofer and Stein, 2010)</ref>. The dataset contains Amazon reviews in three domains: book, DVD and music. Each domain has the reviews in four different languages: English, German, French and Japanese. We treated English as the source language and the rest three as the target languages, respectively. This gives us 9 tasks (the product of the 3 domains and the 3 target languages) in total. For each task, there are 1000 positive and 1000 negative reviews in English and the target language, respectively. ( <ref type="bibr" target="#b20">Prettenhofer and Stein, 2010</ref>) also provides 2000 parallel reviews per task, that were generated using Google Translate 1 , and used by us for cross-language distillation. There are also several thousands of unlabeled reviews in each language. The statistics of unlabeled data is summarized in <ref type="table" target="#tab_1">Table 1</ref>. All the reviews are tok- enized using standard regular expressions except for Japanese, for which we used a publicly avail- able segmenter 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(1) Amazon Reviews</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(2) English-Chinese Yelp Hotel Reviews</head><p>This dataset was firstly used for CLTC by <ref type="bibr" target="#b5">(Chen et al., 2016</ref>). The task is to make sentence-level sentiment classification with 5 labels(rating scale from 1 to 5), using English as the source language and Chinese as the target language. The labeled English data consists of balanced labels of 650k Yelp reviews from . The Chinese data includes 20k labeled Chinese hotel reviews and 1037k unlabeled ones from ( <ref type="bibr">Lin et al., 2015)</ref>. Following the approach by <ref type="bibr" target="#b5">(Chen et al., 2016)</ref>, we use 10k of labeled Chi- nese data as validation set and another 10k hotel reviews as held-out test data. We a random sample of 500k parallel sentences from UM-courpus( <ref type="bibr" target="#b24">Tian et al., 2014</ref>), which is a general-purpose corpus designed for machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We compare the proposed method with other state- of-the-art methods as outlined below.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(3) Adversarial Deep Averaging Network</head><p>Similar to our approach, the adversarial Deep Av- eraging Network (ADAN) also exploits adversar- ial training for CLTC <ref type="bibr" target="#b5">(Chen et al., 2016)</ref>. How- ever, it does not have the parallel-corpus based knowledge distillation part (which we do). In- stead, it uses averaged bilingual embeddings of words as its input and adapts the feature extractor to produce similar features in both languages.</p><p>We also include the results of mSDA for the Yelp Hotel Reviews dataset. mSDA <ref type="bibr" target="#b4">(Chen et al., 2012</ref>) is a domain adaptation method based on   <ref type="table">Table 3</ref>: Accuracy scores of methods on the English-Chinese Yelp Hotel Reviews dataset stacked denoising autoencoders, which has been proved to be effective in cross-domain sentiment classification evaluations. We show the results re- ported by <ref type="bibr" target="#b4">(Chen et al., 2012</ref>), where they used bilingual word embedding as input for mSDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation Detail</head><p>We pre-trained both the source and target classi- fier with unlabeled data in each language. We ran word2vec( <ref type="bibr" target="#b17">Mikolov et al., 2013</ref>) 3 on the tokenized unlabeled corpus. The learned word embeddings are used to initialize the word embedding look-up matrix, which maps input words to word embed- dings and concatenates them into input matrix.</p><p>We fine-tuned the source-language classifier on the English training data with 5-fold cross- validation. For English-Chinese Yelp-hotel review dataset, the temperature T (Section 4.1) in distil- lation is tuned on validation set in the target lan- guage. For Amazon review dataset, since there is no default validation set, we set temperature from low to high in {1, 3, 5, 10} and take the average among all predictions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Main Results</head><p>In tables 2 and 3 we compare the results of our methods (the vanilla version CLD-KCNN and the full version CLDFA-KCNN) with those of other methods based on the published results in the lit- erature. The baseline methods are different in these two tables as they were previously evaluated (by their authors) on different benchmark datasets. Clearly, CLDFA-KCNN outperformed the other methods on all except one task in these two datasets, showing that knowledge distillation is successfully carried out in our approach. Noticing that CLDFA-KCNN outperformed CLD-KCNN, showing the effectiveness of adversarial feature extraction in reducing the distribution mismatch between the parallel corpus and the train/test data in the target domain. We should also point out that in <ref type="table" target="#tab_3">Table 2</ref>, the four baseline methods (PL-LSI, PL-KCCA, PL-OPCA and PL-MC) were evalu- ated under the condition of using additional 100 labeled target documents for training, according to the author's report <ref type="bibr" target="#b27">(Xiao and Guo, 2013</ref>). On the other hand, our methods (CLD-KCNN and CLDFA-KCNN) were evaluated under a tougher condition, i.e., not using any labeled data in the target domains.</p><p>We also test our framework when a few train- ing documents in the target language are available. A simple way to utilize the target-language super- vision is to fit the target-language model with la- beled target data after optimizing with our cross- lingual distillation framework. The performance of CLD-KCNN and CLDFA-KCNN trained with different sizes of labeled target-language data is shown in <ref type="figure" target="#fig_5">figure 3</ref>. We also compare the perfor- mance of training the same classifier using only the target-language labels(Target Only in <ref type="figure" target="#fig_5">figure  3</ref>). As we can see, our framework can efficiently utilize the extra supervision and improve the per- formance over the training using only the target- language labels. The margin is most significant when the size of the target-language label is rela- tively small. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This work introduces a novel framework for dis- tillation of discriminative knowledge across lan- guages, providing effective and efficient algorith- mic solutions for addressing domain/distribution mismatch issues in CLTC. The excellent perfor- mance of our approach is evident in our evalua- tion on two CLTC benchmark datasets, compared to that of other state-of-the-art methods. <ref type="bibr">Saurabh Gupta, Judy Hoffman, and Jitendra Malik. 2015</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Extracted features for source-language documents in the English-Chinese Yelp Hotel Review dataset. Red dots represent features of the documents in L src and green dots represent the features of documents in U parl , which is a generalpurpose parallel corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Extracted features for the source-language documents in the Amazon Reviews dataset. Red dots represent the features of the labeled training documents in L src , and green dots represent the features of the documents in U parl , which are the machine-translated documents from a target language. Below each figure is the target language and the domain of review (Section 5.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 1 )</head><label>1</label><figDesc>Parallel-Corpus based CLTC Methods Methods in this category all use an unlabeled par- allel corpus. Methods named PL-LSI (Littman et al., 1998), PL-OPCA (Platt et al., 2010) and PL-KCAA (Vinokourov et al., 2002) learn la- tent document representations in a shared low- dimensional space by performing the Latent Semantic Indexing (LSI), the Oriented Princi- pal Component Analysis (OPCA) and a kernel (namely KCAA) for the parallel text. PL-MC (Xiao and Guo, 2013) recovers missing features via matrix Completion, and also uses LSI to in- duce a latent space for parallel text. All these methods train a classifier in the shared feature space with labeled training data from both the source and target languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 2 )</head><label>2</label><figDesc>MT-based CLTC Methods The methods in this category all use an MT sys- tem to translate each test document in the tar- get language to the source language in the test- ing phase. The prediction on each translated document is made by a source-language classi- fier, which can be a Logistic Regression model (MT+LR) (Chen et al., 2016) or a deep averaging network (MT+DAN) (Chen et al., 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3</head><label></label><figDesc>https://code.google.com/archive/p/word2vec/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Accuracy scores of methods using varying sizes of target-language labeled data on the Amazon review dataset. The target language is German and the domain is music. The parallel corpus has a fixed size of 1000 and the size of the labeled target-language documents is shown on the x-axis</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Dataset Statistics for the Amazon reviews dataset</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Target Language Domain PL-LSI PL-KCCA PL-OPCA PL-MC CLD-KCNN CLDFA-KCNN</head><label></label><figDesc></figDesc><table>German 

book 
77.59 
79.14 
74.72 
79.22 
82.54 
83.95* 
DVD 
79.22 
76.73 
74.59 
81.34 
82.24 
83.14* 
music 
73.81 
79.18 
74.45 
79.39 
74.65 
79.02 

French 

book 
79.56 
77.56 
76.55 
81.92 
81.6 
83.37 
DVD 
77.82 
78.19 
70.54 
81.97 
82.41 
82.56 
music 
75.39 
78.24 
73.69 
79.3 
83.01 
83.31* 

Janpanese 

book 
72.68 
69.46 
71.41 
72.57 
74.12 
77.36* 
DVD 
72.55 
74.79 
71.84 
76.6 
79.67 
80.52* 
music 
73.44 
73.54 
74.96 
76.21 
73.69 
76.46 
Averaged Accuracy 
75.78 
76.31 
73.64 
78.72 
79.33 
81.08* 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Accuracy scores of methods on the Amazon Reviews dataset: the best score in each row (a 
task) is highlighted in bold face. If the score of CLDFA-KCNN is statistically significantly better (in 
one-sample proportion tests) than the best among the baseline methods, it is marked using a star. 

Model 
Accuracy 
mSDA 
31.44% 
MT-LR 
34.01% 
MT-DAN 
39.66% 
ADAN 
41.04% 

CLD-KCNN 
40.96% 
CLDFA-KCNN 41.82% 

</table></figure>

			<note place="foot" n="1"> translate.google.com 2 https://pypi.python.org/pypi/tinysegmenter</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank the reviewers for their helpful com-ments. This work is supported in part by De-fense Advanced Research Projects Agency Infor-mation Innovation Oce (I2O), the Low Resource Languages for Emergent Incidents (LORELEI) Program, Issued by DARPA/I2O under Contract No. HR0011-15-C-0114, by the National Science Foundation (NSF) under grant IIS-1546329.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning from multiple partially observed views-an application to multilingual text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massih</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Goutte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cross-lingual text categorization. Research and Advanced Technology for Digital Li</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuria</forename><surname>Bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Cornelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Koster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Villegas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="126" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
		<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.4683</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adversarial deep averaging networks for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01614</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge transfer across multilingual corpora via latent topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wim De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="549" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Is machine translation ripe for cross-lingual sentiment classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th</title>
		<meeting>the 49th</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="429" to="433" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7495</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Cross language text classification via subspace co-regularized multiview learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6481</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transductive representation learning for cross-lingual text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM), 2012 IEEE 12th International Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="888" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cross-lingual mixture model for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinfan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="572" to="581" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multilingual subjective language via cross-lingual projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><forename type="middle">M</forename><surname>Wiebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04488</idno>
		<title level="m">Distilling word embeddings: An encoding approach</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Translingual document representations from discriminative projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>John C Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="251" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Crosslanguage text classification using structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross language text classification by model translation and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjun</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1057" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Um-corpus: A large english-chinese parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Liang Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Quaresma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1837" to="1842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Inferring a semantic representation of text via cross-language correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Vinokourov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Co-training for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A novel two-step method for cross language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1259" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-lingual text classification via model translation with limited dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dependency sensitive convolutional neural networks for modeling sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02361</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning bilingual sentiment word embeddings for cross-language sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fulin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Degen</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Cross-lingual sentiment classification with bilingual document representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Attention-based lstm network for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
