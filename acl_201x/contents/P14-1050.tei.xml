<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">New Word Detection for Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
							<email>aihuang@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqiang</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Dept. of Communication Engineering</orgName>
								<orgName type="department" key="dep2">Information Technology Security Evaluation Center</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications *</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Dept. of Communication Engineering</orgName>
								<orgName type="department" key="dep2">Information Technology Security Evaluation Center</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications *</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab. of Intelligent Technology and Systems</orgName>
								<orgName type="department" key="dep2">National Lab. for Information Science and Technology</orgName>
								<orgName type="department" key="dep3">Dept. of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">New Word Detection for Sentiment Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="531" to="541"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Automatic extraction of new words is an indispensable precursor to many NLP tasks such as Chinese word segmentation, named entity extraction, and sentiment analysis. This paper aims at extracting new sentiment words from large-scale user-generated content. We propose a fully unsupervised, purely data-driven framework for this purpose. We design statistical measures respectively to quantify the utility of a lexical pattern and to measure the possibility of a word being a new word. The method is almost free of linguistic resources (except POS tags), and requires no elaborated linguistic rules. We also demonstrate how new sentiment word will benefit sentiment analysis. Experiment results demonstrate the effectiveness of the proposed method.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>New words on the Internet have been emerg- ing all the time, particularly in user-generated con- tent. Users like to update and share their infor- mation on social websites with their own language styles, among which new political/social/cultural words are constantly used.</p><p>However, such new words have made many natural language processing tasks more challeng- ing. Automatic extraction of new words is indis- pensable to many tasks such as Chinese word seg- mentation, machine translation, named entity ex- traction, question answering, and sentiment analy- sis. New word detection is one of the most critical issues in Chinese word segmentation. Recent stud- ies <ref type="bibr" target="#b16">(Sproat and Emerson, 2003)</ref>  <ref type="bibr" target="#b3">(Chen, 2003)</ref> have shown that more than 60% of word segmentation errors result from new words. Statistics show that more than 1000 new Chinese words appear every year <ref type="bibr" target="#b18">(Thesaurus Research Center, 2003)</ref>. These words are mostly domain-specific technical terms and time-sensitive political/social /cultural terms. Most of them are not yet correctly recognized by the segmentation algorithm, and remain as out of vocabulary (OOV) words.</p><p>New word detection is also important for sen- timent analysis such as opinionated phrase ex- traction and polarity classification. A sentiment phrase with complete meaning should have a cor- rect boundary, however, characters in a new word may be broken up. For example, in a sentence " 表 演/ n 非 常/ adv 给/ v 力/ n（artists' perfor- mance is very impressive）" the two Chinese char- acters"给/v 力/n(cool; powerful)"should always be extracted together. In polarity classification, new words can be informative features for clas- sification models. In the previous example, "给 力(cool; powerful)" is a strong feature for clas- sification models while each single character is not. Adding new words as feature in classification models will improve the performance of polarity classification, as demonstrated later in this paper.</p><p>This paper aims to detect new word for senti- ment analysis. We are particulary interested in ex- tracting new sentiment word that can express opin- ions or sentiment, which is of high value toward- s sentiment analysis. New sentiment word, as ex- emplified in <ref type="table">Table 1</ref>, is a sub-class of multi-word expressions which is a sequence of neighboring words "whose exact and unambiguous meaning or connotation cannot be derived from the mean- ing or connotation of its components" <ref type="bibr" target="#b4">(Choueka, 1988)</ref>. Such new words cannot be directly iden- tified using grammatical rules, which poses a ma- jor challenge to automatic analysis. Moreover, ex- isting lexical resources never have adequate and timely coverage since new words appear constant- ly. People thus resort to statistical methods such as Pointwise Mutual Information <ref type="bibr" target="#b5">(Church and Hanks, 1990</ref>), Symmetrical Conditional Probability (da <ref type="bibr" target="#b6">Silva and Lopes, 1999</ref>), Mutual Expectation ( <ref type="bibr" target="#b7">Dias et al., 2000</ref>), Enhanced Mutual Information ( , and Multi-word Expression Distance ( <ref type="bibr" target="#b1">Bu et al., 2010</ref> Our central idea for new sentiment word de- tection is as follows: Starting from very few seed words (for example, just one seed word), we can extract lexical patterns that have strong statistical association with the seed words; the extracted lex- ical patterns can be further used in finding more new words, and the most probable new words can be added into the seed word set for the next iter- ation; and the process can be run iteratively un- til a stop condition is met. The key issues are to measure the utility of a pattern and to quantify the possibility of a word being a new word. The main contributions of this paper are summarized as fol- lows:</p><p>• We propose a novel framework for new word detection from large-scale user-generated da- ta. This framework is fully unsupervised and purely data-driven, and requires very lightweight linguistic resources (i.e., only POS tags).</p><p>• We design statistical measures to quantify the utility of a pattern and to quantify the possi- bility of a word being a new word, respective- ly. No elaborated linguistic rules are needed to filter undesirable results. This feature may enable our approach to be portable to other languages.</p><p>• We investigate the problem of polarity predic- tion of new sentiment word and demonstrate that inclusion of new sentiment word benefits sentiment classification tasks.</p><p>The rest of the paper is structured as follows: we will introduce related work in the next section. We will describe the proposed method in Section 3, including definitions, the overview of the algorith- m, and the statistical measures for addressing the two key issues. We then present the experiments in Section 4. Finally, the work is summarized in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>New word detection has been usually inter- weaved with word segmentation, particularly in Chinese NLP. In these works, new word detection is considered as an integral part of segmentation, where new words are identified as the most proba- ble segments inferred by the probabilistic models; and the detected new word can be further used to improve word segmentation. Typical models in- clude conditional random fields proposed by <ref type="bibr" target="#b13">(Peng et al., 2004)</ref>, and a joint model trained with adap- tive online gradient descent based on feature fre- quency information <ref type="bibr" target="#b17">(Sun et al., 2012)</ref>.</p><p>Another line is to treat new word detection as a separate task, usually preceded by part-of-speech tagging. The first genre of such studies is to lever- age complex linguistic rules or knowledge. For example, <ref type="bibr" target="#b10">Justeson and Katz (1995)</ref> extracted tech- nical terminologies from documents using a regu- lar expression. <ref type="bibr" target="#b0">Argamon et al. (1998)</ref> segmented the POS sequence of a multi-word into small POS tiles, counted tile frequency in the new word and non-new-word on the training set respectively, and detected new words using these counts. <ref type="bibr" target="#b2">Chen and Ma (2002)</ref> employed morphological and statisti- cal rules to extract Chinese new word. The sec- ond genre of the studies is to treat new word de- tection as a classification problem. <ref type="bibr" target="#b24">Zhou (2005)</ref> proposed a discriminative Markov Model to de- tect new words by chunking one or more separat- ed words. In ( <ref type="bibr" target="#b11">Li et al., 2005</ref>), new word detec- tion was viewed as a binary classification problem. However, these supervised models requires not on- ly heavy engineering of linguistic features, but also expensive annotation of training data.</p><p>User behavior data has recently been explored for finding new words. <ref type="bibr" target="#b23">Zheng et al. (2009)</ref> ex- plored user typing behaviors in Sogou Chinese Pinyin input method to detect new words. <ref type="bibr" target="#b22">Zhang et al. (2010)</ref> proposed to use dynamic time warp- ing to detect new words from query logs. Howev- er, both of the work are limited due to the public unavailability of expensive commercial resources.</p><p>Statistical methods for new word detection have been extensively studied, and in some sense exhibit advantages over linguistics-based method- s. In this setting, new word detection is mostly known as multi-word expression extraction. To measure multi-word association, the first model is Pointwise Mutual Information (PMI) <ref type="bibr" target="#b5">(Church and Hanks, 1990)</ref>. Since then, a variety of sta- tistical methods have been proposed to measure bi-gram association, such as Log-likelihood <ref type="bibr" target="#b8">(Dunning, 1993)</ref> and Symmetrical Conditional Proba- bility (SCP) <ref type="bibr" target="#b6">(da Silva and Lopes, 1999</ref>). Among all the 84 bi-gram association measures, PMI has been reported to be the best one in Czech data <ref type="bibr" target="#b12">(Pecina, 2005)</ref>. In order to measure arbitrary n- grams, most common strategies are to separate n- gram into two parts X and Y so that existing bi- gram methods can be used (da <ref type="bibr" target="#b6">Silva and Lopes, 1999;</ref><ref type="bibr" target="#b7">Dias et al., 2000;</ref><ref type="bibr" target="#b15">Schone and Jurafsky, 2001</ref>).  proposed Enhanced Mutual Information (EMI) which measures the co- hesion of n-gram by the frequency of itself and the frequency of each single word. Based on the in- formation distance theory, <ref type="bibr" target="#b1">Bu et al. (2010)</ref> pro- posed multi-word expression distance (MED) and the normalized version, and reported superior per- formance to EMI, SCP, and other measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definitions</head><p>Definition 3.1 (Adverbial word). Words that are used mainly to modify a verb or an adjective, such as "太(too)", "非常(very)", "十分(very)", and "特 别(specially)". Definition 3.2 (Auxiliary word). Words that are auxiliaries, model particles, or punctuation marks. In Chinese, such words are like "着,了,啦,的,啊", and punctuation marks include "，。！？；：" and so on. Definition 3.3 (Lexical Pattern). A lexical pat- tern is a triplet &lt; AD, * , AU &gt;, where AD is an adverbial word, the wildcard * means an arbitrary number of words 1 , and AU denotes an auxiliary word. <ref type="table">Table 2</ref> gives some examples of lexical pat- terns. In order to obtain lexical patterns, we can define regular expressions with POS tags 2 and ap- ply the regular expressions on POS tagged texts. Since the tags of adverbial and auxiliary words are relatively static and can be easily identified, such a method can safely obtain lexical patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pattern</head><p>Frequency &lt;"都",*,"了"&gt; 562,057 &lt;"都",*,"的"&gt; 387,649 &lt;"太",*,"了"&gt; 380,470 &lt;"不",*,"，"&gt; 369,702 <ref type="table">Table 2</ref>: Examples of lexical pattern. The frequen- cy is counted on 237,108,977 Weibo posts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Algorithm Overview</head><p>The algorithm works as follows: starting from very few seed words (for example, a word in <ref type="table">Table 1</ref>), the algorithm can find lexical pattern- s that have strong statistical association with the seed words in which the likelihood ratio test (L- RT) is used to quantify the degree of association. Subsequently, the extracted lexical patterns can be further used in finding more new words. We de- sign several measures to quantify the possibility of a candidate word being a new word, and the top- ranked words will be added into the seed word set for the next iteration. The process can be run iter- atively until a stop condition is met. Note that we do not augment the pattern set (P) at each iteration, instead, we keep a fixed small number of patterns during iteration because this strategy produces op- timal results.</p><p>From linguistic perspectives, new sentiment words are commonly modified by adverbial words and thus can be extracted by lexical patterns. This is the reason why the algorithm will work. Our al- gorithm is in spirit to double propagation ( <ref type="bibr" target="#b14">Qiu et al., 2011</ref>), however, the differences are apparen- t in that: firstly, we use very lightweight linguis- tic information (except POS tags); secondly, our major contributions are to propose statistical mea- sures to address the following key issues: first, to measure the utility of lexical patterns; second, to measure the possibility of a candidate word being a new word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Measuring the Utility of a Pattern</head><p>The first key issue is to quantify the utility of a pattern at each iteration. This can be measured by the association of a pattern to the current word set used in the algorithm. The likelihood ratio test- s <ref type="bibr" target="#b8">(Dunning, 1993)</ref> is used for this purpose. This association model has also been used to model as- sociation between opinion target words by (Hai et</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: New word detection algorithm</head><p>Input: D: a large set of POS tagged posts W s : a set of seed words k p : the number of patterns chosen at each iteration k c : the number of patterns in the candidate pattern set k w : the number of words added at each iteration K: the number of words returned</p><p>Output: A list of ranked new words W 1 Obtain all lexical patterns using regular expressions on D; 2 Count the frequency of each lexical pattern and extract words matched by each pattern ; 3 Obtain top k c frequent patterns as candidate pattern set P c and top 5,000 frequent words as candidate word set W c ; 4 P = Φ; W=W s ; t = 0 ; 5 for |W| &lt; K do <ref type="bibr">6</ref> Use W to score each pattern in P c with U (p) ;</p><formula xml:id="formula_0">7 P = {top k p patterns} ; 8</formula><p>Use P to extract new words and if the words are in W c , score them with F (w) ;</p><formula xml:id="formula_1">9 W = W ∪ {top k w words} ; 10 W c = W c -W ;</formula><p>11 Sort words in W with F (w) ; 12 Output the ranked list of words in W ;</p><p>al., 2012).</p><p>The LRT is well known for not relying crit- ically on the assumption of normality, instead, it uses the asymptotic assumption of the generalized likelihood ratio. In practice, the use of likelihood ratios tends to result in significant improvements in text-analysis performance.</p><p>In our problem, LRT computes a contingency table of a pattern p and a word w, derived from the corpus statistics, as given in <ref type="table" target="#tab_1">Table 3</ref>, where k 1 (w, p) is the number of documents that w match- es pattern p, k 2 (w, ¯ p) is the number of documents that w occurs while p does not, k 3 ( ¯ w, p) is the number of documents that p occurs while w does not, and k 4 ( ¯ w, ¯ p) is the number of documents con- taining neither p nor w. Based on the statistics shown in <ref type="table" target="#tab_1">Table 3</ref>, the likelihood ratio tests (LRT) model captures the sta- tistical association between a pattern p and a word w by employing the following formula:</p><formula xml:id="formula_2">Statistics p ¯ p w k 1 (w, p) k 2 (w, ¯ p) ¯ w k 3 ( ¯ w, p) k 4 ( ¯ w, ¯ p)</formula><formula xml:id="formula_3">LRT (p, w) = log L(ρ1, k1, n1) * L(ρ2, k2, n2) L(ρ, k1, n1) * L(ρ, k2, n2)<label>(1)</label></formula><p>where:</p><formula xml:id="formula_4">L(ρ, k, n) = ρ k * (1 − ρ) n−k ; n 1 = k 1 + k 3 ; n 2 = k 2 + k 4 ; ρ 1 = k 1 /n 1 ; ρ 2 = k 2 /n 2 ; ρ = (k 1 + k 2 )/(n 1 + n 2 ).</formula><p>Thus, the utility of a pattern can be measured as follows:</p><formula xml:id="formula_5">U (p) = ∑ w i ∈W LRT (p, w i )<label>(2)</label></formula><p>where W is the current word set used in the algo- rithm (see Algorithm 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Measuring the Possibility of Being New Words</head><p>Another key issue in the proposed algorithm is to quantify the possibility of a candidate word being a new word. We consider several factors for this purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Likelihood Ratio Test</head><p>Very similar to the pattern utility measure, L- RT can also be used to measure the association of a candidate word to a given pattern set, as follows:</p><formula xml:id="formula_6">LRT (w) = ∑ p i ∈P LRT (w, p i )<label>(3)</label></formula><p>where P is the current pattern set used in the algo- rithm (see Algorithm 1), and p i is a lexical pattern. This measure only quantifies the association of a candidate word to the given pattern set. It tells nothing about the possibility of a word be- ing a new word, however, a new sentiment word, should have close association with the lexical pat- terns. This has linguistic interpretations because new sentiment words are commonly modified by adverbial words and thus should have close associ- ation with lexical patterns. This measure is proved to be an influential factor by our experiments in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Left Pattern Entropy</head><p>If a candidate word is a new word, it will be more commonly used with diversified lexical pat- terns since the non-compositionality of new word means that the word can be used in many differ- ent linguistic scenarios. This can be measured by information entropy, as follows:</p><formula xml:id="formula_7">LP E(w) = − ∑ l i ∈L(Pc,w) c(li, w) N (w) * log c(li, w) N (w)<label>(4)</label></formula><p>where L(P c , w) is the set of left word of all pat- terns by which word w can be matched in P c , c(l i , w) is the count that word w can be matched by patterns whose left word is l i , and N (w) is the count that word w can be matched by the patterns in P c . Note that we use P c , instead of P, because the latter set is very small while computing entropy needs a large number of patterns. Tuning the size of P c will be further discussed in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">New Word Probability</head><p>Some words occur very frequently and can be widely matched by lexical patterns, but they are not new words. For example, " 爱吃(love to eat)" and " 爱说(love to talk)" can be matched by many lexical patterns, however, they are not new words due to the lack of non-compositionality. In such words, each single character has high probability to be a word. Thus, we design the following mea- sure to favor this observation.</p><formula xml:id="formula_8">N W P (w) = n ∏ i=1 p(w i ) 1 − p(w i )<label>(5)</label></formula><p>where w = w 1 w 2 . . . w n , each w i is a single char- acter, and p(w i ) is the probability of the character w i being a word, as computed as follows:</p><formula xml:id="formula_9">p(w i ) = all(w i ) − s(w i ) all(w i )</formula><p>where all(w i ) is the total frequency of w i , and s(w i ) is the frequency of w i being a single char- acter word. Obviously, in order to obtain the value of s(w i ), some particular Chinese word segmen- tation tool is required. In this work, we resort to ICTCLAS ( <ref type="bibr" target="#b20">Zhang et al., 2003</ref>), a widely used tool in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Non-compositionality Measures</head><p>New words are usually multi-word expres- sions, where a variety of statistical measures have been proposed to detect multi-word expressions. Thus, such measures can be naturally incorporated into our algorithm.</p><p>The first measure is enhanced mutual infor- mation (EMI) ( :</p><formula xml:id="formula_10">EM I(w) = log 2 F /N ∏ n i=1 F i −F N (6)</formula><p>where F is the number of posts in which a multi- word expression w = w 1 w 2 . . . w n occurs, F i is the number of posts where w i occurs, and N is the total number of posts. The key idea of EMI is to measure word pair's dependency as the ratio of its probability of being a multi-word to its probability of not being a multi-word. The larger the value, the more possible the expression will be a multi-word expression.</p><p>The second measure we take into account is normalized multi-word expression distance ( <ref type="bibr" target="#b1">Bu et al., 2010)</ref>, which has been proposed to measure the non-compositionality of multi-word expressions.</p><formula xml:id="formula_11">N M ED(w) = log|µ(w)| − log|ϕ(w)| logN − log|ϕ(w)|<label>(7)</label></formula><p>where µ(w) is the set of documents in which all single words in w = w 1 w 2 . . . w n co-occur, ϕ(w) is the set of documents in which word w occurs as a whole, and N is the total number of docu- ments. Different from EMI, this measure is a strict distance metric, meaning that a smaller value in- dicates a larger possibility of being a multi-word expression. As can be seen from the formula, the key idea of this metric is to compute the ratio of the co-occurrence of all words in a multi-word expres- sions to the occurrence of the whole expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.5">Configurations to Combine Various Factors</head><p>Taking into account the aforementioned fac- tors, we have different settings to score a new word, as follows:</p><formula xml:id="formula_12">FLRT (w) = LRT (w)<label>(8)</label></formula><formula xml:id="formula_13">FLP E (w) = LRT (w) * LP E(w)<label>(9)</label></formula><p>FNW P (w) = LRT (w) * LP E(w) * N W P (w) (10)</p><formula xml:id="formula_14">FEMI (w) = LRT (w) * LP E(w) * EM I(w)<label>(11)</label></formula><formula xml:id="formula_15">FNMED(w) = LRT (w) * LP E(w) N M ED(w)<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we will conduct the following experiments: first, we will compare our method to several baselines, and perform parameter tun- ing with extensive experiments; second, we will classify polarity of new sentiment words using t- wo methods; third, we will demonstrate how new sentiment words will benefit sentiment classifica- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Preparation</head><p>We crawled 237,108,977 Weibo posts from http://www.weibo.com, the largest social website in China. These posts range from January of 2011 to December of 2012. The posts were then part-of- speech tagged using a Chinese word segmentation tool named ICTCLAS ( <ref type="bibr" target="#b20">Zhang et al., 2003)</ref>.</p><p>Then, we asked two annotators to label the top 5,000 frequent words that were extracted by lexi- cal patterns as described in Algorithm 1. The an- notators were requested to judge whether a candi- date word is a new word, and also to judge the po- larity of a new word (positive, negative, and neu- tral). If there is a disagreement on either of the two tasks, discussions are required to make the fi- nal decision. The annotation led to 323 new word- s, among which there are 116 positive words, 112 negative words, and 95 neutral words 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metric</head><p>As our algorithm outputs a ranked list of words, we adapt average precision to evaluate the performance of new sentiment word detection. The metric is computed as follows:</p><formula xml:id="formula_16">AP (K) = ∑ K k=1 P (k) * rel(k) ∑ K k=1 rel(k)</formula><p>where P (k) is the precision at cut-off k, rel(k) is 1 if the word at position k is a new word and 0 oth- erwise, and K is the number of words in the ranked list. A perfect list (all top K items are correct) has an AP value of 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation of Different Measures and Comparison to Baselines</head><p>First, we assess the influence of likelihood ra- tio test, which measures the association of a word to the pattern set. As can be seen from <ref type="table">Table 4</ref>, the association model (LRT) remarkably boosts the performance of new word detection, indicating L- RT is a key factor for new sentiment word extrac- tion. From linguistic perspectives, new sentiment words are commonly modified by adverbial words and thus should have close association with lexical patterns.</p><p>Second, we compare different settings of our method to two baselines. The first one is en- hanced mutual information (EMI) where we set F (w) = EM I(w) ( ) and the second baseline is normalized multi-word expres- sion distance (NMED) ( <ref type="bibr" target="#b1">Bu et al., 2010</ref>) where we set F (w) = N M ED(w). The results are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. As can be seen, all the proposed measures outperform the two baselines (EM I and N M ED) remarkably and consistently. The set- ting of F N M ED produces the best performance. Adding N M ED or EM I leads to remarkable im- provements because of their capability of measur- ing non-compositionality of new words. Only us- ing LRT can obtain a fairly good results when K is small, however, the performance drops sharply be- cause it's unable to measure non-compositionality. Comparison between LRT + LP E (or LRT + LP E + N W P ) and LRT shows that inclusion of left pattern entropy also boosts the performance apparently. However, the new word probabili- ty (N W P ) has only marginal contribution to im- provement.</p><p>In the above experiments, we set k p = 5 (the number of patterns chosen at each iteration) and k w = 10 (the number of words added at each iter- ation), which is the optimal setting and will be dis- cussed in the next subsection. And only one seed word "坑爹(reverse one's expectation)" is used.  <ref type="figure" target="#fig_0">259  LRT+LPE 0.743 0.652 0.613 0.582 0.</ref>548 LPE+NWP 0.467 0.400 0.350 0.330 0.320 LRT+LPE+NWP 0.755 0.680 0.612 0.571 0.543 LPE+EMI 0.608 0.551 0.519 0.486 0. <ref type="figure" target="#fig_0">467  LRT+LPE+EMI 0.859 0.759 0.717 0.662 0.</ref>632 LPE+NMED 0.749 0.690 0.641 0.612 0.576 LRT+LPE+NMED 0.907 0.808 0.741 0.723 0.699 <ref type="table">Table 4</ref>: Results with vs. without likelihood ratio test (LRT).</p><note type="other">). top K words ⇒ 100 200 300 400 500 LPE 0.366 0.324 0.286 0.270 0.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameter Tuning</head><p>Firstly, we will show how to obtain the op- timal settings of k p and k w . The measure setting we take here is F N M ED (w), as shown in Formula (12). Again, we choose only one seed word "坑 爹(reverse one's expectation)", and the number of words returned is set to K = 300. Results in <ref type="table">Ta- ble 5</ref> show that the performance drops consistent- ly across different k w settings when the number of patterns increases. Note that at the early stage of Algorithm 1, larger k p (perhaps with noisy pattern- s) may lead to lower quality of new words; while larger k w (perhaps with noisy seed words) may lead to lower quality of lexical patterns. Therefore, we choose the optimal setting to small numbers, as k p = 5, k w = 10.</p><p>Secondly, we justify whether the proposed al- gorithm is sensitive to the number of seed words. We set k p = 5 and k w = 10, and take F N M ED as the weighting measure of new word. We exper- imented with only one seed word, two, three, and four seed words, respectively. The results in Ta- ble 6 show very stable performance when different numbers of seed words are chosen. It's interesting that the performance is totally the same with dif- ferent numbers of seed words. By looking into the pattern set and the selected words at each iteration, we found that the pattern set (P) converges soon to the same set after a few iterations; and at the be- ginning several iterations, the selected words are almost the same although the order of adding the words is different. Since the algorithm will finally sort the words at step (11) and P is the same, the ranking of the words becomes all the same.</p><p>Lastly, we need to decide the optimal number of patterns in P c (that is, k c in Algorithm 1) be- cause the set has been used in computing left pat- tern entropy, see Formula (4). Too small size of P c may lead to insufficient estimation of left pat- tern entropy. Results in <ref type="table">Table 7</ref> shows that larg- er P c decrease the performance, particularly when the number of words returned (K) becomes larger. Therefore, we set |P c | = 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Polarity Prediction of New Sentiment Words</head><p>In this section, we attempt to classifying the polarity of the annotated 323 new words. Two methods are adapted with different settings for this purpose. The first one is majority vote (MV), and the second one is pointwise mutual information, similar to <ref type="bibr" target="#b19">(Turney and Littman, 2003)</ref>. The ma- jority vote method is formulated as below:</p><formula xml:id="formula_17">M V (w) = ∑ wp∈P W #(w, wp) |P W | − ∑ wn∈N W #(w, wn) |N W |</formula><p>where P W and N W are a positive and negative set of emoticons (or seed words) respectively, and #(w, w p ) is the co-occurrence count of the input word w and the item w p . The polarity is judged ac- cording to this rule: if M V (w) &gt; th 1 , the word w is positive; if M V (w) &lt; −th 1 the word negative; otherwise neutral. The threshold th 1 is manually tuned.</p><p>And PMI is computed as follows:</p><formula xml:id="formula_18">P M I(w) = ∑ wp∈P W P M I(w, wp) |P W | − ∑ wn∈N W P M I(w, wn) |N W |</formula><p>where P M I(x, y) = log 2 ( P r(x,y) P r(x) * P r(y) ), and P r(·) denotes probability. The polarity is judged according to the rule: if P M I(w) &gt; th 2 , w is positive; if P M I(w) &lt; −th 2 negative; otherwise neutral. The threshold th 2 is manually tuned.</p><p>As for the resources P W and N W , we have three settings. The first setting (denoted by  <ref type="table">Table 5</ref>: Parameter tuning results for k p and k w . The measure setting is F N M ED (w), the seed word set is {"坑爹(reverse one's expectation)"}, and the number of words returned is K = 300.  Large_Emo) is a set of most frequent 36 emoticons in which there are 21 positive and 15 negative e- moticons respectively. The second one (denoted by Small_Emo) is a set of 10 emoticons, which are chosen from the 36 emoticons, as shown in <ref type="table" target="#tab_4">Table 8</ref>. The third one (denoted by Opin_Words) is two sets of seed opinion words, where P W ={ 高 兴(happy),大 方(generous),漂 亮(beautiful), 善 良(kind), 聪明(smart)} and N W ={伤心(sad),小 气(mean),难看(ugly), 邪恶(wicked), 笨(stupid)}.</p><p>The performance of polarity prediction is shown in <ref type="table" target="#tab_5">Table 9</ref>. In two-class polarity classifi- cation, we remove neutral words and only make prediction with positive/negative classes. The first observation is that the performance of using emoti- cons is much better than that of using seed opin- ion words. We conjecture that this may be be- cause new sentiment words are more frequently co-occurring with emoticons than with these opin- ion words. The second observation is that three- class polarity classification is much more diffi- cult than two-class polarity classification because many extracted new words are nouns such as "基 友(gay)","菇 凉(girl)", and "盆 友(friend)". Such nouns are more difficult to classify sentiment ori- entation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Application of New Sentiment Words to Sentiment Classification</head><p>In this section, we justify whether inclusion of new sentiment word would benefit sentiment clas- sification. For this purpose, we randomly sampled and annotated 4,500 Weibo posts that contain at least one opinion word in the union of the Hownet 4 opinion lexicons and our annotated new word- s. We apply two models for polarity classification. The first model is a lexicon-based model (denot- ed by Lexicon) that counts the number of positive and negative opinion words in a post respective- ly, and classifies a post to be positive if there are more positive words than negative ones, and to be negative otherwise. The second model is a SVM model in which opinion words are used as feature, and 5-fold cross validation is conducted.</p><p>We experiment with different settings of Hownet lexicon resources:</p><p>• Hownet opinion words (denoted by Hownet):</p><p>After removing some obviously inappropri- ate words, the left lexicons have 627 posi- tive opinion words and 1,038 negative opin- ion words, respectively.</p><p>• Compact Hownet opinion words (denoted by cptHownet): we count the frequency of the above opinion words on the training data and remove words whose document frequency is less than 2. This results in 138 positive words and 125 negative words.</p><p>Then, we add into the above resources the la- beled new polar words(denoted by N W , including 116 positive and 112 negative words) and the top 100 words produced by the algorithm (denoted by T 100), respectively. Note that the lexicon-based model requires the sentiment orientation of each dictionary entry 5 , we thus manually label the po-|P c | ⇒ 50 100 200 300 400 500 K=100 0.907 0.905 0.916 0.916 0.888 0.887 K=200 0.808 0.810 0.778 0.776 0.766 0.764 K=300 0.741 0.731 0.722 0.726 0.712 0.713 K=400 0.709 0.708 0.677 0.675 0.656 0.655 K=500 0.685 0.683 0.653 0.646 0.626 0.627 <ref type="table">Table 7</ref>: Tuning the number of patterns in P c . The measure setting is F N M ED (w), k p = 5, k w = 10, and the seed word set is {"坑爹(reverse one's expectation)"}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emoticon</head><p>Polarity    <ref type="table" target="#tab_7">Table 10</ref> show that inclusion of new words in both models improves the perfor- mance remarkably. In the setting of the original lexicon (Hownet), both models obtain 2-3% gains from the inclusion of new words. Similar improve- ment is observed in the setting of the compact lex- icon. Note, that T 100 is automatically obtained from Algorithm 1 so that it may contain words that are not new sentiment words, but the resource also improves performance remarkably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In order to extract new sentiment words from large-scale user-generated content, this paper pro- poses a fully unsupervised, purely data-driven, and  almost knowledge-free (except POS tags) frame- work. We design statistical measures to quantify the utility of a lexical pattern and to measure the possibility of a word being a new word, respec- tively. The method is almost free of linguistic re- sources (except POS tags), and does not rely on elaborated linguistic rules. We conduct extensive experiments to reveal the influence of different sta- tistical measures in new word finding. Compara- tive experiments show that our proposed method outperforms baselines remarkably. Experiments also demonstrate that inclusion of new sentiment words benefits sentiment classification definitely. From linguistic perspectives, our framework is capable to extract adjective new words because the lexical patterns usually modify adjective word- s. As future work, we are considering how to ex- tract other types of new sentiment words, such as nounal new words that can express sentiment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparative results of different measure settings. X-axis is the number of words returned (K), and Y-axis is average precision (AP (K)).</figDesc><graphic url="image-1.png" coords="6,318.20,566.59,196.66,121.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>New word English Translation 
Polarity 
口爱 
lovely 
positive 
杯具 
tragic/tragedy 
negative 
给力 
very cool; powerful 
positive 
坑爹 
reverse one's expectation negative 

Table 1: Examples of new sentiment word. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Contingency table for likelihood ratio test 
(LRT). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Performance with different numbers of 
seed words. The measure setting is F N M ED (w), 
and k p = 5, k w = 10. The seed words are chosen 
from Table 1. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>The ten emoticons used for polarity pre-
diction. 

Methods ⇒ Majority vote PMI 
Two-class polarity classification 
Large_Emo 0.861 
0.865 
Small_Emo 0.846 
0.851 
Opin_Words 0.697 
0.654 
Three-class polarity classification 
Large_Emo 0.598 
0.632 
Small_Emo 0.551 
0.635 
Opin_Words 0.449 
0.486 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>The accuracy of two/three-class polarity 
classification. 

larity of all top 100 words (we did NOT remove 
incorrect new word). This results in 52 positive 
and 34 negative words. 
Results in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table>The accuracy of polarity classfication of 
Weibo post with/without new sentiment words. N-
W includes 116/112 positive/negative words, and 
T100 contains 52/34 positive/negative words. 

</table></figure>

			<note place="foot" n="1"> We set the number to 3 words in this work considering computation costs. 2 Such expressions are very simple and easy to write because we only need to consider POS tags of adverbial and auxiliary word.</note>

			<note place="foot" n="3"> All the resources are available upon request.</note>

			<note place="foot" n="4"> http://www.keenage.com/html/c_index.html. 5 This is not necessary for the SVM model. All words in the top 100 words can be used as feature.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partly supported by the fol-lowing grants from: the National Basic Re-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A memory-based approach to learning shallow natural language patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Krymolowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Computational Linguistics</title>
		<meeting>the 17th International Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="67" to="73" />
		</imprint>
	</monogr>
	<note>COLING &apos;98</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Measuring the non-compositionality of multiword expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics, COLING &apos;10</title>
		<meeting>the 23rd International Conference on Computational Linguistics, COLING &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="116" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unknown word extraction for chinese documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keh-Jiann</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Computational Linguistics</title>
		<meeting>the 19th International Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chinese word segmentation using minimal linguistic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Second SIGHAN Workshop on Chinese Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="148" to="151" />
		</imprint>
	</monogr>
	<note>SIGHAN &apos;03</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Looking for needles in a haystack or locating interesting collocation expressions in large textual databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaacov</forename><surname>Choueka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the RIAO&apos;88 Conference on User-Oriented Content-Based Text and Image Handling</title>
		<meeting>eeding of the RIAO&apos;88 Conference on User-Oriented Content-Based Text and Image Handling</meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A local maxima method and a fair dispersion normalization for extracting multi-word units from corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ferreira Da Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G Pereira</forename><surname>Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Meeting on Mathematics of Language</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="369" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mining textual associations in text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvie</forename><surname>Guilloré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José Gabriel Pereira</forename><surname>Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th ACM SIGKDD Work. Text Mining</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accurate methods for the statistics of surprise and coincidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1993-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">One seed to find them all: Mining opinion features via association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Hai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM &apos;12</title>
		<meeting>the 21st ACM International Conference on Information and Knowledge Management, CIKM &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="255" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Technical terminology: some linguistic properties and an algorithm for identification in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John S Justeson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Slava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural language engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="27" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The use of svm for chinese new word identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongqiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ning</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhong</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="723" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An extensive empirical study of collocation extraction methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Student Research Workshop, ACLstudent &apos;05</title>
		<meeting>the ACL Student Research Workshop, ACLstudent &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Chinese segmentation and new word detection using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangfang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>COL- ING &apos;04</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computational Linguistics</title>
		<meeting>the 20th International Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Opinion word expansion and target extraction through double propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="27" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Is knowledge-free induction of multiword unit dictionary headwords a solved problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Schone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 6th Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of the 6th Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The first international chinese word segmentation bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Emerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Second SIGHAN Workshop on Chinese Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="133" to="143" />
		</imprint>
	</monogr>
	<note>SIGHAN &apos;03</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beijing Thesaurus Research</forename><surname>Center</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Xinhua Xin Ciyu Cidian. Commercial Press</publisher>
			<pubPlace>Beijing</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Measuring praise and criticism: Inference of semantic orientation from association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="315" to="346" />
			<date type="published" when="2003-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hhmm-based chinese lexical analyzer ictclas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua-Ping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Kui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-Yi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Second SIGHAN Workshop on Chinese Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="184" to="187" />
		</imprint>
	</monogr>
	<note>SIGHAN &apos;03. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving effectiveness of mutual information for substantival multiword expression extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taketoshi</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tubao</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="10919" to="10930" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Chinese new word detection from query logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Data Mining and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="233" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Incorporating user behaviors in new word detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyun</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Jont Conference on Artifical Intelligence, IJCAI&apos;09</title>
		<meeting>the 21st International Jont Conference on Artifical Intelligence, IJCAI&apos;09<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2101" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A chunking strategy towards unknown word detection in chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing-IJCNLP 2005</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="530" to="541" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
