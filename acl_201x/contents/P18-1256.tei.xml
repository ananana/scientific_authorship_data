<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Let&apos;s do it &quot;again&quot;: A First Computational Approach to Detecting Adverbial Presupposition Triggers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Cianflone</surname></persName>
							<email>{andre.cianflone@mail,yulan.feng@mail,jad@cs,jcheung@cs}.mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">MILA McGill University Montreal</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC, QC</region>
									<country>Canada, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">MILA McGill University Montreal</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC, QC</region>
									<country>Canada, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jad</forename><surname>Kabbara</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">MILA McGill University Montreal</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC, QC</region>
									<country>Canada, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><surname>Chi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">MILA McGill University Montreal</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC, QC</region>
									<country>Canada, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Cheung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">MILA McGill University Montreal</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC, QC</region>
									<country>Canada, Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Let&apos;s do it &quot;again&quot;: A First Computational Approach to Detecting Adverbial Presupposition Triggers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">July 15 -20, 2018. c 2018 Association for Computational Linguistics 2747</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce the task of predicting adverbial presupposition triggers such as also and again. Solving such a task requires detecting recurring or similar events in the discourse context, and has applications in natural language generation tasks such as summarization and dialogue systems. We create two new datasets for the task, derived from the Penn Treebank and the Annotated English Gigaword corpora, as well as a novel attention mechanism tailored to this task. Our attention mechanism augments a baseline recurrent neural network without the need for additional trainable parameters, minimizing the added computational cost of our mechanism. We demonstrate that our model statistically outperforms a number of baselines, including an LSTM-based language model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In pragmatics, presuppositions are assumptions or beliefs in the common ground between discourse participants when an utterance is made <ref type="bibr">(Frege, 1892;</ref><ref type="bibr" target="#b30">Strawson, 1950;</ref><ref type="bibr" target="#b28">Stalnaker, 1973</ref><ref type="bibr" target="#b29">Stalnaker, , 1998</ref>, and are ubiquitous in naturally occurring discourses <ref type="bibr" target="#b1">(Beaver and Geurts, 2014</ref>). Presuppositions un- derly spoken statements and written sentences and understanding them facilitates smooth commu- nication. We refer to expressions that indicate the presence of presuppositions as presupposition triggers. These include definite descriptions, fac- tive verbs and certain adverbs, among others. For example, consider the following statements:</p><p>(1) John is going to the restaurant again. * Authors (listed in alphabetical order) contributed equally.</p><p>(2) John has been to the restaurant.</p><p>(1) is only appropriate in the context where (2) is held to be true because of the presence of the presupposition trigger again. One distinguishing characteristic of presupposition is that it is unaf- fected by negation of the presupposing context, unlike other semantic phenomena such as entail- ment and implicature. The negation of (1), John is not going to the restaurant again., also presup- poses <ref type="bibr">(2)</ref>.</p><p>Our focus in this paper is on adverbial presup- position triggers such as again, also and still. Ad- verbial presupposition triggers indicate the recur- rence, continuation, or termination of an event in the discourse context, or the presence of a similar event. In one study of presuppositional triggers in English journalistic texts <ref type="bibr" target="#b15">(Khaleel, 2010)</ref>, adver- bial triggers were found to be the most commonly occurring presupposition triggers after existential triggers. <ref type="bibr">1</ref> Despite their frequency, there has been little work on these triggers in the computational literature from a statistical, corpus-driven perspec- tive.</p><p>As a first step towards language technology sys- tems capable of understanding and using presup- positions, we propose to investigate the detec- tion of contexts in which these triggers can be used. This task constitutes an interesting test- ing ground for pragmatic reasoning, because the cues that are indicative of contexts containing re- curring or similar events are complex and often span more than one sentence, as illustrated in Sen- tences (1) and <ref type="bibr">(2)</ref>. Moreover, such a task has im- mediate practical consequences. For example, in language generation applications such as summa- rization and dialogue systems, adding presuppo- sitional triggers in contextually appropriate loca-tions can improve the readability and coherence of the generated output.</p><p>We create two datasets based on the Penn Tree- bank corpus <ref type="bibr" target="#b20">(Marcus et al., 1993</ref>) and the En- glish Gigaword corpus ( <ref type="bibr" target="#b7">Graff et al., 2007)</ref>, extract- ing contexts that include presupposition triggers as well as other similar contexts that do not, in or- der to form a binary classification task. In creat- ing our datasets, we consider a set of five target adverbs: too, again, also, still, and yet. We fo- cus on these adverbs in our investigation because these triggers are well known in the existing lin- guistic literature and commonly triggering presup- positions. We control for a number of potential confounding factors, such as class balance, and the syntactic governor of the triggering adverb, so that models cannot exploit these correlating fac- tors without any actual understanding of the pre- suppositional properties of the context.</p><p>We test a number of standard baseline classifiers on these datasets, including a logistic regression model and deep learning methods based on re- current neural networks (RNN) and convolutional neural networks (CNN).</p><p>In addition, we investigate the potential of attention-based deep learning models for detect- ing adverbial triggers. Attention is a promising approach to this task because it allows a model to weigh information from multiple points in the previous context and infer long-range dependen- cies in the data ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>). For ex- ample, the model could learn to detect multiple instances involving John and restaurants, which would be a good indication that again is appropri- ate in that context. Also, an attention-based RNN has achieved success in predicting article definite- ness, which involves another class of presupposi- tion triggers ( <ref type="bibr" target="#b11">Kabbara et al., 2016)</ref>.</p><p>As another contribution, we introduce a new weighted pooling attention mechanism designed for predicting adverbial presupposition triggers. Our attention mechanism allows for a weighted averaging of our RNN hidden states where the weights are informed by the inputs, as opposed to a simple unweighted averaging. Our model uses a form of self-attention ( <ref type="bibr" target="#b24">Paulus et al., 2018;</ref><ref type="bibr" target="#b33">Vaswani et al., 2017)</ref>, where the input sequence acts as both the attention mechanism's query and key/value. Unlike other attention models, instead of simply averaging the scores to be weighted, our approach aggregates (learned) attention scores by learning a reweighting scheme of those scores through an- other level (dimension) of attention. Additionally, our mechanism does not introduce any new pa- rameters when compared to our LSTM baseline, reducing its computational impact.</p><p>We compare our model using the novel attention mechanism against the baseline classifiers in terms of prediction accuracy. Our model outperforms these baselines for most of the triggers on the two datasets, achieving 82.42% accuracy on predicting the adverb "also" on the Gigaword dataset.</p><p>The contributions of this work are as follows:</p><p>1. We introduce the task of predicting adverbial presupposition triggers.</p><p>2. We present new datasets for the task of detecting adverbial presupposition triggers, with a data extraction method that can be ap- plied to other similar pre-processing tasks.</p><p>3. We develop a new attention mechanism in an RNN architecture that is appropriate for the prediction of adverbial presupposition trig- gers, and show that its use results in bet- ter prediction performance over a number of baselines without introducing additional pa- rameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Presupposition and pragmatic reasoning</head><p>The discussion of presupposition can be traced back to Frege's work on the philosophy of lan- guage (Frege, 1892), which later leads to the most commonly accepted view of presupposition called the Frege-Strawson theory <ref type="bibr" target="#b13">(Kaplan, 1970;</ref><ref type="bibr" target="#b30">Strawson, 1950)</ref>. In this view, presuppositions are pre- conditions for sentences/statements to be true or false. To the best of our knowledge, there is no previous computational work that directly inves- tigates adverbial presupposition. However in the fields of semantics and pragmatics, there exist lin- guistic studies on presupposition that involve ad- verbs such as "too" and "again" (e.g., <ref type="bibr" target="#b2">(Blutner et al., 2003)</ref>, <ref type="bibr" target="#b12">(Kang, 2012)</ref>) as a pragmatic pre- supposition trigger. Also relevant to our work is ( <ref type="bibr" target="#b11">Kabbara et al., 2016)</ref>, which proposes using an attention-based LSTM network to predict noun phrase definiteness in English. Their work demon- strates the ability of these attention-based models to pick up on contextual cues for pragmatic rea- soning.</p><p>Many different classes of construction can trig- ger presupposition in an utterance, this includes but is not limited to stressed constituents, factive verbs, and implicative verbs ( <ref type="bibr" target="#b34">Zare et al., 2012)</ref>. In this work, we focus on the class of adverbial pre- supposition triggers.</p><p>Our task setup resembles the Cloze test used in psychology <ref type="bibr" target="#b32">(Taylor, 1953;</ref><ref type="bibr" target="#b4">E. B. Coleman, 1968;</ref><ref type="bibr" target="#b5">Earl F. Rankin, 1969)</ref> and machine comprehen- sion ( <ref type="bibr" target="#b26">Riloff and Thelen, 2000</ref>), which tests text comprehension via a fill-in-the-blanks task. We similarly pre-process our samples such that they are roughly the same length, and have equal num- bers of negative samples as positive ones. How- ever, we avoid replacing the deleted words with a blank, so that our model has no clue regard- ing the exact position of the possibly missing trig- ger. Another related work on the Children's Book Test <ref type="bibr" target="#b9">(Hill et al., 2015)</ref> notes that memories that encode sub-sentential chunks (windows) of infor- mative text seem to be most useful to neural net- works when interpreting and modelling language. Their finding inspires us to run initial experiments with different context windows and tune the size of chunks according to the Logistic Regression re- sults on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention</head><p>In the context of encoder-decoder models, atten- tion weights are usually based on an energy mea- sure of the previous decoder hidden state and en- coder hidden states. Many variations on atten- tion computation exist. <ref type="bibr" target="#b31">Sukhbaatar et al. (2015)</ref> propose an attention mechanism conditioned on a query and applied to a document. To generate summaries, <ref type="bibr" target="#b24">Paulus et al. (2018)</ref> add an attention mechanism in the prediction layer, as opposed to the hidden states. <ref type="bibr" target="#b33">Vaswani et al. (2017)</ref> suggest a model which learns an input representation by self-attending over inputs. While these methods are all tailored to their specific tasks, they all in- spire our choice of a self-attending mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Corpora</head><p>We extract datasets from two corpora, namely the Penn Treebank (PTB) corpus <ref type="bibr" target="#b20">(Marcus et al., 1993)</ref> and a subset (sections 000-760) of the third edi- tion of the English Gigaword corpus ( <ref type="bibr" target="#b7">Graff et al., 2007)</ref>. For the PTB dataset, we use sections 22 and 23 for testing. For the Gigaword corpus, we <ref type="bibr">('still', ['The', 'Old', 'Granary', ..</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. / * 46 t o k e n s o m i t t e d * / ...,'has', '@@@@', 'included', 'Bertrand', 'Russell', ... / * 6 t o k e n s o m i t t e d * / ... 'Morris '], ['DT', 'NNP', 'NNP', ... / * 46 t o k e n s o m i t t e d * / ..., 'VBZ', '@@@@', 'VBN', 'NNP', 'NNP', ... / * 6 t o k e n s o m i t t e d * / ... 'NNP'])</head><p>Figure 1: An example of an instance containing a presuppositional trigger from our dataset.</p><p>use sections 700-760 for testing. For the remain- ing data, we randomly chose 10% of them for de- velopment, and the other 90% for training.</p><p>For each dataset, we consider a set of five tar- get adverbs: too, again, also, still, and yet. We choose these five because they are commonly used adverbs that trigger presupposition. Since we are concerned with investigating the capacity of at- tentional deep neural networks in predicting the presuppositional effects in general, we frame the learning problem as a binary classification for pre- dicting the presence of an adverbial presupposi- tion (as opposed to the identity of the adverb).</p><p>On the Gigaword corpus, we consider each ad- verb separately, resulting in five binary classifica- tion tasks. This was not feasible for PTB because of its small size.</p><p>Finally, because of the commonalities between the adverbs in presupposing similar events, we create a dataset that unifies all instances of the five adverbs found in the Gigaword corpus, with a la- bel "1" indicating the presence of any of these ad- verbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data extraction process</head><p>We define a sample in our dataset as a 3-tuple, consisting of a label (representing the target ad- verb, or 'none' for a negative sample), a list of tokens we extract (before/after the adverb), and a list of corresponding POS tags ( <ref type="bibr" target="#b18">Klein and Manning, 2002</ref>). In each sample, we also add a special token "@@@@" right before the head word and the corresponding POS tag of the head word, both in positive and negative cases. We add such spe- cial tokens to identify the candidate context in the passage to the model. <ref type="figure">Figure 1</ref> shows a single pos- itive sample in our dataset.</p><p>We first extract positive contexts that contain a triggering adverb, then extract negative contexts that do not, controlling for a number of poten- tial confounds. Our positive data consist of cases where the target adverb triggers presupposition by modifying a certain head word which, in most cases, is a verb. We define such head word as a governor of the target adverb.</p><p>When extracting positive data, we scan through all the documents, searching for target adverbs. For each occurrence of a target adverb, we store the location and the governor of the adverb. Tak- ing each occurrence of a governor as a pivot, we extract the 50 unlemmatized tokens preceding it, together with the tokens right after it up to the end of the sentence (where the adverb is)-with the ad- verb itself being removed. If there are less than 50 tokens before the adverb, we simply extract all of these tokens. In preliminary testing using a lo- gistic regression classifier, we found that limiting the size to 50 tokens had higher accuracy than 25 or 100 tokens. As some head words themselves are stopwords, in the list of tokens, we do not re- move any stopwords from the sample; otherwise, we would lose many important samples.</p><p>We filter out the governors of "too" that have POS tags "JJ" and "RB" (adjectives and adverbs), because such cases corresponds to a different sense of "too" which indicates excess quantity and does not trigger presupposition (e.g., "rely too heavily on", "it's too far from").</p><p>After extracting the positive cases, we then use the governor information of positive cases to ex- tract negative data. In particular, we extract sen- tences containing the same governors but not any of the target adverbs as negatives. In this way, models cannot rely on the identity of the gover- nor alone to predict the class. This procedure also roughly balances the number of samples in the positive and negative classes.</p><p>For each governor in a positive sample, we lo- cate a corresponding context in the corpus where the governor occurs without being modified by any of the target adverbs. We then extract the surrounding tokens in the same fashion as above. Moreover, we try to control position- related confounding factors by two randomization approaches: 1) randomize the order of documents to be scanned, and 2) within each document, start scanning from a random location in the document. Note that the number of negative cases might not be exactly equal to the number of negative cases in all datasets because some governors appearing in positive cases are rare words, and we're unable to find any (or only few) occurrences that match them for the negative cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning Model</head><p>In this section, we introduce our attention-based model. At a high level, our model extends a bidi- rectional LSTM model by computing correlations between the hidden states at each timestep, then applying an attention mechanism over these cor- relations. Our proposed weighted-pooling (WP) neural network architecture is shown in <ref type="figure">Figure 2</ref>.</p><p>The input sequence u = {u 1 , u 2 , . . . , u T } con- sists of a sequence, of time length T , of one- hot encoded word tokens, where the original to- kens are those such as in Listing 1. Each token u t is embedded with pretrained embedding ma- trix W e ∈ R |V |×d , where |V | corresponds to the number of tokens in vocabulary V , and d defines the size of the word embeddings. The embed- ded token vector x t ∈ R d is retrieved simply with x t = u t W e . Optionally, x t may also include the token's POS tag. In such instances, the embedded token at time step t is concatenated with the POS tag's one-hot encoding p t : x t = u t W e ||p t , where || denotes the vector concatenation operator.</p><p>At each input time step t, a bi-directional LSTM (Hochreiter and Schmidhuber, 1997) en- codes x t into hidden state h t ∈ R s :</p><formula xml:id="formula_0">h t = − → h t || ← − h t<label>(1)</label></formula><p>where − → h t = f (x t , h t−1 ) is computed by the for- ward LSTM, and ← − h t = f (x t , h t+1 ) is computed by the backward LSTM. Concatenated vector h t is of size 2s, where s is a hyperparameter determin- ing the size of the LSTM hidden states. Let matrix H ∈ R 2s×T correspond to the concatenation of all hidden state vectors:</p><formula xml:id="formula_1">H = [h 1 ||h 2 || . . . ||h T ].<label>(2)</label></formula><p>Our model uses a form of self-attention ( <ref type="bibr" target="#b24">Paulus et al., 2018;</ref><ref type="bibr" target="#b33">Vaswani et al., 2017)</ref>, where the input sequence acts as both the attention mechanism's query and key/value. Since the location of a pre- supposition trigger can greatly vary from one sam- ple to another, and because dependencies can be long range or short range, we model all possible word-pair interactions within a sequence. We cal- culate the energy between all input tokens with a <ref type="table" target="#tab_1">Training set  Test set  Corpus  Positive  Negative  Total  Positive  Negative  Total  PTB  2,596  2,579  5,175  249  233  482  Gigaword yet  32,024  31,819  63,843  7950  7890  15840  Gigaword too  55,827  29,918  85,745  13987  7514  21501  Gigaword again  43,120  42,824  85,944  10935  10827  21762  Gigaword still  97,670  96,991  194,661  24509  24232  48741  Gigaword also  269,778  267,851  537,626  66878  66050  132928  Gigaword all  498,415  491,173  989,588  124255  123078  247333   Table 1</ref>: Number of training samples in each dataset.</p><p>Figure 2: Our weighted-pooling neural network architecture (WP). The tokenized input is embedded with pretrained word embeddings and possibly concatenated with one-hot encoded POS tags. The input is then encoded with a bi-directional LSTM, followed by our attention mechanism. The computed attention scores are then used as weights to average the encoded states, in turn connected to a fully connected layer to predict presupposition triggering.</p><p>pair-wise matching matrix:</p><formula xml:id="formula_2">M = H H (3)</formula><p>where M is a square matrix ∈ R T ×T . To get a single attention weight per time step, we adopt the attention-over-attention method <ref type="bibr" target="#b3">(Cui et al., 2017)</ref>. With matrix M , we first compute row-wise atten- tion score M r ij over M :</p><formula xml:id="formula_3">M r ij = exp(e ij ) T t=1 exp(e it )<label>(4)</label></formula><p>where e ij = M ij . M r can be interpreted as a word-level attention distribution over all other words. Since we would like a single weight per word, we need an additional step to aggregate these attention scores. Instead of simply averag- ing the scores, we follow ( <ref type="bibr" target="#b3">Cui et al., 2017</ref>)'s ap- proach which learns the aggregation by an addi- tional attention mechanism. We compute column- wise softmax M c ij over M :</p><formula xml:id="formula_4">M c ij = exp(e ij ) T t=1 exp(e tj )<label>(5)</label></formula><p>The columns of M r are then averaged, forming vector β ∈ R T . Finally, β is multiplied with the column-wise softmax matrix M c to get attention vector α:</p><formula xml:id="formula_5">α = M r β.<label>(6)</label></formula><p>Note Equations <ref type="formula" target="#formula_1">(2)</ref> to (6) have described how we derived an attention score over our input with- out the introduction of any new parameters, poten- tially minimizing the computational effect of our attention mechanism.</p><p>As a last layer to their neural network, <ref type="bibr" target="#b3">Cui et al. (2017)</ref> sum over α to extract the most relevant input. However, we use α as weights to combine all of our hidden states h t :</p><formula xml:id="formula_6">c = T t=1 α t h t<label>(7)</label></formula><p>where c ∈ R s . We follow the pooling with a dense layer z = σ(W z c + b z ), where σ is a non-linear function, matrix W z ∈ R 64×s and vector b z ∈ R 64 are learned parameters. The presupposition trigger probability is computed with an affine transform followed by a softmax:</p><formula xml:id="formula_7">ˆ y = softmax(W o z + b o )<label>(8)</label></formula><p>where matrix W o ∈ R 2×64 and vector b o ∈ R 2 are learned parameters. The training objective mini- mizes:</p><formula xml:id="formula_8">J(θ) = 1 m m t=1 E(ˆ y, y)<label>(9)</label></formula><p>where E(· , ·) is the standard cross-entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We compare the performance of our WP model against several models which we describe in this section. We carry out the experiments on both datasets described in Section 3. We also investi- gate the impact of POS tags and attention mecha- nism on the models' prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines</head><p>We compare our learning model against the fol- lowing systems. The first is the most-frequent- class baseline (MFC) which simply labels all sam- ples with the most frequent class of 1. The sec- ond is a logistic regression classifier (LogReg), in which the probabilities describing the possible outcomes of a single input x is modeled using a lo- gistic function. We implement this baseline classi- fier with the scikit-learn package <ref type="bibr" target="#b25">(Pedregosa et al., 2011</ref>), with a CountVectorizer including bi-gram features. All of the other hyperparameters are set to default weights.</p><p>The third is a variant LSTM recurrent neural network as introduced in <ref type="bibr" target="#b8">(Graves, 2013)</ref>. The in- put is encoded by a bidirectional LSTM like the WP model detailed in Section 4. Instead of a self-attention mechanism, we simply mean-pool matrix H, the concatenation of all LSTM hid- den states, across all time steps. This is fol- lowed by a fully connected layer and softmax function for the binary classification. Our WP model uses the same bidirectional LSTM as this baseline LSTM, and has the same number of pa- rameters, allowing for a fair comparison of the two models. Such a standard LSTM model repre- sents a state-of-the-art language model, as it out- performs more recent models on language model- ing tasks when the number of model parameters is controlled for ( <ref type="bibr" target="#b21">Melis et al., 2017)</ref>.</p><p>For the last model, we use a slight variant of the CNN sentence classification model of <ref type="bibr" target="#b16">(Kim, 2014</ref>) based on the Britz tensorflow implementation 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hyperparameters &amp; Additional Features</head><p>After tuning, we found the following hyperparam- eters to work best: 64 units in fully connected lay- ers and 40 units for POS embeddings. We used dropout with probability 0.5 and mini-batch size of 64.</p><p>For all models, we initialize word embeddings with word2vec ( ) pretrained embeddings of size 300. Unknown words are ran- domly initialized to the same size as the word2vec embeddings. In early tests on the development datasets, we found that our neural networks would consistently perform better when fixing the word embeddings. All neural network performance re- ported in this paper use fixed embeddings.</p><p>Fully connected layers in the LSTM, CNN and WP model are regularized with dropout <ref type="bibr" target="#b27">(Srivastava et al., 2014</ref>). The model parameters for these neural networks are fine-tuned with the Adam al- gorithm <ref type="bibr" target="#b17">(Kingma and Ba, 2015)</ref>. To stabilize the RNN training gradients ( <ref type="bibr" target="#b23">Pascanu et al., 2013)</ref>, we perform gradient clipping for gradients below threshold value -1, or above 1. To reduce overfit- ting, we stop training if the development set does not improve in accuracy for 10 epochs. All per- formance on the test set is reported using the best trained model as measured on the development set.</p><p>In addition, we use the CoreNLP Part-of-  <ref type="table">Table 2</ref>: Performance of various models, including our weighted-pooled LSTM (WP). MFC refers to the most-frequent-class baseline, LogReg is the logistic regression baseline. LSTM and CNN correspond to strong neural network baselines. Note that we bold the performance numbers for the best performing model for each of the "+ POS" case and the "-POS" case.</p><p>Speech (POS) tagger <ref type="bibr" target="#b19">(Manning et al., 2014</ref>) to get corresponding POS features for extracted tokens.</p><p>In all of our models, we limit the maximum length of samples and POS tags to 60 tokens. For the CNN, sequences shorter than 60 tokens are zero- padded. <ref type="table">Table 2</ref> shows the performance obtained by the different models with and without POS tags. Over- all, our attention model WP outperforms all other models in 10 out of 14 scenarios (combinations of datasets and whether or not POS tags are used). Importantly, our model outperforms the regular LSTM model without introducing additional pa- rameters to the model, which highlights the ad- vantage of WP's attention-based pooling method. For all models listed in <ref type="table">Table 2</ref>, we find that in- cluding POS tags benefits the detection of adver- bial presupposition triggers in Gigaword and PTB datasets. Note that, in <ref type="table">Table 2</ref>, we bolded ac- curacy figures that were within 0.1% of the best performing WP model as McNemar's test did not show that WP significantly outperformed the other model in these cases (p value &gt; 0.05). <ref type="table" target="#tab_1">Table 3</ref> shows the confusion matrix for the best performing model (WP,+POS). The small differ- ences in the off-diagonal entries inform us that the model misclassifications are not particularly skewed towards the presence or absence of pre- supposition triggers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicted Actual</head><p>Absence Presence Absence 54,658 11,961 Presence 11,776 55,006   <ref type="table" target="#tab_2">Table 4</ref>, shows the distribution of agreed and disagreed classifica- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis</head><p>Consider the following pair of samples that we randomly choose from the PTB dataset (shortened for readability):</p><p>1. ...Taped just as the market closed yesterday , it offers Ms. Farrell advising , " We view the market here as going through a relatively normal cycle ... . We continue to feel that the stock market is the @@@@ place to be for long-term appreciation 2. ...More people are remaining independent longer presumably because they are better off physically and financially . Careers count most for the well-to-do many affluent people @@@@ place personal success and money above family</p><p>In both cases, the head word is place. In Exam- ple 1, the word continue (emphasized in the above text) suggests that adverb still could be used to modify head word place (i.e., ... the stock mar- ket is still the place ...). Further, it is also easy to see that place refers to stock market, which has occurred in the previous context. Our model cor- rectly predicts this sample as containing a presup- position, this despite the complexity of the coref- erence across the text.</p><p>In the second case of the usage of the same main head word place in Example 2, our model falsely predicts the presence of a presupposition. However, even a human could read the sentence as "many people still place personal success and money above family". This underlies the sub- tlety and difficulty of the task at hand. The long- range dependencies and interactions within sen- tences seen in these examples are what motivate the use of the various deep non-linear models pre- sented in this work, which are useful in detecting these coreferences, particularly in the case of at- tention mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this work, we have investigated the task of pre- dicting adverbial presupposition triggers and in- troduced several datasets for the task. Addition- ally, we have presented a novel weighted-pooling attention mechanism which is incorporated into a recurrent neural network model for predicting the presence of an adverbial presuppositional trigger. Our results show that the model outperforms the CNN and LSTM, and does not add any additional parameters over the standard LSTM model. This shows its promise in classification tasks involv- ing capturing and combining relevant information from multiple points in the previous context.</p><p>In future work, we would like to focus more on designing models that can deal with and be optimized for scenarios with severe data imbal- ance. We would like to also explore various ap- plications of presupposition trigger prediction in language generation applications, as well as ad- ditional attention-based neural network architec- tures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Confusion matrix for the best performing 
model, predicting the presence of a presupposition 
trigger or the absence of such as trigger. 

WP Cor. WP Inc. 
LSTM Cor. 
101,443 
6,819 
LSTM Inc. 
8,016 
17,123 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Contingency table for correct (cor.) and 
incorrect (inc.) predictions between the LSTM 
baseline and the attention model (WP) on the 
Giga_also dataset. 

The contingency table, shown in </table></figure>

			<note place="foot" n="1"> Presupposition of existence are triggered by possessive constructions, names or definite noun phrases.</note>

			<note place="foot" n="2"> http://www.wildml.com/2015/12/implementing-a-cnnfor-text-classification-in-tensorflow/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank the reviewers for their valuable comments. This work was sup-ported by the Centre de Recherche d'Informatique de Montréal (CRIM), the Fonds de Recherche du Québec-Nature et Technologies (FRQNT) and the Natural Sciences and Engineering Research Council of Canada (NSERC).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS 2015)</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The Stanford Encyclopedia of Philosophy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">I</forename><surname>Beaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Geurts</surname></persName>
		</author>
		<editor>Edward N. Zalta</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Metaphysics Research Lab, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note>Presupposition. winter 2014 edition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Blutner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeevat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bezuidenhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Breheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Glucksberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Happé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Recanati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
		<title level="m">Optimality Theory and Pragmatics. Palgrave Studies in Pragmatics, Language and Cognition</title>
		<meeting><address><addrLine>Macmillan UK</addrLine></address></meeting>
		<imprint>
			<publisher>Palgrave</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention-overattention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A measure of information gained during prose learning. Reading Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="369" to="386" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comparable cloze and multiple-choice comprehension test scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">W Culhane</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rankin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Reading</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="193" to="198" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<title level="m">Gottlob Frege. 1892. Über sinn und bedeutung. Zeitschrift für Philosophie und philosophische Kritik</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="25" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">English gigaword third edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Linguistic Data Consortium</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno>abs/1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno>abs/1511.02301</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Capturing pragmatic knowledge in article usage prediction using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jad</forename><surname>Kabbara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The use of too as a pragmatic presupposition trigger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Social Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="165" to="169" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">What is Russell&apos;s theory of descriptions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kaplan</surname></persName>
		</author>
		<editor>Wolfgang Yourgrau and Allen D</editor>
		<imprint>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breck</surname></persName>
		</author>
		<title level="m">Physics, Logic, and History</title>
		<imprint>
			<publisher>Plenum Press</publisher>
			<biblScope unit="page" from="277" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An analysis of presupposition triggers in english journalistic texts. Of College Of Education For Women</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaleel</forename><surname>Layth Muthana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="523" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 2015 International Conference on Learning Representation (ICLR 2015)</title>
		<meeting>eeding of the 2015 International Conference on Learning Representation (ICLR 2015)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A generative constituent-context model for improved grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>abs/1707.05589</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A rule-based question answering system for reading comprehension tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Thelen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 ANLP/NAACL Workshop on Reading Comprehension Tests As Evaluation for Computer-based Language Understanding Sytems</title>
		<meeting>the 2000 ANLP/NAACL Workshop on Reading Comprehension Tests As Evaluation for Computer-based Language Understanding Sytems<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="13" to="19" />
		</imprint>
	</monogr>
	<note>ANLP/NAACLReadingComp &apos;00</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Stalnaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Presuppositions. Journal of philosophical logic</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="447" to="457" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the representation of context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Stalnaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Logic, Language and Information</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strawson</surname></persName>
		</author>
		<title level="m">On referring. Mind</title>
		<imprint>
			<date type="published" when="1950" />
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="320" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cloze procedure: A new tool for measuring readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename><forename type="middle">L</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism Quarterly</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">415</biblScope>
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5994" to="6004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Presupposition trigger-a comparative analysis of broadcast news discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javad</forename><surname>Zare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Abbaspour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi Rajaee</forename><surname>Nia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="734" to="743" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
