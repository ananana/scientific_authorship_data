<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Coreference Resolution with Deep Biaffine Attention by Joint Mention Detection and Mention Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
							<email>r.zhang@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Yale University</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">Yale University</orgName>
								<orgName type="institution" key="instit4">IBM Watson</orgName>
								<orgName type="institution" key="instit5">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
							<email>cicerons@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Yale University</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">Yale University</orgName>
								<orgName type="institution" key="instit4">IBM Watson</orgName>
								<orgName type="institution" key="instit5">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dos</forename><surname>Santos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Yale University</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">Yale University</orgName>
								<orgName type="institution" key="instit4">IBM Watson</orgName>
								<orgName type="institution" key="instit5">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
							<email>michihiro.yasunaga@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Yale University</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">Yale University</orgName>
								<orgName type="institution" key="instit4">IBM Watson</orgName>
								<orgName type="institution" key="instit5">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Yale University</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">Yale University</orgName>
								<orgName type="institution" key="instit4">IBM Watson</orgName>
								<orgName type="institution" key="instit5">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
							<email>dragomir.radev@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Yale University</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">Yale University</orgName>
								<orgName type="institution" key="instit4">IBM Watson</orgName>
								<orgName type="institution" key="instit5">Yale University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Coreference Resolution with Deep Biaffine Attention by Joint Mention Detection and Mention Clustering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="102" to="107"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>102</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Coreference resolution aims to identify in a text all mentions that refer to the same real-world entity. The state-of-the-art end-to-end neural coreference model considers all text spans in a document as potential mentions and learns to link an antecedent for each possible mention. In this paper, we propose to improve the end-to-end coreference resolution system by (1) using a biaffine attention model to get antecedent scores for each possible mention, and (2) jointly optimizing the mention detection accuracy and the mention clustering log-likelihood given the mention cluster labels. Our model achieves the state-of-the-art performance on the CoNLL-2012 Shared Task English test set.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>End-to-end coreference resolution is the task of identifying and grouping mentions in a text such that all mentions in a cluster refer to the same en- tity. An example is given below <ref type="bibr" target="#b2">(Björkelund and Kuhn, 2014</ref>) where mentions for two entities are labeled in two clusters: <ref type="bibr">[Drug Emporium Inc.]</ref>  Many traditional coreference systems, either rule- based ( <ref type="bibr" target="#b14">Haghighi and Klein, 2009;</ref><ref type="bibr" target="#b19">Lee et al., 2011</ref>) * Work done during the internship at IBM <ref type="bibr">Watson.</ref> or learning-based ( <ref type="bibr" target="#b1">Bengtson and Roth, 2008;</ref><ref type="bibr" target="#b12">Fernandes et al., 2012;</ref><ref type="bibr" target="#b10">Durrett and Klein, 2013;</ref><ref type="bibr" target="#b2">Björkelund and Kuhn, 2014)</ref>, usually solve the problem in two separate stages: (1) a mention de- tector to propose entity mentions from the text, and (2) a coreference resolver to cluster proposed mentions. At both stages, they rely heavily on complicated, fine-grained, conjoined features via heuristics. This pipeline approach can cause cas- cading errors, and in addition, since both stages rely on a syntactic parser and complicated hand- craft features, it is difficult to generalize to new data sets and languages.</p><p>Very recently, <ref type="bibr" target="#b20">Lee et al. (2017)</ref> proposed the first state-of-the-art end-to-end neural coreference resolution system. They consider all text spans as potential mentions and therefore eliminate the need of carefully hand-engineered mention detec- tion systems. In addition, thanks to the represen- tation power of pre-trained word embeddings and deep neural networks, the model only uses a min- imal set of hand-engineered features (speaker ID, document genre, span distance, span width).</p><p>The core of the end-to-end neural coreference resolver is the scoring function to compute the mention scores for all possible spans and the an- tecedent scores for a pair of spans. Furthermore, one major challenge of coreference resolution is that most mentions in the document are singleton or non-anaphoric, i.e., not coreferent with any pre- vious mention <ref type="bibr" target="#b34">(Wiseman et al., 2015)</ref>. Since the data set only have annotations for mention clus- ters, the end-to-end coreference resolution system needs to detect mentions, detect anaphoricity, and perform coreference linking. Therefore, research questions still remain on good designs of the scor- ing architecture and the learning strategy for both mention detection and antecedent scoring given only the gold cluster labels.</p><p>To this end, we propose to use a biaffine atten- <ref type="figure">Figure 1</ref>: Model architecture. We consider all text spans up to 10-word length as possible mentions. For brevity, we only show three candidate antecedent spans ("Drug Emporium Inc.", "Gary Wilber", "was named CEO") for the current span "this drugstore chain".</p><p>tion model instead of pure feed forward networks to compute antecedent scores. Furthermore, in- stead of training only to maximize the marginal likelihood of gold antecedent spans, we jointly optimize the mention detection accuracy and the mention clustering log-likelihood given the men- tion cluster labels. We optimize mention detection loss explicitly to extract mentions and also per- form anaphoricity detection.</p><p>We evaluate our model on the CoNLL-2012 En- glish data set and achieve new state-of-the-art per- formances of 67.8% F1 score using a single model and 69.2% F1 score using a 5-model ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Formulation</head><p>In end-to-end coreference resolution, the input is a document D with T words, and the output is a set of mention clusters each of which refers to the same entity. A possible span is an N-gram within a single sentence. We consider all possible spans up to a predefined maximum width. To impose an ordering, spans are sorted by the start position START(i) and then by the end position END(i). For each span i the system needs to assign an an- tecedent a i from all preceding spans or a dummy antecedent : a i ∈ {, 1, . . . , i−1}. If a span j is a true antecedent of the span i, then we have a i = j and 1 ≤ j ≤ i−1. The dummy antecedent repre- sents two possibilities: (1) the span i is not an en- tity mention, or (2) the span i is an entity mention but not coreferent with any previous span. Finally, the system groups mentions according to corefer- ence links to form the mention clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Figure 1 illustrates our model. We adopt the same span representation approach as in <ref type="bibr" target="#b20">Lee et al. (2017)</ref> using bidirectional LSTMs and a head- finding attention. Thereafter, a feed forward net- work produces scores for spans being entity men- tions. For antecedent scoring, we propose a bi- affine attention model <ref type="bibr" target="#b9">(Dozat and Manning, 2017)</ref> to produce distributions of possible antecedents. Our training data only provides gold mention clus- ter labels. To make best use of this information, we propose to jointly optimize the mention scor- ing and antecedent scoring in our loss function. Span Representation Suppose the current sen- tence of length L is [w 1 , w 2 , . . . , w L ], we use w t to denote the concatenation of fixed pretrained word embeddings and CNN character embeddings (dos Santos and Zadrozny, 2014) for word w t . Bidi- rectional LSTMs (Hochreiter and Schmidhuber, 1997) recurrently encode each w t :</p><formula xml:id="formula_0">− → h t = LSTM forward ( − → h t−1 , w t ) ← − h t = LSTM backward ( ← − h t+1 , w t ) h t = [ − → h t , ← − h t ]<label>(1)</label></formula><p>Then, the head-finding attention computes a score distribution over different words in a span s i :</p><formula xml:id="formula_1">α t = v α FFNN α (h t ) s i,t = exp(α t ) END(i) k=START(i) exp(α k ) w head-att i = END(i) t=START(i) s i,t w t (2)</formula><p>where FFNN is a feed forward network outputting a vector. Effective span representations encode both con- textual information and internal structure of spans. Therefore, we concatenate different vectors, in- cluding a feature vector φ(i) for the span size, to produce the span representation s i for s i :</p><formula xml:id="formula_2">s i = [h START(i) , h END(i) , w head-att i , φ(i)]<label>(3)</label></formula><p>Mention Scoring The span representation is input to a feed forward network which measures if it is an entity mention using a score m(i):</p><formula xml:id="formula_3">m(i) = v m FFNN m (s i )<label>(4)</label></formula><p>Since we consider all possible spans, the num- ber of spans is O(T 2 ) and the number of span pairs is O(T 4 ). Due to computation efficiency, we prune candidate spans during both inference and training. We keep λT spans with highest mention scores.</p><p>Biaffine Attention Antecedent Scoring Consider the current span s i and its previous spans s j (1 ≤ j ≤ i − 1), we propose to use a biaffine attention model to produce scores c(i, j): </p><formula xml:id="formula_4">ˆ s i = FFNN anaphora (s i ) ˆ s j = FFNN antecedent (s j ), 1 ≤ j ≤ i − 1 c(i, j) = ˆ s j U bî s i + v bî s i<label>(5)</label></formula><formula xml:id="formula_5">s(i, j) = m(i) + m(j) + c(i, j), j = 0, j =<label>(6)</label></formula><p>During inference, the model only creates a link if the highest antecedent score is positive. Joint Mention Detection and Mention Cluster During training, only mention cluster labels are available rather than antecedent links. Therefore, <ref type="bibr" target="#b20">Lee et al. (2017)</ref> train the model end-to-end by maximizing the following marginal log-likelihood where GOLD(i) are gold antecedents for s i :</p><formula xml:id="formula_6">L cluster (i) = log j ∈GOLD(i) exp(s(i, j )) j=,0,...,i−1 exp(s(i, j))<label>(7)</label></formula><p>However, the initial pruning is completely ran- dom and the mention scoring model only receives distant supervision if we only optimize the above mention cluster performance. This makes learning slow and ineffective especially for mention detec- tion. Based on this observation, we propose to di- rectly optimize mention detection:</p><formula xml:id="formula_7">L detect (i) = y i logˆylogˆ logˆy i + (1 − y i ) log(1 − ˆ y i )<label>(8)</label></formula><p>wherê y i = sigmoid(m(i)), y i = 1 if and only if s i is in one of the gold mention clusters. Our final loss combines mention detection and clustering:</p><formula xml:id="formula_8">L loss = −λ detect N i=1 L detect (i) − N i =1 L cluster (i )</formula><p>where N is the number of all possible spans, N is the number of unpruned spans, and λ detection con- trols weights of two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Data Set and Evaluation We evaluate our model on the CoNLL-2012 Shared Task English data ( <ref type="bibr" target="#b28">Pradhan et al., 2012)</ref> which is based on the OntoNotes corpus ( <ref type="bibr" target="#b16">Hovy et al., 2006</ref>  <ref type="formula" target="#formula_0">(2015)</ref> 76.2 69.3 72.6 66.2 55.8 60.5 59.4 54.9 57.1 63.4 <ref type="bibr">Fernandes et al. (2014)</ref> 75.9 65.8 70.5 77.7 65.8 71.2 43.2 55.0 48.4 <ref type="bibr">63.4 Clark and Manning (2015)</ref> 76.1 69.4 72.6 65.6 56.0 60.4 59.4 53.0 56.0 63.0 Martschat and Strube <ref type="formula" target="#formula_0">(2015)</ref> 76.7 68.1 72.2 66.1 54.2 59.6 59.5 52.3 55.7 62.5 <ref type="bibr" target="#b11">Durrett and Klein (2014)</ref> 72.6 69.9 71.2 61.2 56.4 58.7 56.2 54.2 55.2 61.7 <ref type="bibr" target="#b2">Björkelund and Kuhn (2014)</ref> 74.3 67.5 70.7 62.7 55.0 58.6 59.4 52.3 55.6 61.6 <ref type="bibr" target="#b10">Durrett and Klein (2013)</ref> 72.9 65.9 69.2 63.6 52.5 57.5 54.3 54.4 54.3 60.3 <ref type="table">Table 1</ref>: Experimental results on the CoNLL-2012 Englisth test set. The F1 improvements are statistical significant with p &lt; 0.05 under the paired bootstrap resample test <ref type="bibr" target="#b18">(Koehn, 2004</ref>) compared with <ref type="bibr" target="#b20">Lee et al. (2017)</ref>.</p><p>Avg. F1 Our model (single) 67.8 without mention detection loss 67.5 without biaffine attention 67.4 <ref type="bibr" target="#b20">Lee et al. (2017)</ref> 67.3 <ref type="table">Table 2</ref>: Ablation study on the development set.</p><p>genre, span distance, span width) features as 20- dimensional learned embeddings. Word and char- acter embeddings use 0.5 dropout. All hidden lay- ers and feature embeddings use 0.2 dropout. The batch size is 1 document. Based on the results on the development set, λ detection = 0.1 works best from {0.05, 0.1, 0.5, 1.0}. Model is trained with ADAM optimizer ( <ref type="bibr" target="#b17">Kingma and Ba, 2015)</ref> and converges in around 200K updates, which is faster than that of <ref type="bibr" target="#b20">Lee et al. (2017)</ref>. Overall Performance In <ref type="table">Table 1</ref>, we compare our model with previous state-of-the-art systems. We obtain the best results in all F1 metrics. Our single model achieves 67.8% F1 and our 5-model ensem- ble achieves 69.2% F1. In particular, compared with <ref type="bibr" target="#b20">Lee et al. (2017)</ref>, our improvement mainly results from the precision scores. This indicates that the mention detection loss does produce bet- ter mention scores and the biaffine attention more effectively determines if two spans are coreferent. Ablation Study To understand the effect of dif- ferent proposed components, we perform ablation study on the development set. As shown in <ref type="table">Table  2</ref>, removing the mention detection loss term or the biaffine attention decreases 0.3/0.4 final F1 score, but still higher than the baseline. This shows <ref type="figure">Figure 2</ref>: Mention detection subtask on develop- ment set. We plot accuracy and frequency break- down by span widths.</p><p>that both components have contributions and when they work together the total gain is even higher. Mention Detection Subtask To further under- stand our model, we perform a mention detection subtask where spans with mention scores higher than 0 are considered as mentions. We show the mention detection accuracy breakdown by span widths in <ref type="figure">Figure 2</ref>. Our model indeed performs better thanks to the mention detection loss. The advantage is even clearer for longer spans which consist of 5 or more words.</p><p>In addition, it is important to note that our model can detect mentions that do not exist in the training data. While <ref type="bibr" target="#b23">Moosavi and Strube (2017)</ref> observe that there is a large overlap be- tween the gold mentions of the training and dev (test) sets, we find that our model can correctly de-tect 1048 mentions which are not detected by <ref type="bibr" target="#b20">Lee et al. (2017)</ref>, consisting of 386 mentions existing in training data and 662 mentions not existing in training data. From those 662 mentions, some ex- amples are (1) a suicide murder (2) Hong Kong Is- land (3) a US Airforce jet carrying robotic under- sea vehicles (4) the investigation into who was be- hind the apparent suicide attack. This shows that our mention loss helps detection by generalizing to new mentions in test data rather than memoriz- ing the existing mentions in training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>As summarized by <ref type="bibr" target="#b25">Ng (2010)</ref>, learning-based coreference models can be categorized into three types: (1) Mention-pair models train binary clas- sifiers to determine if a pair of mentions are coref- erent ( <ref type="bibr">Soon et al., 2001;</ref><ref type="bibr" target="#b26">Ng and Cardie, 2002;</ref><ref type="bibr" target="#b1">Bengtson and Roth, 2008)</ref>. <ref type="formula">(2)</ref> Mention-ranking models explicitly rank all previous candidate men- tions for the current mention and select a sin- gle highest scoring antecedent for each anaphoric mention <ref type="bibr" target="#b8">(Denis and Baldridge, 2007b;</ref><ref type="bibr" target="#b34">Wiseman et al., 2015;</ref><ref type="bibr" target="#b4">Clark and Manning, 2016a;</ref><ref type="bibr" target="#b20">Lee et al., 2017)</ref>. <ref type="formula" target="#formula_2">(3)</ref> Entity-mention models learn classifiers to determine whether the current mention is coref- erent with a preceding, partially-formed mention cluster <ref type="bibr" target="#b3">(Clark and Manning, 2015;</ref><ref type="bibr" target="#b33">Wiseman et al., 2016;</ref><ref type="bibr" target="#b5">Clark and Manning, 2016b</ref>).</p><p>In addition, we also note latent-antecedent mod- els <ref type="bibr" target="#b12">(Fernandes et al., 2012;</ref><ref type="bibr" target="#b2">Björkelund and Kuhn, 2014;</ref><ref type="bibr" target="#b22">Martschat and Strube, 2015)</ref>. <ref type="bibr" target="#b12">Fernandes et al. (2012)</ref> introduce coreference trees to repre- sent mention clusters and learn to extract the max- imum scoring tree in the graph of mentions.</p><p>Recently, several neural coreference resolution systems have achieved impressive gains ( <ref type="bibr" target="#b34">Wiseman et al., 2015</ref><ref type="bibr" target="#b33">Wiseman et al., , 2016</ref><ref type="bibr">Clark and Manning, 2016b,a)</ref>. They utilize distributed representations of mention pairs or mention clusters to dramatically reduce the number of hand-crafted features. For exam- ple, <ref type="bibr" target="#b34">Wiseman et al. (2015)</ref> propose the first neural coreference resolution system by training a deep feed-forward neural network for mention ranking. However, these models still employ the two-stage pipeline and require a syntactic parser or a sepa- rate designed hand-engineered mention detector.</p><p>Finally, we also note the relevant work on joint mention detection and coreference resolu- tion. <ref type="bibr" target="#b6">Daumé III and Marcu (2005)</ref> propose to model both mention detection and coreference of the Entity Detection and Tracking task simultane- ously. <ref type="bibr" target="#b7">Denis and Baldridge (2007a)</ref> propose to use integer linear programming framework to model anaphoricity and coreference as a joint task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose to use a biaffine attention model to jointly optimize mention detection and mention clustering in the end-to-end neural coref- erence resolver. Our model achieves the state-of- the-art performance on the CoNLL-2012 Shared Task in English.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FFNN</head><label></label><figDesc>anaphora and FFNN antecedent reduce span rep- resentation dimensions and only keep informa- tion relevant to coreference decisions. Compared with the traditional FFNN approach in Lee et al. (2017), biaffine attention directly models both the compatibility of s i and s j byˆsbyˆbyˆs j U bî s i and the prior likelihood of s i having an antecedent by v bî s i . Inference The final coreference score s(i, j) for span s i and span s j consists of three terms: (1) if s i is a mention, (2) if s j is a mention, (3) if s j is an antecedent for s i . Furthermore, for dummy antecedent , we fix the final score to be 0:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe (Pennington et al., 2014) embeddings and 50-dimension embeddings from Turian et al. (2010). Character CNNs use 8- dimension learned embeddings and 50 kernels for each window size in {3,4,5}. LSTMs have hidden size 200, and each FFNN has two hidden layers with 150 units and ReLU (Nair and Hinton, 2010) activations. We include (speaker ID, document</figDesc><table>). It con-
tains 2,802/343/348 train/development/test docu-
ments in different genres. 
We use three standard metrics: MUC (Vilain 
et al., 1995), B 3 (Bagga and Baldwin, 1998), and 
CEAF φ 4 (Luo, 2005MUC 
B 3 
CEAF φ 4 
P 
R 
F1 
P 
R 
F1 
P 
R 
F1 
Avg. F1 
Our work (5-model ensemble) 
82.1 73.6 77.6 73.1 62.0 67.1 67.5 59.0 62.9 
69.2 
Lee et al. (2017) (5-model ensemble) 81.2 73.6 77.2 72.3 61.7 66.6 65.2 60.2 62.6 
68.8 
Our work (single model) 
79.4 73.8 76.5 69.0 62.3 65.5 64.9 58.3 61.4 
67.8 
Lee et al. (2017) (single model) 
78.4 73.4 75.8 68.6 61.8 65.0 62.7 59.0 60.8 
67.2 
Clark and Manning (2016a) 
79.2 70.4 74.6 69.9 58.0 63.4 63.5 55.5 59.2 
65.7 
Clark and Manning (2016b) 
79.9 69.3 74.2 71.0 56.5 63.0 63.8 54.3 58.7 
65.3 
Wiseman et al. (2016) 
77.5 69.8 73.4 66.8 57.0 61.5 62.1 53.9 57.7 
64.2 
Wiseman et al. </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Kenton Lee and three anonymous re-viewers for their helpful discussion and feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Algorithms for scoring coreference chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breck</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The first international conference on language resources and evaluation workshop on linguistics coreference</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding the value of features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bengtson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning structured perceptrons for coreference resolution with latent antecedents and non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Entity-centric coreference resolution with model stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving coreference resolution by learning entitylevel distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A large-scale exploration of effective global features for a joint entity detection and tracking model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint determination of anaphoricity and coreference resolution using integer programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A ranking approach to pronoun resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Easy victories and uphill battles in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A joint model for entity analysis: Coreference, typing, and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="477" to="490" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Latent structure perceptron with feature induction for unrestricted coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eraldo Rezende Fernandes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Cícero Nogueira Dos Santos, and Ruy Luiz Milidiú</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cícero Nogueira dos Santos, and Ruy Luiz Milidiú. 2014. Latent trees for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eraldo Rezende Fernandes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Simple coreference resolution with rich syntactic and semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ontonotes: the 90% solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stanford&apos;s multi-pass sieve coreference resolution system at the conll-2011 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Peirsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifteenth conference on computational natural language learning: Shared task</title>
		<meeting>the fifteenth conference on computational natural language learning: Shared task</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="28" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On coreference resolution performance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Latent structures for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="405" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lexical features in coreference resolution: To be used with caution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadat</forename><surname>Nafise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Moosavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Supervised noun phrase coreference research: The first fifteen years</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving machine learning approaches to coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
		<idno>JMLR: W&amp;CP</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A machine learning approach to coreference resolution of noun phrases</title>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<editor>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim</editor>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="521" to="544" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Vilain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Aberdeen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th conference on Message understanding</title>
		<meeting>the 6th conference on Message understanding</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning global features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart M</forename><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning anaphoricity and antecedent ranking features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam Joshua</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">Matthew</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">Merrill</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
