<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Words: Deep Learning for Multiword Expressions and Collocations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valia</forename><surname>Kordoni</surname></persName>
							<email>kordonie@anglistik.hu-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Humboldt-Universit√§t zu Berlin</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Tutorial Overview</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Words: Deep Learning for Multiword Expressions and Collocations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2017, Tutorial Abstracts</title>
						<meeting>ACL 2017, Tutorial Abstracts <address><addrLine>Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="15" to="16"/>
							<date type="published">July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-5005</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Deep learning has recently shown much promise for NLP applications. Traditionally, in most NLP approaches, documents or sentences are represented by a sparse bag-of-words representation. There is now a lot of work which goes beyond this by adopting a distributed representation of words, by constructing a so-called &quot;neural embedding&quot; or vector space representation of each word or document. The aim of this tutorial is to go beyond the learning of word vectors and present methods for learning vector representations for Multiword Expressions and bilingual phrase pairs, all of which are useful for various NLP applications. This tutorial aims to provide attendees with a clear notion of the linguistic and distributional characteristics of multiword expressions (MWEs), their relevance for the intersection of deep learning and natural language processing, what methods and resources are available to support their use, and what more could be done in the future. Our target audience are researchers and practitioners in machine learning, parsing (syntactic and semantic) and language technology, not necessarily experts in MWEs, who are interested in tasks that involve or could benefit from considering MWEs as a pervasive phenomenon in human language and communication. This tutorial consists of four parts. Part I starts with a thorough introduction to different types of MWEs and collocations, their linguistic dimensions (idiomaticity, syntactic and semantic fixed-ness, specificity, etc.), as well as their statistical characteristics (variability, recurrence, association , etc.). This part concludes with an overview of linguistic and psycholinguistic theories of MWEs to date. For MWEs to be useful for language technology , they must be recognisable automatically. Hence, Part II surveys computational approaches to MWEs recognition, both manually-authored approaches and machine learning ones, as well as computational approaches to MWE elements combination. We will also review type and token evaluation methods for MWE identification. Part III offers a thorough overview of how and where MWEs can contribute to the intersection of NLP and Deep Learning, particularly focusing on recent advances in the computational treatment of MWEs in the framework of Deep Learning. Part IV of the tutorial concludes with concrete examples of where MWEs treatment can contribute to language technology applications such as machine translation, information extraction, information retrieval and parsing, as well as MWE-related multi-level annotation platforms (for instance , pipelines) and resources made available for a wide range of languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tutorial Outline</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 .</head><label>1</label><figDesc>PART I -General overview: (a) Introduction (b) Types and examples of MWEs and col- locations (c) Linguistic dimensions of MWEs: id- iomaticity, syntactic and semantic fixed- ness, specificity, etc. (d) Statistical dimensions of MWEs: vari- ability, recurrence, association, etc. (e) Linguistic and psycholinguistic theories of MWEs 2. PART II -Computational methods (symbolic and statistical) (a) Recognizing the elements of MWEs (b) Recognising how elements are com- bined</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
