<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trans-dimensional Random Fields for Language Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
							<email>wangbin12@mails.tsinghua.edu.cn, ozj@tsinghua.edu.cn, ztan@stat.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08854</postCode>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Trans-dimensional Random Fields for Language Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="785" to="794"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Language modeling (LM) involves determining the joint probability of words in a sentence. The conditional approach is dominant, representing the joint probability in terms of conditionals. Examples include n-gram LMs and neural network LMs. An alternative approach, called the random field (RF) approach, is used in whole-sentence maximum entropy (WSME) LMs. Although the RF approach has potential benefits, the empirical results of previous WSME models are not satisfactory. In this paper, we revisit the RF approach for language modeling, with a number of innovations. We propose a trans-dimensional RF (TDRF) model and develop a training algorithm using joint stochastic approximation and trans-dimensional mixture sampling. We perform speech recognition experiments on Wall Street Journal data, and find that our TDRF models lead to performances as good as the recurrent neural network LMs but are computationally more efficient in computing sentence probability.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language modeling is crucial for a variety of computational linguistic applications, such as speech recognition, machine translation, handwriting recognition, information retrieval and so on. It involves determining the joint probability p(x) of a sentence x, which can be denoted as a pair x = (l, x l ), where l is the length and x l = (x 1 , . . . , x l ) is a sequence of l words.</p><p>Currently, the dominant approach is conditional modeling, which decomposes the joint probability of x l into a product of conditional probabilities 1 <ref type="bibr">1</ref> And the joint probability of x is modeled as p(x) = by using the chain rule,</p><formula xml:id="formula_0">p(x 1 , . . . , x l ) = l i=1</formula><p>p(x i |x 1 , . . . , x i−1 ). <ref type="formula">(1)</ref> To avoid degenerate representation of the con- ditionals, the history of x i , denoted as h i = (x 1 , · · · , x i−1 ), is reduced to equivalence classes through a mapping φ(h i ) with the assumption p(x i |h i ) ≈ p(x i |φ(h i )).</p><p>Language modeling in this conditional approach consists of finding suitable mappings φ(h i ) and effective methods to estimate p(x i |φ(h i )). A classic example is the traditional n-gram LMs with φ(h i ) = (x i−n+1 , . . . , x i−1 ). Various smoothing techniques are used for parameter estimation <ref type="bibr" target="#b4">(Chen and Goodman, 1999)</ref>. Recently, neural network LMs, which have begun to surpass the traditional n-gram LMs, also follow the conditional modeling approach, with φ(h i ) determined by a neural network (NN), which can be either a feedforward NN <ref type="bibr" target="#b25">(Schwenk, 2007)</ref> or a recurrent <ref type="bibr">NN (Mikolov et al., 2011)</ref>.</p><p>Remarkably, an alternative approach is used in whole-sentence maximum entropy (WSME) lan- guage modeling ( <ref type="bibr" target="#b22">Rosenfeld et al., 2001</ref>). Specifi- cally, a WSME model has the form:</p><formula xml:id="formula_2">p(x; λ) = 1 Z exp{λ T f (x)}<label>(3)</label></formula><p>Here f (x) is a vector of features, which can be arbitrary computable functions of x, λ is the cor- responding parameter vector, and Z is the global normalization constant. Although WSME mod- els have the potential benefits of being able to naturally express sentence-level phenomena and integrate features from a variety of knowledge</p><formula xml:id="formula_3">p(x l )p(EOS|x l ),</formula><p>where EOS is a special token placed at the end of every sentence. Thus the distribution of the sentence length is implicitly modeled. sources, their performance results ever reported are not satisfactory ( <ref type="bibr" target="#b22">Rosenfeld et al., 2001;</ref><ref type="bibr" target="#b0">Amaya and Benedí, 2001;</ref><ref type="bibr" target="#b24">Ruokolainen et al., 2010</ref>).</p><p>The WSME model defined in (3) is basically a Markov random field (MRF). A substantial chal- lenge in fitting MRFs is that evaluating the gradi- ent of the log likelihood requires high-dimensional integration and hence is difficult even for mod- erately sized models <ref type="bibr" target="#b28">(Younes, 1989)</ref>, let alone the language model (3). The sampling methods previously tried for approximating the gradient are the Gibbs sampling, the Independence Metropolis- Hasting sampling and the importance sampling ( <ref type="bibr" target="#b22">Rosenfeld et al., 2001</ref>). Simple applications of these methods are hardly able to work efficient- ly for the complex, high-dimensional distribution such as (3), and hence the WSME models are in fact poorly fitted to the data. This is one of the reasons for the unsatisfactory results of previous WSME models.</p><p>In this paper, we propose a new language model, called the trans-dimensional random field (TDRF) model, by explicitly taking account of the empirical distributions of lengths. This formulation subsequently enables us to develop a powerful Markov chain Monte Carlo (MCMC) technique, called trans-dimensional mixture sampling and then propose an effective training algorithm in the framework of stochastic approximation (SA) ( <ref type="bibr" target="#b1">Benveniste et al., 1990;</ref><ref type="bibr" target="#b5">Chen, 2002</ref>). The SA algorithm involves jointly updating the model parameters and normalization constants, in conjunction with trans-dimensional MCMC sampling. Section 2 and 3 present the model definition and estimation respectively. Furthermore, we make several additional in- novations, as detailed in Section 4, to enable successful training of TDRF models. First, the diagonal elements of hessian matrix are estimat- ed during SA iterations to rescale the gradient, which significantly improves the convergence of the SA algorithm. Second, word classing is in- troduced to accelerate the sampling operation and also improve the smoothing behavior of the mod- els through sharing statistical strength between similar words. Finally, multiple CPUs are used to parallelize the training of our RF models.</p><p>In Section 5, speech recognition experiments are conducted to evaluate our TDRF LMs, com- pared with the traditional 4-gram LMs and the re- current neural network LMs (RNNLMs) <ref type="bibr" target="#b15">(Mikolov et al., 2011</ref>) which have emerged as a new state- of-art of language modeling. We explore the use of a variety of features based on word and class information in TDRF LMs. In terms of word error rates (WERs) for speech recognition, our TDRF LMs alone can outperform the KN-smoothing 4- gram LM with 9.1% relative reduction, and per- form comparably to the RNNLM with a slight 0.5% relative reduction. To our knowledge, this result represents the first strong empirical evidence supporting the power of using the whole-sentence language modeling approach. Our open-source TDRF toolkit is released publicly 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Definition</head><p>Throughout, we denote 3 by x l = (x 1 , . . . , x l ) a sentence (i.e., word sequence) of length l ranging from 1 to m. Each element of x l corresponds to a single word. For l = 1, . . . , m, we assume that sentences of length l are distributed from an exponential family model:</p><formula xml:id="formula_4">p l (x l ; λ) = 1 Z l (λ) e λ T f (x l ) ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">f (x l ) = (f 1 (x l ), f 2 (x l ), . . . , f d (x l )) T is</formula><p>the feature vector and λ = (λ 1 , λ 2 , . . . , λ d ) T is the corresponding parameter vector, and Z l (λ) is the normalization constant:</p><formula xml:id="formula_6">Z l (λ) = x l e λ T f (x l )<label>(5)</label></formula><p>Moreover, we assume that length l is associated with probability π l for l = 1, . . . , m. Therefore, the pair (l, x l ) is jointly distributed as</p><formula xml:id="formula_7">p(l, x l ; λ) = π l p l (x l ; λ).<label>(6)</label></formula><p>We provide several comments on the above model definition. First, by making explicit the role of lengths in model definition, it is clear that the model in (6) is a mixture of random fields on sentences of different lengths (namely on sub- spaces of different dimensions), and hence will be called a trans-dimensional random field (TDRF). Different from the WSME model (3), a crucial aspect of the TDRF model (6) is that the mixture weights π l can be set to the empirical length probabilities in the training data. The WSME model (3) is essentially also a mixture of RFs, but the mixture weights implied are proportional to the normalizing constants Z l (λ):</p><formula xml:id="formula_8">p(l, x l ; λ) = Z l (λ) Z(λ) 1 Z l (λ) e λ T f (x l ) ,<label>(7)</label></formula><p>where Z(λ) = m l=1 Z l (λ). A motivation for proposing (6) is that it is very difficult to sample from (3), namely <ref type="formula" target="#formula_8">(7)</ref>, as a mixture distribution with unknown weights which typically differ from each other by orders of magnitudes, e.g. 10 40 or more in our experiments. Setting mixture weights to the known, empirical length probabilities enables us to develop a very effective learning algorithm, as introduced in Sec- tion 3. Basically, the empirical weights serve as a control device to improve sampling from multiple distributions ( <ref type="bibr" target="#b13">Liang et al., 2007;</ref><ref type="bibr" target="#b27">Tan, 2015)</ref> .</p><p>Second, it can be shown that if we incorporate the length features 4 in the vector of features f (x) in (3), then the distribution p(x; λ) in (3) under the maximum entropy (ME) principle will take the form of (6) and the probabilities (π 1 , . . . , π m ) in (6) implied by the parameters for the length fea- tures are exactly the empirical length probabilities.</p><p>Third, a feature f i (x l ), 1 ≤ i ≤ d, can be any computable function of the sentence x l , such as n-grams. In our current experiments, the features f i (x l ) and their corresponding parameters λ i are defined to be position-independent and length- independent. For example,</p><formula xml:id="formula_9">f i (x l ) = k f i (x l , k), where f i (x l , k)</formula><p>is a binary function of x l evaluated at position k. As a result, the feature f i (x l ) takes values in the non-negative integers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Estimation</head><p>We develop a stochastic approximation algorith- m using Markov chain Monte Carlo to estimate the parameters λ and the normalization constants Z 1 (λ), ..., Z m (λ) ( <ref type="bibr" target="#b1">Benveniste et al., 1990;</ref><ref type="bibr" target="#b5">Chen, 2002</ref>). The core algorithms newly designed in this paper are the joint SA for simultaneously estimating parameters and normalizing constants (Section 3.2) and trans-dimensional mixture sam- pling (Section 3.3) which is used as Step I of the joint SA. The most relevant previous works that we borrowed from are (Gu and Zhu, 2001) on SA for fitting a single RF, (Tan, 2015) on sampling and estimating normalizing constants from multiple RFs of the same dimension, and <ref type="bibr" target="#b9">(Green, 1995)</ref> on trans-dimensional MCMC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Maximum likelihood estimation</head><p>Suppose that the training dataset consists of n l sentences of length l for l = 1, . . . , m. First, the maximum likelihood estimate of the length probability π l is easily shown to be n l /n, where n = m l=1 n l . By abuse of notation, we set π l = n l /n hereafter. Next, the log-likelihood of λ given the empirical length probabilities is</p><formula xml:id="formula_10">L(λ) = 1 n m l=1 x l ∈D l log p l (x l ; λ),<label>(8)</label></formula><p>where D l is the collection of sentences of length l in the training set. By setting to 0 the derivative of <ref type="formula" target="#formula_10">(8)</ref> with respect to λ, we obtain that the maximum likelihood estimate of λ is determined by the following equation:</p><formula xml:id="formula_11">∂L(λ) ∂λ = ˜ p[f ] − p λ [f ] = 0,<label>(9)</label></formula><p>where˜pwhere˜ where˜p[f ] is the expectation of the feature vector f with respect to the empirical distribution:</p><formula xml:id="formula_12">˜ p[f ] = 1 n m l=1 x l ∈D l f (x l ),<label>(10)</label></formula><p>and p λ [f ] is the expectation of f with respect to the joint distribution (6) with π l = n l /n:</p><formula xml:id="formula_13">p λ [f ] = m l=1 n l n p λ,l [f ],<label>(11)</label></formula><p>and</p><formula xml:id="formula_14">p λ,l [f ] = x l f (x l )p l (x l ; λ)</formula><p>. Eq. <ref type="formula" target="#formula_11">(9)</ref> has the form of equating empirical expectations˜pexpectations˜ expectations˜p[f ] with theoretical expectations p λ [f ], as similarly found in maximum likelihood estimation of single random field models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Joint stochastic approximation</head><p>Training random field models is challenging due to numerical intractability of the normalizing con- stants Z l (λ) and expectations p λ,l <ref type="bibr">[f ]</ref>. We propose a novel SA algorithm for estimating the parame- ters λ by (9) and, simultaneously, estimating the log ratios of normalization constants:</p><formula xml:id="formula_15">ζ * l (λ) = log Z l (λ) Z 1 (λ) , l = 1, . . . , m<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Joint stochastic approximation</head><p>Input: training set 1: set initial values λ (0) = (0, . . . , 0) T and</p><formula xml:id="formula_16">ζ (0) = ζ * (λ (0) ) − ζ * 1 (λ (0) ) 2: for t = 1, 2, . . . , tmax do 3: set B (t) = ∅ 4: set (L (t,0) , X (t,0) ) = (L (t−1,K) , X (t−1,K) )</formula><p>Step I: MCMC sampling 5:</p><formula xml:id="formula_17">for k = 1 → K do 6: sampling (See Algorithm 3) (L (t,k) , X (t,k) ) = SAM P LE(L (t,k−1) , X (t,k−1) ) 7: set B (t) = B (t) ∪ {(L (t,k) , X (t,k) )} 8:</formula><p>end for</p><p>Step II: SA updating 9:</p><p>Compute λ (t) based on (14) 10:</p><p>Compute ζ (t) based on <ref type="formula" target="#formula_6">(15)</ref> and <ref type="formula" target="#formula_7">(16)  11</ref>: end for where Z 1 (λ) is chosen as the reference value and can be calculated exactly. The algorithm can be obtained by combining the standard SA algorithm for training single random fields ( <ref type="bibr" target="#b10">Gu and Zhu, 2001</ref>) and a trans-dimensional extension of the self-adjusted mixture sampling algorithm <ref type="bibr" target="#b27">(Tan, 2015)</ref>.</p><p>Specifically, consider the following joint distri- bution of the pair (l, x l ):</p><formula xml:id="formula_18">p(l, x l ; λ, ζ) ∝ π l e ζ l e λ T f (x l ) ,<label>(13)</label></formula><p>where π l is set to n l /n for l = 1, . . . , m, but</p><formula xml:id="formula_19">ζ = (ζ 1 , . . . , ζ m ) T with ζ 1 = 0 are hypothesized values of the truth ζ * (λ) = (ζ * 1 (λ), . . . , ζ * m (λ)) T with ζ * 1 (λ) = 0. The distribution p(l, x l ; λ, ζ) reduces to p(l, x l ; λ) in (6) if ζ were identical to ζ * (λ). In general, p(l, x l ; λ, ζ) differs from p(l, x l ; λ)</formula><p>in that the marginal probability of length l is not necessarily π l .</p><p>The joint SA algorithm, whose pseudo-code is shown in Algorithm 1, consists of two steps at each time t as follows.</p><p>Step I: MCMC sampling. Generate a sample set B (t) with p(l, x l ; λ (t−1) , ζ (t−1) ) as the station- ary distribution (see Section 3.3).</p><p>Step II: SA updating. Compute</p><formula xml:id="formula_20">λ (t) = λ (t−1) + γ λ ˜ p[f ] − (l,x l )∈B (t) f (x l ) K (14)</formula><p>where γ λ is a learning rate of λ; compute</p><formula xml:id="formula_21">ζ (t− 1 2 ) = ζ (t) + γ ζ δ1(B (t) ) π1 , . . . , δm(B (t) ) πm<label>(15)</label></formula><formula xml:id="formula_22">ζ (t) = ζ (t− 1 2 ) − ζ (t− 1 2 ) 1 (16)</formula><p>where γ ζ is a learning rate of ζ, and δ l (B <ref type="bibr">(t)</ref> ) is the relative frequency of length l appearing in B (t) :</p><formula xml:id="formula_23">δ l (B (t) ) = (j,x j )∈B (t) 1(j = l) K .<label>(17)</label></formula><p>The rationale in <ref type="formula" target="#formula_6">(15)</ref> is to adjust ζ based on how the relative frequencies of lengths δ l (B (t) ) are compared with the desired length probabili- ties π l . Intuitively, if the relative frequency of some length l in the sample set B (t) is greater (or respectively smaller) than the desired length probability π l , then the hypothesized value ζ (t−1) l is an underestimate (or overestimate) of ζ * l (λ (t−1) ) and hence should be increased (or decreased).</p><p>Following <ref type="bibr" target="#b10">Gu &amp; Zhu (2001)</ref> and Tan <ref type="formula" target="#formula_1">(2015)</ref>, we set the learning rates in two stages:</p><formula xml:id="formula_24">γ λ = t −β λ if t ≤ t 0 1 t−t 0 +t β λ 0 if t &gt; t 0 (18) γ ζ = (0.1t) −β ζ if t ≤ t 0 1 0.1(t−t 0 )+(0.1t 0 ) β ζ if t &gt; t 0<label>(19)</label></formula><p>where 0.5 &lt; β λ , β ζ &lt; 1. In the first stage (t ≤ t 0 ), a slow-decaying rate of t −β is used to introduce large adjustments. This forces the estimates λ (t) and ζ (t) to fall reasonably fast into the true values. In the second stage (t &gt; t 0 ), a fast-decaying rate of t −1 is used. The iteration number t is multiplied by 0.1 in (19), to make the the learning rate of ζ decay more slowly than λ. Commonly, t 0 is selected to ensure there is no more significant adjustment observed in the first stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Trans-dimensional mixture sampling</head><p>We describe a trans-dimensional mixture sam- pling algorithm to simulate from the joint distri- bution p(l, x l ; λ, ζ), which is used with (λ, ζ) = (λ (t−1) , ζ (t−1) ) at time t for MCMC sampling in the joint SA algorithm. The name "mixture sam- pling" reflects the fact that p(l, x l ; λ, ζ) represents a labeled mixture, because l is a label indicating that x l is associated with the distribution p l (x l ; ζ). With fixed (λ, ζ), this sampling algorithm can be seen as formally equivalent to reversible jump MCMC <ref type="bibr" target="#b9">(Green, 1995)</ref>, which was originally pro- posed for Bayes model determination.</p><p>The trans-dimensional mixture sampling algo- rithm consists of two steps at each time t: local jump between lengths and Markov move of sen- tences for a given length. In the following, we de- note by L (t−1) and X (t−1) the length and sequence before sampling, but use the short notation (λ, ζ) for (λ (t−1) , ζ (t−1) ).</p><p>Step I: Local jump. The Metropolis-Hastings method is used in this step to sample the length. Assuming L (t−1) = k, first we draw a new length j ∼ Γ(k, ·). The jump distribution Γ(k, l) is defined to be uniform at the neighborhood of k :</p><formula xml:id="formula_25">Γ(k, l) =          1 3 , if k ∈ [2, m − 1], l ∈ [k − 1, k + 1] 1 2 , if k = 1, l ∈ [1, 2] or k = m, l ∈ [m − 1, m] 0, otherwise<label>(20)</label></formula><p>where m is the maximum length. Eq. <ref type="formula" target="#formula_1">(20)</ref> restricts the difference between j and k to be no more than one. If j = k, we retain the sequence and perform the next step directly, i.e. set L (t) = k and X (t) = X (t−1) . If j = k + 1 or j = k − 1, the two cases are processed differently. If j = k + 1, we first draw an element (i.e., word) Y from a proposal distribution:</p><formula xml:id="formula_26">Y ∼ g k+1 (y|X (t−1) ).</formula><p>Then we set L (t) = j (= k + 1) and X (t) = {X (t−1) , Y } with probability</p><formula xml:id="formula_27">min 1, Γ(j, k) Γ(k, j) p(j, {X (t−1) , Y }; λ, ζ) p(k, X (t−1) ; λ, ζ)g k+1 (Y |X (t−1) )<label>(21)</label></formula><p>where {X (t−1) , Y } denotes a sequence of length k + 1 whose first k elements are X (t−1) and the last element is Y .</p><p>If j = k − 1, we set L (t) = j (= k − 1) and X (t) = X (t−1) 1:j with probability</p><formula xml:id="formula_28">min 1, Γ(j, k) Γ(k, j) p(j, X (t−1) 1:j ; λ, ζ)g k (X (t−1) k |X (t−1) 1:j ) p(k, X (t−1) ; λ, ζ)<label>(22)</label></formula><p>where X (t−1) 1:j is the first j elements of X (t−1) and</p><formula xml:id="formula_29">X (t−1) k</formula><p>is the kth element of X (t−1) . In (21) and <ref type="formula" target="#formula_1">(22)</ref>, g k+1 (y|x k ) can be flexibly specified as a proper density function in y. In our application, we find the following choice works reasonably well:</p><formula xml:id="formula_30">g k+1 (y|x k ) = p(k + 1, {x k , y}; λ, ζ) w p(k + 1, {x k , w}; λ, ζ) .<label>(23)</label></formula><p>Step II: Markov move. After the step of local jump, we obtain</p><formula xml:id="formula_31">X (t) =      X (t−1) if L (t) = k {X (t−1) , Y } if L (t) = k + 1 X (t−1) 1:k−1 if L (t) = k − 1<label>(24)</label></formula><p>Then we perform Gibbs sampling on X (t) , from the first element to the last element (Algorithm 2)</p><formula xml:id="formula_32">Algorithm 2 Markov Move 1: for i = 1 → L (t) do 2: draw W ∼ p(L (t) , {X (t) 1:i−1 , w, X (t) i+1:L (t) }; λ, ζ) 3: set X (t) i = W 4: end for</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Algorithm Optimization and Acceleration</head><p>The joint SA algorithm may still suffer from slow convergence, especially when λ is high- dimensional. We introduce several techniques for improving the convergence of the algorithm and reducing computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Improving SA recursion</head><p>We propose two techniques to effectively improve the convergence of SA recursion. The first technique is to incorporate Hessian information, similarly as in related works on s- tochastic approximation (Gu and ) and stochastic gradient descent algorithms ( <ref type="bibr" target="#b3">Byrd et al., 2014</ref>). But we only use the diagonal elements of the Hessian matrix to re-scale the gradient, due to high-dimensionality of λ.</p><p>Taking the second derivatives of L(λ) yields</p><formula xml:id="formula_33">H i = − d 2 L(λ) dλ 2 i = p[f 2 i ] − m l=1 π l (p l [f i ]) 2 (25)</formula><p>where H i denotes the ith diagonal element of Hessian matrix. At time t, before updating the parameter λ (Step II in Section 3.2), we compute</p><formula xml:id="formula_34">H (t− 1 2 ) i = 1 K (l,x l )∈B (t) f i (x l ) 2 − m l=1 π l (¯ p l [f i ]) 2 ,<label>(26)</label></formula><formula xml:id="formula_35">H (t) i = H (t−1) i + γ H (H (t− 1 2 ) i − H (t−1) i ),<label>(27)</label></formula><p>where</p><formula xml:id="formula_36">¯ p l [f i ] = |B (t) l | −1 (l,x l )∈B (t) l f i (x l )</formula><p>, and</p><formula xml:id="formula_37">B (t) l</formula><p>is the subset, of size |B</p><formula xml:id="formula_38">(t)</formula><p>l |, containing all sentences of length l in B (t) .</p><p>The second technique is to introduce the "mini- batch" on the training set. At each iteration, a subset D (t) of K sentences are randomly selected from the training set. Then the gradient is approx- imated with the overall empirical expectatioñ p[f ] being replaced by the empirical expectation over the subset D <ref type="bibr">(t)</ref> . This technique is reminiscent of stochastic gradient descent using a random sub- sample of training data to achieve fast convergence  of optimization algorithms <ref type="bibr" target="#b2">(Bousquet and Bottou, 2008)</ref>.</p><p>By combining the two techniques, we revise the updating equation <ref type="formula" target="#formula_4">(14)</ref> of λ to</p><formula xml:id="formula_39">λ (t) i = λ (t−1) i + γ λ max(H (t) i , h) × (l,x l )∈D (t) fi(x l ) K − (l,x l )∈B (t) fi(x l ) K<label>(28)</label></formula><p>where 0 &lt; h &lt; 1 is a threshold to avoid H (t) i being too small or even zero. Moreover, a constant t c is added to the denominator of <ref type="formula" target="#formula_10">(18)</ref>, to avoid too large adjustment of λ, i.e. <ref type="figure" target="#fig_0">Fig.1(a)</ref> shows the result after introducing hessian estimation, and <ref type="figure" target="#fig_0">Fig.1(b)</ref> shows the effect of train- ing set mini-batching.</p><formula xml:id="formula_40">γ λ = 1 tc+t β λ if t ≤ t 0 , 1 tc+t−t 0 +t β λ 0 if t &gt; t 0 .<label>(29)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sampling acceleration</head><p>For MCMC sampling in Section 3.3, the Gibbs sampling operation of drawing X (t) i (Step 2 in Al- gorithms 2) involves calculating the probabilities of all the possible elements in position i. This is computationally costly, because the vocabulary size |V| is usually 10 thousands or more in prac- tice. As a result, the Gibbs sampling operation presents a bottleneck limiting the efficiency of sampling algorithms.</p><p>We propose a novel method of using class in- formation to effectively reduce the computational cost of Gibbs sampling. Suppose that each word in vocabulary V is assigned to a single class. If the total class number is |C|, then there are, on average, |V|/|C| words in each class. With the class information, we can first draw the class of X (t) i , denoted by c (t) i , and then draw a word</p><formula xml:id="formula_41">Algorithm 3 Class-based MCMC sampling 1: function SAMPLE((L (t−1) , X (t−1) )) 2: set k = L (t−1) 3:</formula><p>init (L (t) , X (t) ) = (k, X (t−1) )</p><p>Step I: Local jump 4:</p><p>generate j ∼ Γ(k, ·) (Eq. <ref type="formula" target="#formula_1">(20)</ref>) 5:</p><p>if j = k + 1 then 6:</p><p>generate C ∼ Q k+1 (c) 7:</p><p>generate Y ∼ ˘ g k+1 (y|X (t−1) , C) (Eq.31) 8:</p><p>set L (t) = j and X (t) = {X (t−1) , Y } with probability (Eq.21) and (Eq.32) 9:</p><p>end if 10:</p><p>if j = k − 1 then 11:</p><p>set L (t) = j and X (t) = X (t−1)</p><p>1:k−1 with probabil- ity Eq. <ref type="formula" target="#formula_1">(22)</ref> and (Eq.32) 12:</p><p>end if</p><p>Step II: Markov move 13:</p><formula xml:id="formula_42">for i = 1 → L (t) do 14: draw C ∼ Qi(c) 15: set c (t) i = C with probability (Eq.30) 16: draw W ∈ V c (t) i 17: set X (t) i = W 18:</formula><p>end for 19:</p><p>return (L (t) , X (t) ) 20: end function belonging to class c (t) i . The computational cost is reduced from |V| to |C| + |V|/|C| on average.</p><p>The idea of using class information to accel- erate training has been proposed in various con- texts of language modeling, such as maximum entropy models <ref type="bibr" target="#b8">(Goodman, 2001b)</ref> and RNN LMs <ref type="bibr" target="#b15">(Mikolov et al., 2011</ref>). However, the realization of this idea is different for training our models.</p><p>The pseudo-code of the new sampling method is shown in Algorithm 3. Denote by V c the subset of V containing all the words belonging to class c. In the Markov move step (Step 13 to 18 in Algorithm 3), at each position i, we first generate a class C from a proposal distribution Q i (c) and then accept C as the new c (t) i with probability</p><formula xml:id="formula_43">min 1, Qi(c (t) i ) Qi(C) pi(C) pi(c (t) i )<label>(30)</label></formula><p>where</p><formula xml:id="formula_44">p i (c) = w∈Vc p(L (t) , {X (t) 1:i−1 , w, X (t) i+1:L (t) }; λ, ζ).</formula><p>The probabilities Q i (c) and p i (c) depend on {X (t)</p><formula xml:id="formula_45">1:i−1 , X (t)</formula><p>i+1:L (t) }, but this is suppressed in the notation. Then we normalize the probabilities of words belonging to class c i . Similarly, in the local jump step with k = L (t−1) , if the proposal j = k + 1 (Step 5 to 9 in Algorithm 3), we first generate C ∼ Q k+1 (c) and then generate Y from class C by</p><formula xml:id="formula_46">˘ g k+1 (y|x k , C) = p(k + 1, {x k , y}; λ, ζ) w∈V C p(k + 1, {x k , w}; λ, ζ)<label>(31)</label></formula><p>with x k = X (t−1) . Then we set L (t) = j and X (t) = {X (t−1) , Y } with probability as defined in <ref type="formula" target="#formula_1">(21)</ref>, by setting</p><formula xml:id="formula_47">g k+1 (y|x k ) = Q k+1 (C)˘ g k+1 (y|x k , C).<label>(32)</label></formula><p>If the proposal j = k − 1, similarly we use acceptance probability <ref type="formula" target="#formula_1">(22)</ref> with <ref type="formula" target="#formula_1">(32)</ref>. In our application, we construct Q i (c) dynami- cally as follows. Write x l for {X (t−1) , Y } in Step 8 or for X (t) in Step 11 of Algorithm 3. First, we construct a reduced model p c l (x l ), by including only the features that depend on x l i through its class and retaining the corresponding parameters in p l (x l ; λ, ζ). Then we define the distribution</p><formula xml:id="formula_48">Q i (c) = p c l ({x l 1:i−1 , c, x l i+1:l }),</formula><p>which can be directly calculated without knowing the value of x l i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parallelization of sampling</head><p>The sampling operation can be easily parallelized in SA Algorithm 1. At each time t, both the parameters λ and log normalization constants ζ are fixed at λ (t−1) and ζ (t−1) . Instead of simu- lating one Markov Chain, we simulate J Markov Chains on J CPU cores separately. As a result, to generate a sample set B (t) of size K, only K/J sampling steps need to be performed on each CPU core. By parallelization, the sampling operation is completed J times faster than before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">PTB perplexity results</head><p>In this section, we evaluate the performance of LMs by perplexity (PPL). We use the Wall Street Journal (WSJ) portion of Penn Treebank (PTB). Sections 0-20 are used as the training data (about 930K words), sections 21-22 as the development data (74K) and section 23-24 as the test data (82K). The vocabulary is limited to 10K words, with one special token U N K denoting words not in the vocabulary. This setting is the same as that used in other studies <ref type="bibr" target="#b15">(Mikolov et al., 2011</ref>). The baseline is a 4-gram LM with modified Kneser-Ney smoothing (Chen and Goodman, Word classing has been shown to be useful in conditional ME models <ref type="bibr" target="#b6">(Chen, 2009)</ref>. For our TDRF models, we consider a variety of features as shown in <ref type="table" target="#tab_1">Table 1</ref>, mainly based on word and class information. Each word is deterministically assigned to a single class, by running the automat- ic clustering algorithm proposed in <ref type="bibr" target="#b14">(Martin et al., 1998</ref>) on the training data.</p><formula xml:id="formula_49">Type Features w (w−3w−2w−1w0)(w−2w−1w0)(w−1w0)(w0) c (c−3c−2c−1c0)(c−2c−1c0)(c−1c0)(c0) ws (w−3w0)(w−3w−2w0)(w−3w−1w0)(w−2w0) cs (c−3c0)(c−3c−2c0)(c−3c−1c0)(c−2c0) wsh (w−4w0) (w−5w0) csh (c−4c0) (c−5c0) cpw (c−3c−2c−1w0) (c−2c−1w0)(c−1w0)</formula><p>In <ref type="table" target="#tab_1">Table 1</ref>, w i , c i , i = 0, −1, . . . , −5 denote the word and its class at different position offset i, e.g. w 0 , c 0 denotes the current word and its class. We first introduce the classic word/class n-gram features (denoted by "w"/"c") and the word/class skipping n-gram features (denoted by "ws"/"cs") <ref type="bibr" target="#b7">(Goodman, 2001a)</ref>. Second, to demonstrate that long-span features can be naturally integrated in TDRFs, we introduce higher-order features "w- sh"/"csh", by considering two words/classes sep- arated with longer distance. Third, as an example of supporting heterogenous features that combine different information, the crossing features "cp- w" (meaning class-predict-word) are introduced. Note that for all the feature types in <ref type="table" target="#tab_1">Table 1</ref>, only the features observed in the training data are used.</p><p>The joint SA (Algorithm 1) is used to train the TDRF models, with all the acceleration methods described in Section 4 applied. The minibatch size K = 300. The learning rates γ λ and γ ζ are configured as <ref type="bibr">(29)</ref> and <ref type="formula" target="#formula_11">(19)</ref> respectively with β λ = β ζ = 0.6 and t c = 3000. For t 0 , it is first initialized to be 10 4 . During iterations, we monitor the smoothed log-likelihood (moving average of 1000 iterations) on the PTB development data. models PPL (± std. dev.) KN4</p><p>142.72 RNN 128.81 TDRF w+c 130.69±1.64 <ref type="table">Table 2</ref>: The PPLs on the PTB test data. The class number is 200.</p><p>We set t 0 to the current iteration number once the rising percentage of the smoothed log-likelihoods within 100 iterations is below 20%, and then continue 5000 further iterations before stopping. The configuration of hessian estimation (Section 4.1) is γ H = γ λ and h = 10 −4 . L 2 regularization with constant 10 −5 is used to avoid over-fitting. 8 CPU cores are used to parallelize the algorithm, as described in Section 4.3, and the training of each TDRF model takes less than 20 hours. The perplexity results on the PTB test data are given in <ref type="table">Table 2</ref>. As the normalization constants of TDRF models are estimated stochastically, we report the Monte Carlo mean and standard devi- ation from the last 1000 iterations for each PPL. The TDRF model using the basic "w+c" features performs close to the RNNLM in perplexity. To be compact, results with more features are presented in the following WSJ experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">WSJ speech recognition results</head><p>In this section, we continue to use the LMs ob- tained above (using PTB training and develop- ment data), and evaluate their performance mea- sured by WERs in speech recognition, by re- scoring 1000-best lists from WSJ'92 test data (330 sentences). The oracle WER of the 1000-best lists is 3.4%, which are generated from using the Kaldi toolkit 7 with a DNN-based acoustic model. TDRF LMs using a variety of features and different number of classes are tested. The results are shown in <ref type="table">Table 3</ref>. Different types of features, like the skipping features, the higher-order fea- tures and the crossing features can all be easily supported in TDRF LMs, and the performance is improved to varying degrees. Particularly, the TDRF using the "w+c+ws+cs+cpw" features with class number 200 performs comparable to the RNNLM in both perplexity and WER. Numerical- ly, the relative reduction is 9.1% compared with the KN4 LMs, and 0.5% compared with the RNN LM.  <ref type="table">Table 3</ref>: The WERs and PPLs on the WSJ'92 test data. "#feat" denotes the feature number. Differ- ent TDRF models with class number 100/200/500 are reported (denoted by "100c"/"200c"/"500c")</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison and discussion</head><p>TDRF vs WSME. For comparison, <ref type="table">Table 3</ref> also presents the results from our implementation of the WSME model (3), using the same features as in <ref type="table" target="#tab_1">Table 1</ref>. This WSME model is the same as in <ref type="bibr" target="#b23">(Rosenfeld, 1997)</ref>, but different from ( <ref type="bibr" target="#b22">Rosenfeld et al., 2001</ref>), which uses the traditional n-gram LM as the priori distribution p 0 . For the WSME model (3), we can still use a SA training algorithm, similar to that developed in Section 3.2, to estimate the parameters λ. But in this case, there is no need to introduce ζ l , because the normalizing constants Z l (λ) are canceled out as seen from <ref type="formula" target="#formula_8">(7)</ref>. Specifically, the learning rate γ λ and the L 2 regularization are configured the same as in TDRF training. A fixed number of iterations with t 0 = 5000 is performed. The total iteration number is 10000, which is similar to the iteration number used in TDRF training.</p><p>In order to calculate perplexity, we need to estimate the global normalizing constant Z(λ) = m l=1 Z l (λ) for the WSME model. Similarly as in <ref type="bibr" target="#b27">(Tan, 2015)</ref>, we apply the SA algorithm in Section 3.2 to estimate the log normalizing constants ζ, while fixing the parameters λ to be those already estimated from the WSME model and using uniform probabilities π l ≡ m −1 .</p><p>The resulting PPLs of these WSME models are extremely poor. The average test log-likelihoods per sentence for these two WSME models are −494 and −509 respectively. However, the W- ERs from using the trained WSME models in hypothesis re-ranking are not as poor as would be expected from their PPLs. This appears to indicate that the estimated WSME parameters are not so bad for relative ranking. Moreover, when the estimated λ and ζ are substituted into our TDRF model (6) with the empirical length probabilities π l , the "corrected" average test log-likelihoods per sentence for these two sets of parameters are improved to be −152 and −119 respectively. The average test log-likelihoods are both −96 for the two corresponding TDRF models in <ref type="table">Table 3</ref>. This is some evidence for the model deficiency of the WSME distribution as defined in <ref type="formula" target="#formula_2">(3)</ref>, and intro- ducing the empirical length probabilities gives a more reasonable model assumption.</p><p>TDRF vs conditional ME. After training, TDRF models are computationally more efficient in com- puting sentence probability, simply summing up weights for the activated features in the sentence. The conditional ME models <ref type="bibr" target="#b11">(Khudanpur and Wu, 2000;</ref><ref type="bibr" target="#b20">Roark et al., 2004</ref>) suffer from the expen- sive computation of local normalization factors. This computational bottleneck hinders their use in practice <ref type="bibr" target="#b8">(Goodman, 2001b;</ref><ref type="bibr" target="#b22">Rosenfeld et al., 2001</ref>). Partly for this reason, although building conditional ME models with sophisticated features as in <ref type="table" target="#tab_1">Table 1</ref> is theoretically possible, such work has not been pursued so far.</p><p>TDRF vs RNN. The RNN models suffer from the expensive softmax computation in the output layer <ref type="bibr">8</ref> . Empirically in our experiments, the aver- age time costs for re-ranking of the 1000-best list for a sentence are 0.16 sec vs 40 sec, based on TDRF and RNN respectively (no GPU used).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>While there has been extensive research on con- ditional LMs, there has been little work on the whole-sentence LMs, mainly in ( <ref type="bibr" target="#b22">Rosenfeld et al., 2001;</ref><ref type="bibr" target="#b0">Amaya and Benedí, 2001;</ref><ref type="bibr" target="#b24">Ruokolainen et al., 2010)</ref>. Although the whole-sentence approach has potential benefits, the empirical results of pre- vious WSME models are not satisfactory, almost the same as traditional n-gram models. After incorporating lexical and syntactic information, a mere relative improvement of 1% and 0.4% respectively in perplexity and in WER is reported for the resulting WSEM ( <ref type="bibr" target="#b22">Rosenfeld et al., 2001</ref>). Subsequent studies of using WSEMs with gram- matical features, as in <ref type="bibr" target="#b0">(Amaya and Benedí, 2001)</ref> and <ref type="bibr" target="#b24">(Ruokolainen et al., 2010)</ref>, report perplexity improvement above 10% but no WER improve- ment when using WSEMs alone.</p><p>Most RF modeling has been restricted to fixed- dimensional spaces <ref type="bibr">9</ref> . Despite recent progress, fitting RFs of moderate or large dimensions re- mains to be challenging ( <ref type="bibr" target="#b12">Koller and Friedman, 2009;</ref><ref type="bibr">Mizrahi et al., 2013</ref>). In particular, the work of ( <ref type="bibr" target="#b19">Pietra et al., 1997</ref>) is inspiring to us, but the improved iterative scaling (IIS) method for parameter estimation and the Gibbs sampler are not suitable for even moderately sized models. Our TDRF model, together with the joint SA al- gorithm and trans-dimensional mixture sampling, are brand new and lead to encouraging results for language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In summary, we have made the following contri- butions, which enable us to successfully train T- DRF models and obtain encouraging performance improvement.</p><p>• The new TDRF model and the joint SA train- ing algorithm, which simultaneously updates the model parameters and normalizing con- stants while using trans-dimensional mixture sampling.</p><p>• Several additional innovations including ac- celerating SA iterations by using Hessian information, introducing word classing to ac- celerate the sampling operation and improve the smoothing behavior of the models, and parallelization of sampling. In this work, we mainly explore the use of fea- tures based on word and class information. Future work with other knowledge sources and larger- scale experiments is needed to fully exploit the advantage of TDRFs to integrate richer features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgments</head><p>This work is supported by Toshiba Corporation, National Natural Science Foundation of China (NSFC) via grant 61473168, and Tsinghua Ini- tiative. We thank the anonymous reviewers for helpful comments on this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of convergence curves on training set after introducing hessian and training set mini-batching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>i</head><label></label><figDesc>and draw a word as the new X (t) i from the class c (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Feature definition in TDRF LMs 

1999), denoted by KN4. We use the RNNLM 
toolkit 5 to train a RNNLM (Mikolov et al., 2011). 
The number of hidden units is 250 and other 
configurations are set by default 6 . 
</table></figure>

			<note place="foot" n="2"> http://oa.ee.tsinghua.edu.cn/ ˜ ouzhijian/software.htm 3 We add sup or subscript l, e.g. in x l , p l (), to make clear that the variables and distributions depend on length l.</note>

			<note place="foot" n="4"> The length feature corresponding to length l is a binary feature that takes one if the sentence x is of length l, and otherwise takes zero.</note>

			<note place="foot" n="5"> http://rnnlm.org/ 6 Minibatch size=10, learning rate=0.1, BPTT steps=5. 17 sweeps are performed before stopping, which takes about 25 hours. No word classing is used, since classing in RNNLMs reduces computation but at cost of accuracy. RNNLMs were experimented with varying numbers of hidden units (100500). The best result from using 250 hidden units is reported.</note>

			<note place="foot" n="8"> This deficiency could be partly alleviated with some speed-up methods, e.g. using word clustering (Mikolov, 2012) or noise contrastive estimation (Mnih and Kavukcuoglu, 2013).</note>

			<note place="foot" n="9"> Using local fixed-dimensional RFs in sequential models was once explored, e.g. temporal restricted Boltzmann machine (TRBM) (Sutskever and Hinton, 2007).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improvement of a whole sentence maximum entropy language model using grammatical features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredy</forename><surname>Amaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José Miguel</forename><surname>Benedí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Adaptive algorithms and stochastic approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Benveniste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Métivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Priouret</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The tradeoffs of large scale learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A stochastic quasi-newton method for large-scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.7020</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="359" to="394" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Stochastic approximation and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanfu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shrinking exponential language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A bit of progress in language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="403" to="434" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Classes for fast maximum entropy training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>of International Conference on Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reversible jump markov chain monte carlo computation and bayesian model determination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="711" to="732" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation for spatial models by markov chain monte carlo stochastic approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><forename type="middle">Gao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Tu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="339" to="355" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Maximum entropy techniques for exploiting syntactic, semantic and collocational dependencies in language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="355" to="372" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic approximation in monte carlo computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanhai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">477</biblScope>
			<biblScope unit="page" from="305" to="320" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Algorithms for bigram and trigram word clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Liermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="19" to="37" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">H</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>of International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Statistical language models based on neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yariv</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mizrahi</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1308.6342</idno>
		<title level="m">Misha Denil, and Nando de Freitas. 2013. Linear and parallel learning of markov random fields</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inducing features of random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">Della</forename><surname>Stephen Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="380" to="393" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminative language modeling with conditional random fields and the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Saraclar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd</title>
		<meeting>the 42nd</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Annual Meeting on Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Whole-sentence exponential language models: a vehicle for linguistic-statistical integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="55" to="73" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A whole sentence maximum entropy language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Automatic Speech Recognition and Understanding (ASRU)</title>
		<meeting>of Automatic Speech Recognition and Understanding (ASRU)</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Using dependency grammar features in whole sentence maximum entropy language model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanel</forename><surname>Teemu Ruokolainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Alumäe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dobrinkat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Baltic HLT</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Continuous space language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="492" to="518" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning multilevel distributed representations for highdimensional sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>AISTATS</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Optimally adjusted mixture sampling and locally weighted histogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Department of Statistics, Rutgers University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Younes</surname></persName>
		</author>
		<title level="m">Parametric inference for imperfectly observed gibbsian fields. Probability theory and related fields</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="625" to="645" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
