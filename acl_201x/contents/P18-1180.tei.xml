<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Distributional and Orthographic Aggregation Model for English Derivational Morphology</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 1938</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Deutsch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hewitt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Distributional and Orthographic Aggregation Model for English Derivational Morphology</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1938" to="1947"/>
							<date type="published">July 15-20, 2018. 2018. 1938</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Modeling derivational morphology to generate words with particular semantics is useful in many text generation tasks, such as machine translation or abstractive question answering. In this work, we tackle the task of derived word generation. That is, given the word &quot;run,&quot; we attempt to generate the word &quot;runner&quot; for &quot;someone who runs.&quot; We identify two key problems in generating derived words from root words and transformations: suffix ambiguity and orthographic irregularity. We contribute a novel aggregation model of derived word generation that learns derivational transformations both as orthographic functions using sequence-to-sequence models and as functions in distributional word embedding space. Our best open-vocabulary model, which can generate novel words, and our best closed-vocabulary model, show 22% and 37% relative error reductions over current state-of-the-art systems on the same dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The explicit modeling of morphology has been shown to improve a number of tasks (Seeker and C ¸ etinoglu, 2015; <ref type="bibr" target="#b14">Luong et al., 2013)</ref>. In a large number of the world's languages, many words are composed through morphological operations on subword units. Some languages are rich in inflec- tional morphology, characterized by syntactic trans- formations like pluralization. Similarly, languages like English are rich in derivational morphology, where the semantics of words are composed from * These authors contributed equally; listed alphabetically. smaller parts. The AGENT derivational transforma- tion, for example, answers the question, what is the word for 'someone who runs'? with the answer, a runner. 1 Here, AGENT is spelled out as suffixing -ner onto the root verb run.</p><p>We tackle the task of derived word generation. In this task, a root word x and a derivational trans- formation t are given to the learner. The learner's job is to produce the result of the transformation on the root word, called the derived word y. <ref type="table">Table  1</ref> gives examples of these transformations.</p><p>Previous approaches to derived word genera- tion model the task as a character-level sequence- to-sequence (seq2seq) problem ( <ref type="bibr" target="#b3">Cotterell et al., 2017b</ref>). The letters from the root word and some encoding of the transformation are given as input to a neural encoder, and the decoder is trained to pro- duce the derived word, one letter at a time. We iden- tify the following problems with these approaches:</p><p>First, because these models are unconstrained, they can generate sequences of characters that do  <ref type="table">Table 1</ref>: The goal of derived word generation is to produce the derived word, y, given both the root word, x, and the transformation t, as demonstrated here with examples from the dataset.</p><p>not form actual words. We argue that requiring the model to generate a known word is a reasonable constraint in the special case of English derivational morphology, and doing so avoids a large number of common errors. Second, sequence-based models can only gen- eralize string manipulations (such as "add -ment") if they appear frequently in the training data. Be- cause of this, they are unable to generate derived words that do not follow typical patterns, such as generating truth as the nominative derivation of true. We propose to learn a function for each trans- formation in a low dimensional vector space that corresponds to mapping from representations of the root word to the derived word. This eliminates the reliance on orthographic information, unlike re- lated approaches to distributional semantics, which operate at the suffix level ( <ref type="bibr" target="#b8">Gupta et al., 2017)</ref>.</p><p>We contribute an aggregation model of derived word generation that produces hypotheses inde- pendently from two separate learned models: one from a seq2seq model with only orthographic in- formation, and one from a feed-forward network using only distributional semantic information in the form of pretrained word vectors. The model learns to choose between the hypotheses accord- ing to the relative confidence of each. This system can be interpreted as learning to decide between positing an orthographically regular form or a se- mantically salient word. See <ref type="figure" target="#fig_0">Figure 1</ref> for a diagram of our model.</p><p>We show that this model helps with two open problems with current state-of-the-art seq2seq de- rived word generation systems, suffix ambiguity and orthographic irregularity (Section 2). We also improve the accuracy of seq2seq-only derived word systems by adding external information through constrained decoding and hypothesis rescoring. These methods provide orthogonal gains to our main contribution.</p><p>We evaluate models in two categories: open vo- cabulary models that can generate novel words unattested in a preset vocabulary, and closed- vocabulary models, which cannot. Our best open- vocabulary and closed-vocabulary models demon- strate 22% and 37% relative error reductions over the current state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Derivational Morphology</head><p>Derivational transformations generate novel words that are semantically composed from the root word and the transformation. We identify two unsolved problems in derived word transformation, each of which we address in Sections 3 and 4.</p><p>First, many plausible choices of suffix for a sin- gle pair of root word and transformation. For ex- ample, for the verb ground, the RESULT transfor- mation could plausibly take as many forms as 2</p><formula xml:id="formula_0">(ground, RESULT) → grounding (ground, RESULT) → *groundation (ground, RESULT) → *groundment (ground, RESULT) → *groundal</formula><p>However, only one is correct, even though each suffix appears often in the RESULT transformation of other words. We will refer to this problem as "suffix ambiguity."</p><p>Second, many derived words seem to lack a gen- eralizable orthographic relationship to their root words. For example, the RESULT of the verb speak is speech. It is unlikely, given an orthographically similar verb creak, that the RESULT be creech in- stead of, say, creaking. Seq2seq models must grap- ple with the problem of derived words that are the result of unlikely or potentially unseen string transformations. We refer to this problem as "or- thographic irregularity."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sequence Models and Corpus Knowledge</head><p>In this section, we introduce the prior state-of-the- art model, which serves as our baseline system. Then we build on top of this system by incorpo- rating a dictionary constraint and rescoring the model's hypotheses with token frequency informa- tion to address the suffix ambiguity problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline Architecture</head><p>We begin by formalizing the problem and defin- ing some notation. For source word x = x 1 , x 2 , . . . x m , a derivational transformation t, and target word y = y 1 , y 2 , . . . y n , our goal is to learn some function from the pair (x, t) to y. Here, x i and y j are the ith and jth characters of the input strings x and y. We will sometimes use x 1:i to denote x 1 , x 2 , . . . x i , and similarly for y 1:j .</p><p>The current state-of-the-art model for derived- form generation approaches this problem by learn- ing a character-level encoder-decoder neural net- work with an attention mechanism ( <ref type="bibr" target="#b3">Cotterell et al., 2017b;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2014)</ref>.</p><p>The input to the bidirectional LSTM en- coder <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b7">Graves and Schmidhuber, 2005</ref>) is the sequence #, x 1 , x 2 , . . . x m , #, t, where # is a special symbol to denote the start and end of a word, and the encoding of the derivational transformation t is concatenated to the input characters. The model is trained to minimize the cross entropy of the training data. We refer to our reimplementation of this model as SEQ.</p><p>For a more detailed treatment of neural sequence- to-sequence models with attention, we direct the reader to <ref type="bibr" target="#b13">Luong et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dictionary Constraint</head><p>The suffix ambiguity problem poses challenges for models which rely exclusively on input charac- ters for information. As previously demonstrated, words derived via the same transformation may take different suffixes, and it is hard to select among them based on character information alone. Here, we describe a process for restricting our inference procedure to only generate known English words, which we call a dictionary constraint. We believe that for English morphology, a large enough cor- pus will contain the vast majority of derived forms, so while this approach is somewhat restricting, it removes a significant amount of ambiguity from the problem.</p><p>To describe how we implemented this dictionary constraint, it is useful first to discuss how decoding in a seq2seq model is equivalent to solving a short- est path problem. The notation is specific to our model, but the argument is applicable to seq2seq models in general.</p><p>The goal of decoding is to find the most probable structurê y conditioned on some observation x and transformation t. That is, the problem is to solvê</p><formula xml:id="formula_1">solvê y = arg max y∈Y p(y | x, t) (1) = arg min y∈Y − log p(y | x, t) (2)</formula><p>where Y is the set of valid structures. Sequential models have a natural ordering y = y 1 , y 2 , . . . y n over which − log p(y | x, t) can be decomposed</p><formula xml:id="formula_2">− log p(y | x, t) = n t=1 − log p(y t | y 1:t−1 , x, t)</formula><p>(3) Solving Equation 2 can be viewed as solving a shortest path problem from a special starting state to a special ending state via some path which uniquely represents y. Each vertex in the graph represents some sequence y 1:i , and the weight of the edge from y 1:i to y 1:i+1 is given by</p><formula xml:id="formula_3">− log p(y i+1 | y 1:i−1 , x, t)<label>(4)</label></formula><p>The weight of the path from the start state to the end state via the unique path that describes y is exactly equal to Equation 3. When the vocabulary size is too large, the exact shortest path is intractable, and approximate search methods, such as beam search, are used instead. In derived word generation, Y is an infinite set of strings. Since Y is unrestricted, almost all of the strings in Y are not valid words. Given a dic- tionary Y D , the search space is restricted to only those words in the dictionary by searching over the trie induced from Y D , which is a subgraph of the unrestricted graph. By limiting the search space to Y D , the decoder is guaranteed to generate some known word. Models which use this dictionary- constrained inference procedure will be labeled with +DICT. Algorithm 1 has the pseudocode for our decoding procedure.</p><p>We discuss specific details of the search pro- cedure and interesting observations of the search space in Section 6. Section 5.2 describes how we obtained the dictionary of valid words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word Frequency Knowledge through Rescoring</head><p>We also consider the inclusion of explicit word frequency information to help solve suffix ambi- guity, using the intuition that "real" derived words are likely to be frequently attested. This permits a high-recall, potentially noisy dictionary. We are motivated by very high top-10 accu- racy compared to top-1 accuracy, even among dictionary-constrained models. By rescoring the hypotheses of a model using word frequency (a word-global signal) as a feature, attempt to recover a portion of this top-10 accuracy.</p><p>When a model has been trained, we query it for its top-10 most likely hypotheses. The union of all hypotheses for a subset of the training observations forms the training set for a classifier that learns to predict whether a hypothesis generated by the model is correct. Each hypothesis is labelled with its correctness, a value in {±1}. We train a sim- ple combination of two scores: the seq2seq model score for the hypothesis, and the log of the word frequency of the hypothesis.</p><p>To permit a nonlinear combination of word fre- quency and model score, we train a small multi- layer perceptron with the model score and the fre- quency of a derived word hypothesis as features.</p><p>At testing time, the 10 hypotheses generated by a single seq2seq model for a single observation are rescored. The new model top-1 hypothesis, then, is the argmax over the 10 hypotheses according to the rescorer. In this way, we are able to incorporate word-global information, e.g. word frequency, that is ill-suited for incorporation at each character pre- diction step of the seq2seq model. We label models that are rescored in this way +FREQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Distributional Models</head><p>So far, we have presented models that learn deriva- tional transformations as orthographic operations. Such models struggle by construction with the or- thographic irregularity problem, as they are trained to generalize orthographic information. However, the semantic relationships between root words and derived words are the same even when the orthog- raphy is dissimilar. It is salient, for example, that irregular word speech is related to its root speak in about the same way as how exploration is related to the word explore.</p><p>We model distributional transformations as func- tions in dense distributional word embedding spaces, crucially learning a function per deriva- tional transformation, not per suffix pair. In this way, we aim to explicitly model the semantic trans- formation, not the othographic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Feed-forward derivational transformations</head><p>For all source words x and all target words y, we look up static distributional embeddings v x , v y ∈ R d . For each derivational transformation t, we learn a function f t :</p><formula xml:id="formula_4">R d → R d that maps v x to v y .</formula><p>f t is parametrized as two-layer perceptron, trained using a squared loss,</p><formula xml:id="formula_5">L = b T b (5) b = f t (v x ) − v y<label>(6)</label></formula><p>We perform inference by nearest neighbor search in the embedding space. This inference strategy requires a subset of strings for our embedding dic- tionary, Y V . Upon receiving (x, t) at test time, we compute f t (v x ) and find the most similar embeddings in Y V . Specifically, we find the top-k most similar embed- dings, and take the most similar derived word that starts with the same 4 letters as the root word, and is not identical to it. This heuristic filters out highly implausible hypotheses.</p><p>We use the single-word subset of the Google News vectors ( <ref type="bibr" target="#b17">Mikolov et al., 2013)</ref> as Y V , so the size of the vocabulary is 929k words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SEQ and DIST Aggregation</head><p>The seq2seq and distributional models we have pre- sented learn with disjoint information to solve sepa- rate problems. We leverage this intuition to build a model that chooses, for each observation, whether to generate according to orthographic information via the SEQ model, or produce a potentially irregu- lar form via the DIST model. To train this model, we use a held-out portion of the training set, and filter it to only observations for which exactly one of the two models produces the correct derived form. Finally, we make the strong assumption that the probability of a derived form being generated correctly according to 1 model as opposed to the other is dependent only on the unnormalized model score from each. We model this as a logistic regression (t is omitted for clarity):</p><formula xml:id="formula_6">P (·|y D , y S , x) = softmax(W e [DIST(y D |x); SEQ(y S |x)] + b e )</formula><p>where W e and b e are learned parameters, y D and y S are the hypotheses of the distributional and seq2seq models, and DIST(·) and SEQ(·) are the models' likelihood functions. We denote this ag- gregate AGGR in our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Datasets</head><p>In this section we describe the derivational mor- phology dataset used in our experiments and how we collected the dictionary and token frequencies used in the dictionary constraint and rescorer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Derivational Morphology</head><p>In our experiments, we use the derived word gen- eration derivational morphology dataset released in <ref type="bibr" target="#b3">Cotterell et al. (2017b)</ref>. The dataset, derived from NomBank ( <ref type="bibr" target="#b15">Meyers et al., 2004</ref>) , consists of 4,222 training, 905 validation, and 905 test triples of the form (x, t, y). The transformations are from the following categories: ADVERB (ADJ → ADV), RESULT (V → N), AGENT (V → N), and NOMI- NAL (ADJ → N). Examples from the dataset can be found in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dictionary and Token Frequency Statistics</head><p>The dictionary and token frequency statistics used in the dictionary constraint and frequency rerank- ing come from the Google Books NGram corpus <ref type="bibr" target="#b16">(Michel et al., 2011</ref>). The unigram frequency counts were aggregated across years, and any to- kens which appear fewer than approximately 2,000 times, do not end in a known possible suffix, or contain a character outside of our vocabulary were removed. The frequency threshold was determined using development data, optimizing for high recall. We collect a set of known suffixes from the training data by removing the longest common prefix be- tween the source and target words from the target word. The result is a dictionary with frequency information for around 360k words, which covers 98% of the target words in the training data. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Inference Procedure Discussion</head><p>In many sequence models where the vocabulary size is large, exact inference by finding the true shortest path in the graph discussed in Section 3.2 is intractable. As a result, approximate inference techniques such as beam search are often used, or the size of the search space is reduced, for exam- ple, by using a Markov assumption. We, however, observed that exact inference via a shortest path algorithm is not only tractable in our model, but  only slightly more expensive than greedy search and significantly less expensive than beam search.</p><p>To quantify this claim, we measured the ac- curacy and number of states explored by greedy search, beam search, and shortest path with and without a dictionary constraint on the development data. <ref type="table" target="#tab_2">Table 2</ref> shows the results averaged over 30 runs. As expected, beam search and shortest path have higher accuracies than greedy search and ex- plore more of the search space. Surprisingly, beam search and shortest path have nearly identical ac- curacies, but shortest path explores significantly fewer hypotheses.</p><p>At least two factors contribute to the tractability of exact search in our model. First, our character- level sequence model has a vocabulary size of 63, which is significantly smaller than token-level mod- els, in which a vocabulary of 50k words is not un- common. The search space of sequence models is dependent upon the size of the vocabulary, so the model's search space is dramatically smaller than for a token-level model. Second, the inherent structure of the task makes it easy to eliminate large subgraphs of the search space. The first several characters of the input word and output word are almost always the same, so the model assigns very low probability to any se- quence with different starting characters than the input. Then, the rest of the search procedure is dedicated to deciding between suffixes. Any suffix which does not appear frequently in the training data receives a low score, leaving the search to de- cide between a handful of possible options. The result is that the learned probability distribution is very spiked; it puts very high probability on just a few output sequences. It is empirically true that the top few most probable sequences have signif- icantly higher scores than the next most probable sequences, which supports this hypothesis.</p><p>In our subsequent experiments, we decode using Algorithm 1 The decoding procedure uses a shortest-path algorithm to find the most probable output sequence. The dictionary constraint is (op- tionally) implemented on line 9 by only considering prefixes that are contained in some trie T .</p><p>1: procedure DECODE(x, t, V , T )</p><p>2:</p><p>H ← Heap() 3:</p><formula xml:id="formula_7">H.insert(0, #) 4:</formula><p>while H is not empty do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>y ← H.remove()</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>if y is a complete word then return y</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>for y ∈ V do 8:</p><formula xml:id="formula_8">y ← y + y 9:</formula><p>if y ∈ T then 10:</p><p>s ← FORWARD(x, t, y )</p><p>11:</p><p>H.insert(s, y ) exact inference by running a shortest path algo- rithm (see Algorithm 1). For reranking models, instead of typically using a beam of size k, we use the top k most probable sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>In all of our experiments, we use the training, de- velopment, and testing splits provided by <ref type="bibr" target="#b3">Cotterell et al. (2017b)</ref> and average over 30 random restarts. <ref type="table" target="#tab_4">Table 3</ref> displays the accuracies and average edit distances on the test set of each of the systems pre- sented in this work and the state-of-the-art model from <ref type="bibr" target="#b3">Cotterell et al. (2017b)</ref>. First, we observed that SEQ outperforms the re- sults reported in <ref type="bibr" target="#b3">Cotterell et al. (2017b)</ref> by a large margin, despite the fact that the model architectures are the same. We attribute this difference to better hyperparameter settings and improved learning rate annealing.</p><p>Then, it is clear that the accuracy of the distri- butional model, DIST, is significantly lower than any seq2seq model. We believe the orthography- informed models perform better because most ob- servations in the dataset are orthographically regu- lar, providing low-hanging fruit.  depth analysis of the strengths of SEQ and DIST in Section 7.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open-vocabulary models</head><p>Closed-vocabulary models We now consider closed-vocabulary models that improve upon the seq2seq model in AGGR. First, we see that re- stricting the decoder to only generate known words is extremely useful, with SEQ+DICT improving over SEQ by 6.2 points. Qualitatively, we note that this constraint helps solve the suffix ambiguity problem, since orthographically plausible incorrect hypotheses are pruned as non-words. See <ref type="table">Table 6</ref> for examples of this phenomenon. Additionally, we observe that the dictionary-constrained model outperforms the unconstrained model according to top-10 accuracy (see <ref type="table">Table 5</ref>). Rescoring (+FREQ) provides further improve- ment of 0.8 points, showing that the decoding dic- tionary constraint provides a higher-quality beam that still has room for top-1 improvement. All to- gether, AGGR+FREQ+DICT provides a 4.4 point improvement over the best open-vocabulary model, AGGR. This shows the disambiguating power of assuming a closed vocabulary.</p><p>Edit Distance One interesting side effect of the dictionary constraint appears when comparing AGGR+FREQ with and without the dictionary con- straint. Although the accuracy of the dictionary- constrained model is better, the average edit dis- tance is worse. The unconstrained model is free to put invalid words which are orthographically similar to the target word in its top-k, however the constrained model can only choose valid words. This means it is easier for the unconstrained model to generate words which have a low edit distance to the ground truth, whereas the constrained model <ref type="bibr">Cotterell</ref>   <ref type="table">Table 4</ref>: The accuracies and edit distances of our best open-vocabulary and closed-vocabulary models, AGGR and AGGR+FREQ+DICT compared to prior work, evaluated at the transformation level. For edit distance, lower is better.</p><p>can only do that if such a word exists. The result is a more accurate, yet more orthographically diverse, set of hypotheses.</p><p>Results by Transformation Next, we compare our best open vocabulary and closed vocabulary models to previous work across each derivational transformation. These results are in <ref type="table">Table 4</ref>. The largest improvement over the baseline sys- tem is for NOMINAL transformations, in which the AGGR has a 49% reduction in error. We attribute most of this gain to the difficulty of this particular transformation. NOMINAL is challenging because there are several plausible endings (e.g. -ity, -ness, -ence) which occur at roughly the same rate. Addi- tionally, NOMINAL examples are the least frequent transformation in the dataset, so it is challenging for a sequential model to learn to generalize. The distributional model, which does not rely on suffix information, does not have this same weakness, so the aggregation AGGR model has better results.</p><p>The performance of AGGR+FREQ+DICT is worse than AGGR, however. This is surprising because, in all other transformations, adding dic- tionary information improves the accuracies. We believe this is due to the ambiguity of the ground truth: Many root words have seemingly multi- ple plausible nominal transformations, such as rigid → {rigidness, rigidity} and equivalent → {equivalence, equivalency}. The dictionary con- straint produces a better set of hypotheses to rescore, as demonstrated in <ref type="table">Table 5</ref>. Therefore, the dictionary-constrained model is likely to have more of these ambiguous cases, which makes the task more difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Strengths of SEQ and DIST</head><p>In this subsection we explore why AGGR improves consistently over SEQ even though it maintains an open vocabulary. We have argued that DIST is able to correctly produce derived words that are <ref type="bibr" target="#b3">Cotterell et al. (2017b)</ref> SEQ SEQ+DICT top-10-acc top-10-acc top-10-acc  <ref type="table">Table 5</ref>: The accuracies of the top-10 best outputs for the SEQ, SEQ+DICT, and prior work demonstrate that the dictionary constraint significantly improves the overall candidate quality. orthographically irregular or infrequent in the train- ing data. <ref type="figure" target="#fig_1">Figure 2</ref> quantifies this phenomenon, analyzing the difference in accuracy between the two models, and plotting this in relationship to the frequency of the suffix in the training data. The plot shows that SEQ excels at generating derived words ending in -ly, -ion, and other suffixes that appeared frequently in the training data. DIST's improvements over SEQ are generally much less frequent in the training data, or as in the case of -ment, are less frequent than other suffixes for the same transformation (like -ion.) By producing de- rived words whose suffixes show up rarely in the training data, DIST helps solve the orthographic irregularity problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Prior Work</head><p>There has been much work on the related task of inflected word generation (Durrett and DeNero,  <ref type="table">Table 6</ref>: Sample output from a selection of models. The words in bold mark the correct derivations. "Hindrance" and "vacancy" are the expected derived words for the last two rows.</p><p>2013; <ref type="bibr" target="#b20">Rastogi et al., 2016;</ref><ref type="bibr" target="#b11">Hulden et al., 2014</ref>). It is a structurally similar task to ours, but does not have the same difficulty of challenges ( <ref type="bibr">Cotterell et al., 2017a,b)</ref>, which we have addressed in our work. The paradigm completion for derivational morphol- ogy dataset we use in this work was introduced in <ref type="bibr" target="#b3">Cotterell et al. (2017b)</ref>. They apply the model that won the 2016 SIGMORPHON shared task on in- flectional morphology to derivational morphology ( <ref type="bibr" target="#b12">Kann and Schütze, 2016;</ref>). We use this as our baseline.</p><p>Our implementation of the dictionary constraint is an example of a special constraint which can be directly incorporated into the inference algo- rithm at little additional cost. <ref type="bibr">Yih (2004, 2007)</ref> propose a general inference procedure that naturally incorporates constraints through recasting inference as solving an integer linear program.</p><p>Beam or hypothesis rescoring to incorporate an expensive or non-decomposable signal into search has a history in machine translation <ref type="bibr" target="#b10">(Huang and Chiang, 2007)</ref>. In inflectional morphology, <ref type="bibr" target="#b19">Nicolai et al. (2015)</ref> use this idea to rerank hypothe- ses using orthographic features and <ref type="bibr" target="#b5">Faruqui et al. (2016)</ref> use a character-level language model. Our approach is similar to <ref type="bibr" target="#b5">Faruqui et al. (2016)</ref> in that we use statistics from a raw corpus, but at the token level.</p><p>There have been several attempts to use distri- butional information in morphological generation and analysis. <ref type="bibr" target="#b24">Soricut and Och (2015)</ref> collect pairs of words related by any morphological change in an unsupervised manner, then select a vector off- set which best explains their observations. There has been subsequent work exploring the vector offset method, finding it unsuccessful in captur- ing derivational transformations ( <ref type="bibr" target="#b6">Gladkova et al., 2016)</ref>. However, we use more expressive, non- linear functions to model derivational transforma- tions and report positive results. <ref type="bibr" target="#b8">Gupta et al. (2017)</ref> then learn a linear transformation per orthographic rule to solve a word analogy task. Our distribu- tional model learns a function per derivational trans- formation, not per orthographic rule, which allows it to generalize to unseen orthography.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Implementation Details</head><p>Our models are implemented in Python using the DyNet deep learning library ( <ref type="bibr">Neubig et al., 2017</ref>). The code is freely available for download. <ref type="bibr">4</ref> Sequence Model The sequence-to-sequence model uses character embeddings of size 20, which are shared across the encoder and decoder, with a vocabulary size of 63. The hidden states of the LSTMs are of size 40.</p><p>For training, we use Adam with an initial learn- ing rate of 0.005, a batch size of 5, and train for a maximum of 30 epochs. If after one epoch of the training data, the loss on the validation set does not decrease, we anneal the learning rate by half and revert to the previous best model. During decoding, we find the top 1 most prob- able sequence as discussed in Section 6 unless rescoring is used, in which we use the top 10.</p><p>Rescorer The rescorer is a 1-hidden-layer per- ceptron with a tanh nonlinearity and 4 hidden units. It is trained for a maximum of 5 epochs.</p><p>Distributional Model The DIST model is a 1- hidden-layer perceptron with a tanh nonlinearity and 100 hidden units. It is trained for a maximum of 25 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>In this work, we present a novel aggregation model for derived word generation. This model learns to choose between the predictions of orthographically- and distributionally-informed models. This amelio- rates suffix ambiguity and orthographic irregularity, the salient problems of the generation task. Concur- rently, we show that derivational transformations can be usefully modeled as nonlinear functions on distributional word embeddings. The distributional and orthographic models aggregated contribute or- thogonal information to the aggregate, as shown by substantial improvements over state-of-the-art results, and qualitative analysis. Two ways of incor- porating corpus knowledge -constrained decoding and rescoring -demonstrate further improvements to our main contribution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Diagram depicting the flow of our aggregation model. Two models generate a hypothesis according to orthogonal information; then one is chosen as the final model generation. Here, the hypothesis from the distributional model is chosen.</figDesc><graphic url="image-1.png" coords="1,307.28,222.54,218.26,148.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Aggregating across 30 random restarts, we tallied when SEQ and DIST correctly produced derived forms of each suffix. The y-axis shows the logarithm of the difference, per suffix, between the tally for DIST and the tally for SEQ. On the x-axis is the logarithm of the frequency of derived words with each suffix in the training data. A linear regression line is plotted to show the relationship between log suffix frequency and log difference in correct predictions. Suffixes that differ only by the first letter, as with-ger and-er, have been merged and represented by the more frequent of the two.</figDesc><graphic url="image-2.png" coords="7,307.28,209.10,218.27,162.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The average accuracies and number of states explored 
in the search graph of 30 runs of the SEQ model with various 
search procedures. The BEAM models use a beam size of 10. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Our open-vocabulary aggregation model AGGR improves performance by 3.8 points accuracy over SEQ, indicating that the sequence models and the distributional model are contributing complementary signals. AGGR is an open-vocabulary model like Cotterell et al. (2017b) and improves upon it by 6.3 points, making it our best comparable model. We provide an in-</figDesc><table>Model 
Accuracy Edit 

Cotterell et al. (2017b) 
71.7 
0.97 

DIST 
54.9 
3.23 
SEQ 
74.2 
0.88 
AGGR 
78.0 
0.83 
SEQ+FREQ 
79.3 
0.71 
DUAL+FREQ 
82.0 
0.64 

SEQ+DICT 
80.4 
0.72 
AGGR+DICT 
81.0 
0.78 
SEQ+FREQ+DICT 
81.2 
0.71 
AGGR+FREQ+DICT 
82.4 
0.67 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The accuracies and edit distances of the models 
presented in this paper and prior work. For edit distance, 
lower is better. The dictionary-constrained models are on the 
lower half of the table. 

</table></figure>

			<note place="foot" n="1"> We use the verb run as a demonstrative example; the transformation can be applied to most verbs.</note>

			<note place="foot" n="2"> The * indicates a non-word.</note>

			<note place="foot" n="3"> The remaining 2% is mostly words with hyphens or mistakes in the dataset.</note>

			<note place="foot" n="4"> https://github.com/danieldeutsch/ acl2018</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Shyam Upadhyay, Jordan Kodner, and Ryan Cotterell for insightful discus-sions about derivational morphology. We would also like to thank our anonymous reviewers for helpful feedback on clarity and presentation. This work was supported by Contract HR0011-15-2-0025 with the US Defense Advanced Re-search Projects Agency (DARPA). Approved for Public Release, Distribution Unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Conll-sigmorphon 2017 shared task: Universal morphological reinflection in 52 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christo</forename><surname>Kirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sylak-Glassman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Géraldine</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vylomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/K17-2001" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection. Association for Computational Linguistics</title>
		<meeting>the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection. Association for Computational Linguistics<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="30" />
		</imprint>
	</monogr>
	<note>Jason Eisner, and Mans Hulden</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Jason Eisner, and Mans Hulden. 2016. The sigmorphon 2016 shared taskmorphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christo</forename><surname>Kirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sylak-Glassman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Paradigm completion for derivational morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vylomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huda</forename><surname>Khayrallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christo</forename><surname>Kirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D17-1074" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="714" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised learning of complete morphological paradigms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1185" to="1195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Morphological inflection generation using character sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1077" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="634" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn&apos;t</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Gladkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Drozd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Matsuoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL Student Research Workshop. Association for Computational Linguistics</title>
		<meeting>the NAACL Student Research Workshop. Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploiting morphological regularities in distributional word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arihant</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avijit</forename><surname>Syed Sarfaraz Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjit</forename><surname>Vajpayee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Madan Gopal Jhanwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shrivastava</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D17-1028" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="292" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Forest rescoring: Faster decoding with integrated language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of morphological paradigms and lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Mans Hulden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malin</forename><surname>Forsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="569" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Singlemodel encoder-decoder with explicit morphological representation for reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<ptr target="http://anthology.aclweb.org/P16-2090" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="555" to="560" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W13-3512" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The nombank project: An interim report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zielinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop: Frontiers in Corpus Annotation</title>
		<editor>A. Meyers</editor>
		<meeting><address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quantitative analysis of culture using millions of digitized books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan Kui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviva</forename><surname>Presser Aiden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Veres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Pickett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Hoiberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Clancy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Norvig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Orwant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Pinker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erez</forename><surname>Lieberman Aiden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="page" from="176" to="82" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1301.3781.pdf" />
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop Papers</title>
		<meeting><address><addrLine>Scottsdale, Arizona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomi</forename><surname>Saphra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
		<title level="m">Swabha Swayamdipta, and Pengcheng Yin. 2017. Dynet: The dynamic neural network toolkit</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inflection generation as discriminative string transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><surname>Nicolai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N15-1093" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weighting finite-state transductions with neural context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1076" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="623" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<ptr target="http://cogcomp.org/papers/RothYi04.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Computational Natural Language Learning (CoNLL). Association for Computational Linguistics</title>
		<editor>Hwee Tou Ng and Ellen Riloff</editor>
		<meeting>of the Conference on Computational Natural Language Learning (CoNLL). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Global inference for entity and relation identification via a linear programming formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<ptr target="http://cogcomp.org/papers/RothYi07.pdf" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A graphbased lattice dependency parser for joint morphological segmentation and syntactic analysis. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Seeker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="359" to="373" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised morphology induction using word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1627" to="1637" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
