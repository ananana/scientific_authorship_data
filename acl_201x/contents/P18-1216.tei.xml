<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Embedding of Words and Labels for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Embedding of Words and Labels for Text Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2321" to="2331"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2321</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word embeddings are effective intermediate representations for capturing semantic regularities between words, when learning the representations of text sequences. We propose to view text classification as a label-word joint embedding problem: each label is embedded in the same space with the word vectors. We introduce an attention framework that measures the compatibility of embeddings between text sequences and labels. The attention is learned on a training set of labeled samples to ensure that, given a text sequence, the relevant words are weighted higher than the irrelevant ones. Our method maintains the interpretability of word embeddings, and enjoys a built-in ability to leverage alternative sources of information, in addition to input text sequences. Extensive results on the several large text datasets show that the proposed framework out-performs the state-of-the-art methods by a large margin, in terms of both accuracy and speed.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text classification is a fundamental problem in natural language processing (NLP). The task is to annotate a given text sequence with one (or multiple) class label(s) describing its textual con- tent. A key intermediate step is the text rep- resentation. Traditional methods represent text with hand-crafted features, such as sparse lexi- cal features (e.g., n-grams) ( <ref type="bibr" target="#b38">Wang and Manning, 2012)</ref>. Recently, neural models have been em- ployed to learn text representations, including con- volutional neural networks (CNNs) (Kalchbrenner * Corresponding author et al., <ref type="bibr" target="#b15">2014;</ref><ref type="bibr" target="#b46">Zhang et al., 2017b;</ref><ref type="bibr" target="#b31">Shen et al., 2017)</ref> and recurrent neural networks (RNNs) based on long short-term memory (LSTM) <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b39">Wang et al., 2018)</ref>.</p><p>To further increase the representation flexibil- ity of such models, attention mechanisms <ref type="bibr" target="#b2">(Bahdanau et al., 2015</ref>) have been introduced as an in- tegral part of models employed for text classifi- cation ( <ref type="bibr" target="#b41">Yang et al., 2016</ref>). The attention module is trained to capture the dependencies that make significant contributions to the task, regardless of the distance between the elements in the sequence. It can thus provide complementary information to the distance-aware dependencies modeled by RNN/CNN. The increasing representation power of the attention mechanism comes with increased model complexity.</p><p>Alternatively, several recent studies show that the success of deep learning on text classification largely depends on the effectiveness of the word embeddings ( <ref type="bibr" target="#b13">Joulin et al., 2016;</ref><ref type="bibr" target="#b40">Wieting et al., 2016;</ref><ref type="bibr" target="#b1">Arora et al., 2017;</ref><ref type="bibr" target="#b30">Shen et al., 2018a</ref>). Par- ticularly, <ref type="bibr" target="#b30">Shen et al. (2018a)</ref> quantitatively show that the word-embeddings-based text classifica- tion tasks can have the similar level of difficulty regardless of the employed models, using the con- cept of intrinsic dimension ( <ref type="bibr" target="#b20">Li et al., 2018</ref>). Thus, simple models are preferred. As the basic build- ing blocks in neural-based NLP, word embed- dings capture the similarities/regularities between words ( <ref type="bibr" target="#b26">Pennington et al., 2014</ref>). This idea has been extended to compute embeddings that capture the semantics of word se- quences (e.g., phrases, sentences, paragraphs and documents) ( <ref type="bibr" target="#b19">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b18">Kiros et al., 2015</ref>). These representations are built upon vari- ous types of compositions of word vectors, rang- ing from simple averaging to sophisticated archi- tectures. Further, they suggest that simple models are efficient and interpretable, and have the poten-tial to outperform sophisticated deep neural mod- els.</p><p>It is therefore desirable to leverage the best of both lines of works: learning text representations to capture the dependencies that make significant contributions to the task, while maintaining low computational cost. For the task of text classifica- tion, labels play a central role of the final perfor- mance. A natural question to ask is how we can directly use label information in constructing the text-sequence representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Contribution</head><p>Our primary contribution is therefore to pro- pose such a solution by making use of the la- bel embedding framework, and propose the Label- Embedding Attentive Model (LEAM) to improve text classification. While there is an abundant lit- erature in the NLP community on word embed- dings (how to describe a word) for text representa- tions, much less work has been devoted in compar- ison to label embeddings (how to describe a class). The proposed LEAM is implemented by jointly embedding the word and label in the same latent space, and the text representations are constructed directly using the text-label compatibility.</p><p>Our label embedding framework has the fol- lowing salutary properties: (i) Label-attentive text representation is informative for the downstream classification task, as it directly learns from a shared joint space, whereas traditional methods proceed in multiple steps by solving intermediate problems.</p><p>(ii) The LEAM learning procedure only involves a series of basic algebraic operations, and hence it retains the interpretability of simple mod- els, especially when the label description is avail- able. (iii) Our attention mechanism (derived from the text-label compatibility) has fewer parameters and less computation than related methods, and thus is much cheaper in both training and test- ing, compared with sophisticated deep attention models. (iv) We perform extensive experiments on several text-classification tasks, demonstrating the effectiveness of our label-embedding attentive model, providing state-of-the-art results on bench- mark datasets. (v) We further apply LEAM to predict the medical codes from clinical text. As an interesting by-product, our attentive model can highlight the informative key words for prediction, which in practice can reduce a doctor's burden on reading clinical notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Label embedding has been shown to be effective in various domains and tasks. In computer vi- sion, there has been a vast amount of research on leveraging label embeddings for image clas- sification ( <ref type="bibr" target="#b0">Akata et al., 2016)</ref>, multimodal learn- ing between images and text ( <ref type="bibr" target="#b9">Frome et al., 2013;</ref><ref type="bibr" target="#b17">Kiros et al., 2014)</ref>, and text recognition in im- ages ( <ref type="bibr" target="#b28">Rodriguez-Serrano et al., 2013)</ref>. It is par- ticularly successful on the task of zero-shot learn- ing ( <ref type="bibr" target="#b25">Palatucci et al., 2009;</ref><ref type="bibr" target="#b43">Yogatama et al., 2015;</ref><ref type="bibr" target="#b21">Ma et al., 2016)</ref>, where the label correlation cap- tured in the embedding space can improve the prediction when some classes are unseen. In NLP, labels embedding for text classification has been studied in the context of heterogeneous net- works in <ref type="bibr" target="#b36">(Tang et al., 2015)</ref> and multitask learning in ( <ref type="bibr" target="#b44">Zhang et al., 2017a</ref>), respectively. To the au- thors' knowledge, there is little research on inves- tigating the effectiveness of label embeddings to design efficient attention models, and how to joint embedding of words and labels to make full use of label information for text classification has not been studied previously, representing a contribu- tion of this paper.</p><p>For text representation, the currently best- performing models usually consist of an encoder and a decoder connected through an attention mechanism ( <ref type="bibr" target="#b37">Vaswani et al., 2017;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>, with successful applications to sentiment classification ( , sentence pair modeling ( <ref type="bibr" target="#b42">Yin et al., 2016)</ref> and sentence sum- marization ( <ref type="bibr">Rush et al., 2015</ref>). Based on this success, more advanced attention models have been developed, including hierarchical attention networks ( <ref type="bibr" target="#b41">Yang et al., 2016)</ref>, attention over at- tention ( <ref type="bibr" target="#b6">Cui et al., 2016)</ref>, and multi-step atten- tion ( <ref type="bibr" target="#b10">Gehring et al., 2017)</ref>. The idea of attention is motivated by the observation that different words in the same context are differentially informative, and the same word may be differentially important in a different context. The realization of "context" varies in different applications and model architec- tures. Typically, the context is chosen as the target task, and the attention is computed over the hidden layers of a CNN/RNN. Our attention model is di- rectly built in the joint embedding space of words and labels, and the context is specified by the label embedding.</p><p>Several recent works ( <ref type="bibr" target="#b37">Vaswani et al., 2017;</ref><ref type="bibr">Shen et al., 2018b,c)</ref> have demonstrated that sim-ple attention architectures can alone achieve state- of-the-art performance with less computational time, dispensing with recurrence and convolutions entirely. Our work is in the same direction, shar- ing the similar spirit of retaining model simplicity and interpretability. The major difference is that the aforementioned work focused on self attention, which applies attention to each pair of word tokens from the text sequences. In this paper, we investi- gate the attention between words and labels, which is more directly related to the target task. Further- more, the proposed LEAM has much less model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Throughout this paper, we denote vectors as bold, lower-case letters, and matrices as bold, upper- case letters. We use for element-wise division when applied to vectors or matrices. We use • for function composition, and ∆ p for the set of one hot vectors in dimension p.</p><p>Given a training set S = {(X n , y n )} N n=1 of pair-wise data, where X ∈ X is the text sequence, and y ∈ Y is its corresponding label. Specifically, y is a one hot vector in single-label problem and a binary vector in multi-label problem, as defined later in Section 4.1. Our goal for text classification is to learn a function f : X → Y by minimizing an empirical risk of the form:</p><formula xml:id="formula_0">min f ∈F 1 N N n=1 δ(y n , f (X n ))<label>(1)</label></formula><p>where δ : Y × Y → R measures the loss incurred from predicting f (X) when the true label is y, where f belongs to the functional space F. In the evaluation stage, we shall use the 0/1 loss as a tar- get loss: δ(y, z) = 0 if y = z, and 1 otherwise.</p><p>In the training stage, we consider surrogate losses commonly used for structured prediction in differ- ent problem setups (see Section 4.1 for details on the surrogate losses used in this paper). More specifically, an input sequence X of length L is composed of word tokens:</p><formula xml:id="formula_1">X = {x 1 , · · · , x L }.</formula><p>Each token x l is a one hot vec- tor in the space ∆ D , where D is the dictionary size. Performing learning in ∆ D is computation- ally expensive and difficult. An elegant frame- work in NLP, initially proposed in ( <ref type="bibr" target="#b19">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b26">Pennington et al., 2014;</ref><ref type="bibr" target="#b18">Kiros et al., 2015)</ref>, allows to concisely per- form learning by mapping the words into an em- bedding space. The framework relies on so called word embedding: ∆ D → R P , where P is the dimensionality of the embedding space. There- fore, the text sequence X is represented via the respective word embedding for each token: V = {v 1 , · · · , v L }, where v l ∈ R P . A typical text classification method proceeds in three steps, end- to-end, by considering a function decomposition f = f 0 • f 1 • f 2 as shown in <ref type="figure" target="#fig_1">Figure 1</ref>(a):</p><p>• f 0 : X → V, the text sequence is represented as its word-embedding form V, which is a matrix of P × L.</p><p>• f 1 : V → z, a compositional function f 1 ag- gregates word embeddings into a fixed-length vector representation z.</p><p>• f 2 : z → y, a classifier f 2 annotates the text representation z with a label.</p><p>A vast amount of work has been devoted to de- vising the proper functions f 0 and f 1 , i.e., how to represent a word or a word sequence, respec- tively. The success of NLP largely depends on the effectiveness of word embeddings in f 0 ( <ref type="bibr" target="#b3">Bengio et al., 2003;</ref><ref type="bibr" target="#b4">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b26">Pennington et al., 2014</ref>). They are often pre-trained offline on large corpus, then re- fined jointly via f 1 and f 2 for task-specific rep- resentations. Furthermore, the design of f 1 can be broadly cast into two categories. The popu- lar deep learning models consider the mapping as a "black box," and have employed sophisticated CNN/RNN architectures to achieve state-of-the- art performance ( <ref type="bibr" target="#b45">Zhang et al., 2015;</ref><ref type="bibr" target="#b41">Yang et al., 2016</ref>). On the contrary, recent studies show that simple manipulation of the word embeddings, e.g., mean or max-pooling, can also provide surpris- ingly excellent performance ( <ref type="bibr" target="#b13">Joulin et al., 2016;</ref><ref type="bibr" target="#b40">Wieting et al., 2016;</ref><ref type="bibr" target="#b1">Arora et al., 2017;</ref><ref type="bibr" target="#b30">Shen et al., 2018a</ref>). Nevertheless, these methods only lever- age the information from the input text sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Label-Embedding Attentive Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model</head><p>By examining the three steps in the traditional pipeline of text classification, we note that the use of label information only occurs in the last step, when learning f 2 , and its impact on learning the representations of words in f 0 or word sequences in f 1 is ignored or indirect. Hence, we propose a new pipeline by incorporating label information in every step, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>(b):  We focus on learning label embedding C (how to embed class labels in a Eu- clidean space), and leveraging the "compatibility" G between embedded words and labels to derive the attention score β for improved z. Note that ⊗ denotes the cosine similarity between C and V. In this figure, there are K=2 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 a r K u f y R 9 c N 9 Z i N s z n h Y 2 l 3 6 Q x I = " &gt; A A A C U n i c b V J N T w I x E O 3 i F y I q 6 t F L I z H x R B Z j o t 6 I X j x i F C U B Q r r d W a j 2 Y 9 N 2 M b D h P x o T D / 4 R L x 6 0 u 3 I Q c J K m L + + 9 a W e m D W L O j P X 9 D 6 + w s r q 2 v l H c L G 2 V t 3 d 2 K 3 v 7 D 0 Y l m k K L K q 5 0 O y A G O J P Q s s x y a M c a i A g 4 P A b P 1 5 n + O A J t m J L 3 d h x D T 5 C B Z B G j x D q q X 3 n q S n i h S g g i w 7 R 7 J 4 i d p m k 3 i P D d d F q a 0 w K w Z J S L i o d m L N y G c 3 L R O F l 0 T T L H Z N S v V P 2 a n w d e B v U Z q K J Z N P u V t 2 6 o a C J A W s q J M Z 2 6 H 9 t e S r R l l I M 7 M z E Q E / p M B t B x U B I B p p f m M 5 n i Y 8 e E O F L a L W l x z v 7 N S I k w W X 3 O 6 Z o e m k U t I / / T O o m N L n o p k 3 F i Q d L f i 6 K E Y 6 t w N m A c M g 3 U 8 r E D h G r m a s V 0 S D S h 1 j 1 D y Q 2 h v t j y M m i d 1 i 5 r / u 1 Z t X E 1 m 0 Y R H a I j d I L q 6 B w 1 0 A 1 q o h a i 6 B V 9 o m 8 P e e / e V 8 H 9 k l 9 r w Z v l H K C 5 K J R / A H p C t 3 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 a r K u f y R 9 c N 9 Z i N s z n h Y 2 l 3 6 Q x I = " &gt; A A A C U n i c b V J N T w I x E O 3 i F y I q 6 t F L I z H x R B Z j o t 6 I X j x i F C U B Q r r d W a j 2 Y 9 N 2 M b D h P x o T D / 4 R L x 6 0 u 3 I Q c J K m L + + 9 a W e m D W L O j P X 9 D 6 + w s r q 2 v l H c L G 2 V t 3 d 2 K 3 v 7 D 0 Y l m k K L K q 5 0 O y A G O J P Q s s x y a M c a i A g 4 P A b P 1 5 n + O A J t m J L 3 d h x D T 5 C B Z B G j x D q q X 3 n q S n i h S g g i w 7 R 7 J 4 i d p m k 3 i P D d d F q a 0 w K w Z J S L i o d m L N y G c 3 L R O F l 0 T T L H Z N S v V P 2 a n w d e B v U Z q K J Z N P u V t 2 6 o a C J A W s q J M Z 2 6 H 9 t e S r R l l I M 7 M z E Q E / p M B t B x U B I B p p f m M 5 n i Y 8 e E O F L a L W l x z v 7 N S I k w W X 3 O 6 Z o e m k U t I / / T O o m N L n o p k 3 F i Q d L f i 6 K E Y 6 t w N m A c M g 3 U 8 r E D h G r m a s V 0 S D S h 1 j 1 D y Q 2 h v t j y M m i d 1 i 5 r / u 1 Z t X E 1 m 0 Y R H a I j d I L q 6 B w 1 0 A 1 q o h a i 6 B V 9 o m 8 P e e / e V 8 H 9 k l 9 r w Z v l H K C 5 K J R / A H p C t 3 E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 a r K u f y R 9 c N 9 Z i N s z n h Y 2 l 3 6 Q x I = " &gt; A A A C U n i c b V J N T w I x E O 3 i F y I q 6 t F L I z H x R B Z j o t 6 I X j x i F C U B Q r r d W a j 2 Y 9 N 2 M b D h P x o T D / 4 R L x 6 0 u 3 I Q c J K m L + + 9 a W e m D W L O j P X 9 D 6 + w s r q 2 v l H c L G 2 V t 3 d 2 K 3 v 7 D 0 Y l m k K L K q 5 0 O y A G O J P Q s s x y a M c a i A g 4 P A b P 1 5 n + O A J t m J L 3 d h x D T 5 C B Z B G j x D q q X 3 n q S n i h S g g i w 7 R 7 J 4 i d p m k 3 i P D d d F q a 0 w K w Z J S L i o d m L N y G c 3 L R O F l 0 T T L H Z N S v V P 2 a n w d e B v U Z q K J Z N P u V t 2 6 o a C J A W s q J M Z 2 6 H 9 t e S r R l l I M 7 M z E Q E / p M B t B x U B I B p p f m M 5 n i Y 8 e E O F L a L W l x z v 7 N S I k w W X 3 O 6 Z o e m k U t I / / T O o m N L n o p k 3 F i Q d L f i 6 K E Y 6 t w N m A c M g 3 U 8 r E D h G r m a s V 0 S D S h 1 j 1 D y Q 2 h v t j y M m i d 1 i 5 r / u 1 Z t X E 1 m 0 Y R H a I j d I L q 6 B w 1 0 A 1 q o h a i 6 B V 9 o m 8 P e e / e V 8 H 9 k l 9 r w Z v l H K C 5 K J R / A H p C t 3 E = &lt; / l a t e x i t &gt;</head><formula xml:id="formula_2">f 1 f 0 X f 2 y (a) Traditional method C G &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u s N I E C u G P 8 Y y D G t y I d e s r n E + V f U = " &gt; A A A C C X i c b V C 7 T s M w F H V 4 l v I K M L I Y K i S m K k V I w F b B U M Y i E V q p i S r H c V q r t h P Z D q i K M r P w K y w M g F j 5 A z b + B j f N A C 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H s n R 8 z r 3 2 v S d I G F X a c b 6 t h c W l 5 Z X V y l p 1 f W N z a 9 v e 2 b 1 T c S o x c X H M Y t k N k C K M C u J q q h n p J p I g H j D S C U Z X E 7 9 z T 6 S i s b j V 4 4 T 4 H A 0 E j S h G 2 k h 9 + 8 A T 5 A H H n C M R Z l 6 L I 5 1 n m R d E s J X n 1 e L e t 2 t O 3 S k A 5 0 m j J D V Q o t 2 3 v 7 w w x i k n Q m O G l O o 1 n E T 7 G Z K a Y k b M q 6 k i C c I j N C A 9 Q w X i R P l Z s U o O j 4 w S w i i W 5 g g N C / V 3 R 4 a 4 U m M e m E o z 2 1 D N e h P x P 6 + X 6 u j c z 6 h I U k 0 E n n 4 U p Q z q G E 5 y g S G V B G s 2 N g R h S c 2 s E A + R R F i b 9 K o m h M b s y v P E P a l f 1 J 2 b 0 1 r z s k y j A v b B I T g G D X A G m u A a t I E L M H g E z + A V v F l P 1 o v 1 b n 1 M S x e s s m c P / I H 1 + Q P m w p q U &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u s N I E C u G P 8 Y y D G t y I d e s r n E + V f U = " &gt; A A A C C X i c b V C 7 T s M w F H V 4 l v I K M L I Y K i S m K k V I w F b B U M Y i E V q p i S r H c V q r t h P Z D q i K M r P w K y w M g F j 5 A z b + B j f N A C 1 H s n R 8 z r 3 2 v S d I G F X a c b 6 t h c W l 5 Z X V y l p 1 f W N z a 9 v e 2 b 1 T c S o x c X H M Y t k N k C K M C u J q q h n p J p I g H j D S C U Z X E 7 9 z T 6 S i s b j V 4 4 T 4 H A 0 E j S h G 2 k h 9 + 8 A T 5 A H H n C M R Z l 6 L I 5 1 n m R d E s J X n 1 e L e t 2 t O 3 S k A 5 0 m j J D V Q o t 2 3 v 7 w w x i k n Q m O G l O o 1 n E T 7 G Z K a Y k b M q 6 k i C c I j N C A 9 Q w X i R P l Z s U o O j 4 w S w i i W 5 g g N C / V 3 R 4 a 4 U m M e m E o z 2 1 D N e h P x P 6 + X 6 u j c z 6 h I U k 0 E n n 4 U p Q z q G E 5 y g S G V B G s 2 N g R h S c 2 s E A + R R F i b 9 K o m h M b s y v P E P a l f 1 J 2 b 0 1 r z s k y j A v b B I T g G D X A G m u A a t I E L M H g E z + A V v F l P 1 o v 1 b n 1 M S x e s s m c P / I H 1 + Q P m w p q U &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u s N I E C u G P 8 Y y D G t y I d e s r n E + V f U = " &gt; A A A C C X i c b V C 7 T s M w F H V 4 l v I K M L I Y K i S m K k V I w F b B U M Y i E V q p i S r H c V q r t h P Z D q i K M r P w K y w M g F j 5 A z b + B j f N A C 1 H s n R 8 z r 3 2 v S d I G F X a c b 6 t h c W l 5 Z X V y l p 1 f W N z a 9 v e 2 b 1 T c S o x c X H M Y t k N k C K M C u J q q h n p J p I g H j D S C U Z X E 7 9 z T 6 S i s b j V 4 4 T 4 H A 0 E j S h G 2 k h 9 + 8 A T 5 A H H n C M R Z l 6 L I 5 1 n m R d E s J X n 1 e L e t 2 t O 3 S k A 5 0 m j J D V Q o t 2 3 v 7 w w x i k n Q m O G l O o 1 n E T 7 G Z K a Y k b M q 6 k i C c I j N C A 9 Q w X i R P l Z s U o O j 4 w S w i i W 5 g g N C / V 3 R 4 a 4 U m M e m E o z 2 1 D N e h P x P 6 + X 6 u j c z 6 h I U k 0 E n n 4 U p Q z q G E 5 y g S G V B G s 2 N g R h S c 2 s E A + R R F i b 9 K o m h M b s y v P E P a l f 1 J 2 b 0 1 r z s k y j A v b B I T g G D X A G m u A a t I E L M H g E z + A V v F l P 1 o v 1 b n 1 M S x e s s m c P / I H 1 + Q P m w p q U &lt; / l a t e x i t &gt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T s M w F H X K q 5 R X g J H F U C E x V S l C A r Y K F s Y i k b Z S E 1 W O 4 7 R W b S e y H V A V Z W b h V 1 g Y A L H y B 2 z 8 D W 6 a A V q O Z O n 4 n H v t e 0 + Q M K q 0 4 3 x b l a X l l d W 1 6 n p t Y 3 N r e 8 f e 3 e u o O J W Y u D h m s e w F S B F G B X E 1 1 Y z 0 E k k Q D x j p B u P r q d + 9 J 1 L R W N z p S U J 8 j o a C R h Q j b a S B f e g J 8 o B j z p E I M 6 / D k c 6 z z A s i 2 M n z W n E f 2 H W n 4 R S A i 6 R Z k j o o 0 R 7 Y X 1 4 Y 4 5 Q T o T F D S v W b T q L 9 D E l N M S P m 1 V S R B O E x G p K + o Q J x o v y s W C W H x 0 Y J Y R R L c 4 S G h f q 7 I 0 N c q Q k P T K W Z b a T m v a n 4 n 9 d P d X T h Z 1 Q k q S Y C z z 6 K U g Z 1 D K e 5 w J B K g j W b G I K w p G Z W i E d I I q x N e j U T Q n N + 5 U X i n j Y u G 8 7 t W b 1 1 V a Z R B Q f g C J y A J j g H L X A D 2 s A F G D y C Z / A K 3 q w n 6 8 V 6 t z 5 m p R W r 7 N k H f 2 B 9 / g A s m p r B &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U Z h 7 Y j k s Y l F N c + x w F 1 G V x 2 T 7 S F E = " &gt; A A A C C X i c b V C 7 T s M w F H X K q 5 R X g J H F U C E x V S l C A r Y K F s Y i k b Z S E 1 W O 4 7 R W b S e y H V A V Z W b h V 1 g Y A L H y B 2 z 8 D W 6 a A V q O Z O n 4 n H v t e 0 + Q M K q 0 4 3 x b l a X l l d W 1 6 n p t Y 3 N r e 8 f e 3 e u o O J W Y u D h m s e w F S B F G B X E 1 1 Y z 0 E k k Q D x j p B u P r q d + 9 J 1 L R W N z p S U J 8 j o a C R h Q j b a S B f e g J 8 o B j z p E I M 6 / D k c 6 z z A s i 2 M n z W n E f 2 H W n 4 R S A i 6 R Z k j o o 0 R 7 Y X 1 4 Y 4 5 Q T o T F D S v W b T q L 9 D E l N M S P m 1 V S R B O E x G p K + o Q J x o v y s W C W H x 0 Y J Y R R L c 4 S G h f q 7 I 0 N c q Q k P T K W Z b a T m v a n 4 n 9 d P d X T h Z 1 Q k q S Y C z z 6 K U g Z 1 D K e 5 w J B K g j W b G I K w p G Z W i E d I I q x N e j U T Q n N + 5 U X i n j Y u G 8 7 t W b 1 1 V a Z R B Q f g C J y A J j g H L X A D 2 s A F G D y C Z / A K 3 q w n 6 8 V 6 t z 5 m p R W r 7 N k H f 2 B 9 / g A s m p r B &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U Z h 7 Y j k s Y l F N c + x w F 1 G V x 2 T 7 S F E = " &gt; A A A C C X i c b V C 7 T s M w F H X K q 5 R X g J H F U C E x V S l C A r Y K F s Y i k b Z S E 1 W O 4 7 R W b S e y H V A V Z W b h V 1 g Y A L H y B 2 z 8 D W 6 a A V q O Z O n 4 n H v t e 0 + Q M K q 0 4 3 x b l a X l l d W 1 6 n p t Y 3 N r e 8 f e 3 e u o O J W Y u D h m s e w F S B F G B X E 1 1 Y z 0 E k k Q D x j p B u P r q d + 9 J 1 L R W N z p S U J 8 j o a C R h Q j b a S B f e g J 8 o B j z p E I M 6 / D k c 6 z z A s i 2 M n z W n E f 2 H W n 4 R S A i 6 R Z k j o o 0 R 7 Y X 1 4 Y 4 5 Q T o T F D S v W b T q L 9 D E l N M S P m 1 V S R B O E x G p K + o Q J x o v y s W C W H x 0 Y J Y R R L c 4 S G h f q 7 I 0 N c q Q k P T K W Z b a T m v a n 4 n 9 d P d X T h Z 1 Q k q S Y C z z 6 K U g Z 1 D K e 5 w J B K g j W b G I K w p G Z W i E d I I q x N e j U T Q n N + 5 U X i n j Y u G 8 7 t W b 1 1 V a Z R B Q f g C J y A J j g H L X A D 2 s A F G D y C Z / A K 3 q w n 6 8 V 6 t z 5 m p R W r 7 N k H f 2 B 9 / g A s m p r B &lt; / l a t e x i t &gt;</head><p>• f 0 : Besides embedding words, we also em- bed all the labels in the same space, which act as the "anchor points" of the classes to in- fluence the refinement of word embeddings.</p><p>• f 1 : The compositional function aggregates word embeddings into z, weighted by the compatibility between labels and words.</p><p>• f 2 : The learning of f 2 remains the same, as it directly interacts with labels.</p><p>Under the proposed label embedding framework, we specifically describe a label-embedding atten- tive model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Embeddings of Words and Labels</head><p>We propose to embed both the words and the labels into a joint space i.e.,</p><formula xml:id="formula_3">∆ D → R P and Y → R P . The label embeddings are C = [c 1 , · · · , c K ],</formula><p>where K is the number of classes. A simple way to measure the compatibility of label-word pairs is via the cosine similarity</p><formula xml:id="formula_4">G = (C V) ˆ G,<label>(2)</label></formula><p>wherê G is the normalization matrix of size K×L, with each element obtained as the multiplication of 2 norms of the c-th label embedding and l-th word embedding:</p><formula xml:id="formula_5">ˆ g kl = c k v l .</formula><p>To further capture the relative spatial informa- tion among consecutive words (i.e., phrases 1 ) and introduce non-linearity in the compatibility mea- sure, we consider a generalization of (2). Specif- ically, for a text phase of length 2r + 1 cen- tered at l, the local matrix block G l−r:l+r in G measures the label-to-token compatibility for the "label-phrase" pairs. To learn a higher-level com- patibility stigmatization u l between the l-th phrase and all labels, we have:</p><formula xml:id="formula_6">u l = ReLU(G l−r:l+r W 1 + b 1 ),<label>(3)</label></formula><p>where W 1 ∈ R 2r+1 and b 1 ∈ R K are parameters to be learned, and u l ∈ R K . The largest com- patibility value of the l-th phrase wrt the labels is collected:</p><formula xml:id="formula_7">m l = max-pooling(u l ).<label>(4)</label></formula><p>Together, m is a vector of length L. The compat- ibility/attention score for the entire text sequence is:</p><formula xml:id="formula_8">β = SoftMax(m),<label>(5)</label></formula><p>where the l-th element of SoftMax is</p><formula xml:id="formula_9">β l = exp(m l ) L l =1 exp(m l )</formula><p>.</p><p>The text sequence representation can be sim- ply obtained via averaging the word embeddings, weighted by label-based attention score:</p><formula xml:id="formula_10">z = l β l v l .<label>(6)</label></formula><p>Relation to Predictive Text Embeddings Pre- dictive Text Embeddings (PTE) ( <ref type="bibr" target="#b36">Tang et al., 2015)</ref> is the first method to leverage label embeddings to improve the learned word embeddings. We discuss three major differences between PTE and our LEAM: (i) The general settings are different. PTE casts the text representation through hetero- geneous networks, while we consider text repre- sentation through an attention model. (ii) In PTE, the text representation z is the averaging of word embeddings. In LEAM, z is weighted averaging of word embeddings through the proposed label- attentive score in (6). (iii) PTE only considers the linear interaction between individual words and la- bels. LEAM greatly improves the performance by considering nonlinear interaction between phrase and labels. Specifically, we note that the text em- bedding in PTE is similar with a very special case of LEAM, when our window size r = 1 and at- tention score β is uniform. As shown later in <ref type="figure">Fig- ure 2(c)</ref> of the experimental results, LEAM can be significantly better than the PTE variant.</p><p>Training Objective The proposed joint embed- ding framework is applicable to various text clas- sification tasks. We consider two setups in this paper. For a learned text sequence representation z = f 1 •f 0 (X), we jointly optimize f = f 0 •f 1 •f 2 over F, where f 2 is defined according to the spe- cific tasks:</p><p>• Single-label problem: categorizes each text instance to precisely one of K classes, y ∈</p><formula xml:id="formula_11">∆ K min f ∈F 1 N N n=1 CE(y n , f 2 (z n )),<label>(7)</label></formula><p>where CE(·, ·) is the cross entropy between two probability vectors, and</p><formula xml:id="formula_12">f 2 (z n ) = SoftMax (z n ), with z n = W 2 z n + b 2 and W 2 ∈ R K×P , b 2 ∈ R K are trainable param- eters.</formula><p>• Multi-label problem: categorizes each text instance to a set of K target labels {y k ∈ ∆ 2 |k = 1, · · · , K}; there is no constraint on how many of the classes the instance can be assigned to, and</p><formula xml:id="formula_13">min f ∈F 1 N K N n=1 K k=1 CE(y nk , f 2 (z nk ),<label>(8)</label></formula><p>where</p><formula xml:id="formula_14">f 2 (z nk ) = 1 1+exp(z nk )</formula><p>, and z nk is the kth column of z n .</p><p>To summarize, the model parameters θ = {V, C, W 1 , b 1 , W 2 , b 2 }. They are trained end- to-end during learning. {W 1 , b 1 } and {W 2 , b 2 } are weights in f 1 and f 2 , respectively, which are treated as standard neural networks. For the joint embeddings {V, C} in f 0 , the pre-trained word embeddings are used as initialization if available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning &amp; Testing with LEAM</head><p>Learning and Regularization The quality of the jointly learned embeddings are key to the model performance and interpretability. Ide- ally, we hope that each label embedding acts as the "anchor" points for each classes: closer to the word/sequence representations that are in the same classes, while farther from those in different classes. To best achieve this property, we consider to regularize the learned label embeddings c k to be on its corresponding manifold. This is imposed by the fact c k should be easily classified as the correct label y k :</p><formula xml:id="formula_15">min f ∈F 1 K K n=1 CE(y k , f 2 (c k )),<label>(9)</label></formula><p>where f 2 is specficied according to the problem in either <ref type="formula" target="#formula_11">(7)</ref> or <ref type="formula" target="#formula_13">(8)</ref>. This regularization is used as a penalty in the main training objective in <ref type="formula" target="#formula_11">(7)</ref> or <ref type="formula" target="#formula_13">(8)</ref>, and the default weighting hyperparameter is set as 1. It will lead to meaningful interpretabil- ity of learned label embeddings as shown in the experiments.</p><p>Interestingly in text classification, the class itself is often described as a set of E words {e i , i = 1, · · · , E}. These words are consid- ered as the most representative description of each class, and highly distinguishing between different classes. For example, the Yahoo! Answers Topic dataset ( <ref type="bibr" target="#b45">Zhang et al., 2015</ref>) contains ten classes, most of which have two words to precisely de- scribe its class-specific features, such as "Comput- ers &amp; Internet", "Business &amp; Finance" as well as "Politics &amp; Government" etc. We consider to use each label's corresponding pre-trained word em- beddings as the initialization of the label embed- dings. For the datasets without representative class descriptions, one may initialize the label embed- dings as random samples drawn from a standard Gaussian distribution.</p><p>Testing Both the learned word and label embed- dings are available in the testing stage. We clar- ify that the label embeddings C of all class candi- dates Y are considered as the input in the testing stage; one should distinguish this from the use of groundtruth label y in prediction. For a text se- quence X, one may feed it through the proposed pipeline for prediction: (i) f 1 : harvesting the word embeddings V, (ii) f 2 : V interacts with C to ob- tain G, pooled as β, which further attends V to derive z, and (iii) f 3 : assigning labels based on the tasks. To speed up testing, one may store G offline, and avoid its online computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Parameters </p><formula xml:id="formula_16">Complexity Seq. Operation CNN m · h · P O(m · h · L · P ) O(1) LSTM 4 · h · (h + P ) O(L · h 2 + h · L · P ) O(L) SWEM 0 O(L · P ) O(1) Bi-BloSAN 7·P 2 +5·P O(P 2 ·L 2 /R+P 2 ·L+P 2 ·R 2 ) O(1) Our model K · P O(K · L · P ) O(1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Complexity</head><p>We compare CNN, LSTM, Simple Word Embeddings-based Models (SWEM) (Shen et al., 2018a) and our LEAM wrt the parameters and computational speed. For the CNN, we assume the same size m for all filters. Specifically, h represents the dimension of the hidden units in the LSTM or the number of filters in the CNN; R denotes the number of blocks in the Bi-BloSAN; P denotes the final sequence representation dimension. Similar to ( <ref type="bibr" target="#b37">Vaswani et al., 2017;</ref><ref type="bibr" target="#b30">Shen et al., 2018a</ref>), we examine the number of compositional parameters, computational com- plexity and sequential steps of the four methods. As shown in <ref type="table" target="#tab_0">Table 1</ref>, both the CNN and LSTM have a large number of compositional parameters.</p><p>Since K m, h, the number of parameters in our models is much smaller than for the CNN and LSTM models. For the computational complexity, our model is almost same order as the most simple SWEM model, and is smaller than the CNN or LSTM by a factor of mh/K or h/K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>Setup We use 300-dimensional GloVe word em- beddings <ref type="bibr" target="#b26">Pennington et al. (2014)</ref> as initializa- tion for word embeddings and label embeddings in our model. Out-Of-Vocabulary (OOV) words are initialized from a uniform distribution with range [−0.01, 0.01]. The final classifier is imple- mented as an MLP layer followed by a sigmoid or softmax function depending on specific task. We train our model's parameters with the Adam Optimizer ( <ref type="bibr" target="#b16">Kingma and Ba, 2014)</ref>, with an ini- tial learning rate of 0.001, and a minibatch size of 100. Dropout regularization ( <ref type="bibr" target="#b35">Srivastava et al., 2014</ref>) is employed on the final MLP layer, with dropout rate 0.5. The model is implemented using Tensorflow and is trained on GPU Titan X. The code to reproduce the experimental results is at https://github.com/guoyinwang/LEAM <ref type="table" target="#tab_0">AGNews  4  120k  7.6k  Yelp Binary  2  560 k  38k  Yelp Full  5  650k  38k  DBPedia  14  560k  70k  Yahoo  10  1400k  60k   Table 2</ref>: Summary statistics of five datasets, in- cluding the number of classes, number of training samples and number of testing samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset # Classes # Training # Testing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classification on Benchmark Datasets</head><p>We test our model on the same five standard benchmark datasets as in ( <ref type="bibr" target="#b45">Zhang et al., 2015</ref>). The summary statistics of the data are shown in <ref type="table">Table  2</ref>, with content specified below:</p><p>• AGNews: Topic classification over four cat- egories of Internet news articles <ref type="bibr" target="#b8">(Del Corso et al., 2005</ref>) composed of titles plus descrip- tion classified into: World, Entertainment, Sports and Business.</p><p>• Yelp Review Full: The dataset is obtained from the Yelp Dataset Challenge in 2015, the task is sentiment classification of polarity star labels ranging from 1 to 5.</p><p>• Yelp Review Polarity: The same set of text reviews from Yelp Dataset Challenge in 2015, except that a coarser sentiment defini- tion is considered: 1 and 2 are negative, and 4 and 5 as positive.</p><p>• DBPedia: Ontology classification over four- teen non-overlapping classes picked from DBpedia 2014 (Wikipedia).</p><p>• </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Yahoo DBPedia AGNews Yelp P. Yelp F. Bag-of-words ( <ref type="bibr" target="#b45">Zhang et al., 2015)</ref> 68.90 96.60 88.80 92.20 58.00 Small word CNN ( <ref type="bibr" target="#b45">Zhang et al., 2015)</ref> 69   <ref type="table" target="#tab_2">Table 3</ref>.</p><p>Testing accuracy Simple compositional meth- ods indeed achieve comparable performance as the sophisticated deep CNN/RNN models. On the other hand, deep hierarchical attention model can improve the pure CNN/RNN models. The recently proposed self-attention network generally yield higher accuracy than previous methods. All ap- proaches are better than traditional bag-of-words method. Our proposed LEAM outperforms the state-of-the-art methods on two largest datasets, i.e., Yahoo and DBPedia. On other datasets, LEAM ranks the 2nd or 3rd best, which are simi- lar to top 1 method in term of the accuracy. This is probably due to two reasons: (i) the number of classes on these datasets is smaller, and (ii) there is no explicit corresponding word embed- ding available for the label embedding initializa- tion during learning. The potential of label embed- ding may not be fully exploited. As the ablation study, we replace the nonlinear compatibility (3) to the linear one in (2) . The degraded performance demonstrates the necessity of spatial dependency and nonlinearity in constructing the attentions. Nevertheless, we argue LEAM is favorable for text classification, by comparing the model size and time cost <ref type="table" target="#tab_4">Table 4</ref>, as well as convergence speed in <ref type="figure">Figure 2(a)</ref>  faster than Bi-BloSAN. We also compare the per- formance when only a partial dataset is labeled, the results are shown in <ref type="figure">Figure 2</ref>(b). LEAM con- sistently outperforms other methods with different proportion of labeled data.</p><p>Hyper-parameter Our method has an addi- tional hyperparameter, the window size r to define the length of "phase" to construct the attention. Larger r captures long term dependency, while smaller r enforces the local dependency. We study its impact in <ref type="figure">Figure 2</ref>(c). The topic classification tasks generally requires a larger r, while senti- ment classification tasks allows relatively smaller r. One may safely choose r around 50 if not fine- tuning. We report the optimal results in <ref type="table" target="#tab_2">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Representational Ability</head><p>Label embeddings are highly meaningful To provide insight into the meaningfulness of the learned representations, in <ref type="figure">Figure 3</ref> we visual- ize the correlation between label embeddings and document embeddings based on the Yahoo date- set. First, we compute the averaged document em- beddings per class:  text manifold for class k. Ideally, the perfect label embedding c k should be the representative anchor point for class k. We compute the cosine similar- ity between ¯ z k and c k across all the classes, shown in <ref type="figure">Figure 3(a)</ref>. The rows are averaged per-class document embeddings, while columns are label embeddings. Therefore, the on-diagonal elements measure how representative the learned label em- beddings are to describe its own classes, while off-diagonal elements reflect how distinctive the label embeddings are to be separated from other classes. The high on-diagonal elements and low off-diagonal elements in <ref type="figure">Figure 3(a)</ref> indicate the superb ability of the label representations learned from LEAM.</p><formula xml:id="formula_17">¯ z k = 1 |S k | i∈S k z i ,</formula><p>Further, since both the document and label em- beddings live in the same high-dimensional space, we use t-SNE <ref type="bibr" target="#b22">(Maaten and Hinton, 2008)</ref> to vi- sualize them on a 2D map in <ref type="figure">Figure 3(b)</ref>. Each color represents a different class, the point clouds are document embeddings, and the label embed- dings are the large dots with black circles. As can be seen, each label embedding falls into the inter- nal region of the respective manifold, which again demonstrate the strong representative power of la- bel embeddings.</p><p>Interpretability of attention Our attention score β can be used to highlight the most infor- mative words wrt the downstream prediction task. We visualize two examples in <ref type="figure" target="#fig_4">Figure 4</ref>(a) for the Yahoo dataset. The darker yellow means more im- portant words. The 1st text sequence is on the topic of "Sports", and the 2nd text sequence is "Entertainment". The attention score can correctly detect the key words with proper scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Applications to Clinical Text</head><p>To demonstrate the practical value of label embed- dings, we apply LEAM for a real health care sce- nario: medical code prediction on the Electronic Health Records dataset. A given patient may have multiple diagnoses, and thus multi-label learning is required.</p><p>Specifically, we consider an open-access dataset, MIMIC-III ( <ref type="bibr" target="#b12">Johnson et al., 2016)</ref>, which</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUC F1 Model</head><p>Macro Micro Macro Micro P@5 Logistic Regression 0.829 0.864 0.477 0.533 0.546 Bi-GRU 0.828 0.868 0.484 0.549 0.591 CNN <ref type="bibr" target="#b15">(Kim, 2014)</ref> 0.876 0.907 0.576 0.625 0.620 C-MemNN ( <ref type="bibr" target="#b27">Prakash et al., 2017)</ref> 0.833 - - - 0.42 Attentive LSTM ( <ref type="bibr" target="#b34">Shi et al., 2017)</ref> - 0.900 - 0.532 - CAML ( <ref type="bibr" target="#b24">Mullenbach et al., 2018)</ref> 0.875 0.909 0.532 0.614 0.609 LEAM 0.881 0.912 0.540 0.619 0.612 <ref type="table">Table 5</ref>: Quantitative results for doctor-notes multi-label classification task.</p><p>contains text and structured records from a hospital intensive care unit. Each record includes a variety of narrative notes describing a patients stay, including diagnoses and procedures. They are accompanied by a set of metadata codes from the International Classification of Diseases (ICD), which present a standardized way of indicating diagnoses/procedures. To compare with previous work, we follow ( <ref type="bibr" target="#b34">Shi et al., 2017;</ref><ref type="bibr" target="#b24">Mullenbach et al., 2018)</ref>, and preprocess a dataset consisting of the most common 50 labels. It results in 8,067 documents for training, 1,574 for validation, and 1,730 for testing.</p><p>Results We compare against the three base- lines: a logistic regression model with bag-of- words, a bidirectional gated recurrent unit (Bi- GRU) and a single-layer 1D convolutional net- work <ref type="bibr" target="#b15">(Kim, 2014</ref>). We also compare with three recent methods for multi-label classification of clinical text, including Condensed Memory Net- works (C-MemNN) ( <ref type="bibr" target="#b27">Prakash et al., 2017)</ref>, Atten- tive LSTM ( <ref type="bibr" target="#b34">Shi et al., 2017)</ref> and Convolutional Attention (CAML) ( <ref type="bibr" target="#b24">Mullenbach et al., 2018)</ref>. To quantify the prediction performance, we fol- low ( <ref type="bibr" target="#b24">Mullenbach et al., 2018)</ref> to consider the micro-averaged and macro-averaged F1 and area under the ROC curve (AUC), as well as the preci- sion at n (P@n). Micro-averaged values are cal- culated by treating each (text, code) pair as a sep- arate prediction. Macro-averaged values are cal- culated by averaging metrics computed per-label. P@n is the fraction of the n highestscored labels that are present in the ground truth.</p><p>The results are shown in <ref type="table">Table 5</ref>. LEAM pro- vides the best AUC score, and better F1 and P@5 values than all methods except CNN. CNN con- sistently outperforms the basic Bi-GRU architec- ture, and the logistic regression baseline performs worse than all deep learning architectures. We emphasize that the learned attention can be very useful to reduce a doctor's reading burden. As shown in <ref type="figure" target="#fig_4">Figure 4(b)</ref>, the health related words are highlighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we first investigate label embed- dings for text representations, and propose the label-embedding attentive models. It embeds the words and labels in the same joint space, and mea- sures the compatibility of word-label pairs to at- tend the document representations. The learn- ing framework is tested on several large standard datasets and a real clinical text application. Com- pared with the previous methods, our LEAM al- gorithm requires much lower computational cost, and achieves better if not comparable performance relative to the state-of-the-art. The learned atten- tion is highly interpretable: highlighting the most informative words in the text sequence for the downstream classification task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of different schemes for document representations z. (a) Much work in NLP has been devoted to directly aggregating word embedding V for z. (b) We focus on learning label embedding C (how to embed class labels in a Euclidean space), and leveraging the "compatibility" G between embedded words and labels to derive the attention score β for improved z. Note that ⊗ denotes the cosine similarity between C and V. In this figure, there are K=2 classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Yahoo! Answers Topic: Topic classifica- tion over ten largest main categories from Ya- hoo! Answers Comprehensive Questions and Answers version 1.0, including question title, question content and best answer. We compare with a variety of methods, in- cluding (i) the bag-of-words in (Zhang et al., 2015); (ii) sophisticated deep CNN/RNN models: large/small word CNN, LSTM reported in (Zhang et al., 2015; Dai and Le, 2015) and deep CNN (29 layer) (Conneau et al., 2017); (iii) simple compo- sitional methods: fastText (Joulin et al., 2016) and simple word embedding models (SWEM) (Shen et al., 2018a); (iv) deep attention models: hier- archical attention network (HAN) (Yang et al.,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Comprehensive study of LEAM, including convergence speed, performance vs proportion of labeled data, and impact of hyper-parameter</figDesc><graphic url="image-12.png" coords="8,81.90,253.09,118.76,118.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of learned attention β.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Comparisons of CNN, LSTM, SWEM 
and our model architecture. Columns correspond 
to the number of compositional parameters, com-
putational complexity and sequential operations 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Test Accuracy on document classification tasks, in percentage. We ran Bi-BloSAN using the 
authors' implementation; all other results are directly cited from the respective papers. 

2016); (v) simple attention models: bi-directional 
block self-attention network (Bi-BloSAN) (Shen 
et al., 2018c). The results are shown in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>. The time cost is reported as the wall-clock time for 1000 iterations. LEAM maintains the simplicity and low cost of SWEM, compared with other models. LEAM uses much less model parameters, and converges significantly</figDesc><table>Model 
# Parameters Time cost (s) 
CNN 
541k 
171 
LSTM 
1.8M 
598 
SWEM 
61K 
63 
Bi-BloSAN 
3.6M 
292 
LEAM 
65K 
65 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Comparison of model size and speed.</figDesc><table></table></figure>

			<note place="foot" n="1"> We call it &quot;phrase&quot; for convenience; it could be any longer word sequence such as a sentence and paragraph etc. when a larger window size r is considered.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label-embedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Attention-overattention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04423</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ranking a stream of news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianna M Del</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Gulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Romani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th international conference on World Wide Web</title>
		<meeting>the 14th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mimic-iii, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Lehman</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengling</forename><surname>Li-Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">Anthony</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Scientific data</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 deep learning workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Measuring the intrinsic dimension of objective landscapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heerad</forename><surname>Farkhoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Label embedding for zero-shot fine-grained named entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sa</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Explainable prediction of medical codes from clinical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mullenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05695</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Condensed memory networks for clinical diagnostic inferencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaditya</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oladimeji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Label embedding for text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Jose A Rodriguez-Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">France</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexander M Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<title level="m">Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deconvolutional latent-variable model for text sequence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Bi-directional block selfattention for fast and memory-efficient sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04075</idno>
		<title level="m">Towards automated icd coding using deep learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pte: Predictive text embedding through large-scale heterogeneous text networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1165" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<title level="m">Topic compositional neural language model</title>
		<imprint>
			<publisher>AISTATS</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Abcnn: Attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Wenpeng Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Embedding methods for fine grained entity type classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07210</idno>
		<title level="m">Multitask label embedding for text classification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deconvolutional paragraph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention-based lstm network for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
