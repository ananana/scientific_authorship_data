<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reinforced Extractive Summarization with Question-Focused Rewards</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristjan</forename><surname>Arumae</surname></persName>
							<email>k.arumae@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<postCode>32816</postCode>
									<settlement>Orlando</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
							<email>feiliu@cs.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<postCode>32816</postCode>
									<settlement>Orlando</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reinforced Extractive Summarization with Question-Focused Rewards</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2018, Student Research Workshop</title>
						<meeting>ACL 2018, Student Research Workshop <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="105" to="111"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>105</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate a new training paradigm for extractive summarization. Traditionally, human abstracts are used to derive gold-standard labels for extraction units. However , the labels are often inaccurate, because human abstracts and source documents cannot be easily aligned at the word level. In this paper we convert human abstracts to a set of Cloze-style comprehension questions. System summaries are encouraged to preserve salient source content useful for answering questions and share common words with the abstracts. We use reinforcement learning to explore the space of possible extractive summaries and introduce a question-focused reward function to promote concise, fluent, and informative summaries. Our experiments show that the proposed method is effective. It surpasses state-of-the-art systems on the standard summarization dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We study extractive summarization in this work where salient word sequences are extracted from the source document and concatenated to form a summary ( <ref type="bibr" target="#b10">Nenkova and McKeown, 2011</ref>). Exist- ing supervised approaches to extractive summa- rization frequently use human abstracts to create annotations for extraction units <ref type="bibr">(Gillick and Favre, 2009;</ref><ref type="bibr" target="#b3">Li et al., 2013;</ref><ref type="bibr">Cheng and Lapata, 2016</ref>). E.g., a source word is labelled 1 if it appears in the abstract, 0 otherwise. Despite the usefulness, there are two issues with this scheme. First, a vast majority of the source words are tagged 0s, only a small portion are 1s. This is due to the fact that human abstracts are short and concise; they often contain words not present in the source. Second, <ref type="table">Table 1</ref>: Example source document, the top sentence of the abstract, and system-generated Cloze-style questions. Source content related to the abstract is italicized.</p><p>not all labels are accurate. Source words that are labelled 0 may be paraphrases, generalizations, or otherwise related to words in the abstracts. These source words are often mislabelled. Consequently, leveraging human abstracts to provide supervision for extractive summarization remains a challenge.</p><p>Neural abstractive summarization can alleviate this issue by allowing the system to either copy words from the source texts or generate new words from a vocabulary ( <ref type="bibr" target="#b16">Rush et al., 2015;</ref><ref type="bibr" target="#b8">Nallapati et al., 2016;</ref><ref type="bibr" target="#b17">See et al., 2017)</ref>. While the techniques are promising, they face other challenges, such as ensuring the summaries remain faithful to the orig- inal. Failing to reproduce factual details has been revealed as one of the main obstacles for neural abstractive summarization <ref type="bibr" target="#b0">(Cao et al., 2018;</ref><ref type="bibr" target="#b18">Song and Liu, 2018)</ref>. This study thus chooses to focus on neural extractive summarization.</p><p>We explore a new training paradigm for extrac- tive summarization. We convert human abstracts to a set of Cloze-style comprehension questions, where the question body is a sentence of the ab- stract with a blank, and the answer is an entity or a keyword. <ref type="table">Table 1</ref> shows an example. Because the questions cannot be answered by applying general world knowledge, system summaries are encour- aged to preserve salient source content that is rele- vant to the questions (≈ human abstract) such that the summaries can work as a document surrogate to predict correct answers. We use an attention mechanism to locate segments of a summary that are relevant to a given question so that the sum- mary can be used to answer multiple questions.</p><p>This study extends the work of ( <ref type="bibr" target="#b2">Lei et al., 2016</ref>) to use reinforcement learning to explore the space of extractive summaries. While the original work focuses on generating rationales to support super- vised classification, the goal of our study is to pro- duce fluent, generic document summaries. The question-answering (QA) task is designed to ful- fill this goal and the QA performance is only sec- ondary. Our research contributions can be summa- rized as follows:</p><p>• we investigate an alternative training scheme for extractive summarization where the summaries are encouraged to be semantically close to hu- man abstracts in addition to sharing common words;</p><p>• we compare two methods to convert human ab- stracts to Cloze-style questions and investigate its impact on QA and summarization perfor- mance. Our results surpass those of previous systems on a standard summarization dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This study focuses on generic summarization. It is different from the query-based summariza- tion <ref type="bibr">(Daumé III and Marcu, 2006;</ref><ref type="bibr">Dang and Owczarzak, 2008)</ref>, where systems are trained to select text pieces related to predefined queries. In this work we have no predefined queries but the system carefully generates questions from human abstracts and learns to produce generic summaries that are capable of answering all questions. Cloze questions have been used in reading com- prehension ( <ref type="bibr" target="#b15">Richardson et al., 2013;</ref><ref type="bibr" target="#b23">Weston et al., 2016;</ref><ref type="bibr" target="#b7">Mostafazadeh et al., 2016;</ref><ref type="bibr" target="#b13">Rajpurkar et al., 2016</ref>) to test the system's ability to perform rea- soning and language understanding. <ref type="bibr">Hermann et al. (2015)</ref> describe an approach to extract (context, question, answer) triples from news articles. Our work draws on this approach to automatically cre- ate questions from human abstracts.</p><p>Reinforcement learning (RL) has been recently applied to a number of NLP applications, includ- ing dialog generation ( <ref type="bibr" target="#b4">Li et al., 2017)</ref>, machine translation (MT) ( <ref type="bibr" target="#b14">Ranzato et al., 2016;</ref><ref type="bibr">Gu et al., 2018)</ref>, question answering ( <ref type="bibr">Choi et al., 2017)</ref>, and summarization and sentence simplification <ref type="bibr" target="#b24">(Zhang and Lapata, 2017;</ref><ref type="bibr" target="#b11">Paulus et al., 2017;</ref><ref type="bibr">Chen and Bansal, 2018;</ref><ref type="bibr" target="#b9">Narayan et al., 2018)</ref>. This study leverages RL to explore the space of possible ex- tractive summaries. The summaries are encour- aged to preserve salient source content useful for answering questions as well as sharing common words with the abstracts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>Given a source document X, our system generates a summary Y = (y 1 , y 2 , · · · , y |Y | ) by identifying consecutive sequences of words: y t is 1 if the t-th source word is included in the summary, 0 oth- erwise. In this section we investigate a question- oriented reward R(Y ) that encourages summaries to contain sufficient content useful for answering key questions about the document ( §3.1); we then use reinforcement learning to explore the space of possible extractive summaries ( §3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question-Focused Reward</head><p>We reward a summary if it can be used as a docu- ment surrogate to answer important questions. Let</p><formula xml:id="formula_0">{(Q k , e * k )} K k=1</formula><p>be a set of question-answer pairs for a source document, where e * k is the ground- truth answer corresponding to an entity or a key- word. We encode the question Q k into a vector:</p><formula xml:id="formula_1">q k = Bi-LSTM(Q k ) ∈ R d using a bidirectional LSTM,</formula><p>where the last outputs of the forward and backward passes are concatenated to form a ques- tion vector. We use the same Bi-LSTM to en- code the summary Y to a sequence of vectors:</p><formula xml:id="formula_2">(h S 1 , h S 2 , · · · , h S |S| ) = Bi-LSTM(Y ),</formula><p>where |S| is the number of words in the summary; h S t ∈ R d is the concatenation of forward and backward hidden states at time step t. <ref type="figure">Figure 1</ref> provides an illustra- tion of the system framework.</p><p>An attention mechanism is used to locate parts of the summary that are relevant to Q k . We de- fine α k,i ∝ exp(q k W a h S i ) to represent the impor- tance of the i-th summary word (h S i ) to answering the k-th question (q k ), characterized by a bilin- ear term <ref type="bibr" target="#b1">(Chen et al., 2016a)</ref>. A context vector c k is constructed as a weighted sum of all summary words relevant to the k-th question, and it is used to predict the answer. We define the QA reward R a (Y ) as the log-likelihood of correctly predict- c 2 @entity1: "Bayern Munich" @entity2: "Manchester United" <ref type="figure">Figure 1</ref>: System framework. The model uses an extractive summary as a document surrogate to answer important questions about the document. The questions are automatically derived from the human abstract.</p><p>ing all answers. {W a , W c } are learnable model parameters.</p><formula xml:id="formula_3">α k,i = exp(q k W a h S i ) |S| i=1 exp(q k W a h S i )<label>(1)</label></formula><formula xml:id="formula_4">c k = |S| i=1 α k,i h S i (2) P (e k |Y, Q k ) = softmax(W c c k ) (3) R a (Y ) = 1 K K k=1 log P (e * k |Y, Q k )<label>(4)</label></formula><p>In the following we describe approaches to ob- tain a set of question-answer pairs</p><formula xml:id="formula_5">{(Q k , e * k )} K k=1</formula><p>from a human abstract. In fact, this formula- tion has the potential to make use of multiple hu- man abstracts (subject to availability) in a unified framework; in that case, the QA pairs will be ex- tracted from all abstracts. According to Eq. (4), the system is optimized to generate summaries that preserve salient source content sufficient to answer all questions (≈ human abstract). We expect to harvest one question-answer pair from each sentence of the abstract. More are pos- sible, but the QA pairs will contain duplicate con- tent. There are a few other noteworthy issues. If we do not collect any QA pairs from a sentence of the abstract, its content will be left out of the sys- tem summary. It is thus crucial for the system to extract at least one QA pair from any sentence in an automatic manner. Further, the questions must not be answered by simply applying general world knowledge. We expect the adequacy of the sum- mary to have a direct influence on whether or not the questions will be correctly answered. Moti- vated by these considerations, we perform the fol- lowing steps. We split a human abstract to a set of sentences, identify an answer token from each sentence, then convert the sentence to a question by replacing the token with a placeholder, yield- ing a Cloze question. We explore two approaches to extract answer tokens:</p><p>• Entities. We extract four types of named enti- ties {PER, LOC, ORG, MISC} from sentences and treat them as possible answer tokens.</p><p>• Keywords. This approach identifies the ROOT word of a sentence dependency parse tree and treats it as a keyword-based answer token. Not all sentences contain entities, but every sentence has a root word; it is often the main verb of the sentence.</p><p>We obtain K question-answer pairs from each hu- man abstract, one pair per sentence. If there are less than K sentences in the abstract, the QA pairs of the top sentences will be duplicated, with the assumption that the top sentences are more impor- tant than others. If multiple entities reside in a sen- tence, we randomly pick one as the answer token; otherwise if there are no entities, we use the root word instead. To ensure that the extractive summaries are con- cise, fluent, and close to the original wording, we add additional components to the reward function:</p><formula xml:id="formula_6">(i) we define R s (Y ) = | 1 |Y |</formula><p>|Y | t=1 y t − δ| to re- strict the summary size. We require the percentage of selected source words to be close to a prede- fined threshold δ. This constraint works well at re- stricting length, with the average summary size ad- hering to this percentage; (ii) we further introduce R f (Y ) = |Y | t=2 |y t − y t−1 | to encourage the sum- maries to be fluent. This component is adopted from ( <ref type="bibr" target="#b2">Lei et al., 2016)</ref>, where few 0/1 switches between y t−1 and y t indicates the system is se- lecting consecutive word sequences; (iii) we en- courage system and reference summaries to share common bigrams. This practice has shown suc-cess in earlier studies <ref type="bibr">(Gillick and Favre, 2009)</ref>. R b (Y ) is defined as the percentage of reference bigrams successfully covered by the system sum- mary. These three components together ensure the well-formedness of extractive summaries. The fi- nal reward function R(Y ) is a linear interpolation of all the components; γ, α, β are coefficients and we describe their parameter tuning in §4.</p><formula xml:id="formula_7">R(Y )=R a (Y )+γR b (Y )−αR f (Y )−βR s (Y )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reinforcement Learning</head><p>In the following we seek to optimize a policy P (Y |X) for generating extractive summaries so that the expected reward E P (Y |X) [R(Y )] is max- imized. Taking derivatives of this objective with respect to model parameters θ involves repeat- edly sampling summariesˆYsummariesˆ summariesˆY = (ˆ y 1 , ˆ y 2 , · · · , ˆ y |Y | ) (illustrated in Eq. <ref type="formula" target="#formula_8">(6)</ref>). In this way reinforce- ment learning exploits the space of extractive sum- maries of a source document.</p><formula xml:id="formula_8">θ E P (Y |X) [R(Y )] = E P (Y |X) [R(Y ) θ log P (Y |X)] ≈ 1 N N n=1 R( ˆ Y (n) ) θ log P ( ˆ Y (n) |X)<label>(6)</label></formula><p>To calculate P (Y |X) and then samplê Y from it, we use a bidirectional LSTM to en- code a source document to a sequence of vectors:</p><formula xml:id="formula_9">(h D 1 , h D 2 , · · · , h D |X| ) = Bi-LSTM(X).</formula><p>Whether to include the t-th source word in the summary (ˆ y t ) thus can be decided based on h D t . However, we also want to accommodate the previous t-1 sampling decisions (ˆ y 1:t−1 ) to improve the fluency of the extractive summary. Following ( <ref type="bibr" target="#b2">Lei et al., 2016)</ref>, we introduce a single-direction LSTM en- coder whose hidden state s t tracks the sampling decisions up to time step t (Eq. 8). It represents the semantic meaning encoded in the current sum- mary. To sample the t-th word, we concatenate the two vectors [h D t ||s t−1 ] and use it as input to a feed- forward layer with sigmoid activation to estimatêestimatê y t ∼ P (y t |ˆy|ˆy 1:t−1 , X) (Eq. 7). P (y t |ˆy|ˆy 1:</p><formula xml:id="formula_10">t−1 , X) = σ(W h [h D t ||s t−1 ] + b h ) (7) s t = LSTM([h D t ||ˆy||ˆy t ], s t−1 )<label>(8)</label></formula><formula xml:id="formula_11">P ( ˆ Y |X) = |Y | t=1 P (ˆ y t |ˆy|ˆy 1:t−1 , X)<label>(9)</label></formula><p>Note that Eq. <ref type="formula">(7)</ref> can be pretrained using goldstan- dard summary sequence Y * = (y * 1 , y * 2 , · · · , y * |Y | ) to minimize the word-level cross-entropy loss,</p><formula xml:id="formula_12">System R-1 R-2 R-L</formula><p>LSA <ref type="bibr" target="#b20">(Steinberger and Jezek, 2004)</ref> 21.2 6.2 14.0 LexRank <ref type="bibr">(Erkan and Radev, 2004)</ref> 26.1 9.6 17.7 TextRank <ref type="bibr" target="#b6">(Mihalcea and Tarau, 2004)</ref> 23.3 7.7 15.8 SumBasic <ref type="bibr" target="#b22">(Vanderwende et al., 2007)</ref> 22.9 5.5 14.8 KL-Sum <ref type="bibr">(Haghighi and Vanderwende, 2009)</ref> 20.7 5.9 13.7 Distraction-M3 <ref type="bibr">(Chen et al., 2016b)</ref> 27.1 8.2 18.7 Seq2Seq w/ Attn <ref type="bibr" target="#b17">(See et al., 2017)</ref> 25.0 7.7 18.8 Pointer-Gen w/ Cov <ref type="bibr" target="#b17">(See et al., 2017)</ref> 29.9 10.9 21.1 Graph-based Attn <ref type="bibr" target="#b21">(Tan et al., 2017)</ref> 30.3 9.8 20.0</p><p>Extr+EntityQ (this paper) 31.4 11.5 21.7 Extr+KeywordQ (this paper) 31.7 11.6 21.5 where we set y * t as 1 if (x t , x t+1 ) is a bigram in the human abstract. For reinforcement learning, our goal is to optimize the policy P (Y |X) using the reward function R(Y ) ( §3.1) during the train- ing process. Once the policy P (Y |X) is learned, we do not need the reward function (or any QA pairs) at test time to generate generic summaries. Instead we choosê y t that yields the highest prob- abilityˆyabilityˆ abilityˆy t = arg max P (y t |ˆy|ˆy 1:t−1 , X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>All training, validation, and testing was performed using the CNN dataset ( <ref type="bibr">Hermann et al., 2015;</ref><ref type="bibr" target="#b8">Nallapati et al., 2016</ref>) containing news articles paired with human-written highlights (i.e., abstracts). We observe that a source article contains 29.8 sen- tences and an abstract contains 3.54 sentences on average. The train/valid/test splits contain 90,266, 1,220, 1,093 articles respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hyperparameters</head><p>The hyperparameters, tuned on the validation set, include the following: the hidden state size of the Bi-LSTM is 256; the hidden state size of the single-direction LSTM encoder is 30. Dropout rate <ref type="bibr" target="#b19">(Srivastava, 2013)</ref>, used twice in the sampling component, is set to 0.2. The minibatch size is set to 256. We apply early stopping on the vali- dation set, where the maximum number of epochs is set to 50. Our source vocabulary contains 150K words; words not in the vocabulary are replaced by the unk token. We use 100-dimensional word embeddings, initialized by GloVe ( <ref type="bibr" target="#b12">Pennington et al., 2014</ref>) and remain trainable. We set β = 2α and select the best α ∈ {10, 20, 50} and γ ∈ {5, 6, 7, 8} using the valid set (best value un- derlined). The maximum length of input is set to 100 words; δ is set to be 0.4 (≈40 words). We use the Adam optimizer ( <ref type="bibr">Kingma and Ba, 2015</ref>) with an initial learning rate of 1e-4 and halve the learn- ing rate if the objective worsens beyond a thresh- old (&gt; 10%). As mentioned we utilized a bigram based pretraining method. We found that this sta- bilized the training of the full model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>We compare our methods with state-of-the-art published systems, including both extractive and abstractive approaches (their details are summa- rized below). We experiment with two variants of our approach. "EntityQ" uses QA pairs whose an- swers are named entities. "KeywordQ" uses pairs whose answers are sentence root words. Accord- ing to the R-1, R-2, and R-L scores <ref type="bibr" target="#b5">(Lin, 2004</ref>) presented in <ref type="table" target="#tab_0">Table 2</ref>, both methods are superior to the baseline systems on the benchmark dataset, yielding 11.5 and 11.6 R-2 F-scores, respectively.</p><p>• LSA ( <ref type="bibr" target="#b20">Steinberger and Jezek, 2004</ref>) uses the la- tent semantic analysis technique to identify se- mantically important sentences.</p><p>• LexRank ( <ref type="bibr">Erkan and Radev, 2004</ref>) is a graph- based approach that computes sentence impor- tance based on the concept of eigenvector cen- trality in a graph representation of source sen- tences.</p><p>• TextRank ( <ref type="bibr" target="#b6">Mihalcea and Tarau, 2004</ref>) is an un- supervised graph-based ranking algorithm in- spired by algorithms PageRank and HITS.</p><p>• SumBasic ( <ref type="bibr" target="#b22">Vanderwende et al., 2007</ref>) is an ex- tractive approach that assumes words occurring frequently in a document cluster have a higher chance of being included in the summary.</p><p>• KL-Sum (Haghighi and Vanderwende, 2009) describes a method that greedily adds sentences to the summary so long as it decreases the KL divergence.</p><p>• Distraction-M3 ( <ref type="bibr">Chen et al., 2016b</ref>) trains the summarization model to not only attend to to specific regions of input documents, but also distract the attention to traverse different con- tent of the source document.</p><p>• Pointer-Generator (See et al., 2017) allows the system to not only copy words from the source text via pointing but also generate novel words through the generator.</p><p>• Graph-based Attention ( <ref type="bibr" target="#b21">Tan et al., 2017</ref>) in- troduces a graph-based attention mechanism to enhance the encoder-decoder framework.  <ref type="table">Table 3</ref>: Train/valid accuracy and R-2 F-scores when using varying numbers of QA pairs (K=1 to 5) in the reward func.</p><p>In <ref type="table">Table 3</ref>, we vary the number of QA pairs used per article in the reward function (K=1 to 5). The summaries are encouraged to contain compre- hensive content useful for answering all questions. When more QA pairs are used (K1→K5), we ob- serve that the number of answer tokens has in- creased and almost doubled: 23.7K (K1)→50.3K (K5) for entities as answers, and 7.3K→13.7K for keywords. The enlarged answer space has an im- pact on QA accuracies. When using entities as answers, the training accuracy is 34.8% (Q5) and validation is 15.4% (Q5), and there appears to be a considerable gap between the two. In contrast, the gap is quite small when using keywords as an- swers (27.5% and 21.9% for Q5), suggesting that using sentence root words as answers is a more vi- able strategy to create QA pairs.</p><p>Comparing to QA studies (Chen et al., 2016a), we remove the constraint that requires answer en- tities (or keywords) to reside in the source docu- ments. Adding this constraint improves the QA accuracy for a standard QA system. However, be- cause our system does not perform QA during test- ing (the question-answer pairs are not available for the test set) but only generate generic summaries, we do not enforce this requirement and report no testing accuracies. We observe that the R-2 scores only present minor changes from K1 to K5. We conjecture that more question-answer pairs do not make the summaries contain more comprehensive content because the input and the summary are rel- atively short; K=1 yields the best results.</p><p>In <ref type="table">Table 4</ref>, we present example system and ref- erence summaries. Our extractive summaries can be overlaid with the source documents to assist people with browsing through the documents. In this way the summaries stay true to the original and do not contain information that was not in the source documents.</p><p>Future work. We are interested in investigating</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Document</head><p>It was all set for a fairytale ending for record breaking jockey AP Mc- Coy. In the end it was a different but familiar name who won the Grand National on Saturday.</p><p>25-1 outsider Many Clouds, who had shown little form going into the race, won by a length and a half, ridden by jockey Leighton Aspell.</p><p>Aspell won last year's Grand National too, making him the first jockey since the 1950s to ride back-to-back winners on different horses.</p><p>"It feels wonderful, I asked big questions," Aspell said...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>25-1 shot Many Clouds wins Grand National</head><p>Second win a row for jockey Leighton Aspell First jockey to win two in a row on different horses since 1950s <ref type="table">Table 4</ref>: Example system summary and human abstract. The summary words are shown in bold in the source document.</p><p>approaches that automatically group selected sum- mary segments into clusters. Each cluster can cap- ture a unique aspect of the document, and clus- ters of text segments can be color-highlighted. In- spired by the recent work of <ref type="bibr" target="#b9">Narayan et al. (2018)</ref>, we are also interested in conducting the usability study to test how well the summary highlights can help users quickly answer key questions about the documents. This will provide an alternative strat- egy for evaluating our proposed method against both extractive and abstractive baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we explore a new training paradigm for extractive summarization. Our system converts human abstracts to a set of question-answer pairs. We use reinforcement learning to exploit the space of extractive summaries and promote summaries that are concise, fluent, and adequate for answer- ing questions. Results show that our approach is effective, surpassing state-of-the-art systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 : Results on the CNN test set (full-length F1 scores).</head><label>2</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their valu-able suggestions. This work is in part supported by an unrestricted gift from Bosch Research. Krist-jan Arumae gratefully acknowledges a travel grant provided by the National Science Foundation.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Faithful to the original: Fact aware neural abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A thorough examination of the cnn/daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rationalizing neural predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Document summarization via guided sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adversarial learning for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ROUGE: a package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Workshop on Text Summarization Branches Out</title>
		<meeting>ACL Workshop on Text Summarization Branches Out</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">TextRank: Bringing order into text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-tosequence RNNs and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ranking sentences for extractive summarization with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT)</title>
		<meeting>the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Automatic summarization. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MCTest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A neural attention model for sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structure-infused copy mechanisms for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING)</title>
		<meeting>the International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving Neural Networks with Dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Toronto, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using latent semantic analysis in text summarization and summary evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Steinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Jezek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISIM</title>
		<meeting>ISIM</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Abstractive document summarization with a graphbased attentional neural model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond SumBasic: Taskfocused summarization with sentence simplification and lexical expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1606" to="1618" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards AI-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sentence simplification with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
