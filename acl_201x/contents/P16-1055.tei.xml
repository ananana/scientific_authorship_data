<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><forename type="middle">Tejasvi</forename><surname>Chaganty</surname></persName>
							<email>chaganty@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<email>pliang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="578" to="587"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>How much is 131 million US dollars? To help readers put such numbers in context , we propose a new task of automatically generating short descriptions known as perspectives, e.g. &quot;$131 million is about the cost to employ everyone in Texas over a lunch period&quot;. First, we collect a dataset of numeric mentions in news articles , where each mention is labeled with a set of rated perspectives. We then propose a system to generate these descriptions consisting of two steps: formula construction and description generation. In construction, we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on familiarity , numeric proximity and semantic compatibility. In generation, we convert a formula into natural language using a sequence-to-sequence recurrent neu-ral network. Our system obtains a 15.2% F 1 improvement over a non-compositional baseline at formula construction and a 12.5 BLEU point improvement over a baseline description generation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When posed with a mention of a number, such as "Cristiano Ronaldo, the player who Madrid ac- quired for <ref type="bibr">[. .</ref> . ] a $131 million" <ref type="figure">(Figure 1</ref>), it is often difficult to comprehend the scale of large (or small) absolute values like $131 million <ref type="bibr" target="#b16">(Paulos, 1988;</ref><ref type="bibr" target="#b17">Seife, 2010)</ref>. Studies have shown that pro- viding relative comparisons, or perspectives, such as "about the cost to employ everyone in Texas over a lunch period" significantly improves com- prehension when measured in terms of memory re- tention or outlier detection ( <ref type="bibr" target="#b2">Barrio et al., 2016)</ref>. Figure 1: An overview of the perspective gen- eration task: given a numeric mention, generate a short description (a perspective) that allows the reader to appreciate the scale of the mentioned number. In our system, we first construct a for- mula over facts in our knowledge base and then generate a description of that formula.</p><p>Previous work in the HCI community has relied on either manually generated perspectives ( <ref type="bibr" target="#b2">Barrio et al., 2016</ref>) or present a fact as is from a knowl- edge base <ref type="bibr" target="#b8">(Chiacchieri, 2013)</ref>. As a result, these approaches are limited to contexts in which a rele- vant perspective already exists.</p><p>In this paper, we generate perspectives by com- posing facts from a knowledge base. For example, we might describe $100,000 to be "about twice the median income for a year", and describe $5 mil- lion to be the "about how much the average per- son makes over their lifetime". Leveraging com- positionality allows us to achieve broad coverage of numbers from a relatively small collection of familiar facts, e.g. median income and a person's lifetime.</p><p>Using compositionality in perspectives is also concordant with our understanding of how people learn to appreciate scale. <ref type="bibr" target="#b11">Jones and Taylor (2009)</ref> find that students learning to appreciate scale do so mainly by anchoring with familiar concepts, e.g. $50,000 is slightly less than the median income in the US, and by unitization, i.e. improvising a system of units that is more relatable, e.g. using the Earth as a measure of mass when describing the mass of Jupiter to be that of 97 Earths. Here, compositionality naturally unitizes the constituent facts: in the examples above, money was unitized in terms of median income, and time was unitized in a person's lifetime. Unitization and anchoring have also been proposed by <ref type="bibr" target="#b7">Chevalier et al. (2013)</ref> as the basis of a design methodology for construct- ing visual perspectives called concrete scales.</p><p>When generating compositional perspectives, we must address two key challenges: construct- ing familiar, relevant and meaningful formulas and generating easy-to-understand descriptions or perspectives. We tackle the first challenge us- ing an overgenerate-and-rank paradigm, selecting formulas using signals from familiarity, composi- tionality, numeric proximity and semantic similar- ity. We treat the second problem of generation as a translation problem and use a sequence-to- sequence recurrent neural network (RNN) to gen- erate perspectives from a formula.</p><p>We evaluate individual components of our sys- tem quantitatively on a dataset collected using crowdsourcing. Our formula construction method improves on F 1 over a non-compositional baseline by about 17.8%. Our generation method improves over a simple baseline by 12.5 BLEU points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem statement</head><p>The input to the perspective generation task is a sentence s containing a numeric mention x: a span of tokens within the sentence which describes a quantity with value x.value and of unit x.unit. In <ref type="figure">Figure 1</ref>, the numeric mention x is "$131 million", x.value = 1.31e8 and x.unit = $. The output is a description y that puts x in perspective.</p><p>We have access to a knowledge base K with nu- meric tuples t = (t.value, t.unit, t.description). <ref type="table">Table 1</ref> has a few examples of tuples in our knowl- edge base. Units (e.g. $/per/yr) are fractions com- posed either of fundamental units (length, area, volume, mass, time) or of ordinal units (e.g. cars,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description</head><p>Value <ref type="table" target="#tab_3">Unit   cost of an employee  71e3  $/year/person  population of Texas  27e3 person  number of employees at Google 57e3 person  average household size  2.54 person  time taken for a basketball game 60 minute  average lifetime for a person  79 year  a week  1 week  time taken for lunch</ref> 30 minute cost of property in the Bay area 1e3 $/ft 2 area of a city block 10e3 m 2 <ref type="table">Table 1</ref>: A subset of our knowledge base of nu- meric tuples. Tuples with fractional units (e.g. $/ft 2 ) can be combined with other tuples to create formulas.</p><p>people, etc.). The first step of our task, described in Section 4, is to construct a formula f over numeric tuples in K that has the same value and unit as the nu- meric mention x. A valid formula comprises of an arbitrary multiplier f.m and a sequence of tuples f.tuples. The value of a formula, f.value, is sim- ply the product of the multiplier and the values of the tuples, and the unit of the formula, f.unit, is the product of the units of the tuples. In <ref type="figure">Figure 1</ref>, the formula has a multiplier of 1 and is composed of tuples 1 , 2 and 3 ; it has a value of 1.3e8 and a unit of $.</p><p>The second step of our task, described in Sec- tion 5, is to generate a perspective y, a short noun phrase that realizes f . Typically, the utterance will be formed using variations of the descriptions of the tuples in f.tuples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset construction</head><p>We break our data collection task into two steps, mirroring formula selection and description gen- eration: first, we collect descriptions of formulas constructed exhaustively from our knowledge base (for generation), and then we use these descrip- tions to collect preferences for perspectives (for construction).</p><p>Collecting the knowledge base. We manually constructed a knowledge base with 142 tuples and 9 fundamental units 1 from the United States Bu-reau of Statistics, the orders of magnitude topic on Wikipedia and other Wikipedia pages. The facts chosen are somewhat crude; for example, though "the cost of an employee" is a very context depen- dent quantity, we take its value to be the median cost for an employer in the United States, $71,000. Presenting facts at a coarse level of granularity makes them more familiar to the general reader while still being appropriate for perspective gen- eration: the intention is to convey the right scale, not necessarily the precise quantity.</p><p>Collecting numeric mentions. We collected 53,946 sentences containing numeric mentions from the newswire section of LDC2011T07 using simple regular expression patterns like</p><formula xml:id="formula_0">$([0-9]+(,[0-9]+) * (.[0-9]+)? ((hundred)|(thousand)|(million)| (billion)|(trillion))).</formula><p>The values and units of the numeric mentions in each sentence were normalized and converted to fundamental units (e.g. from miles to length). We then randomly selected up to 200 mentions of each of the 9 types in bins with boundaries 10 −3 , 1, 10 3 , 10 6 , 10 9 , 10 12 leading to 4,931 mentions that are stratified by unit and magni- tude. <ref type="bibr">2</ref> Finally, we chose mentions which could be described by at least one numeric expression, resulting in the 2,041 mentions that we use in our experiments ( <ref type="figure">Figure 2</ref>). We note that there is a slight bias towards mentions of money and people because these are more common in the news corpus.</p><p>Generating formulas. Next, we exhaustively generate valid formulas from our knowledge base. We represent the knowledge base as a graph over units with vertices and edges annotated with tu- ples ( <ref type="figure">Figure 3</ref>). Every vertex in this graph is la- beled with a unit u and contains the set of tuples with this unit: {t ∈ K : t.unit = u}. Addition- ally, for every vertex in the graph with a unit of the form u 1 /u 2 , where u 2 has no denominator, we add an edge from u 1 /u 2 to u 1 , annotated with all tuples of type u 2 : in <ref type="figure">Figure 3</ref> we add an edge from money/person to money annotated with the three person tuples in <ref type="table">Table 1</ref>. The set of formulas with unit u is obtained by enumerating all paths in the graph which terminate at the vertex u. The mul- tiplier of the formula is set so that the value of were well represented in the corpus.</p><p>2 Some types had fewer than 200 mentions for some bins.  <ref type="table">Table 1.</ref> the formula matches the value of the mention. For example, the formula in <ref type="figure">Figure 1</ref> was constructed by traversing the graph from money/time/person to money: we start with a tuple in money/time/person (cost of an employee) and then multiply by a tu- ple with unit time (time for lunch) and then by unit person (population of Texas), thus traversing two edges to arrive at money. Using the 142 tuples in our knowledge base, we generate a total of 1,124 formulas sans multiplier.</p><p>Collecting descriptions of formulas. The main goal of collecting descriptions of formulas is to train a language generation system, though these descriptions will also be useful while collecting training data for formula selection. For every unit in our knowledge base and every value in the set {10 −7 , 10 −6 . . . , 10 10 }, we generated all valid for- mulas. We further restricted this set to formulas with a multiplier between 1/100 and 100, based on the rationale that human cognition of scale sharply drops beyond an order of magnitude (Tretter et al., 2006). In total, 5000 formulas were presented to crowdworkers on Amazon Mechanical Turk, with a prompt asking them to rephrase the formula as an English expression <ref type="figure" target="#fig_2">(Figure 4</ref>). <ref type="bibr">3</ref> We obtained 5-7 descriptions of each formula, leading to a total of 31,244 unique descriptions.</p><p>Collecting data on formula preference. Fi- nally, given a numeric mention, we ask crowd- workers which perspectives from the description dataset they prefer. Note that formulas gener- ated for a particular mention may differ in mul- tiplier with a formula in the description dataset. We thus relax our constraints on factual accuracy while collecting this formula preference dataset: for each mention x, we choose a random perspec- tive from the description dataset described above corresponding to a formula whose value is within a factor of 2 from the mention's value, x.value. A smaller factor led to too many mentions without a valid comparison, while a larger one led to blatant factual inaccuracies. The perspectives were par- titioned into sets of four and displayed to crowd- workers along with a "None of the above" option with the following prompt: "We would like you to pick up to two of these descriptions that are useful in understanding the scale of the highlighted num- ber" ( <ref type="figure" target="#fig_3">Figure 5</ref>). A formula is rated to be useful by simple majority. <ref type="bibr">4</ref> Figure 6 provides a summary of the dataset col- lected, visualizing how many formulas are use- ful, controlling for the size of the formula. The exhaustive generation procedure produces a large number of spurious formulas like "20 × trash gen- erated in the US × a minute × number of employ- ees on Medicare". Nonetheless, compositional formulas are quite useful in the appropriate con- text; <ref type="table">Table 2</ref> presents some mentions with highly rated perspectives and formulas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Formula selection</head><p>We now turn to the first half of our task: given a numeric mention x and a knowledge base K, select a formula f over K with the same value and unit as the mention. It is easy to generate a very large number of formulas for any mention. For the ex- ample, "Cristiano Ronaldo, the player who Madrid acquired for [. . . ] $131 million.", the small knowl- edge base in <ref type="table">Table 1</ref> can generate the 12 different formulas, 5 including the following:</p><p>1. 1 × the cost of an employee × the population of Texas × the time taken for lunch.</p><p>2. 400 × the cost of an employee × average household size × a week.</p><p>3. 1 × the cost of an employee × number of employees at Google × a week.</p><p>4. 1 × cost of property in the Bay Area × area of a city block.</p><p>Some of the formulas above are clearly worse than others: the key challenge is picking a for- mula that will lead to a meaningful and relevant perspective.</p><p>Criteria for ranking formulas. We posit the following principles to guide our choice in fea- tures <ref type="table">(Table 3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence</head><p>That's about . . . Formula</p><p>The Billings-based Stillwater Min- ing produced 601,000 ounces of platinum.</p><p>4 times the weight of an ele- phant. 4 × weight of an elephant.</p><p>Authorities estimate there are about 60 million guns in Yemen. twice the gun ownership of the population of Texas 2 × gun ownership × popula- tion of Texas Water is flowing into Taihu lake at a rate of 150 cubic meters per sec- ond.</p><p>how much water would flow from a tap left on for a week.</p><p>rate of flow of water from tap × a week</p><p>The bank had held auctions, sell- ing around US$1 billion worth of three-month bills.</p><p>half the cost of employing the population of Texas for a work day.</p><p>1/2 × cost of an employee × time taken for a work day × population of Texas The government <ref type="bibr">[s]</ref> have promised to rent about 1.2 million sq. feet.</p><p>the area of forest logged in a single minute 90 × area of forest logged × a minute <ref type="table">Table 2</ref>: Examples of numeric mentions, perspectives and their corresponding formulas in the dataset. All the examples except the last one are rated to be useful by crowdworkers. Non-compositional perspectives with a single tu- ple are broadly useful. Useful compositional per- spectives tend to be more context-specific than non-compositional ones, and many of the formu- las that can be generated from the knowledge base are spurious.</p><p>Proximity: A numeric perspective should be within an order of magnitude of the mentioned value. Conception of scale quickly fails with quantities that exceed "human scales" <ref type="bibr" target="#b20">(Tretter et al., 2006</ref>): numbers that are significantly away from 1/10 and 10. We use this principle to prune formulas with multipliers not in the range [1/100, 100] (e.g. example 2 above) and introduce features for numeric proximity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>Features # Proximity sign(log(f.m)), | log(f.m)|</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Familiarity I[t]</head><p>142 Compatibility I <ref type="bibr">[t, t ]</ref> 20022 Similarity wvec(s) wvec(t.description) 1 <ref type="table">Table 3</ref>: Feature templates used to score a formu- las f and their counts (#), where f.m is the for- mula's multiplier and t, t ∈ f.tuples are tuples in the formula.</p><p>Familiarity: A numeric perspective should be composed of concepts familiar to the reader. The most common technique cited by those who do well at scale cognition tests is reasoning in terms of familiar objects <ref type="bibr" target="#b20">(Tretter et al., 2006;</ref><ref type="bibr" target="#b11">Jones and Taylor, 2009;</ref><ref type="bibr" target="#b7">Chevalier et al., 2013)</ref>. Intuitively, the average American reader may not know ex- actly how many people are in Texas, but is familiar enough with the quantity to effectively reason us- ing Texas' population as a unit. On the other hand, it is less likely that the same reader is familiar with even the concept of Angola's population.</p><p>Of course, because it is so personal, familiarity is difficult to capture. With additional information about the reader, e.g. their location, it is possible to personalize the chosen tuples ( <ref type="bibr" target="#b12">Kim et al., 2016)</ref>. Without this information, we back off to a global preference on tuples by using indicator features for each tuple in the formula.   <ref type="table">Table 4</ref>: The top three examples outputted by the ranking system with the scores reported by the system.</p><p>Compatibility: Similarly, some tuple combi- nations are more natural ("median income × a month") while others are less so ("weight of a per- son × population of Texas"). We model compati- bility between tuples in a formula using an indica- tor feature.</p><p>Similarity: A numeric perspective should be relevant to the context. Apart from helping with scale cognition, a perspective should also place the mentioned quantity in appropriate context: for ex- ample, NASA's budget of $17 billion could be de- scribed as 0.1% of the United States' budget or the amount of money it could cost to feed Los Angeles for a year. While both perspectives are appropri- ate, the former is more relevant than the latter.</p><p>We model context relevance using word vector similarity between the tuples of the formula and the sentence containing the mention as a proxy for semantic similarity. Word vectors for a sentence or tuple description are computed by taking the mean of the word vectors for every non-stop-word token. The word vectors at the token level are computed using word2vec <ref type="bibr" target="#b15">(Mikolov et al., 2013</ref>).</p><p>Evaluation. We train a logistic regression clas- sifier using the features described in <ref type="table">Table 3</ref> using the perspective ratings collected in Section 3. Re- call that the formula for each perspective in the dataset is assigned a positive ("useful") label if it was labeled to be useful to the majority of the workers. <ref type="table" target="#tab_3">Table 5a</ref> presents results on classifying formulas as useful with a feature ablation. <ref type="bibr">6</ref> Familiarity and compatibility are the most use- ful features when selecting formulas, each hav- ing a significant increase in F 1 over the proximity baseline. There are minor gains from combining these two features. On the other hand, semantic similarity does not affect performance relative to the baseline. We find that this is mainly due to the disproportionate number of unfamiliar formu- las present in the dataset that drown out any sig- nal. <ref type="table">Table 4</ref> presents two examples of the system's ranking of formulas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Perspective generation</head><p>Our next goal is to generate natural language de- scriptions, also known as perspectives, given a formula. Our approach models the task as a sequence-to-sequence translation task from for- mulas to natural language. We first describe a rule- based baseline and then describe a recurrent neural network (RNN) with an attention-based copying mechanism (Jia and Liang, 2016).</p><p>Baseline. As a simple approach to generate per- spectives, we just combine tuples in the formula with the neutral prepositions of and for, e.g. "1/5th of the cost of an employee for the population of Texas for the time taken for lunch."</p><p>Sequence-to-sequence RNN. We use formula- perspective pairs from the dataset to create a sequence-to-sequence task: the input is composed using the formula's multiplier and descriptions of its tuples connected with the symbol '*'; the out- put is the perspective <ref type="figure">(Figure 7)</ref>.</p><p>Our system is based on the model described in <ref type="bibr" target="#b10">Jia and Liang (2016)</ref>. Given a sequence of input tokens (x = (x i )), the model computes a context- dependent vector (b = (b i )) for each token using a bidirectional RNN with LSTM units. We then generate the output sequence (y j ) left to right as follows. At each output position, we have a hid- den state vector (s j ) which is used to produce an "attention" distribution (α j = (α ji )) over input tokens: α ji = Attend(s j , b i ). This distribution is used to generate the output token and update the hidden state vector. To generate the token, we ei-  73.8 70.3 72.1 71.5 68.9 70.1 F + C + P † 73.8 70.3 72.1 71.5 68.9 70.1 F + C + P + S † 73.8 70.3 72.0 71.4 68.6 69.9</p><p>(a) the formula construction system. Precision, Recall and F1 are cross- validated on 10-folds. * significant F1 versus P and S with p &lt; 0.01. + significant F1 versus P, S and F with p &lt; 0.01. † significant F1 versus P, S, F and C with p &lt; 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Train (b) the description generation system. * significant BLEU score versus the baseline with p &lt; 0.01.  Figure 7: We model description generation as a sequence transduction task, with input as formu- las (at bottom) and output as perspectives (at top). We use a RNN with an attention-based copying mechanism.</p><p>ther sample a word from the current state or copy a word from the input using attention. Allowing our model to copy from the input is helpful for our task, since many of the entities are repeated verba- tim in both input and output. We refer the reader to Jia and Liang (2016) for more details.</p><p>Evaluation. We split the perspective descrip- tion dataset into a training and test set such that no formula in the test set contains the same set of tuples as a formula in the training set. 7 Ta- ble 5b compares the performance of the base- line and sequence-to-sequence RNN using BLEU.</p><p>The sequence-to-sequence RNN performs signif- icantly better than the baseline, producing more natural rephrasings. <ref type="table">Table 6</ref> shows some output generated by the system (see <ref type="table">Table 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Human evaluation</head><p>In addition to the automatic evaluations for each component of the system, we also ran an end-to- end human evaluation on an independent set of 211 mentions collected using the same methodol- ogy described in Section 3. Crowdworkers were asked to choose between perspectives generated by our full system (LR+RNN) and those generated by the baseline of picking the numerically closest tuple in the knowledge base (BASELINE). They could also indicate if either both or none of the shown perspectives appeared useful. 8 <ref type="table">Table 7</ref> summarizes the results of the evalua- tion and an error analysis conducted by the au- thors. Errors were characterized as either be- ing errors in generation (e.g. <ref type="table">Table 6</ref>) or viola- tions of the criteria in selecting good formulas de- scribed in Section 4 <ref type="table">(Table 7c</ref>). The other category mostly contains cases where the output generated by LR+RNN appears reasonable by the above cri- teria but was not chosen by a majority of workers. A few of the mentions shown did not properly de- scribe a numeric quantity, e.g. ". . . claimed respon- sibility for a 2009 gun massacre . . . " and were la- beled invalid mentions. The most common error is the selection of a formula that is not contextu- ally relevant to the mentioned text because no such Input formula Generated perspective 7 × the cost of an employee × a week 7 times the cost of employing one person for one week 1/10 × the cost of an employee × the population of California × the time taken for a football game one tenth the cost of an employee during a foot- ball game by the population of California 1 × coffee consumption × a minute × population of the world the amount of coffee consumed in one minute on the world 6 × weight of a person × population of California six times the weight of the people who is worth <ref type="table">Table 6</ref>: Examples of perspectives generated by the sequence-to-sequence RNN. The model is able to capture rephrasings of fact descriptions and reordering of the facts. However, it often confuses preposi- tions and, very rarely, can produce nonsensical utterances.  <ref type="table">Table 7</ref>: Results of an end-to-end human evaluation of the output produced by our perspective generation system (LR+RNN) and a baseline (BASELINE) that picks the numerically closest tuple in the knowledge base for each mention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mention</head><p>Perspective (that's about. . . )</p><p>+ In 2007, Turkmenistan exported 50 billion cu- bic meters of gas to Russia. the amount of oil produced by the US during a lifetime + It can carry up to 10 nuclear warheads and has a range of 8,000 km. the distance from San Francisco to Beijing -the 2.7 million square feet that Mission Bay's largest developer is entitled to build twice the area of forest logged in a minute -Las Vegas Sands claims the 10.5 million square feet is the largest building in Asia. one half of an area of an average farm <ref type="table">Table 8</ref>: Examples of perspectives generated by our system that frame the mentioned quantity to be larger or smaller (top to bottom) than initially the authors thought.</p><p>formula exists within the knowledge base (within an order of magnitude of the mentioned value): a larger knowledge base would significantly de- crease these errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work and discussion</head><p>We have proposed a new task of perspective gen- eration. Compositionality is the key ingredient of our approach, which allows us synthesize informa- tion across multiple sources of information. At the same time, compositionality also poses problems for both formula selection and description genera- tion.</p><p>On the formula selection side, we must com- pose facts that make sense. For semantic com- patibility between the mention and description, we have relied on simple word vectors ( <ref type="bibr" target="#b15">Mikolov et al., 2013)</ref>, but more sophisticated forms of semantic relations on larger units of text might yield better results <ref type="bibr" target="#b5">(Bowman et al., 2015)</ref>.</p><p>On the description generation side, there is a long line of work in generating natural language descriptions of structured data or logical forms <ref type="bibr" target="#b22">Wong and Mooney (2007)</ref>; <ref type="bibr" target="#b6">Chen and Mooney (2008)</ref>; <ref type="bibr" target="#b13">Lu and Ng (2012)</ref>; <ref type="bibr" target="#b0">Angeli et al. (2010)</ref>. We lean on the recent developments of neural sequence-to-sequence models <ref type="bibr" target="#b18">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b14">Luong et al., 2015)</ref>. Our problem bears some similarity to the semantic parsing work of <ref type="bibr" target="#b21">Wang et al. (2015)</ref>, who connect generated canonical utterances (representing logi- cal forms) to real utterances.</p><p>If we return to our initial goal of helping peo- ple understand numbers, there are two important directions to explore. First, we have used a small knowledge base, which limits the coverage of per- spectives we can generate. Using Freebase <ref type="bibr" target="#b4">(Bollacker et al., 2008)</ref> or even open information ex- traction <ref type="bibr" target="#b9">(Fader et al., 2011</ref>) would dramatically in- crease the number of facts and therefore the scope of possible perspectives.</p><p>Second, while we have focused mostly on ba- sic compatibility, it would be interesting to explore more deeply how the juxtaposition of facts affects framing. <ref type="table">Table 8</ref> presents several examples gener- ated by our system that frame the mentioned quan- tities to be larger or smaller than the authors orig- inally thought. We think perspective generation is an exciting setting to study aspects of numeric framing <ref type="bibr" target="#b19">(Teigen, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility</head><p>All code, data, and experiments for this paper are avail- able on the CodaLab platform at https: //worksheets.codalab.org/worksheets/ 0x243284b4d81d4590b46030cdd3b72633/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Cristiano Ronaldo, the player who Madrid acquired for [. . . ] $131 million. $1.3e8 ≈ 71e3 $/per/yr 1 × 27e6 per 2 × 30 min 3 1 = cost of an employee 2 = population of Texas 3 = time taken for lunch about the cost to employ everyone in Texas over a lunch period. construction generation mention formula perspective</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: A histogram of the absolute values of numeric mentions by type. There are 100-300 mentions of each unit. person (3 tuples)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A screenshot of the crowdsourced task to generate natural language descriptions, or perspectives, from formulas.</figDesc><graphic url="image-1.png" coords="4,72.00,62.81,218.26,104.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A screenshot of the crowdsourced task to identify which formulas are useful to crowdworkers in understanding the highlighted mentioned number.</figDesc><graphic url="image-2.png" coords="4,307.28,62.80,218.27,164.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: A histogram comparing formula length to ratings of usefulness (clipped for readability). Non-compositional perspectives with a single tuple are broadly useful. Useful compositional perspectives tend to be more context-specific than non-compositional ones, and many of the formulas that can be generated from the knowledge base are spurious.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Formula</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>7</head><label></label><figDesc>* the cost of an employee * a week x i input bidirectional RNN b i s j attend α ji output copy 7 times the cost of employing one person for one week y j output input encoding state vector attention vector</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Score Studies estimate 36,000 people die on aver- age each year from seasonal flu.</figDesc><table>1/4 × global death rate × a day 
0.67 
5 × death rate in the US × a day 
0.64 
1/3 × number of employees at Mi-
crosoft 
0.60 

Gazprom's exports to Europe [. . . ] will total 
60 billion cubic meters . . . 

oil produced by the US × average life-
time 
0.78 

average coffee consumption × popula-
tion of the world × average lifetime 
0.78 

2 × average coffee consumption × pop-
ulation of Asia × average lifetime 
0.73 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 : Evaluation of perspective generation subsystems.</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Namely, length, area, volume, time, weight, money, people, cars and guns. These units were chosen because they</note>

			<note place="foot" n="3"> Crowdworkers were paid $0.08 per description. 4 Crowdworkers were paid $0.06 to vote on each set of perspectives.</note>

			<note place="foot" n="5"> The full knowledge base described in Section 3 can generate 242 formulas with the unit money (sans multiplier).</note>

			<note place="foot" n="6"> Significance results are computed by the bootstrap test as described in Berg-Kirkpatrick et al. (2012) using the output of classifiers trained on the entire training set.</note>

			<note place="foot" n="7"> Note that formulas with the same set of tuples can occur multiple times in the either the training or test set with different multipliers.</note>

			<note place="foot" n="8"> Crowdworkers were paid $0.06 per to choose a perspective for each mention. Each mention and set of perspectives were presented to 5 crowdworkers.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Glen Chiacchieri for providing us information about the Dictionary of Numbers, Maneesh Agarwala for useful dis-cussions and references, Robin Jia for sharing code for the sequence-to-sequence RNN, and the anonymous reviewers for their constructive feed-back. This work was partially supported by the Sloan Research fellowship to the second author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A simple domain-independent probabilistic approach to generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learn586 ing to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving the comprehension of numbers in the news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Barrio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hofman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Human Factors in Computing Systems (CHI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical investigation of statistical significance in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burkett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="995" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Management of Data (SIGMOD)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to sportscast: A test of grounded language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using concrete scales: A practical framework for effective visual depiction of complex measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vuillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2426" to="2435" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chiacchieri</surname></persName>
		</author>
		<ptr target="http://www.dictionaryofnumbers.com/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identifying relations for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Developing a sense of scale: Looking backward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Research in Science Teaching</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="460" to="475" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating personalized spatial analogies for distances and areas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Human Factors in Computing Systems (CHI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A probabilistic forestto-string model for language generation from typed lambda calculus expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1611" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Innumeracy: Mathematical illiteracy and its consequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Paulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Macmillan</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Proofiness: How you&apos;re being fooled by the numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seife</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Penguin</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Framing of numeric quantities. The Wiley Blackwell Handbook of Judgment and Decision Making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Teigen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="568" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Accuracy of scale conceptions in science: Mental maneuverings across many orders of spatial magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Tretter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Minogue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Research in Science Teaching</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1061" to="1085" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building a semantic parser overnight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generation by inverting a semantic parser that uses statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
