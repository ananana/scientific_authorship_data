<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Addressing Limited Data for Textual Entailment Across Domains</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Shivade</surname></persName>
							<email>shivade@cse.ohio-state.edu †</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="laboratory">IBM T. J. Watson Research Center</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<addrLine>1101 Kitchawan Road</addrLine>
									<postCode>43210, 10598</postCode>
									<settlement>Columbus, Yorktown Heights</settlement>
									<region>OH, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preethi</forename><surname>Raghavan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="laboratory">IBM T. J. Watson Research Center</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<addrLine>1101 Kitchawan Road</addrLine>
									<postCode>43210, 10598</postCode>
									<settlement>Columbus, Yorktown Heights</settlement>
									<region>OH, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="laboratory">IBM T. J. Watson Research Center</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<addrLine>1101 Kitchawan Road</addrLine>
									<postCode>43210, 10598</postCode>
									<settlement>Columbus, Yorktown Heights</settlement>
									<region>OH, NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Addressing Limited Data for Textual Entailment Across Domains</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1245" to="1255"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We seek to address the lack of labeled data (and high cost of annotation) for textual entailment in some domains. To that end, we first create (for experimental purposes) an entailment dataset for the clinical domain , and a highly competitive supervised entailment system, ENT, that is effective (out of the box) on two domains. We then explore self-training and active learning strategies to address the lack of labeled data. With self-training, we successfully exploit unlabeled data to improve over ENT by 15% F-score on the newswire domain, and 13% F-score on clinical data. On the other hand, our active learning experiments demonstrate that we can match (and even beat) ENT using only 6.6% of the training data in the clinical domain, and only 5.8% of the training data in the newswire domain.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Textual entailment is the task of automatically de- termining whether a natural language hypothesis can be inferred from a given piece of natural lan- guage text. The RTE challenges <ref type="bibr" target="#b0">(Bentivogli et al., 2009;</ref><ref type="bibr" target="#b2">Bentivogli et al., 2011</ref>) have spurred considerable research in textual entailment over newswire data. This, along with the availability of large-scale datasets labeled with entailment in- formation ( <ref type="bibr" target="#b4">Bowman et al., 2015)</ref>, has resulted in a variety of approaches for textual entailment recog- nition. * This work was conducted during an internship at IBM A variation of this task, dubbed textual entail- ment search, has been the focus of RTE-5 and sub- sequent challenges, where the goal is to find all sentences in a corpus that entail a given hypoth- esis. The mindshare created by those challenges and the availability of the datasets has spurred many creative solutions to this problem. How- ever, the evaluations have been restricted primarily to these datasets, which are in the newswire do- main. Thus, much of the existing state-of-the-art research has focused on solutions that are effective in this domain.</p><p>It is easy to see though, that entailment search has potential applications in other domains too. For instance, in the clinical domain we imagine entailment search can be applied for clinical trial matching as one example. Inclusion criteria for a clinical trial (for e.g., patient is a smoker) be- come the hypotheses, and the patient's electronic health records are the text for entailment search. Clearly, an effective textual entailment search sys- tem could possibly one day fully automate clinical trial matching.</p><p>Developing an entailment system that works well in the clinical domain and, thus, automates this matching process, requires lots of labeled data, which is extremely scant in the clinical do- main. Generating such a dataset is tedious and costly, primarily because it requires medical do- main expertise. Moreover, there are always pri- vacy concerns in releasing such a dataset to the community. Taking this into consideration, we in- vestigate the problem of textual entailment in a low-resource setting.</p><p>We begin by creating a dataset in the clinical domain, and a supervised entailment system that is competitive on multiple domains -newswire as well as clinical. We then present our work on self- training and active learning to address the lack of a large-scale labeled dataset. Our self-training sys- tem results in significant gains in performance on clinical (+13% F-score) and on newswire (+15% F-score) data. Further, we show that active learn- ing with uncertainty sampling reduces the number of required annotations for the entailment search task by more than 90% in both domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Recognizing Textual Entailment (RTE) shared tasks ( <ref type="bibr" target="#b8">Dagan et al., 2013</ref>) conducted annually from 2006 up until 2011 have been the primary drivers of textual entailment research in recent years. Initially the task was defined as that of entailment recognition. RTE-5 ( <ref type="bibr" target="#b0">Bentivogli et al., 2009</ref>) then introduced the task of entailment search as a pilot. Subsequently, RTE-6 ( <ref type="bibr" target="#b1">Bentivogli et al., 2010)</ref> and RTE-7 ( <ref type="bibr" target="#b2">Bentivogli et al., 2011</ref>) featured entailment search as the primary task, but constrained the search space to only those candi- date sentences that were first retrieved by Lucene, an open source search engine <ref type="bibr">1</ref> . Based on the 80% recall from Lucene in RTE-5, the organizers of RTE-6 and RTE-7 deemed this filter to be an appropriate compromise between the size of the search space and the cost and complexity of the human annotation task.</p><p>Annotating data for these tasks has remained a challenge since they were defined in the RTE challenges. Successful approaches for entailment <ref type="bibr" target="#b22">(Mirkin et al., 2009;</ref><ref type="bibr" target="#b14">Jia et al., 2010;</ref><ref type="bibr" target="#b30">Tsuchida and Ishikawa, 2011</ref>) have relied on annotated data to either train classifiers, or to develop rules for detecting entailing sentences. Operating under the assumption that more labeled data would im- prove system performance, some researchers have sought to augment their training data with auto- matically or semi-automatically obtained labeled pairs <ref type="bibr" target="#b5">(Burger and Ferro, 2005;</ref><ref type="bibr" target="#b11">Hickl et al., 2006;</ref><ref type="bibr" target="#b10">Hickl and Bensley, 2007;</ref><ref type="bibr" target="#b35">Zanzotto and Pennacchiotti, 2010;</ref><ref type="bibr" target="#b6">Celikyilmaz et al., 2009)</ref>. <ref type="bibr" target="#b5">Burger and Ferro (2005)</ref> automatically create an entailment recognition corpus using the news headline and the first paragraph of a news article as near-paraphrases. Their approach has an estimated accuracy of 70% on a held out set of 500 pairs. The primary limitation of the approach is that it 1 http://lucene.apache.org only generates positive training examples. <ref type="bibr" target="#b11">Hickl et al. (2006)</ref> improves upon this work by including negative examples selected using heuristic rules (e.g., sentences connected by although, otherwise, and but). On RTE-2 their method achieves accu- racy improvements of upto 10%. However, <ref type="bibr" target="#b10">Hickl and Bensley (2007)</ref> achieves only a 1% accuracy improvement on RTE-3 using the same method, suggesting that it is not always as beneficial.</p><p>Recent work by <ref type="bibr" target="#b4">Bowman et al. (2015)</ref> describes a method for generating large scale annotated datasets, viz., the Stanford Natural Language In- ference (SNLI) Corpus, for the problem of entail- ment recognition. They use Amazon Mechanical Turk to very inexpensively produce a large entail- ment annotated data set from image captions. <ref type="bibr" target="#b35">Zanzotto and Pennacchiotti (2010)</ref> create an en- tailment corpus using Wikipedia data. They hand- annotate original Wikipedia entries, and their as- sociated revisions for entailment recognition. Us- ing a previously published system for RTE <ref type="bibr" target="#b34">(Zanzotto and Moschitti, 2006</ref>), they show that their expanded corpus does not result in improvement for RTE-1, RTE-2 or RTE-3.</p><p>Similarly, <ref type="bibr" target="#b6">Celikyilmaz et al. (2009)</ref> address the lack of labeled data by semi-automatically creat- ing an entailment corpus, which they use within their question answering system. They reuse text- hypothesis pairs from RTE challenges in addition to manually annotated pairs from a newswire cor- pus (with pairs for annotation obtained through a Lucene search over the corpus).</p><p>Note that all of the above research on expand- ing the labeled data for entailment has focused on entailment recognition. Our focus in this paper is on improving entailment search by exploiting un- labeled data with self-training and active learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>In this section, we describe the data sets from two domains, newswire and clinical, that we use in the development and evaluation of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Newswire Domain</head><p>For the newswire domain, we use entailment search data from the PASCAL RTE-5, RTE-6 and RTE-7 challenges ( <ref type="bibr" target="#b0">Bentivogli et al., 2009;</ref><ref type="bibr" target="#b1">Bentivogli et al., 2010;</ref><ref type="bibr" target="#b2">Bentivogli et al., 2011</ref>). The dataset consists of a corpus of news documents, along with a set of hypotheses. The hypotheses come from a separate summarization task, where</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Size</head><p>Entailing Newswire-train 20,104 810 (4.0%) Newswire-dev 35,927 1,842 (5.1%) Newswire-test 17,280 800 (4.6%) Newswire-unlabeled 43,485 - Clinical-train 7,026 293 (4.1%) Clinical-dev 8,092 324 (4.0%) Clinical-test 10,466 596 (5.6%) Clinical-unlabeled 623,600 - </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FAMILY HISTORY:</head><p>Father with T2DM age unknown <ref type="figure">Figure 1</ref>: Excerpt from a sample clinical note the summary sentences about a news story (given a topic) were manually created by human anno- tators. These summary sentences are used as hy- potheses in the dataset. Entailment annotations are then provided for a subset of sentences from the document corpus, based on a Lucene filter for each hypothesis.</p><p>In this work, we use the RTE-5 development data to train our system (Newswire-train), RTE-5 test data for evaluation of our systems (Newswire- test), and we use the combined RTE-6 develop- ment and test data for our system development and parameter estimation (Newswire-dev). We use all of the development and test data from RTE-7, without the human annotation labels, as our unla- beled data (Newswire-unlabeled) for self-training and active learning experiments. A summary of the newswire data is shown in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Clinical Domain</head><p>There are no public datasets available for textual entailment search in the clinical domain. In cre- ating this dataset, we imagine a real-world clin- ical situation where hypotheses are facts about a patient that a physician seeing the patient might want to learn (e.g., The patient underwent a surgi- cal procedure within the last three months.). The unstructured notes in the patients electronic med- ical record (EMR) is the text against which a sys- tem would determine the entailment status of the given hypotheses.</p><p>Observe that the aforementioned real-world clinical scenario is very closely related to a ques- tion answering problem, where instead of hy- potheses a physician may pose natural language questions seeking information about the patient (e.g., Has this patient undergone a surgical pro- cedure within the past three months?). Answers to such questions are words, phrases or passages from the patient's EMR. Since we have access to a patient-specific question answering dataset over EMRs 2 (henceforth, referred to as the QA dataset), we use it here as our starting point in constructing the clinical domain textual entailment dataset.</p><p>Given a question answering dataset, how might one go about creating a dataset on textual entail- ment? We follow a methodology similar to that of RTE-1 through RTE-5 for entailment set derived from question answering data. The text corpus in our entailment dataset is the set of de-identified patient records associated with the QA dataset. To generate hypotheses, human annotators converted questions into multiple assertive sentences, which is somewhat similar to what was done in the first five RTE challenges (RTE-1 through RTE-5). For a given question, the human annotators plugged in clinically-plausible answers to convert the ques- tion into a statement that may or may not be true about a given patient. <ref type="table" target="#tab_1">Table 2</ref> shows example hypotheses and their source questions. Note that this procedure for hypothesis generation diverges slightly from the RTE procedure, where answers from a question answering system were plugged into the questions to produce assertive sentences.</p><p>To generate entailment annotations, we paired a hypothesis with every sentence in a subset of clini- cal notes of the EHR, and asked human annotators to determine if the note sentence enabled them to conclude an entailment relationship with the hy- pothesis. For example, the text: "The appearance is felt to be classic for early MS." entails the hy- pothesis: "She has multiple sclerosis". While in the RTE procedure, a Lucene search was used as a filter to limit the number of hypothesis-sentence pairs that are annotated, in our clinical dataset we</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypotheses</head><p>When was the patient diagnosed with dermatomyositis?</p><p>The patient was diagnosed with dermatomyositis two years ago.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Any creatinine elevation?</head><p>Creatinine is elevated.</p><p>Creatinine is normal.</p><p>Why were xrays done on the forearm and hand?</p><p>Xrays were done on the forearm and hand for suspected fracture. The entailment annotations were generated by two medical students with the help of the annota- tions generated for QA. 11 medical students cre- ated our QA dataset of 5696 questions over 71 pa- tient records, of which 1747 questions have cor- responding answers. This was generated intermit- tently over a period of 11 months. Given the QA dataset, the time taken to generate entailment an- notations includes conversion of questions to hy- potheses, and annotating entailment. While con- version of questions to hypotheses took approx. 2 hours for 20 questions, generating about 3000 hy- pothesis and text pairs took approx. 16 hours.</p><p>At the end of this process, we had a total of 243 hypotheses annotated against sentences from 380 clinical notes, to generate 25,584 text-hypothesis pairs. We split this into train, development and test sets, summarized in <ref type="table" target="#tab_0">Table 1</ref>. Although we have a fairly limited number of labeled text-hypothesis pairs, we do have a large number of patient health records (besides the ones in the annotated set). We generated unlabeled data in the clinical domain, by pairing the hypotheses from our training data with sentences from a set of randomly sampled subset of health records outside of the annotated data.</p><p>Datasets for the textual entailment search task are highly skewed towards the non-entailment class. Note that our clinical data, while smaller in size than the newswire data, maintains a similar class imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Supervised Entailment System</head><p>We begin by defining, in this section, our super- vised entailment system (called ENT) that is used as the basis of our self-training and active learn- ing experiments. Our system draws upon charac- teristics and features of systems that have previ- ously been successful in the RTE challenges in the newswire domain. We further enhance this sys- tem with new features targeting the clinical do- main. The purpose of this section is to demon- strate, through an experimental comparison with other entailment systems, that ENT is competi- tive on both domains, and is a reasonable super- vised system to use in our investigations into self- training and active learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">System Description</head><p>Top systems ( <ref type="bibr" target="#b30">Tsuchida and Ishikawa, 2011;</ref><ref type="bibr" target="#b22">Mirkin et al., 2009</ref>) in the RTE challenges have used various types of passage matching ap- proaches in combination with machine learning for entailment. We follow along these lines, and design a classifier-based entailment system. For every text-hypothesis pair in the dataset we ex- tract a feature vector representative of that pair. Then, using the training data, we train a classi- fier to make entailment decisions on unseen exam- ples. In our system, we employ a logistic regres- sion with ridge estimator (the Weka implementa- tion <ref type="figure">(Hall et al., 2009)</ref>), powered by a variety of passage matching features described below.</p><p>Underlying many of our passage match features is a more fine-grained notion of "term match". Term matchers are a set of algorithms that at- tempt to match tokens (including multi-word to- kens, such as New York or heart attack) across a pair of passages. One of the simplest exam- ples of these is exact string matcher. A token in one text passage that matches exactly, character- for-character, with a token in another text passage would be considered a term match by this sim- ple term matcher. However, these term match- ers could be more sophisticated and match pairs of terms that are synonyms, or paraphrases, or Exact String match, ignore case</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-word</head><p>Overlapping terms in multi-word token</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head</head><p>String match head of multi-word token   <ref type="table" target="#tab_3">Table 3</ref>. Each of these may also produce a confidence score for every match they find. Be- cause we are working with clinical data, we added some medical domain term matchers as well -us- ing UMLS <ref type="bibr" target="#b3">(Bodenreider, 2004</ref>) and a rule-based "translator" of medical terms to layman terms 3 .</p><p>Listed below are all of our features used in the ENT's classifier. Most passage match features ag- gregate the output of the term matchers along var- ious linguistic dimensions -lexical, syntactic, se- mantic, and document/passage characteristics. Lexical: This set includes a feature aggregating exact string matches across text-hypothesis, one aggregating all term matchers, a feature count- ing skip-bigram matches (using all matchers), a measure of matched term coverage of text (ratio of matched terms to unmatched terms). Addi- tionally, we have some medical domain features, viz. UMLS concept overlap, and a measure of UMLS-based similarity <ref type="bibr" target="#b29">(Shivade et al., 2015;</ref><ref type="bibr" target="#b25">Pedersen et al., 2007</ref>) using the UMLS::Similarity tool ( <ref type="bibr" target="#b20">McInnes et al., 2009)</ref>. Syntactic: Following the lead of several ap- proaches textual entailment ( <ref type="bibr" target="#b31">Wang and Zhang, 2009;</ref><ref type="bibr" target="#b22">Mirkin et al., 2009;</ref><ref type="bibr" target="#b15">Kouylekov and Negri, 2010)</ref> we have a features measuring the similar- ity of parse trees. Our rule-based syntactic parser <ref type="bibr" target="#b19">(McCord, 1989</ref>) produces dependency parses the text-hypothesis pair, whose nodes are aligned us- ing all of the term matchers. The tree match fea- ture is an aggregation of the aligned subgraphs in the tree (somewhat similar to a tree kernel <ref type="bibr" target="#b23">(Moschitti, 2004)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic:</head><p>We apply open domain as well as medi- cal entity and relation detectors ( <ref type="bibr" target="#b32">Wang et al., 2011;</ref><ref type="bibr" target="#b33">Wang et al., 2012</ref>) to the texts, and post features measuring overlap in detected entities and overlap in the detected relations across the text-hypothesis pair. We also have a rule-based semantic frame de- tector for a "medical finding" frame (patient pre- senting with symptom or disease). We post a fea- ture that aggregates matched elements of detected frames. Passage Characteristics: Clinical notes typically have a structure and the content is often orga- nized in sections (e.g. History of Illness followed by Physical Examination and ending with Assess- ment and Plan). We identified the section in which each note sentence was located and used them as features in the classifier. Clinical notes are also classified into many different categories (e.g., dis- charge summary, radiology report, etc.), which we generate features from. We also generate sev- eral features capturing the "readability" of the text segments -parse failure, list detector, number of verbs, word capitalization, no punctuation and sentence size. We also have a measure of passage topic relevance based on medical concepts in the pair of texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">System Performance</head><p>To compare effectiveness of ENT on the entail- ment task, we chose two publicly available sys- tems -EDITS and TIE -for comparison. Both these system are available under the Excitement Open Platform (EOP), an initiative ( <ref type="bibr" target="#b17">Magnini et al., 2014</ref>) to make tools for textual entailment freely available <ref type="bibr">4</ref> to the NLP community. EDITS (Edit Distance Textual Entailment Suite) by <ref type="bibr" target="#b15">Kouylekov and Negri (2010)</ref> is an open source textual entail- ment system that uses a set of rules and resources to perform "edit" operations on the text to con- vert it into the hypothesis. There are costs as- sociated with the operations, and an overall cost is computed for the text-hypothesis pair, which determines the decision for that pair. This sys- tem has placed third (out of eight teams) in RTE- 5, and seventh (out of thirteen teams) in RTE-7. The Textual Inference Engine (TIE) ( <ref type="bibr" target="#b31">Wang and Zhang, 2009</ref>) is a maximum entropy based entail- ment system relying on predicate argument struc- ture matching. While this system did not partici-  pate in the RTE challenges, it has been shown to be effective on the RTE datasets. In our experiments, we trained the EDITS system optimizing for F- score (the default optimization criterion is accu- racy) and TIE with its default settings. We also used a Lucene baseline similar to the one used in RTE-5, RTE-6 and RTE-7 entailment challenges. We trained the systems on the training set of each domain and tested on the test set. The Lucene baseline considers the first N sentences (where N is 5, 10, 15 or 20) top-ranked by the search engine to be entailing the hypothesis. The configuration with the top 10 sentences performed the best, and is reported in the results. Note that this baseline is a strong one, and none of the systems participating in RTE-5 could beat it. <ref type="table" target="#tab_5">Table 4</ref> summarizes the system performance on newswire and clinical data. We observe that sys- tems that did well on RTE datasets, were mediocre on the clinical dataset. We did not, however, put any effort into adaption of TIE and EDITS to the clinical data. So the mediocre performance on clinical is understandable. It is interesting to see though that ENT did well (comparatively) on both domains.</p><p>We note that our problem setting is most similar to the RTE-5 entailment search task. Of the 20 runs across eight teams that participated in RTE-5, the median F-Score was 0.30 and the best system ( <ref type="bibr" target="#b22">Mirkin et al., 2009</ref>) achieved an F-Score of 0.46. EDITS and TIE perform slightly above the median and ENT (with 0.39 F-score) would have ranked third in the challenge.</p><p>The performance of all systems on the clin- ical data is noticeably low as compared to the newswire data. An obvious difference in the two domains is the training data size (see <ref type="table" target="#tab_0">Table 1</ref>). However, obtaining annotations for textual entail- ment search is expensive, particularly in the clin- ical domain. The remaining sections present our investigations into self-training and active learn- ing, to overcome the lack of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Self-Training</head><p>Our goal is to exploit unlabeled data, with the hope of augmenting the limited annotated data in a given domain. Self-training is a method that has been successfully used to address limited training data on many NLP tasks, such as pars- ing ( <ref type="bibr" target="#b18">McClosky et al., 2006</ref>), information extrac- tion ( <ref type="bibr" target="#b12">Huang and Riloff, 2012;</ref><ref type="bibr" target="#b24">Patwardhan and Riloff, 2007)</ref>, word sense disambiguation <ref type="bibr" target="#b21">(Mihalcea, 2004</ref>), etc. Self-training iteratively in- creases the size of the training set, by automati- cally assigning labels to unlabeled examples, us- ing a model trained in a previous iteration of the self-training regime.</p><p>For our newswire and clinical datasets, using the set of unlabeled text-hypothesis pairs U , we ran the following training regime: A model was created using the training data L n , and applied it to the unlabeled data U . From U , all such pairs that were classified by the model as entailing pairs with high confidence (above a threshold τ ) were added to the labeled training data L n to generate L n+1 . Non-entailing pairs were ignored. A new model is trained on data L n+1 , and the above pro- cess repeated iteratively, until a stopping criteria is reached (in our case, all pairs from U are ex- hausted).</p><p>The threshold τ determines the confidence of our model for a text-hypothesis pair being classi- fied to the entailment class. This threshold was tuned by varying it incrementally from 0.1 to 0.9 in steps of 0.1. The best τ was determined on the development set, and chosen for the self-training system. <ref type="figure">Figure 2</ref> shows the effect of τ on the de- velopment data.</p><p>As such, we see that the F-score of the self- trained model is always above that of the baseline ENT system. The F-score increases upto a peak of 0.33 at threshold τ of 0.2 before dropping at higher thresholds. Using this tuned threshold on test set, the comparitive performance on the test set is outlined in <ref type="table" target="#tab_6">Table 5</ref>. We observe an F-score  of 0.36, which is significantly greater than that of the vanilla ENT system (0.23).</p><p>The effect of the threshold on performance cor- relates with the number of instances added to the training set. When the threshold is low, there are more instances being added (10,799 at threshold of 0.1) into the training set. Therefore, recall is likely to benefit, since the model is exposed to a larger variety of text-hypothesis pairs. However, the precision is low since noisy pairs are likely to be added. When the threshold is high, fewer in- stances are added (316 at threshold of 0.9). These are the ones that the model is most certain about, suggesting that these are likely to be less noisy. Therefore, the precision is comparatively high.</p><p>We also ran our self-training approach on the Newswire datasets. We observed similar varia- tions in performance with newswire data as with the clinical data. At threshold of 0.9, fewer in- stances (49) are added to the training set from the unlabeled data, while a large number of instances (2,861) are added at a lower threshold τ of 0.1.</p><p>The best performance (F-score of 0.52) was ob- tained at threshold of 0.3, on the development set.</p><p>This threshold also resulted in the best perfor- mance (0.54) on the test set. Similar to the clinical domain, precision increased but recall decreased as the threshold increased. Again, it is evident from <ref type="table" target="#tab_6">Table 5</ref> that gains obtained from self-training are due to recall. It should be noted that the self- trained system achieves an F-score of 0.54 -sub- stantially better than the best performing system of <ref type="bibr" target="#b22">Mirkin et al. (2009)</ref> (F-score, 0.46) in RTE-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Active Learning</head><p>Active learning is a popular training paradigm in machine learning <ref type="bibr" target="#b28">(Settles, 2012</ref>) where a learning agent interacts with its environment in acquiring a training set, rather than passively receiving inde- pendent samples from an underlying distribution. This is especially pertinent in the clinical domain, where input from a medical professional should be sought only when really necessary, because of the high cost of such input. The purpose of exploring this paradigm is to achieve the best possible gen- eralization performance at the lowest cost.</p><p>Active learning is an iterative process, and typi- cally works as follows: a model M is trained using a minimal training dataset L. A query framework is used to identify an instance from an unlabeled set U that, if added to L, will result in maximum expected benefit. Gold standard annotations are obtained for this instance and added to the original training set L to generate a new training set L . In the next iteration, a new model M is trained using L and used to identify the next most beneficial in- stance for the training set L . This is repeated until a stopping criterion is met. This approach is often simulated using a training dataset L of reasonable size. The initial model M is created using a subset A of L. Further, instead of querying a large unla- beled set U , the remaining training data (L − A) is treated as an unlabeled dataset and queried for the most beneficial addition.</p><p>We carried out active learning in this setting us- ing a querying framework known as uncertainty sampling ( <ref type="bibr" target="#b16">Lewis and Gale, 1994)</ref>. Here, the model M trained using A, queries the instances in (L−A) for instance(s) it is least certain for a prediction label. For probabilistic classifiers the most uncer- tain instance is the one where posterior probability for a given class is nearest to 0.5. To estimate the effectiveness of this framework, it is always com- pared with a random sampling framework, where random instances from the training data are incre- mentally added to the model. Starting with a model trained using a single randomly chosen instance, we carried out active learning using uncertainty sampling, adding one instance at a time. After the addition of each in- stance, the model was retrained and tested on a held out set. To minimize the effect of randomiza- tion associated with the first instance, we repeated the experiment ten times and averaged the perfor- mance scores across the ten runs.</p><p>Following previous work <ref type="bibr" target="#b27">(Settles and Craven, 2008;</ref><ref type="bibr" target="#b26">Reichart et al., 2008</ref>) we evaluate active learning using learning curves on the test set. <ref type="figure" target="#fig_1">Fig- ure 3</ref> shows the learning curves for newswire and clinical data.</p><p>On clinical data, uncertainty sampling achieves a performance equal to the baseline ENT with only 470 instances. With random sampling, over 2,200 instances are required. The active learner matches the performance of the ENT with only 6.6% of training data. Newswire shows a similar trend, with both sampling strategies outperforming ENT, using less than half the training, and uncertainty sampling learning faster than random. While un- certainty sampling matches ENT F-score with only 1,169 instances, random sampling requires 2,305. Here, the active learner matches ENT performance using only 5.8% of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Effect of Class Distribution</head><p>After analyzing our experimental results, we con- sidered that one possible explanation for the im- provements over baseline ENT could plausibly be because of changes in the class distribution. From <ref type="table" target="#tab_0">Table 1</ref>, we observe that the distribution of classes in both domains is highly skewed (only 4-5% pos- itive instances). Self-training and active learn- ing dramatically change the class distribution in training. To assess the effect of class distribution changes on performance, we ran additional exper- iments, described here.</p><p>We first investigated sub-sampling <ref type="bibr" target="#b13">(Japkowicz, 2000</ref>) the training data to address class imbalance. This includes down-sampling the majority class or up-sampling the minority class until the classes are balanced. We found no significant gains over the vanilla ENT baseline with both strategies. Specif- ically, down-sampling resulted in gains of only 0.002 and 0.001 F-score and up-sampling resulted in a drop of 0.011 and 0.013 F-score on clinical- dev and newswire-dev, respectively.</p><p>Another approach to addressing class imbal- ance is to apply Synthetic Minority Oversam- pling Technique (SMOTE) ( <ref type="bibr" target="#b7">Chawla et al., 2002</ref>). SMOTE creates instances of the minority class by taking a minority class sample and introducing synthetic examples between its k nearest neigh- bors. Using SMOTE on newswire and clinical datasets resulted in improvements over baseline ENT in both domains. The improvements us- ing self-training, however, are significantly higher than SMOTE. <ref type="figure" target="#fig_2">Figure 4</ref> shows a comparison of SMOTE and self-training on newswire data, where equal number of instances are added to the training set by both techniques.</p><p>Finally, for active learning, we consider random sampling as a competing approach to uncertainty sampling. <ref type="figure" target="#fig_3">Figure 5</ref> illustrates the percentage of positive and negative instances that get included in the training set for both sampling strategies, as active learning proceeds. The blue solid line shows that positive instances are consumed faster than the negative instances with uncertainty sam- pling. Thus, a higher percentage of positive in- stances (that approximately equals the number of negative instances getting added) get added and this helps maintain a balanced class distribution.</p><p>Once the positive instances are exhausted, more negative instances are added, resulting in some class imbalance that hurts performance (even though more training data is being added over- all). In contrast, random sampling does not change the class balance, as it consumes a proportional number of positive and negative instances (result- ing in more negative than positive instances). The plot indicates that when using uncertainty sam- pling 80% of the positive examples are added to the training set with less than 50% of the data. This also explains how the active learner matches the performance of the model using the entire la- beled set, but with fewer training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We explored the problem of textual entailment search in two domains -newswire and clinical - and focused a spotlight on the cost of obtaining labeled data in certain domains. In the process, we first created an entailment dataset for the clin- ical domain, and a highly competitive supervised entailment system, called ENT, which is effective (out of the box) on two domains. We then explored two strategies -self-training and active learning - to address the lack of labeled data, and observed some interesting results. Our self-training sys- tem substantially improved over ENT, achieving an F-score gain of 15% on newswire and 13% on clinical, using only additional unlabeled data. On the other hand, our active learning experiments demonstrated that we could match (and even beat) the baseline ENT system with only 6.6% of the training data in the clinical domain, and only 5.8% of the training data in the newswire domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2: Self-training on development data Newswire Clinical System Precision Recall F-score Precision Recall F-score ENT 0.77 0.26 0.39 0.42 0.15 0.23 ENT + Self-Training 0.62 0.48 0.54 * 0.34 0.39 0.36 *</figDesc><graphic url="image-4.png" coords="7,303.31,190.99,219.42,113.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning curves for uncertainty sampling and random sampling on test data</figDesc><graphic url="image-5.png" coords="8,72.00,62.81,221.81,115.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of SMOTE and selftraining (on newswire development set)</figDesc><graphic url="image-7.png" coords="9,72.00,62.81,216.18,113.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of sampling strategies for active learning (on newswire development set)</figDesc><graphic url="image-8.png" coords="9,307.28,62.81,215.45,126.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Summary of datasets 

** NAME[XX (YY) ZZ] has no liver 
problems. 
PAST MEDICAL HISTORY 
1. Htn 
Well controlled 
2. Diabetes mellitus 
On regular dose of insulin. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Example question → hypotheses mappings limit the number of annotations by pairing each hypothesis only with sentences from EMR notes containing an answer to the original question in the QA dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>ENT term matchers 

equivalent to one another according to other crite-
ria. ENT employs a series of term matchers listed 
in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>System performance on test data (* indicates statistical significance) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Self-training results on test data (* indicates statistical significance) 

</table></figure>

			<note place="foot" n="2"> a publication describing the question-answering dataset is currently under review at another venue</note>

			<note place="foot" n="3"> Rules for medical term translator were derived from http://www.globalrph.com/medterm.htm</note>

			<note place="foot" n="4"> http://hltfbk.github.io/ Excitement-Open-Platform/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank our in-house medical expert, Jennifer Liang, for guidance on the data annotation task, our medical annotators for annotating clinical data for us, and Murthy Devarakonda for valuable in-sights during the project. We also thank Eric Fosler-Lussier and Albert M. Lai for their help in conceptualizing this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Fifth PASCAL Recognizing Textual Entailment Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Text Analysis Conference</title>
		<meeting>the Second Text Analysis Conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Sixth PASCAL Recognizing Textual Entailment Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Text Analysis Conference</title>
		<meeting>the Third Text Analysis Conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Seventh PASCAL Recognizing Textual Entailment Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Text Analysis Conference</title>
		<meeting>the Fourth Text Analysis Conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Unified Medical Language System (UMLS): Integrating Biomedical Terminology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generating an Entailment Corpus from News Headlines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment</title>
		<meeting>the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment<address><addrLine>Ann Arbor, MI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Graph-based Semi-Supervised Learning for Question-Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Thint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="719" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SMOTE: Synthetic Minority Over-sampling Technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recognizing Textual Entailment: Models and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Massimo</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zanzotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="220" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The WEKA Data Mining Software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Discourse Commitment-based Framework for Recognizing Textual Entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Bensley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="171" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recognizing Textual Entailment with LCC&apos;s GROUNDHOG System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Bensley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Second PASCAL Recognizing Textual Entailment Challenge: Proceedings of the Challenges Workshop</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bootstrapped Training of Event Extraction Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="286" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning from Imbalanced Data Sets: A Comparison of Various Strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Japkowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Workshop on Learning from Imbalanced Data Sets</title>
		<meeting>the AAAI Workshop on Learning from Imbalanced Data Sets<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">PKUTM Participation at TAC 2010 RTE and Summarization Track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houping</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Text Analysis Conference</title>
		<meeting>the Third Text Analysis Conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An Open-Source Package for Recognizing Textual Entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milen</forename><surname>Kouylekov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 System Demonstrations</title>
		<meeting>the ACL 2010 System Demonstrations<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Sequential Algorithm for Training Text Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">A</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Excitement Open Platform for Textual Inferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zanoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathrin</forename><surname>Eichler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neumann</forename><surname>Guenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Gil</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asher</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Effective Self-Training for Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics<address><addrLine>New York City, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Slot Grammar: A System for Simpler Construction of Practical Natural Language Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Natural Language and Logic</title>
		<meeting>the International Symposium on Natural Language and Logic<address><addrLine>Hamburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="118" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">UMLS-Interface and UMLSSimilarity : Open Source Software for Measuring Paths and Semantic Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Bridget T Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Serguei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pakhomov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Symposium of the American Medical Informatics Association</title>
		<meeting>the Annual Symposium of the American Medical Informatics Association<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Co-Training and Self-Training for Word Sense Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Conference on Natural Language Learning</title>
		<meeting>the Eighth Conference on Natural Language Learning<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Addressing Discourse and Document Structure in the RTE Search Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shachar</forename><surname>Mirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Shnarch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Text Analysis Conference</title>
		<meeting>the Second Text Analysis Conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Asher Stern, and Idan Szpektor</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Study on Convolution Kernels for Shallow Statistic Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="335" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="717" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Measures of Semantic Similarity and Relatedness in the Biomedical Domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Serguei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Pakhomov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher G</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chute</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="288" to="99" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-Task Active Learning for Linguistic Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Tomanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udo</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, OH</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="861" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An Analysis of Active Learning Strategies for Sequence Labeling Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1070" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Active Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="114" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Textual Inference for Eligibility Criteria Resolution in Clinical trials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Shivade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Loptegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Fosler-Lussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><forename type="middle">M</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">IKOMA at TAC2011 : A Method for Recognizing Textual Entailment using Lexical-level and Sentence Structurelevel Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Tsuchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Text Analysis Conference</title>
		<meeting>the Fourth Text Analysis Conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recognizing Textual Relatedness with Predicate-Argument Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="784" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Relation Extraction with Relation Topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1426" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Relation Extraction and Scoring in DeepQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Branimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Boguraev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gondek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic Learning of Textual Entailments with Cross-Pair Similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Zanzotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Expanding Textual Entailment Corpora from Wikipedia using Co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Zanzotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on The People&apos;s Web Meets NLP: Collaboratively Constructed Semantic Resources</title>
		<meeting>the 2nd Workshop on The People&apos;s Web Meets NLP: Collaboratively Constructed Semantic Resources<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
