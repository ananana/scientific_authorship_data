<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Document Classification by Inversion of Distributed Language Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Taddy</surname></persName>
							<email>taddy@chicagobooth.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chicago Booth School of Business</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Document Classification by Inversion of Distributed Language Representations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="45" to="49"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>There have been many recent advances in the structure and measurement of distributed language models: those that map from words to a vector-space that is rich in information about word choice and composition. This vector-space is the distributed language representation. The goal of this note is to point out that any distributed representation can be turned into a classifier through inversion via Bayes rule. The approach is simple and modular, in that it will work with any language representation whose training can be formulated as optimizing a probability model. In our application to 2 million sentences from Yelp reviews, we also find that it performs as well as or better than complex purpose-built algorithms.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed, or vector-space, language representa- tions V consist of a location, or embedding, for every vocabulary word in R K , where K is the di- mension of the latent representation space. These locations are learned to optimize, perhaps approx- imately, an objective function defined on the origi- nal text such as a likelihood for word occurrences.</p><p>A popular example is the Word2Vec machinery of <ref type="bibr" target="#b2">Mikolov et al. (2013)</ref>. This trains the distributed representation to be useful as an input layer for prediction of words from their neighbors in a Skip- gram likelihood. That is, to maximize t+b j =t, j=t−b log p V (w sj | w st )</p><p>summed across all words w st in all sentences w s , where b is the skip-gram window (truncated by the ends of the sentence) and p V (w sj |w st ) is a neural network classifier that takes vector representations for w st and w sj as input (see Section 2). Distributed language representations have been studied since the early work on neural networks <ref type="bibr" target="#b7">(Rumelhart et al., 1986)</ref> and have long been ap- plied in natural language processing <ref type="bibr" target="#b3">(Morin and Bengio, 2005</ref>). The models are generating much recent interest due to the large performance gains from the newer systems, including Word2Vec and the Glove model of <ref type="bibr" target="#b5">Pennington et al. (2014)</ref>, ob- served in, e.g., word prediction, word analogy identification, and named entity recognition.</p><p>Given the success of these new models, re- searchers have begun searching for ways to adapt the representations for use in document classifica- tion tasks such as sentiment prediction or author identification. One naive approach is to use ag- gregated word vectors across a document (e.g., a document's average word-vector location) as input to a standard classifier (e.g., logistic regression). However, a document is actually an ordered path of locations through R K , and simple averaging de- stroys much of the available information.</p><p>More sophisticated aggregation is proposed in <ref type="bibr" target="#b8">Socher et al. (2011;</ref>, where recursive neu- ral networks are used to combine the word vectors through the estimated parse tree for each sentence. Alternatively, <ref type="bibr">Le and Mikolov's Doc2Vec (2014)</ref> adds document labels to the conditioning set in (1) and has them influence the skip-gram likelihood through a latent input vector location in V. In each case, the end product is a distributed representa- tion for every sentence (or document for Doc2Vec) that can be used as input to a generic classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Bayesian Inversion</head><p>These approaches all add considerable model and estimation complexity to the original underlying distributed representation. We are proposing a simple alternative that turns fitted distributed lan- guage representations into document classifiers without any additional modeling or estimation.</p><p>Write the probability model that the represen- tation V has been trained to optimize (likeli- hood maximize) as p V (d), where document d = {w 1 , ...w S } is a set of sentences -ordered vectors of word identities. For example, in Word2Vec the skip-gram likelihood in (1) yields log p V (d) = s t t+b j =t, j=t−b log p Vy (w sj | w st ).</p><p>(2) Even when such a likelihood is not explicit it will be implied by the objective function that is opti- mized during training. Now suppose that your training documents are grouped by class label, y ∈ {1 . . . C}. We can train separate distributed language representations for each set of documents as partitioned by y; for example, fit Word2Vec independently on each sub-corpus D c = {d i : y i = c} and obtain the labeled distributed representation map V c . A new document d has probability p Vc (d) if we treat it as a member of class c, and Bayes rule implies</p><formula xml:id="formula_1">p(y|d) = p Vy (d)π y c p Vc (d)π c<label>(3)</label></formula><p>where π c is our prior probability on class label c. Thus distributed language representations trained separately for each class label yield directly a document classification rule via (3). This approach has a number of attractive qualities.</p><p>Simplicity: The inversion strategy works for any model of language that can (or its training can) be interpreted as a probabilistic model. This makes for easy implementation in systems that are al- ready engineered to fit such language represen- tations, leading to faster deployment and lower development costs. The strategy is also inter- pretable: whatever intuition one has about the dis- tributed language model can be applied directly to the inversion-based classification rule. Inversion adds a plausible model for reader understanding on top of any given language representation.</p><p>Scalability: when working with massive corpora it is often useful to split the data into blocks as part of distributed computing strategies. Our model of classification via inversion provides a convenient top-level partitioning of the data. An efficient sys- tem could fit separate by-class language represen- tations, which will provide for document classi- fication as in this article as well as class-specific answers for NLP tasks such as word prediction or analogy. When one wishes to treat a document as unlabeled, NLP tasks can be answered through en- semble aggregation of the class-specific answers.</p><p>Performance: We find that, in our examples, in- version of Word2Vec yields lower misclassifica- tion rates than both Doc2Vec-based classification and the multinomial inverse regression (MNIR) of Taddy (2013b). We did not anticipate such out- right performance gain. Moreover, we expect that with calibration (i.e., through cross-validation) of the many various tuning parameters available when fitting both Word and Doc 2Vec the perfor- mance results will change. Indeed, we find that all methods are often outperformed by phrase-count logistic regression with rare-feature up-weighting and carefully chosen regularization. However, the out-of-the-box performance of Word2Vec inver- sion argues for its consideration as a simple default in document classification.</p><p>In the remainder, we outline classification through inversion of a specific Word2Vec model and illustrate the ideas in classification of Yelp reviews. The implementation requires only a small extension of the popular gensim python library <ref type="bibr" target="#b6">(Rehurek and Sojka, 2010)</ref>; the ex- tended library as well as code to reproduce all of the results in this paper are available on github.</p><p>In addition, the yelp data is publicly available as part of the correspond- ing data mining contest at kaggle.com. See github.com/taddylab/deepir for detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Implementation</head><p>Word2Vec trains V to maximize the skip-gram likelihood based on (1). We work with the Huff- man softmax specification ( <ref type="bibr" target="#b2">Mikolov et al., 2013)</ref>, which includes a pre-processing step to encode each vocabulary word in its representation via a binary Huffman tree (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>Each individual probability is then only the input space V = [v w 1 · · · v wp ], for a p- word vocabulary, is reported as the language rep- resentation -these vectors are used as input for NLP tasks. However, the full representation V in- cludes mapping from each word to both V and U.</p><formula xml:id="formula_2">p V (w|w t ) = L(w)−1 j=1 σ ch [η(w, j + 1)] u η(w,j) v wt</formula><p>We apply the gensim python implementation of Word2Vec, which fits the model via stochastic gradient descent (SGD), under default specifica- tion. This includes a vector space of dimension K = 100 and a skip-gram window of size b = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Word2Vec Inversion</head><p>Given Word2Vec trained on each of C class- specific corpora D 1 . . . D C , leading to C distinct language representations V 1 . . . V C , classification for new documents is straightforward. Consider the S-sentence document d: each sentence w s is given a probability under each representation V c by applying the calculations in (1) and (4). This leads to the S ×C matrix of sentence probabilities, p Vc (w s ), and document probabilities are obtained</p><formula xml:id="formula_3">p Vc (d) = 1 S s p Vc (w s ).<label>(5)</label></formula><p>Finally, class probabilities are calculated via Bayes rule as in (3). We use priors π c = 1/C, so that classification proceeds by assigning the classˆy classˆ classˆy = argmax c p Vc (d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Illustration</head><p>We consider a corpus of reviews provided by Yelp for a contest on kaggle.com. The text is tok- enized simply by converting to lowercase before splitting on punctuation and white-space. The training data are 230,000 reviews containing more than 2 million sentences. Each review is marked by a number of stars, from 1 to 5, and we fit separate Word2Vec representations V 1 . . . V 5 for the documents at each star rating. The valida- tion data consist of 23,000 reviews, and we ap- ply the inversion technique of Section 2 to score each validation document d with class probabili- ties q = [q 1 · · · q 5 ], where q c = p(c|d). The probabilities will be used in three different classification tasks; for reviews as a. negative at 1-2 stars, or positive at 3-5 stars;</p><p>b. negative 1-2, neutral 3, or positive 4-5 stars;</p><p>c. corresponding to each of 1 to 5 stars.</p><p>In each case, classification proceeds by sum- ming across the relevant sub-class probabilities. For example, in task a, p(positive) = q 3 + q 4 + q 5 . Note that the same five fitted Word2Vec representations are used for each task.</p><p>We consider a set of related comparator tech- niques. In each case, some document repre- sentation (e.g., phrase counts or Doc2Vec vec- tors) is used as input to logistic regression pre- diction of the associated review rating. The lo- gistic regressions are fit under L 1 regularization with the penalties weighted by feature standard deviation (which, e.g., up-weights rare phrases) and selected according to the corrected AICc cri- teria ( <ref type="bibr" target="#b0">Flynn et al., 2013</ref>) via the gamlr R pack- age of <ref type="bibr" target="#b13">Taddy (2014)</ref>. For multi-class tasks b-c, we use distributed Multinomial regression (DMR; Taddy 2015) via the distrom R package. DMR fits multinomial logistic regression in a factorized representation wherein one estimates independent Poisson linear models for each response category. Document representations and logistic regressions are always trained using only the training corpus.</p><p>Doc2Vec is also fit via gensim, using the same latent space specification as for Word2Vec: K = 100 and b = 5. As recommended in the doc- umentation, we apply repeated SGD over 20 re- orderings of each corpus (for comparability, this was also done when fitting Word2Vec). Le and Mikolov provide two alternative Doc2Vec specifi- cations: distributed memory (DM) and distributed bag-of-words (DBOW). We fit both. Vector rep- resentations for validation documents are trained without updating the word-vector elements, lead- ing to 100 dimensional vectors for each docu- ment for each of DM and DCBOW. We input q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q     q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q    <ref type="figure">Figure 2</ref>: Out-of-Sample fitted probabilities of a review being positive (having greater than 2 stars) as a function of the true number of review stars. Box widths are proportional to number of observations in each class; roughly 10% of reviews have each of 1-3 stars, while 30% have 4 stars and 40% have 5 stars. each, as well as the combined 200 dimensional DM+DBOW representation, to logistic regression.</p><note type="other">q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q</note><p>Phrase regression applies logistic regression of re- sponse classes directly onto counts for short 1-2 word 'phrases'. The phrases are obtained using gensim's phrase builder, which simply combines highly probable pairings; e.g., first date and chicken wing are two pairings in this corpus. <ref type="bibr" target="#b10">Taddy (2013a;</ref><ref type="bibr" target="#b11">2013b;</ref><ref type="bibr" target="#b14">2015)</ref> is applied as im- plemented in the textir package for R. MNIR maps from text to the class-space of inter- est through a multinomial logistic regression of phrase counts onto variables relevant to the class- space. We apply MNIR to the same set of 1-2 word phrases used in phrase regression. Here, we regress phrase counts onto stars expressed numeri- cally and as a 5-dimensional indicator vector, lead- ing to a 6-feature multinomial logistic regression. The MNIR procedure then uses the 6 × p matrix of feature-phrase regression coefficients to map from phrase-count to feature space, resulting in 6 di- mensional 'sufficient reduction' statistics for each document. These are input to logistic regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIR, the multinomial inverse regression of</head><p>Word2Vec aggregation averages fitted word rep- resentations for a single Word2Vec trained on all sentences to obtain a fixed-length feature vector for each review (K = 100, as for inversion). This vector is then input to logistic regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head><p>Misclassification rates for each task on the valida- tion set are reported in <ref type="table">Table 1</ref>. Simple phrase- count regression is consistently the strongest per- former, bested only by Word2Vec inversion on task b. This is partially due to the relative strengths of discriminative (e.g., logistic regression) vs gen- a (NP) b (NNP) c <ref type="formula" target="#formula_0">(1-5</ref>  <ref type="table">Table 1</ref>: Out-of-sample misclassification rates.</p><p>erative (e.g., all others here) classifiers: given a large amount of training text, asymptotic effi- ciency of logistic regression will start to work in its favor over the finite sample advantages of a generative classifier ( <ref type="bibr" target="#b4">Ng and Jordan, 2002;</ref><ref type="bibr" target="#b12">Taddy, 2013c)</ref>. However, the comparison is also unfair to Word2Vec and Doc2Vec: both phrase regres- sion and MNIR are optimized exactly under AICc selected penalty, while Word and Doc 2Vec have only been approximately optimized under a sin- gle specification. The distributed representations should improve with some careful engineering.</p><p>Word2Vec inversion outperforms the other doc- ument representation-based alternatives (except, by a narrow margin, MNIR in task a). Doc2Vec under DBOW specification and MNIR both do worse, but not by a large margin. In contrast to Le and Mikolov, we find here that the Doc2Vec DM model does much worse than DBOW. Re- gression onto simple within-document aggrega- tions of Word2Vec perform slightly better than any Doc2Vec option (but not as well as the Word2Vec inversion). This again contrasts the results of Le and Mikolov and we suspect that the more com- plex Doc2Vec model would benefit from a careful tuning of the SGD optimization routine. <ref type="bibr">1</ref> Looking at the fitted probabilities in detail we see that Word2Vec inversion provides a more use- ful document ranking than any comparator (in- cluding phrase regression). For example, <ref type="figure">Figure  2</ref> shows the probabilities of a review being 'pos- itive' in task a as a function of the true star rat- ing for each validation review. Although phrase regression does slightly better in terms of misclas- sification rate, it does so at the cost of classifying many terrible (1 star) reviews as positive. This oc- curs because 1-2 star reviews are more rare than 3- 5 star reviews and because words of emphasis (e.g. very, completely, and !!!) are used both in very bad and in very good reviews. Word2Vec inversion is the only method that yields positive- document probabilities that are clearly increasing in distribution with the true star rating. It is not dif- ficult to envision a misclassification cost structure that favors such nicely ordered probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>The goal of this note is to point out inversion as an option for turning distributed language representa- tions into classification rules. We are not arguing for the supremacy of Word2Vec inversion in par- ticular, and the approach should work well with al- ternative representations (e.g., Glove). Moreover, we are not even arguing that it will always outper- form purpose-built classification tools. However, it is a simple, scalable, interpretable, and effective option for classification whenever you are working with such distributed representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Binary Huffman encoding of a 4 word vocabulary, based upon 18 total utterances. At each step proceeding from left to right the two nodes with lowest count are combined into a parent node. Binary encodings are read back off of the splits moving from right to left.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficiency for Regularization Parameter Selection in Penalized Likelihood Estimation of Misspecified Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheryl</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clifford</forename><surname>Hurvich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jefferey</forename><surname>Simonoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="1031" to="1043" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pro1 Note also that the unsupervised document representations-Doc2Vec or the single Word2Vec used in Word2Vec aggregation-could be trained on larger unlabeled corpora. A similar option is available for Word2Vec inversion: one could take a single Word2Vec model trained on a large unlabeled corpora as a shared baseline (prior) and update separate models with additional training on each labeled sub-corpora. The representations will all be shrunk towards a baseline language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Distributed representations of sentences and documents. but will differ according to distinctions between the language in each labeled sub-corpora. ceedings of the 31 st</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Artificial Intelligence and Statistics</title>
		<meeting>the International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On Discriminative vs Generative Classifiers: A Comparison of Logistic Regression and naive Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radim</forename><surname>Rehurek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Measuring Political Sentiment on Twitter: Factor Optimal Design for Multinomial Inverse Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Taddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="415" to="425" />
			<date type="published" when="2013-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multinomial Inverse Regression for Text Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Taddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="755" to="770" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rejoinder: Efficiency and structure in MNIR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Taddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="772" to="774" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">One-step estimator paths for concave regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Taddy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.5623</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed Multinomial Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Taddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Applied Statistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
