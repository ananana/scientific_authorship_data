<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language Modeling for Code-Mixing: The Role of Linguistic Theory based Synthetic Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adithya</forename><surname>Pratapa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gayatri</forename><surname>Bhat</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Language Technology Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monojit</forename><surname>Choudhury</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunayana</forename><surname>Sitaram</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandipan</forename><surname>Dandapat</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft R&amp;D</orgName>
								<address>
									<region>Hyderabad</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalika</forename><surname>Bali</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Language Modeling for Code-Mixing: The Role of Linguistic Theory based Synthetic Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1543" to="1553"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1543</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Training language models for Code-mixed (CM) language is known to be a difficult problem because of lack of data compounded by the increased confusability due to the presence of more than one language. We present a computational technique for creation of grammatically valid artificial CM data based on the Equivalence Constraint Theory. We show that when training examples are sampled appropriately from this synthetic data and presented in certain order (aka training curriculum) along with monolingual and real CM data, it can significantly reduce the perplexity of an RNN-based language model. We also show that randomly generated CM data does not help in decreasing the perplexity of the LMs.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Code-switching or code-mixing (CM) refers to the juxtaposition of linguistic units from two or more languages in a single conversation or sometimes even a single utterance. <ref type="bibr">1</ref> It is quite commonly ob- served in speech conversations of multilingual so- cieties across the world. Although, traditionally, CM has been associated with informal or casual speech, there is evidence that in several societies, such as urban India and Mexico, CM has become the default code of communication ( <ref type="bibr" target="#b22">Parshad et al., 2016)</ref>, and it has also pervaded written text, espe- cially in computer-mediated communication and social media ( <ref type="bibr" target="#b25">Rijhwani et al., 2017)</ref>. * Work done during author's internship at Microsoft Re- search <ref type="bibr">1</ref> According to some linguists, code-switching refers to inter-sentential mixing of languages, whereas code-mixing refers to intra-sentential mixing. Since the latter is more gen- eral, we will use code-mixing in this paper to mean both.</p><p>It is, therefore, imperative to build NLP tech- nology for CM text and speech. There have been some efforts towards building of Automatic Speech Recognition Systems and TTS for CM speech ( <ref type="bibr">Fung, 2013, 2014;</ref><ref type="bibr" target="#b13">Gebhardt, 2011;</ref><ref type="bibr" target="#b30">Sitaram et al., 2016)</ref>, and tasks like language identification ( <ref type="bibr" target="#b32">Solorio et al., 2014;</ref><ref type="bibr" target="#b5">Barman et al., 2014</ref>), POS tagging ( <ref type="bibr" target="#b35">Vyas et al., 2014;</ref><ref type="bibr" target="#b31">Solorio and Liu, 2008)</ref>, parsing and sentiment analy- sis ( <ref type="bibr" target="#b29">Sharma et al., 2016;</ref><ref type="bibr" target="#b24">Prabhu et al., 2016;</ref><ref type="bibr" target="#b27">Rudra et al., 2016</ref>) for CM text. Nevertheless, the accura- cies of all these systems are much lower than their monolingual counterparts, primarily due to lack of enough data.</p><p>Intuitively, since CM happens between two (or more languages), one would typically need twice as much, if not more, data to train a CM sys- tem. Furthermore, any CM corpus will contain large chunks of monolingual fragments, and rel- atively far fewer code-switching points, which are extremely important to learn patterns of CM from data. This implies that the amount of data required would not just be twice, but probably 10 or 100 times more than that for training a monolingual system with similar accuracy. On the other hand, apart from user-generated content on the Web and social media, it is extremely difficult to gather large volumes of CM data because (a) CM is rare in formal text, and (b) speech data is hard to gather and even harder to transcribe.</p><p>In order to circumvent the data scarcity issue, in this paper we propose the use of linguistically- motivated synthetically generated CM data (as a supplement to real CM data) for development of CM NLP systems. In particular, we use the Equivalence Constraint Theory <ref type="bibr" target="#b23">(Poplack, 1980;</ref><ref type="bibr" target="#b28">Sankoff, 1998</ref>) for generating linguistically valid CM sentences from a pair of parallel sentences in the two languages. We then use these gener- ated sentences, along with monolingual and little amount of real CM data to train a CM Language Model (LM). Our experiments show that, when trained following certain sampling strategies and training curriculum, the synthetic CM sentences are indeed able to improve the perplexity of the trained LM over a baseline model that uses only monolingual and real CM data.</p><p>LM is useful for a variety of downstream NLP tasks such as Speech Recognition and Machine Translation. By definition, it is a discriminator be- tween natural and unnatural language data. The fact that linguistically constrained synthetic data can be used to develop better LM for CM text is, on one hand an indirect statistical and task-based validation of the linguistic theory used to generate the data, and on the other hand an indication that the approach in general is promising and can help solve the issue of data scarcity for a variety of NLP tasks for CM text and speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Generating Synthetic Code-mixed Data</head><p>There is a large and growing body of linguis- tic research regarding the occurrence, syntac- tic structure and pragmatic functions of code- mixing in multilingual communities across the world. This includes many attempts to explain the grammatical constraints on CM, with three of the most widely-accepted being the <ref type="bibr">EmbeddedMatrix (Joshi, 1985;</ref><ref type="bibr" target="#b20">Myers-Scotton, 1993</ref><ref type="bibr" target="#b21">, 1995</ref>, the Equivalence Constraint (EC) <ref type="bibr" target="#b23">(Poplack, 1980;</ref><ref type="bibr" target="#b28">Sankoff, 1998</ref>) and the Functional Head Con- straint ( <ref type="bibr" target="#b9">DiSciullo et al., 1986;</ref><ref type="bibr" target="#b6">Belazi et al., 1994)</ref> theories.</p><p>For our experiments, we generate CM sentences as per the EC theory, since it explains a range of interesting CM patterns beyond lexical substitu- tion and is also suitable for computational model- ing. Further, in a brief human-evaluation we con- ducted, we found that it is representative of real CM usage. In this section, we list the assumptions made by the EC theory, briefly explain the theory, and then describe how we generate CM sentences as per this theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Assumptions of the EC Theory</head><p>Consider two languages L 1 and L 2 that are be- ing mixed. The EC Theory assumes that both languages are defined by context-free grammars G 1 and G 2 . It also assumes that every non- terminal category X 1 in G 1 has a corresponding non-terminal category X 2 in G 2 and that every ter- minal symbol (or word) w 1 in G 1 has a corre- sponding terminal symbol w 2 in G 2 . Finally, it assumes that every production rule in L 1 has a cor- responding rule in L 2 -i.e, the non-terminal cate- gories on the left-hand side of the two rules cor- respond to each other, and every category/symbol on the right-hand side of one rule corresponds to a category/symbol on the right-hand side of the other rule.</p><p>All these correspondences must also hold vice- versa (between languages L 2 and L 1 ), which im- plies that the two grammars can only differ in the ordering of categories/symbols on the right-hand side of any production rule. As a result, any sen- tence in L 1 has a corresponding translation in L 2 , with their parse trees being equivalent except for the ordering of sibling nodes. <ref type="figure">Fig.1</ref>(a) and (b) illustrate one such sentence pair in English and Spanish and their parse-trees. The EC Theory de- scribes a CM sentence as a constrained combina- tion of two such equivalent sentences.</p><p>While the assumptions listed above are quite strong, they do not prevent the EC Theory from being applied to two natural languages whose grammars do not correspond as described above. We apply a simple but effective strategy to recon- cile the structures of a sentence and its translation -if any corresponding subtrees of the two parse trees do not have equivalent structures, we col- lapse each of these subtrees to a single node. Ac- counting for the actual asymmetry between a pair of languages will certainly allow for the genera- tion of more CM variants of any L 1 -L 2 sentence pair. However, in our experiments, this strategy retains most of the structural information in the parse trees, and allows for the generation of up to thousands of CM variants of a single sentence pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Equivalence Constraint Theory</head><p>Sentence production. Given two monolingual sentences (such as those introduced in <ref type="figure">Fig.1</ref>), a CM sentence is created by traversing all the leaf nodes in the parse tree of either of the two sen- tences. At each node, either the word at that node or at the corresponding node in the other sentence's parse is generated. While the traver- sal may start at any leaf node, once the produc- tion enters one constituent, it will exhaust all the lexical slots (leaf nodes) in that constituent or its equivalent constituent in the other language before entering into a higher level constituent or a sister <ref type="figure">Figure 1</ref>: Parse trees of a pair of equivalent (a) English and (b) Spanish sentences, with corresponding hierarchical structure (due to production rules), internal nodes (non-terminal categories) and leaf nodes (terminal symbols), and parse trees of (c) incorrectly code-mixed and (d) correctly code-mixed variants of these sentences (as per the EC theory).</p><formula xml:id="formula_0">(a) S E VP E PP E NP E NN E house JJ E white DT E a IN E in VBZ E lives NP E PRP E She (b) S S VP S PP S NP S JJ S blanca NN S casa DT S una IN S en VBZ S vive NP S PRP S Elle (c) S VP PP NP JJ* white NN S casa DT S una IN S en VBZ E lives NP S PRP S Elle (d) S VP PP NP S JJ S blanca NN S casa DT S una IN E in VBZ E lives NP S PRP S Elle</formula><p>constituent. <ref type="bibr" target="#b28">(Sankoff, 1998)</ref> This guarantees that the parse tree of a sentence so produced will have the same hierarchical structure as the two mono- lingual parse trees <ref type="figure">(Fig. 1(c)</ref> and <ref type="formula">(d)</ref>).</p><p>The EC theory also requires that any mono- lingual fragment that occurs in the CM sentence must occur in one of the monolingual sentences (in the running example, the fragment una blanca would be disallowed since it does not appear in the Spanish sentence).</p><p>Switch-point identification. To ensure that the CM sentence does not at any point deviate from both monolingual grammars, the EC theory im- poses certain constraints on its parse tree. To this end and in order to identify the code-switching points in a generated sentence, nodes in its parse tree are assigned language labels according to the following rules: All leaf nodes are labeled by the languages of their symbols. If all the children of any internal node share a common label, the inter- nal node is also labeled with that language. Any node that is out of rank-order among its siblings according to one language is labeled with the other language. (See labeling in <ref type="figure">Fig.1</ref>(c) and (d)) If any node acquires labels of both languages during this process (such as the node marked with an asterisk in <ref type="figure">Fig.1(c)</ref>), the sentence is disallowed as per the EC theory. In the labeled tree, any pair of adjacent sibling nodes with contrasting labels are said to be at a switch-point (SP).</p><p>Equivalence constraint. Every switch-point identified in the generated sentence must abide by the EC. Let U → U 1 U 2 ...U n and V → V 1 V 2 ...V n be corresponding rules applied in the two mono- lingual parse trees, and nodes U i and V i+1 be ad- jacent in the CM parse tree. This pair of nodes is a switch-point, and it only abides by the EC if every node in U 1 ...U i has a corresponding node in V 1 ...V i . This is true for the switch-point in <ref type="figure">Fig.1(d)</ref>, and indicates that the two grammars are 'equivalent' at the code-switch point. More im- portantly, it shows that switching languages at this point does not require another switch later in the sentence. If every switch-point in the generated sentence abides by the EC, the generated sentence is allowed by the EC theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">System Description</head><p>We assume that the input to the generation model is a pair of parallel sentences in L 1 and L 2 , along with word level alignments. For our experiments, L 1 and L 2 are English and Spanish, and Sec 3.2 describes how we create the input set. We use the Stanford Parser ( <ref type="bibr" target="#b16">Klein and Manning, 2003)</ref> to parse the English sentence.</p><p>Projecting parses. We use the alignments to project the English parse tree onto the Spanish sentence in two steps: (1) We first replace every word in the English parse tree with its Spanish equivalent (2) We re-order the child nodes of each internal node in the tree such that their left-to-right order is as in the Spanish sentence. For instance, after replacing every English word in <ref type="figure">Fig.1</ref>(a) with its corresponding Spanish word, we interchange the positions of casa and blanca to arrive <ref type="figure">Fig.1(b)</ref>. For a pair of parallel sentences that follow all the assumptions of the EC theory, these steps can be performed without exception and result in the cre- ation of a Spanish parse tree with the same hierar- chical structure as the English parse.</p><p>We use various techniques to address cases in which the grammatical structures of the two sen- tences deviate. English words that are unaligned to any Spanish words are replaced by empty strings. (See <ref type="figure">Fig.2</ref> wherein the English word she has no Spanish counterpart, since this pronoun is dropped in the Spanish sentence.) Contiguous word se- quences in one sentence that are aligned to the <ref type="figure">Figure 2</ref>: (a) The parse of an English sentence as per Stanford CoreNLP. This parse is projected onto the parallel Spanish sentence Lo hará and modified during this process, to produce corre- sponding (b) English and (c) Spanish parse trees. same word(s) in the other language are collapsed into a single multi-word node, and the entire sub- tree between these collapsed nodes and their clos- est common ancestor is flattened to accommo- date this change (example in <ref type="figure">Fig.2</ref>). While these changes do result in slightly unnatural or simpli- fied parse trees, they are used very sparingly since English and Spanish have very compatible gram- mars.</p><formula xml:id="formula_1">(a) S E VP E VP E NP E PRP E it VB E do MD E will NP E NNP E She (b) S E VP E NP E PRP E it MD+VB E do will NP E NNP E She (c) S S VP S MD+VB S hará NP S PRP S lo NP S NNP S &lt;&gt;</formula><p>Generating CS sentences. The number of CS sentences that can be produced by combining a corresponding pair of English and Spanish sen- tences increases exponentially with the length of the sentences. Instead of generating these sen- tences exhaustively, we use the parses to construct a finite-state automaton that succinctly captures the acceptable CS sentences. Since the CS sen- tence must have the same hierarchical structure as the monolingual sentences, we construct the au- tomaton during a post-order traversal of the mono- lingual parses. An automaton is constructed at each node by (1) concatenating the automatons constructed at its child nodes, (2) splitting states and removing transitions to ensure that the EC the- ory is not violated. The last automaton to be con- structed, which is associated with the root node, accepts all the CS sentences that can be generated using the monolingual parses. We do not provide the exact details of automaton construction here, but we plan to release our code in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>In this work, we use three types of language data: monolingual data in <ref type="bibr">English and Spanish (Mono)</ref>, real code-mixed data (rCM), and artificial or gen- erated code-mixed data (gCM). In this section, we describe these datasets and their CM properties. We begin with description of some metrics that we shall use for quantification of the complexity of a CM dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Measuring CM Complexity</head><p>The CM data, both real and artificial, can vary in the their relative usage and ordering of L1 and L2 words, and thereby, significantly affect down- stream applications like language modeling. We use the following metrics to estimate the amount and complexity of code-mixing in the datasets.</p><p>Switch-point (SP): As defined in the last sec- tion, switch-points are points within a sentence where the languages of the words on the two sides are different. Intuitively, sentences that have more number of SPs are inherently more complex. We also define the metric SP Fraction (SPF) as the number of SP in a sentence divided by the total number of word boundaries in the sentence.</p><p>Code mixing index (CMI): Proposed by Gam- back and Das <ref type="bibr">(2014,</ref><ref type="bibr">2016)</ref>, CMI quantifies the amount of code mixing in a corpus by accounting for the language distribution as well as the switch- ing between them. Let N be the number of lan- guage tokens, x an utterance; let t L i be the tokens in language L i , P be the number of code switch- ing points in x. Then, the Code mixed index per utterance, C u (x) for x computed as follows,</p><formula xml:id="formula_2">Cu(x) = (N (x) − maxL i ∈L{tL i }(x)) + P (x) N (x)<label>(1)</label></formula><p>Note that all the metrics can be computed at the sentence level as well as at the corpus level by av- eraging the values for all the sentences in a corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Real Datasets</head><p>We chose to conduct all our experiments on English-Spanish CM tweets because English- Spanish CM is well documented <ref type="bibr" target="#b31">(Solorio and Liu, 2008)</ref>, is one of the most commonly mixed language pairs on social media ( <ref type="bibr" target="#b25">Rijhwani et al., 2017)</ref>, and a couple of CM tweet datasets are read- ily available ( <ref type="bibr" target="#b32">Solorio et al., 2014;</ref><ref type="bibr" target="#b25">Rijhwani et al., 2017</ref>). <ref type="table" target="#tab_0">Mono  English  100K  850K (48K) 0  0  Spanish  100K  860K (61K) 0  0  rCM  Train  100K</ref> 1.4M (91K) 0.31 0.105 Validation 100K</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset # Tweets # Words CMI SPF</head><p>1.4M (91K) <ref type="table">Table 1</ref>: Size of the datasets. Numbers in paren- thesis show the vocabulary size, i.e., the no. of unique words.</p><note type="other">0.31 0.106 Test-17 83K 1.1M (82K) 0.31 0.104 Test-14 13K 138K (16K) 0.12 0.06 gCM 31M 463M (79K) 0.75 0.35</note><p>For our experiments, we use a subset of the tweets collected by <ref type="bibr" target="#b25">Rijhwani et al. (2017)</ref> that were automatically identified as English, Span- ish or English-Spanish CM. The authors provided us around 4.5M monolingual tweets per language, and 283K CM tweets. These were already dedu- plicated and tagged for hashtags, URLs, emoti- cons and language labels automatically through the method proposed in the paper. <ref type="table">Table 1</ref> shows the sizes of the various datasets, which are also de- scribed below.</p><p>Mono: 50K tweets were sampled for Spanish and English from the entire collection of monolin- gual tweets. The Spanish tweets were translated to English and vice versa, which gives us a total of 100K monolingual tweets in each language. We shall refer to this dataset as Mono. The sampling strategy and reason for generating translations will become apparent in Sec. 3.3.</p><p>rCM: We use two real CM datasets in our ex- periment. The 283K real CM tweets provided by <ref type="bibr" target="#b25">Rijhwani et al. (2017)</ref> were randomly divided into training, validation and test sets of nearly equal sizes. Note that for most of our experiments, we will use a very small subset of the training set con- sisting of 5000 tweets as train data, because the fundamental assumption of this work is that very little amount of CM data is available for most lan- guage pairs (which is in fact true for most pairs beyond some very popularly mixed languages like English-Spanish). Nevertheless, the much larger training set is required for studying the effect of varying the amount of real CM data on our mod- els. We shall refer to this training dataset as rCM. The test set with 83K tweets will be re- ferred to as Test-17. We also use another dataset of English-Spanish CM tweets for testing our mod- els which was released during the language la- beling shared task at the Workshop on "Compu- tational Approaches to Code-switching, EMNLP 2014" ( <ref type="bibr" target="#b32">Solorio et al., 2014</ref>). We mixed the train- ing, validation and test datasets released during this shared task to construct a set of 13K tweets, which we shall refer to as Test-14. The two test datasets are tweets that were collected three years apart, and therefore, will help us estimate the ro- bustness of the language models. As shown in Ta- ble 1, these datasets are quite different in terms of CMI and average number of SP per tweet. For computing the CMI and SP, we used a English- Spanish LID to language tag the words. In fact, 9500 tweets in the Test-14 dataset are monolin- gual, but we chose to retain them because it re- flects the real distribution of CM data. Further, Test-14 also has manually annotated language la- bels, which will be helpful while conducting an in-depth analysis of the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Synthetic Code-Mixed Data</head><p>As described in the previous section, we use par- allel monolingual sentences to generate grammat- ically valid code mixed sentences. The entire pro- cess involves the following four steps.</p><p>Step 1: We created the parallel corpus by gen- erating translations for all the monolingual En- glish and Spanish tweets (4.5M each) using the Bing Translator API. <ref type="bibr">2</ref> We have found, that the translation quality varies widely across different sentences. Thus, we rank the translated sen- tences using Pseudo Fuzzy-match Score (PFS) <ref type="bibr" target="#b14">(He et al., 2010)</ref>. First, the forward translation engine (eg. English-to-Spanish) translates mono- lingual source sentence s into target t. Then the reverse translation system (eg. Spanish-English) translates target t into pseudo source s . Equa- tion 2 computes the PFS between s and s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P F S =</head><p>EditDistance(s, s ) max(|s|, |s |)</p><p>After manual inspection, we decided to select translation pairs whose P F S ≤ 0.7. The edit dis- tance is based on <ref type="bibr" target="#b36">Wagner and Fischer (1974)</ref>.</p><p>Step 2: We used the fast align toolkit 3 ( <ref type="bibr" target="#b10">Dyer et al., 2013)</ref>, to generate the word align- ments from these parallel sentences.</p><p>Step 3: The constituency parses for all the English tweets were obtained using the Stanford PCFG parser ( <ref type="bibr" target="#b16">Klein and Manning, 2003)</ref>.</p><p>Step 4: Using the parallel sentences, alignments and parse trees, we apply the Equivalent constraint theory (Sec 2.2) to generate all syntactically valid CM sentences while allowing for lexical substitu- tion.</p><p>We randomly selected 50K monolingual Span- ish and English tweets whose PFS ≤ 0.7. This gave us 200K monolingual tweets in all (Mono dataset) and the total amount of generated CM sentences from these 100K translation pairs was 31M, which we shall refer to as gCM. Note that even though we consider the Mono and gCM as two separate sets, in reality the EC model also generates the monolingual sentences; further, existence of gCM presumes existence of Mono. Hence, we also use Mono as part of all training experiments which use gCM.</p><p>We would also like to point out that the choice of experimenting with a much smaller set of tweets, only 50K per language, was made because the number of generated tweets even from this small set of monolingual tweet pairs is almost pro- hibitively large to allow experimentation with sev- eral models and their respective configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>Language modeling is a very widely researched topic <ref type="bibr" target="#b26">(Rosenfeld, 2000;</ref><ref type="bibr" target="#b7">Bengio et al., 2003;</ref><ref type="bibr" target="#b33">Sundermeyer et al., 2015)</ref>. In recent times, deep learn- ing has been successfully employed to build ef- ficient LMs ( <ref type="bibr" target="#b19">Mikolov et al., 2010;</ref><ref type="bibr" target="#b34">Sundermeyer et al., 2012;</ref><ref type="bibr" target="#b3">Arisoy et al., 2012;</ref><ref type="bibr">Che et al., 2017</ref>). <ref type="bibr" target="#b4">Baheti et al. (2017)</ref> recently showed that there is significant effect of the training curriculum, that is the order in which data is presented to an RNN- based LM, on the perplexity of the learnt English- Spanish CM language model on tweets. Along similar lines, in this study we focus our experi- ments on training curriculum, especially regarding the use of gCM data during training, which is the primary contribution of this paper.</p><p>We do not attempt to innovate in terms of the architecture or computational structure of the LM, and use a standard LSTM-based RNN LM <ref type="bibr" target="#b34">(Sundermeyer et al., 2012</ref>) for all our experiments. In- deed, there are enough reasons to believe that CM language is not fundamentally different from non- CM language, and therefore, should not require an altogether different LM architecture. Rather, the difference arises in terms of added complexity due to the presence of lexical items and syntactic struc- tures from two linguistic systems that blows up the space of valid grammatical and lexical configura- tions, which makes it essential to train the models on large volumes of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Curricula</head><p>Baheti et al. <ref type="formula" target="#formula_2">(2017)</ref> showed that rather than ran- domly mixing the monolingual and CM data dur- ing training, the best performance is achieved when the LM is first trained with a mixture of monolingual texts from both languages in nearly equal proportions, and ending with CM data. Mo- tivated by this finding, we define the following ba- sic training curricula ("X | Y" indicates training the model first with data X and then data Y):</p><formula xml:id="formula_4">(1) rCM, (2) Mono, (3) Mono | rCM, (4a) Mono | gCM, (4b) gCM | Mono, (5a) Mono | gCM | rCM, (5b) gCM | Mono | rCM Curricula 1-3 are baselines,</formula><p>where gCM data is not used. Note that curriculum 3 is the best case according to <ref type="bibr" target="#b4">Baheti et al. (2017)</ref>. Curricula 4a and 4b help us examine how far generated data can substitute real data. Finally, curricula 5a and 5b use all the data, and we would expect them to per- form the best.</p><p>Note that we do not experiment with other po- tential combinations (e.g., rCM | gCM | Mono) be- cause it is known (and we also see this in our ex- periments) that adding rCM data at the end always leads to better models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sampling from gCM</head><p>As we have seen in Sec 3.3 <ref type="figure" target="#fig_0">(Fig. 3)</ref>, in the EC model, a pair of monolingual parallel tweets gives rise to a large number (typically exponential in the length of the tweet) of CM tweets. On the other hand, in reality, only a few of those tweets would be observed. Further, if all the generated sentences are used to train an LM, it is not only computation- ally expensive, it also leads to undesirable results because the statistical properties of the distribution of the gCM corpus is very different from real data. We see this in our experiments (not reported in this paper for paucity of space), and also in <ref type="figure" target="#fig_1">Fig 4,</ref> where we plot the ratio of the frequencies of the words in gCM and Mono corpora (y-axis) against their original frequencies in Mono (x-axis). We can clearly see that the frequencies of the words are scaled up non-uniformly, the ratios varying be- tween 1 and 500,000 for low frequency words.</p><p>In order to reduce this skew, instead of select- ing the entire gCM data, we propose three sam- pling techniques for creating the training data from gCM:</p><p>Random: For each monolingual pair of parallel tweets, we randomly pick a fixed number, k, of CM tweets. We shall refer to the resultant training corpus as χ-gCM.</p><p>CMI-based: For each monolingual pair of par- allel tweets, we randomly pick k CM tweets and bucket them using CMI (in 0.1 intervals). Thus, in this case we can define two different curric- ula, where we present the data in increasing or decreasing order of CMI during training, which will be represented by the notations ↑-gCM and ↓-gCM respectively.</p><p>SPF-based: For each monolingual pair of par- allel tweets, we randomly pick k CM tweets such that the SPF distribution (section 3.1) of these tweets is similar to that of rCM data (as estimated from the validation set). This strategy will be re- ferred to as ρ-gCM.</p><p>Thus, depending on the gCM sampling strategy used, curricula 4a-b and 5a-b can have three differ- ent versions each. Note that since CMI for Mono is 0, ↑-gCM is not meaningful for 4b and 5b and similarly, ↓-gCM not for 4a and 5a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>For all our experiments, we use a 2 layered RNN with LSTM units and hidden layer dimension of 100. While training, we use sampled softmax with 5000 samples instead of a full softmax to speed up the training process. The sampling is based on the word frequency in the training corpus. We use momentum SGD with a learning rate of 0.002. We have used the CNTK toolkit for building our mod- els. <ref type="bibr">4</ref> We use a fixed k=5 (from each monolingual pair) for sampling the gCM data. We observed the performance on ↑-gCM to be the best when trained till CMI 0.4 and similarly on ↓-gCM when trained from 1.0 to 0.6. <ref type="table">Table 2</ref> presents the perplexities on validation, Test-14 and Test-17 datasets for all the models (Col. 3, 4 and 5). We observe the following trends: (1) Model 5(b)-ρ has the least perplex- ity value (significantly different from the second lowest value in the column, p &lt; 0.00001 for a paired t-test). (2) There is 55 and 90 point re- duction in perplexity on Test-17 and Test-14 sets respectively from the baseline experiment 3, that does not use gCM data. Thus, addition of gCM data is helpful. (3) Only the 4a and 4b models are worse than 3, while 5a and 5b models are better. Hence, rCM is indispensable, even though gCM helps. (4) SPF based sampling performs signifi- cantly better (again p &lt; 0.00001) than other sam- pling techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results</head><p>To put these numbers in perspective, we also trained our model on 50k monolingual English data, which gave a PPL of 264. This shows that the high PPL values our models obtain are due to the inherent complexity of modeling CM lan- guage. This is further substantiated by the PPL values computed only at the code-switch points, which are shown in <ref type="table">Table 2</ref>, col. 6, 7 and 8. Even for the best model, which in this case is 5(a)-χ, PPL is four times higher than the overall PPL on Test-17.</p><p>Run length: The complexity of modeling CM is also apparent from <ref type="table">Table 3</ref>, which reports the perplexity value of the 3 and 5 models for mono- lingual fragments of various run lengths. We de- fine run length as the number of words in a max- imal monolingual fragment or run within a tweet. In our analysis, we only consider runs of the em- bedded language, defined as the language that has fewer words. As one would expect, model 5(a)- χ performs the best for run length 1 (recall that it has lowest PPL at SP), but as the run length in- creases, the models sampling the gCM data us- ing CMI (5(a)-↑ and 5(b)-↓) are better than the randomly sampled (χ) models. Run length 1 are typically cases of word borrowing and lexical sub- stitution; higher run length segments are typically an indication of CM. Clearly, modeling the shorter runs of the embedded language seems to be one of the most challenging aspect of CM LM.</p><formula xml:id="formula_5">Sample size (k) 1 2 5 10 # tweets 93K 184K 497K 952K 5(b)-ρ 1081 1053 986 1019</formula><p>Significance of Linguistic Constraints: To understand the importance of the linguistic con- straints imposed by EC on generation of gCM, we conducted an experiment where a synthetic CM corpus was created by combining random contigu- ous segments from the monolingual tweets such that the generated CM tweets' SPF distribution matched that of rCM. When we replaced gCM by this corpus in 5(b)-ρ, the PPL on test-17 was 1060, which is worse than the baseline PPL.</p><p>Effect of rCM size: <ref type="table" target="#tab_0">Table 4</ref> shows the PPL values for models 3 and 5(b)-ρ when trained with different amounts of rCM data, keeping other pa- rameters constant. As expected, the PPL drops for both models as rCM size increases. However, even with high rCM data, gCM does help in improv- ing the LM until we have 50k rCM data (compa- rable to monolingual, and an unrealistic scenario in practice), where the returns of adding gCM starts diminishing. We also observe that in gen-eral, model 3 needs twice the amount of rCM data to perform as well as model 5(b)-ρ.</p><p>Effect of gCM size: In our sampling methods on gCM data, we fixed our sample size, k as 5 for consistency and feasibility of experiments. To un- derstand the effect of k (and hence the size of the gCM data), we experimented with k = 1, 2, and 10 keeping everything else fixed. <ref type="table" target="#tab_1">Table 5</ref> reports the results for the models 3 and 5(b)-ρ. We observe that unlike rCM data, increasing gCM data or k does not necessarily decrease PPL after a point. We speculate that there is trade-off between k and the amount of rCM data, and also probably be- tween these and the amount of monolingual data. We plan to explore this further in future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>We briefly describe the various types of ap- proaches used for building LM for CM text.</p><p>Bilingual models: These models combine data from monolingual data sources in both languages ( <ref type="bibr" target="#b37">Weng et al., 1997</ref>). Factored models: Geb- hardt (2011) uses Factored Language Models for rescoring n-best lists during ASR decoding. The factors used include POS tags, CS point prob- ability and LID. In <ref type="bibr" target="#b1">Adel et al.(2014b;</ref><ref type="bibr" target="#b0">2014a;</ref> RNNLMs are combined with n-gram based models, or converted to backoff models, giv- ing improvements in perplexity and mixed error rate. Models that incorporate linguistic con- straints: Li and Fung (2013) use inversion con- straints to predict CS points and integrates this prediction into the ASR decoding process. <ref type="bibr" target="#b18">Li and Fung (2014)</ref> integrates Functional Head con- straints (FHC) for code-switching into the Lan- guage Model for Mandarin-English speech recog- nition. This work uses parsing techniques to re- strict the lattice paths during decoding of speech to those permissible under the FHC theory. Our method instead imposes grammatical constraints (EC theory) to generate synthetic data, which can potentially be used to augment real CM data. This allows flexibility to deploy any sophisticated LM architecture and the synthetic data generated can also be used for CM tasks other than speech recog- nition. Training curricula for CM: <ref type="bibr" target="#b4">Baheti et al. (2017)</ref> show that a training curriculum where an RNN-LM is trained first with interleaved monolin- gual data in both languages followed by CM data gives the best results for English-Spanish LM. The perplexity of this model is 4544, which then re- duces to 298 after interpolation with a statistical n-gram LM. However, these numbers are not di- rectly comparable to our work because the datasets are different. Our work is an extension of this ap- proach showing that adding synthetic data further improves results.</p><p>We do not know of any work that uses syntheti- cally generated CM data for training LMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we presented a computational method for generating synthetic CM data based on the EC theory of code-mixing, and showed that sampling text from the synthetic corpus (according to the distribution of SPF found in real CM data) helps in reduction of PPL of the RNN-LM by an amount which is equivalently achieved by doubling the amount of real CM data. We also showed that randomly generated CM data doesn't improve the LM. Thus, the linguistic theory based generation is of crucial significance. There is no unanimous theory in linguistics on syntactic structure of CM language. Hence, as a future work, we would like to compare the useful- ness of different linguistic theories and different constraints within each theory in our proposed LM framework. This can also provide an indirect validation of the theories. Further, we would like to study sampling techniques motivated by natural distributions of linguistic structures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average number of gCM sentences (yaxis) vs mean input sentence length (x-axis)</figDesc><graphic url="image-1.png" coords="5,307.28,62.81,226.78,135.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Scatter plot of fractional increase in word frequency in gCM (y-axis) vs original frequency (x-axis).</figDesc><graphic url="image-2.png" coords="7,72.00,62.81,226.78,135.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Perplexity variation on Test-17 with changes in amount of rCM train data. Similar trends for other models (left for paucity of space)</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Variation of PPL on Test-17 with gCM 
sample size k. Similar trends for other models. 

</table></figure>

			<note place="foot" n="2"> https://www.microsoft.com/enus/translator/translatorapi.aspx</note>

			<note place="foot" n="3"> https://github.com/clab/fast align</note>

			<note place="foot" n="4"> https://www.microsoft.com/en-us/cognitive-toolkit/</note>

			<note place="foot"># rCM 0.5K 1K 2.5K 5K 10K 50K 3 1238 1186 1120 1041 991 812 5(b)-ρ 1181 1141 1068 986 951 808</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous reviewers for their valuable suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training curriculum Overall PPL</head><p>Avg. SP PPL <ref type="table">Valid Test-17 Test-14  Valid  Test-17 Test-14  1  rCM  1995  2018  1822  5598  5670  8864  2  Mono  1588  1607  892  23378  23790  26901  3  Mono | rCM  1029  1041  861  4734  4824</ref>    <ref type="table">Table 2</ref>: Perplexity of the LM Models on all tweets and only on SP (right block).  </p><note type="other">| rCM 1026 1038 836 4317 4386 5958 5(a)-↑ Mono | ↑-gCM | rCM 1045 1058 961 4983 5078 6861 5(a)-ρ Mono | ρ-gCM | rCM</note></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combining recurrent neural networks and factored language models during decoding of code-switching speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Telaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1415" to="1419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparing approaches to convert recurrent neural networks into backoff language models for efficient decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N T</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Telaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="651" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combination of recurrent neural networks and factored language models for code-switching language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (2)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="206" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebru</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvana</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACLHLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</title>
		<meeting>the NAACLHLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Curriculum design for code-switching: Experiments with language identification and language modeling with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Baheti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunayana</forename><surname>Sitaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monojit</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalika</forename><surname>Bali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICON-2017</title>
		<meeting>of ICON-2017<address><addrLine>Kolkata, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Code mixing: A challenge for language identification in the language of social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utsab</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amitava</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st Workshop on Computational Approaches to Code Switching</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Code switching and x-bar theory: The functional head constraint. Linguistic inquiry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Almeida Jacqueline</forename><surname>Edward J Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toribio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="221" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Tong Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07983</idno>
		<title level="m">Yangqiu Song, and Yoshua Bengio. 2017. Maximum-likelihood augmented discrete generative adversarial networks</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Government and code-mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-M</forename><surname>Disciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Muysken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Linguistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of ibm model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2013</title>
		<meeting>NAACL-HLT 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On measuring the complexity of code-mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gamback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1st Workshop on Language Technologies for Indian Social Media (Social-India)</title>
		<meeting>of the 1st Workshop on Language Technologies for Indian Social Media (Social-India)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comparing the level of code-switching in corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gamback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>of the 10th International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Speech recognition on englishmandarin code-switching data using factored language models</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Integrating n-best smt outputs into a tm system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="374" to="382" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Processing of Sentences with Intrasentential Code Switching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Parsing: Psychological, Computational, and Theoretical Perspectives</title>
		<editor>D. R. Dowty, L. Karttunen, and A. M. Zwicky</editor>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1985" />
			<biblScope unit="page" from="190" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st annual meeting of the association for computational linguistics. Association of Computational Linguistics</title>
		<meeting>the 41st annual meeting of the association for computational linguistics. Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved mixed language speech recognition using asymmetric acoustic model and language model with code-switch inversion constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="7368" to="7372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Language modeling with functional head constraint for code switching speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">JaňJaň Cernock`Cernock`y, and Sanjeev Khudanpur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Recurrent neural network based language model</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Duelling Languages:Grammatical structure in Code-switching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Myers-Scotton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Clarendon Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A lexically based model of code-switching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Myers-Scotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">One Speaker, Two Languages: Cross-disciplinary Perspectives on Code-switching</title>
		<editor>Lesley Milroy and Pieter Muysken</editor>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="233" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What is India speaking? Exploring the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rana</forename><forename type="middle">D</forename><surname>Parshad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Bhowmick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineeta</forename><surname>Chand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitu</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neha</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hinglish&quot; invasion. Physica A</title>
		<imprint>
			<biblScope unit="volume">449</biblScope>
			<biblScope unit="page" from="375" to="389" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sometimes Ill start a sentence in Spanish y termino en espaol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shana</forename><surname>Poplack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="581" to="618" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards sub-word level compositions for sentiment analysis of hindi-english code mixed text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameya</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasudeva</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2482" to="2491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Estimating code-switching on Twitter with a novel generalized word-level language identification technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shruti Rijhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sequiera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C S</forename><surname>Bali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maddila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two decades of statistical language modeling: Where do we go from here?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1270" to="1278" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding language preference for expression of opinion and sentiment: What do Hindi-English speakers do on Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koustav</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rijhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Begum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1131" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A formal production-based explanation of the facts of code-switching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sankoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bilingualism: language and cognition</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="39" to="50" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shallow parsing pipeline for hindi-english code-mixed social media text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motlani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mamidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Experiments with cross-lingual systems for synthesis of code-mixed text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunayana</forename><surname>Sitaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Sai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Rallabandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Rijhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th ISCA Speech Synthesis Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging for english-spanish code-switched text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Overview for the first shared task on language identification in codeswitched data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st Workshop on Computational Approaches to Code Switching</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="62" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">From feedforward to recurrent lstm neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="517" to="529" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lstm neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">POS Tagging of EnglishHindi Code-Mixed Social Media Content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogarshi</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="974" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The string-to-string correction problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="168" to="173" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A study of multilingual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neumeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUROSPEECH</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">1997</biblScope>
			<biblScope unit="page" from="359" to="362" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
