<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TranscRater: a Tool for Automatic Speech Recognition Quality Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Jalalvand</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><forename type="middle">G C</forename><surname>De Souza</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Falavigna</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><forename type="middle">R H</forename><surname>Qwaider</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">)</forename><surname>Fondazione</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Trento, Italy</roleName><forename type="first">Bruno</forename><surname>Kessler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TranscRater: a Tool for Automatic Speech Recognition Quality Estimation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics-System Demonstrations</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics-System Demonstrations <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="43" to="48"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present TranscRater, an open-source tool for automatic speech recognition (ASR) quality estimation (QE). The tool allows users to perform ASR evaluation bypassing the need of reference transcripts and confidence information, which is common to current assessment protocols. TranscRater includes: i) methods to extract a variety of quality indicators from (signal, transcription) pairs and ii) machine learning algorithms which make possible to build ASR QE models exploiting the extracted features. Confirming the positive results of previous evaluations, new experiments with TranscRater indicate its effectiveness both in WER prediction and transcription ranking tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>How to determine the quality of an automatic tran- scription without reference transcripts and with- out confidence information? This is the key prob- lem addressed by research on ASR quality estima- tion ( <ref type="bibr" target="#b3">C. de Souza et al., 2015;</ref><ref type="bibr" target="#b10">Jalalvand et al., 2015b)</ref>, and the task for which TranscRater, the tool described in this paper, has been designed.</p><p>The work on ASR quality estimation (ASR QE) has several motivations. First, the steady increase of applications involving automatic speech recog- nition (e.g. video/TV programs subtitling, voice search engines, voice question answering, spoken dialog systems, meeting and broadcast news tran- scriptions) calls for an accurate method to esti- mate ASR output quality at run-time. Often, in- deed, the nature of such applications (consider for instance spoken dialog systems) requires quick re- sponse capabilities that are incompatible with tra- ditional reference-based protocols.</p><p>Second, even when real-time processing is not a priority, standard evaluation based on computing word-error rate (WER) against gold references is not always a viable solution. In many situations (as in the case of languages for which even the ASR training data is scarce), the bottleneck repre- sented by the limited availability of reference tran- scripts and the costs of their manual creation calls for a method to predict ASR output quality that is reference-independent.</p><p>Third, even when designed to bypass the need of references, current quality prediction methods heavily depend on confidence information about the inner workings of the ASR system that pro- duced the transcriptions ( <ref type="bibr" target="#b5">Evermann and Woodland, 2000;</ref><ref type="bibr" target="#b20">Wessel et al., 2001</ref>). Such informa- tion, describing how the system is certain about the quality of its own hypotheses, often reflects a biased perspective influenced by individual de- coder features. More importantly, it is not always accessible and, in this frequent case, the sole ele- ments available for quality prediction are the sig- nal and its transcription (consider, for instance, the increasing amount of captioned Youtube videos generated by a "black-box" ASR system 1 ). These issues call for a method to predict ASR output quality that is also confidence-independent.</p><p>TranscRater (Transcription Rater) provides a unified ASR QE framework designed to meet the three aforementioned requirements. Its develop- ment was inspired by software previously released for the machine translation (MT) ( <ref type="bibr" target="#b18">Specia et al., 2013;</ref><ref type="bibr" target="#b17">Shah et al., 2014;</ref><ref type="bibr" target="#b16">Servan et al., 2015</ref>) equiv- alent of ASR QE, in which MT quality has to be estimated at run-time and without reference trans-lations ( <ref type="bibr" target="#b11">Mehdad et al., 2012;</ref><ref type="bibr" target="#b4">Camargo de Souza et al., 2013;</ref>). Indeed, the two tasks deal with similar issues. In both cases, we have an input "source" (a written sentence and a recorded signal) and an output text (a translation and a transcription) that has to be assessed with- out any pre-created term of comparison. They can also be approached with similar supervised clas- sification (C. de <ref type="bibr" target="#b3">Souza et al., 2015</ref>) or regression strategies ( <ref type="bibr" target="#b3">C. de Souza et al., 2015)</ref>. Finally, they have similar applications like:</p><p>• Deciding if an input source has been correctly processed;</p><p>• Ranking the output of multiple independent systems (Jalalvand et al., 2015b);</p><p>• Estimating the human effort required to man- ually revise an output segment;</p><p>• Performing data selection for system im- provement based on active learning.</p><p>To support these applications, TranscRater pro- vides an extensible ASR QE framework consist- ing of a variety of feature extractors and ma- chine learning algorithms. The implemented fea- ture extraction methods allow capturing predictive quality indicators both from the input signal and from the output transcription. This basic set of "black box" indicators has been successfully eval- uated in a number of experiments, both on regres- sion and on classification tasks, showing that ASR QE predictions can closely approximate the qual- ity scores obtained with standard reference-based methods. The existing feature extractors can be easily extended to integrate new features, either capturing additional system-independent aspects, or relying on confidence information about the ASR system that produced the transcriptions, if available. Experimental results demonstrate that, also in the "glass-box" scenario in which the ASR system is known, the available features are able to improve the performance obtained with confi- dence information.</p><p>The integration of different machine learning algorithms makes TranscRater a powerful frame- work to quickly set up an ASR QE model given some training data, tune it by choosing among the possible feature configurations and process new, unseen test data to predict their quality. As a stand- alone environment, with few documented external dependencies, TranscRater provides the first off- the-shelf solution to approach ASR QE and extend its application to new scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ASR QE</head><p>The basic ASR QE task consists in training a model from (signal, transcription, label) triplets, and using it to return quality predictions for a test set of unseen (signal, transcription) instances. In this supervised learning setting, the training la- bels can be either numeric scores ( ) or class identifiers (binary or multi-class) (C. de <ref type="bibr" target="#b3">Souza et al., 2015)</ref>. Class assignments can be manually done according to some criteria, or inferred by thresholding numeric scores. Nu- meric quality indicators can be easily obtained by measuring the similarity (or the distance) between the transcription and its manually-created refer- ence. For instance, the models described in pre- vious works on ASR QE learn from training data labelled with real values obtained by computing the transcription word error rate (WER 2 ).</p><p>According to the type of training labels, the problem can be approached either as a regression or as a classification task. As a consequence, also the evaluation metrics will change. Preci- sion/recall/F1 (or other metrics, such as balanced accuracy, in case of very unbalanced distributions) will be used for classification while, similar to MT QE, the mean absolute error (MAE) or sim- ilar metrics will be used for regression.</p><p>A variant of the basic ASR QE task is to con- sider it as a QE-based ranking problem (Jalalvand et al., 2015b), in which each utterance is captured by multiple microphones or transcribed by multi- ple ASR systems. In this case, the capability to rank transcriptions from the best to the worst can be evaluated in terms of normalized discounted cu- mulative gain (NDCG) or similar metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The TranscRater tool</head><p>TranscRater combines in a single open-source framework: i) a set of features capturing differ- ent aspects of transcription quality and ii) different learning algorithms suitable to address the chal- lenges posed by different application scenarios.</p><p>TranscRater internally consists of two main modules: feature extraction and machine learn- ing. At training stage, the tool receives as in- put a set of signal recordings, their transcriptions and the corresponding reference transcripts. The speech signals are provided as separate files in the RIFF Microsoft PCM format with 16K sampling rate. Their transcriptions and the corresponding references are provided in single separate text files (one transcription per row). References are used to compute the WER label of each training in- stance, thus connecting the problem to the task formulation provided in §2. The features extracted from each training instance are passed to the learn- ing module, together with the corresponding label. The label is a WER score which, depending on the type of problem addressed, can be used either to directly train a regressor or to infer a ranking for multiple hypotheses. In either case, the learning module will train the corresponding model with the proper learning algorithm, and tune it using k- fold cross-validation. At test stage, the model is used to predict the label of new, unseen (signal, transcription) in- stances. For each test point, the output is either a WER prediction or a rank, whose reliability can be respectively evaluated in terms of MAE or NDCG (as discussed in §2). Output predictions are pro- vided in a single file (one WER prediction per row for regression and one rank prediction per row for ranking). MAE or NDCG scores are provided as the standard output of the test functions.</p><p>Internally, TranscRater stores the extracted fea- tures in the SVM-light 3 format. This makes pos- sible to use the tool as a feature extractor and to embed it in applications different from the ones described in this paper. The features to be used, the type of learning algorithm, the input files and the links to resources and libraries can be easily set through a configuration file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature sets</head><p>The feature extraction module of TranscRater al- lows the user to extract 72 features that can be cat- egorized in the following four groups:</p><p>• Signal (SIG) features, designed to capture the difficulty of transcribing the input sig- nal given the general recording conditions in which it was acquired;</p><p>• Lexical (LEX) features, designed to capture 3 http://svmlight.joachims.org/ the difficulty to transcribe the input signal given the pronunciation difficulty and the am- biguity of the terms it contains;</p><p>• Language model (LM) features, designed to capture the plausibility of the transcription from the fluency point of view;</p><p>• Part-of-speech (POS) features, designed to capture the plausibility of the transcription from the syntax point of view.</p><p>SIG (44). Signal features are extracted using the OpenSmile 4 toolkit ( <ref type="bibr" target="#b6">Eyben et al., 2013)</ref>. Each speech signal is broken down into 25ms length frames with 10ms overlap. For each frame, we compute 13 Mel Frequency Cepstral Coefficients (MFCC), their delta, acceleration and log-energy as well as the prosody features like fundamental frequency (F0), voicing probability, loudness con- tours and pitch. The final SIG feature vector for the entire input signal is obtained by averaging the values of each feature computed on all the frames. LEX (7). Lexicon-based features are extracted using a lexical feature dictionary (optionally pro- vided by the user). In this dictionary each indi- vidual word is assigned to a feature vector con- taining the frequency of fricatives, liquids, nasals, stops and vowels in its pronunciation. Other ele- ments of the vector are the number of homophones (words with the same pronunciation) and quasi- homophones (words with similar pronunciation).</p><p>LM (12). Language model features include the mean of word probabilities, the sum of the log probabilities and the perplexity score for each transcription. In previous experiments <ref type="bibr" target="#b10">(Jalalvand et al., 2015b;</ref><ref type="bibr" target="#b8">Jalalvand and Falavigna, 2015)</ref> we showed that, instead of only one LM, using a combination of neural network and n-gram LMs trained on task-specific and generic data can sig- nificantly improve the accuracy of quality predic- tion. For this reason, TranscRater allows using up to four different language models: two RNNLM ( <ref type="bibr" target="#b12">Mikolov et al., 2010</ref>) trained on generic and spe- cific data and two n-gramLM trained on generic and specific data. To work with neural network LMs, the tool makes use of RNNLM, 5 while for n-gram LMs it uses SRILM 6 ( <ref type="bibr" target="#b19">Stolcke et al., 2000</ref>). POS (9). Part-of-speech features are extracted using the TreeTagger. <ref type="bibr">7</ref> For each word in the tran- scription, they consider the score assigned to the predicted POS of the word itself, the previous and the following one. This sliding window is used to compute the average value for the entire tran- scription and obtain the sentence-level POS fea- ture vector. The intuition is that a low confidence of the POS tagger in labeling a sentence is an indi- cator of possible syntax issues and, in turn, of poor transcription quality. POS features also include the number and the percentage of content words (numbers, nouns, verbs, adjectives, adverbs).</p><p>These feature groups were successfully tested in various conditions including clean/noisy data, sin- gle/multiple microphones and ASR systems <ref type="bibr" target="#b10">(Jalalvand et al., 2015b;</ref><ref type="bibr" target="#b9">Jalalvand et al., 2015a</ref>). In such conditions, they proved to be a reliable predictor when confidence information about the ASR sys- tem inner workings is not accessible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning algorithms</head><p>For regression-based tasks (WER prediction), TranscRater includes an interface to the Scikit- learn package <ref type="bibr" target="#b14">(Pedregosa et al., 2011</ref>), a Python machine learning library that contains a large set of classification and regression algorithms. Based on the empirical results reported in ( <ref type="bibr" target="#b3">C. de Souza et al., 2015;</ref><ref type="bibr" target="#b10">Jalalvand et al., 2015b)</ref>, which indicate that Extremely Random- ized Trees (XRT ( <ref type="bibr" target="#b7">Geurts et al., 2006</ref>)) is a very competitive algorithm in several WER prediction tasks, the current version of the tool exploits XRT. However, adapting the interface to apply other al- gorithms is an easy task and one of the future extension directions. The main hyper-parameters of the model, such as the number of tree bags, the number of trees per bag, the number of fea- tures per tree and the number of instances in the leaves, are tuned using grid search with k-fold cross-validation on the training set to minimize the mean absolute error (MAE) between the true WERs and the predicted ones.</p><p>As mentioned before, TranscRater provides the possibility to evaluate multiple transcriptions (e.g. obtained from different microphones or ASR sys- tems) and rank them based on their quality. This can be done either indirectly, by exploiting the pre- dicted WER labels in a "ranking by regression" approach (RR) or directly, by exploiting machine- learned ranking methods (MLR). To train and test MLR models, TranscRater exploits RankLib 8 , a li- brary of learning-to-rank algorithms. The current version of the tool includes an interface to the Ran- dom Forest algorithm (RF <ref type="figure">(Breiman, 2001)</ref>), the same used in <ref type="figure">(Jalalvand et al., 2015b)</ref>.</p><p>MLR predicts ranks through pairwise compari- son between the transcriptions. The main param- eters such as the number of bags, the number of trees per bag and the number of leaves per tree are tuned on training set using k-fold cross-validation to maximize the NDCG measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation</head><p>TranscRater is written in Python and is made of several parts linked together using bash scripts. In order to run the toolkit on Linux, the follow- ing libraries are required: i) Java 8 (JDK-1.8); ii) Python 2.7 (or above) and iii) Scikit-learn (ver- sion 0.15.2). Moreover, the user has to download and compile the following libraries: OpenSmile, RNNLM, SRILM and TreeTagger for the feature extraction module as well as RankLib for using machine-learned ranking option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Benchmarking</head><p>The features and algorithms contained in Tran- scRater have been successfully used in previous works ( <ref type="bibr" target="#b3">C. de Souza et al., 2015;</ref><ref type="bibr" target="#b10">Jalalvand et al., 2015b;</ref><ref type="bibr" target="#b9">Jalalvand et al., 2015a</ref>). To further investigate their effectiveness, in this section we provide new results, both in WER pre- diction (MAE) and transcription ranking (NDCG), together with some efficiency analysis (Time in seconds 9 ). To this aim, we use data from the 3 rd CHiME challenge, 10 which were collected for multiple distant microphone speech recognition in noisy environments ( <ref type="bibr" target="#b0">Barker et al., 2015)</ref>. CHiME- 3 data consists of sentences of the Wall Street Journal corpus, uttered by four speakers in four noisy environments, and recorded by five micro- phones placed on the frame of a tablet PC (a sixth one, placed on the back, mainly records back- ground noise). Training and test respectively con- tain 1,640 and 1,320 sentences. Transcriptions are produced by a baseline ASR system, provided by the task organizers, which uses the deep neural network recipe of <ref type="bibr">Kaldi (Povey et al., 2011</ref>).</p><p>In WER prediction, different models built with TranscRater are compared with a baseline com- monly used for regression tasks, which labels all the test instances with the average WER value computed on the training set. In ranking mode, baseline results are computed by averaging the NDCG scores obtained in one hundred iterations in which test instances are randomly ranked.  <ref type="table">Table 1</ref> shows the results of models trained with different feature groups for WER prediction with a single microphone. In terms of time, in this as in the following experiments, the total time (fea- ture extraction + training + test) is mostly de- termined by feature extraction and the bottleneck is clearly represented by the extraction of signal (SIG) features. In terms of MAE, SIG features are also those achieving the worst result. Although they significantly improve over the baseline, they are outperformed by LEX+LM+POS and, even in combination with them, they do not help. How- ever, as suggested by previous works like ) in which some of the SIG fea- tures are among the most predictive ones, the use- fulness of signal features highly depends on data and, in specific conditions, they definitely improve results. Their ineffectiveness in the experiments of this paper likely depends on the lack of word- level time boundaries, which prevented us to com- pute more discriminative features like word log- energies, noise log-energies and signal-to-noise ratio (the best indicator of the acoustic quality of an input utterance).  <ref type="table">Table 2</ref> shows the results achieved by the same feature groups when ranking by regression (RR) the transcriptions from five microphones. In terms of computation time, the higher costs of SIG fea- tures are still evident (the significant increase for all groups is due to the higher number of audio files to be processed). Also in this case, SIG features do not help, neither alone nor in combi- nation with the other groups. Indeed, the high- est results are achieved by the combination of LEX+LM+POS. Their large NDCG improvement over the baseline (+6.8), combined with the sig- nificantly lower computation time, seems to make this combination particularly suitable for the rank- ing by regression strategy.  <ref type="table">Table 3</ref> shows the results achieved, in the same multi-microphone scenario, by the machine- learned ranking approach (MLR). In terms of time, MLR is slightly more efficient than RR, at least on this dataset. Though surprising (MLR performs lots of pairwise comparisons, which are in principle more demanding), such difference is not very informative as it might depend on hyper- parameter settings (e.g. the number of iterations for XRT, manually set to 20), whose optimization was out of the scope of our analysis. In terms of NDCG, the results are higher compared to RR but the differences between feature groups are con- firmed. Interestingly, with MLR even the SIG fea- tures in isolation significantly improve over the baseline (+4.5 points). The NDCG improvement with the combined feature groups is up to 9.5 points, confirming the effectiveness of the com- bined features shown in previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented TranscRater, an open-source tool for ASR quality estimation. TranscRater pro- vides an extensible framework including feature extractors, machine learning algorithms (for WER prediction and transcription ranking), optimiza- tion and evaluation functions. Its source code can be downloaded from https://github. com/hlt-mt/TranscRater. Its license is FreeBSD, a lax permissive non-copyleft license, compatible with the GNU GPL and with any use, including commercial.</p></div>
			<note place="foot" n="1"> More than 157 millions in 10 languages, as announced by Google already in 2012 (source: http://goo.gl/ 5Wlkjl).</note>

			<note place="foot" n="2"> WER is the minimum edit distance between the transcription and the reference. Edit distance is calculated as the number of edits (word insertions, deletions, substitutions) divided by the number of words in the reference.</note>

			<note place="foot" n="4"> http://www.audeering.com/research/ opensmile\#download 5 http://www.fit.vutbr.cz/ ˜ imikolov/ rnnlm/rnnlm-0.3e.tgz 6 http://www.speech.sri.com/projects/ srilm/download.html</note>

			<note place="foot" n="7"> http://www.cis.uni-muenchen. de/ ˜ schmid/tools/TreeTagger/data/ tree-tagger-linux-3.2.tar.gz</note>

			<note place="foot" n="8"> https://people.cs.umass.edu/ ˜ vdang/ data/RankLib-v2.1.tar.gz 9 Experiments were run with a PC with 8 Intel Xeon processors 3.4 GHz and 8 GB RAM. 10 http://spandh.dcs.shef.ac.uk/chime_ challenge/chime2015/data.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The third &apos;CHiME&apos; Speech Separation and Recognition Challenge: Dataset, Task and Baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marxer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 15th IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<meeting>of the 15th IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)<address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Random Forests. Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FBK-UPVUEdin participation in the WMT14 Quality Estimation shared-task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G C</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>González-Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>of the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="322" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multitask Learning for Adaptive Quality Estimation of Automatically Transcribed Utterances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G C</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Falavigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies (NAACL-HLT)</title>
		<meeting>of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies (NAACL-HLT)<address><addrLine>Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="714" to="724" />
		</imprint>
	</monogr>
	<note>Denver</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FBK-UEdin Participation to the WMT13 Quality Estimation Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Camargo De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>of the Eighth Workshop on Statistical Machine Translation<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="352" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large Vocabulary Decoding and Confidence Estimation using Word Posterior Probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>of the IEEE International Conference on Acoustics, Speech, and Signal essing (ICASSP)<address><addrLine>Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1655" to="1658" />
		</imprint>
	</monogr>
	<note>Istanbul</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recent developments in opensmile, the munich open-source multimedia feature extractor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Multimedia</title>
		<meeting>of ACM Multimedia<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="835" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extremely Randomized Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="3" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stacked AutoEncoder for ASR Error Detection and Word Error Rate Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jalalvand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Falavigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 16th Annual Conference of the International Speech Communication Association (INTERPSEECH)</title>
		<meeting>of the 16th Annual Conference of the International Speech Communication Association (INTERPSEECH)<address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2142" to="2146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boosted Acoustic Model Learning and Hypotheses Rescoring on the CHiME3 Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jalalvand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Falavigna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matassoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Svaizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<meeting>of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)<address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="409" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Driving ROVER With Segment-based ASR Quality Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jalalvand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Falavigna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1095" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Match without a Referee: Evaluating MT Adequacy without Reference Translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Machine Translation Workshop (WMT2012)</title>
		<meeting>of the Machine Translation Workshop (WMT2012)<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent Neural Network Based Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERSPEECH</title>
		<meeting>of INTERSPEECH<address><addrLine>Makuhari, Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Quality Estimation for Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G C</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Daniele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 25th International Conference on Computational Linguistics: Technical Papers (COLING)</title>
		<meeting>of the 25th International Conference on Computational Linguistics: Technical Papers (COLING)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1813" to="1823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Kaldi Speech Recognition Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<meeting>of the IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)<address><addrLine>Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An open source toolkit for wordlevel confidence estimation in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Q</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lecouteux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th International Workshop on Spoken Language Translation (IWSLT)</title>
		<meeting>of the 12th International Workshop on Spoken Language Translation (IWSLT)<address><addrLine>Vietnam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An Efficient and User-friendly Tool for Machine Translation Quality Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quest-a translation quality estimation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The SRI March 2000 HUBS Conversational Speech Transcription System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Butzberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plauche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sonmez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the NIST Speech Transcription Workshop</title>
		<meeting>of the NIST Speech Transcription Workshop</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Confidence Measures for Large Vocabulary Continuous Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schluter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="288" to="298" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
