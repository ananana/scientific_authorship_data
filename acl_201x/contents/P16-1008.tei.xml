<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Coverage for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Coverage for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="76" to="85"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Attention mechanism has enhanced state-of-the-art Neural Machine Translation (NMT) by jointly learning to align and translate. It tends to ignore past alignment information, however, which often leads to over-translation and under-translation. To address this problem, we propose coverage-based NMT in this paper. We maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust future attention, which lets NMT system to consider more about untranslated source words. Experiments show that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The past several years have witnessed the rapid progress of end-to-end Neural Machine Transla- tion (NMT) <ref type="bibr" target="#b22">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. Unlike conventional Statistical Ma- chine Translation (SMT) ( <ref type="bibr" target="#b12">Koehn et al., 2003;</ref><ref type="bibr" target="#b4">Chiang, 2007</ref>), NMT uses a single and large neural network to model the entire translation process. It enjoys the following advantages. First, the use of distributed representations of words can alleviate the curse of dimensionality ( <ref type="bibr" target="#b1">Bengio et al., 2003)</ref>. Second, there is no need to explicitly design fea- tures to capture translation regularities, which is quite difficult in SMT. Instead, NMT is capable of learning representations directly from the training data. Third, Long Short-Term Memory <ref type="bibr">(Hochreiter and Schmidhuber, 1997</ref>) enables NMT to cap-ture long-distance reordering, which is a signifi- cant challenge in SMT.</p><p>NMT has a serious problem, however, namely lack of coverage. In phrase-based SMT ( <ref type="bibr" target="#b12">Koehn et al., 2003)</ref>, a decoder maintains a coverage vec- tor to indicate whether a source word is translated or not. This is important for ensuring that each source word is translated in decoding. The decod- ing process is completed when all source words are "covered" or translated. In NMT, there is no such coverage vector and the decoding process ends only when the end-of-sentence mark is pro- duced. We believe that lacking coverage might result in the following problems in conventional NMT:</p><p>1. Over-translation: some words are unneces- sarily translated for multiple times;</p><p>2. Under-translation: some words are mistak- enly untranslated.</p><p>Specifically, in the state-of-the-art attention-based NMT model ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>, generating a target word heavily depends on the relevant parts of the source sentence, and a source word is in- volved in generation of all target words. As a result, over-translation and under-translation in- evitably happen because of ignoring the "cover- age" of source words (i.e., number of times a source word is translated to a target word). Fig- ure 1(a) shows an example: the Chinese word "gu¯ anb`anb`ı" is over translated to "close(d)" twice, while "b` eipò" (means "be forced to") is mistak- enly untranslated.</p><p>In this work, we propose a coverage mechanism to NMT (NMT-COVERAGE) to alleviate the over- translation and under-translation problems. Basi- cally, we append a coverage vector to the inter- mediate representations of an NMT model, which are sequentially updated after each attentive read (a) Over-translation and under-translation generated by NMT.</p><p>(b) Coverage model alleviates the problems of over-translation and under-translation. In conven- tional NMT without coverage, the Chinese word "gu¯ anb`anb`ı" is over translated to "close(d)" twice, while "b` eipò" (means "be forced to") is mistakenly untranslated. Coverage model alleviates these problems by tracking the "coverage" of source words.</p><p>during the decoding process, to keep track of the attention history. The coverage vector, when en- tering into attention model, can help adjust the fu- ture attention and significantly improve the over- all alignment between the source and target sen- tences. This design contains many particular cases for coverage modeling with contrasting character- istics, which all share a clear linguistic intuition and yet can be trained in a data driven fashion. No- tably, we achieve significant improvement even by simply using the sum of previous alignment prob- abilities as coverage for each word, as a success- ful example of incorporating linguistic knowledge into neural network based NLP models. Experiments show that NMT-COVERAGE sig- nificantly outperforms conventional attention- based NMT on both translation and alignment tasks. <ref type="figure" target="#fig_0">Figure 1(b)</ref> shows an example, in which NMT-COVERAGE alleviates the over-translation and under-translation problems that NMT without coverage suffers from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Our work is built on attention-based NMT <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>, which simultaneously con- ducts dynamic alignment and generation of the target sentence, as illustrated in <ref type="figure">Figure 2</ref>. It <ref type="figure">Figure 2</ref>: Architecture of attention-based NMT. Whenever possible, we omit the source index j to make the illustration less cluttered. produces the translation by generating one target word y i at each time step. Given an input sentence x = {x 1 , . . . , x J } and previously generated words {y 1 , . . . , y i−1 }, the probability of generating next word y i is</p><formula xml:id="formula_0">P (y i |y &lt;i , x) = sof tmax g(y i−1 , t i , s i )<label>(1)</label></formula><p>where g is a non-linear function, and t i is a decod- ing state for time step i, computed by</p><formula xml:id="formula_1">t i = f (t i−1 , y i−1 , s i )<label>(2)</label></formula><p>Here the activation function f (·) is a Gated Re- current Unit (GRU) ( <ref type="bibr" target="#b6">Cho et al., 2014b)</ref>, and s i is a distinct source representation for time i, calcu- lated as a weighted sum of the source annotations:</p><formula xml:id="formula_2">s i = J j=1 α i,j · h j<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">h j = [ − → h j ; ← − h j ]</formula><p>is the annotation of x j from a bi-directional Recurrent Neural Net- work (RNN) <ref type="bibr">(Schuster and Paliwal, 1997</ref>), and its weight α i,j is computed by</p><formula xml:id="formula_4">α i,j = exp(e i,j ) J k=1 exp(e i,k )<label>(4)</label></formula><p>and</p><formula xml:id="formula_5">e i,j = a(t i−1 , h j ) = v a tanh(W a t i−1 + U a h j )<label>(5)</label></formula><p>is an attention model that scores how well y i and h j match. With the attention model, it avoids the need to represent the entire source sentence with a single vector. Instead, the decoder selects parts of the source sentence to pay attention to, thus exploits an expected annotation s i over possible alignments α i,j for each time step i. However, the attention model fails to take ad- vantage of past alignment information, which is found useful to avoid over-translation and under- translation problems in conventional SMT ( <ref type="bibr" target="#b12">Koehn et al., 2003)</ref>. For example, if a source word is translated in the past, it is less likely to be trans- lated again and should be assigned a lower align- ment probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Coverage Model for NMT</head><p>In SMT, a coverage set is maintained to keep track of which source words have been translated ("cov- ered") in the past. Let us take x = {x 1 , x 2 , x 3 , x 4 } as an example of input sentence. The initial cov- erage set is C = {0, 0, 0, 0} which denotes that no source word is yet translated. When a trans- lation rule bp = (x 2 x 3 , y m y m+1 ) is applied, we produce one hypothesis labelled with coverage C = {0, 1, 1, 0}. It means that the second and third source words are translated. The goal is to gener- ate translation with full coverage C = {1, 1, 1, 1}. A source word is translated when it is covered by one translation rule, and it is not allowed to be translated again in the future (i.e., hard coverage). In this way, each source word is guaranteed to be translated and only be translated once. As shown, coverage is essential for SMT since it avoids gaps and overlaps in translation of source words.</p><p>Modeling coverage is also important for attention-based NMT models, since they gener- ally lack a mechanism to indicate whether a cer- tain source word has been translated, and there- fore are prone to the "coverage" mistakes: some parts of source sentence have been translated more than once or not translated. For NMT models, di- rectly modeling coverage is less straightforward, but the problem can be significantly alleviated by keeping track of the attention signal during the de- coding process. The most natural way for doing that would be to append a coverage vector to the annotation of each source word (i.e., h j ), which is initialized as a zero vector but updated after ev- ery attentive read of the corresponding annotation. The coverage vector is fed to the attention model to help adjust future attention, which lets NMT system to consider more about untranslated source words, as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>Formally, the coverage model is given by</p><formula xml:id="formula_6">C i,j = g update C i−1,j , α i,j , Φ(h j ), Ψ<label>(6)</label></formula><p>where</p><p>• g update (·) is the function that updates C i,j af- ter the new attention α i,j at time step i in the decoding process;</p><p>• C i,j is a d-dimensional coverage vector sum- marizing the history of attention till time step i on h j ;</p><p>• Φ(h j ) is a word-specific feature with its own parameters;</p><p>• Ψ are auxiliary inputs exploited in different sorts of coverage models.</p><p>Equation 6 gives a rather general model, which could take different function forms for g update (·) and Φ(·), and different auxiliary inputs Ψ (e.g., previous decoding state t i−1 ). In the rest of this section, we will give a number of representative implementations of the coverage model, which either leverage more linguistic information (Sec- tion 3.1.1) or resort to the flexibility of neural net- work approximation (Section 3.1.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Linguistic Coverage Model</head><p>We first consider at linguistically inspired model which has a small number of parameters, as well as clear interpretation. While the linguistically- inspired coverage in NMT is similar to that in SMT, there is one key difference: it indicates what percentage of source words have been translated (i.e., soft coverage). In NMT, each target word y i is generated from all source words with probabil- ity α i,j for source word x j . In other words, the source word x j is involved in generating all tar- get words and the probability of generating target word y i at time step i is α i,j . Note that unlike in SMT in which each source word is fully trans- lated at one decoding step, the source word x j is partially translated at each decoding step in NMT. Therefore, the coverage at time step i denotes the translated ratio of that each source word is trans- lated. We use a scalar (d = 1) to represent linguis- tic coverage for each source word and employ an accumulate operation for g update . The initial value of linguistic coverage is zero, which de- notes that the corresponding source word is not translated yet. We iteratively construct linguis- tic coverages through accumulation of alignment probabilities generated by the attention model, each of which is normalized by a distinct context- dependent weight. The coverage of source word x j at time step i is computed by</p><formula xml:id="formula_7">C i,j = C i−1,j + 1 Φ j α i,j = 1 Φ j i k=1 α k,j<label>(7)</label></formula><p>where Φ j is a pre-defined weight which indicates the number of target words x j is expected to gener- ate. The simplest way is to follow <ref type="bibr" target="#b23">Xu et al. (2015)</ref> in image-to-caption translation to fix Φ = 1 for all source words, which means that we directly use the sum of previous alignment probabilities with- out normalization as coverage for each word, as done in ( <ref type="bibr" target="#b8">Cohn et al., 2016)</ref>. However, in machine translation, different types of source words may contribute differently to the generation of target sentence. Let us take the sentence pairs in <ref type="figure" target="#fig_0">Figure 1</ref> as an example. The noun in the source sentence "j¯ ıchˇangıchˇang" is translated into one target word "airports", while the adjec- tive "b` eipò" is translated into three words "were forced to". Therefore, we need to assign a dis- tinct Φ j for each source word. Ideally, we expect Φ j = I i=1 α i,j with I being the total number of time steps in decoding. However, such desired value is not available before decoding, thus is not suitable in this scenario.</p><p>Fertility To predict Φ j , we introduce the con- cept of fertility, which is firstly proposed in word- level SMT ( <ref type="bibr" target="#b2">Brown et al., 1993</ref>). Fertility of source word x j tells how many target words x j produces. In SMT, the fertility is a random variable Φ j , whose distribution p(Φ j = φ) is determined by the parameters of word alignment models (e.g., IBM models). In this work, we simplify and adapt fertility from the original model and compute the fertility Φ j by 2</p><formula xml:id="formula_8">Φ j = N (x j |x) = N · σ(U f h j ) (8)</formula><p>where N ∈ R is a predefined constant to denote the maximum number of target words one source . Since Φ j does not depend on i, we can pre-compute it before decoding to mini- mize the computational cost. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Neural Network Based Coverage Model</head><formula xml:id="formula_9">C i,j = f (C i−1,j , α i,j , h j , t i−1 )</formula><p>where f (·) is a nonlinear activation function and t i−1 is the auxiliary input that encodes past trans- lation information. Note that we leave out the word-specific feature function Φ(·) and only take the input annotation h j as the input to the cov- erage RNN. It is important to emphasize that the NN-based coverage model is able to be fed with arbitrary inputs, such as the previous attentional context s i−1 . Here we only employ C i−1,j for past alignment information, t i−1 for past translation in- formation, and h j for word-specific bias. 3</p><p>Gating The neural function f (·) can be either a simple activation function tanh or a gating func- tion that proves useful to capture long-distance <ref type="bibr">3</ref> In our preliminary experiments, considering more inputs (e.g., current and previous attentional contexts, unnormal- ized attention weights ei,j) does not always lead to better translation quality. Possible reasons include: 1) the inputs contains duplicate information, and 2) more inputs introduce more back-propagation paths and therefore make it difficult to train. In our experience, one principle is to only feed the coverage model inputs that contain distinct information, which are complementary to each other. dependencies. In this work, we adopt GRU for the gating activation since it is simple yet power- ful ( <ref type="bibr" target="#b7">Chung et al., 2014</ref>). Please refer to ( <ref type="bibr" target="#b6">Cho et al., 2014b</ref>) for more details about GRU.</p><p>Discussion Intuitively, the two types of models summarize coverage information in "different lan- guages". Linguistic models summarize coverage information in human language, which has a clear interpretation to humans. Neural models encode coverage information in "neural language", which can be "understood" by neural networks and let them to decide how to make use of the encoded coverage information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Integrating Coverage into NMT</head><p>Although attention based model has the capabil- ity of jointly making alignment and translation, it does not take into consideration translation his- tory. Specifically, a source word that has sig- nificantly contributed to the generation of target words in the past, should be assigned lower align- ment probabilities, which may not be the case in attention based NMT. To address this problem, we propose to calculate the alignment probabilities by incorporating past alignment information embed- ded in the coverage model. Intuitively, at each time step i in the decoding phase, coverage from time step (i − 1) serves as an additional input to the attention model, which provides complementary information of that how likely the source words are translated in the past. We expect the coverage information would guide the attention model to focus more on untranslated source words (i.e., assign higher alignment prob- abilities). In practice, we find that the coverage model does fulfill the expectation (see Section 5). The translated ratios of source words from lin- guistic coverages negatively correlate to the cor- responding alignment probabilities.</p><p>More formally, we rewrite the attention model in Equation 5 as</p><formula xml:id="formula_10">e i,j = a(t i−1 , h j , C i−1,j ) = v a tanh(W a t i−1 + U a h j + V a C i−1,j )</formula><p>where C i−1,j is the coverage of source word x j be- fore time i. V a ∈ R n×d is the weight matrix for coverage with n and d being the numbers of hid- den units and coverage units, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>80</head><p>We take end-to-end learning for the NMT- COVERAGE model, which learns not only the pa- rameters for the "original" NMT (i.e., θ for encod- ing RNN, decoding RNN, and attention model) but also the parameters for coverage modeling (i.e., η for annotation and guidance of attention) . More specifically, we choose to maximize the like- lihood of reference sentences as most other NMT models (see, however (Shen et al., 2016)):</p><formula xml:id="formula_11">(θ * , η * ) = arg max θ,η N n=1</formula><p>log P (y n |x n ; θ, η) (9)</p><p>No auxiliary objective For the coverage model with a clearer linguistic interpretation (Section 3.1.1), it is possible to inject an auxiliary objec- tive function on some intermediate representation.</p><p>More specifically, we may have the following ob- jective:</p><formula xml:id="formula_12">(θ * , η * ) = arg max θ,η N n=1 log P (y n |x n ; θ, η) − λ J j=1 (Φ j − I i=1 α i,j ) 2 ; η</formula><p>where the term J j=1 (Φ j − I i=1 α i,j ) 2 ; η pe- nalizes the discrepancy between the sum of align- ment probabilities and the expected fertility for linguistic coverage. This is similar to the more explicit training for fertility as in <ref type="bibr" target="#b23">Xu et al. (2015)</ref>, which encourages the model to pay equal attention to every part of the image (i.e., Φ j = 1). However, our empirical study shows that the combined ob- jective consistently worsens the translation quality while slightly improves the alignment quality. Our training strategy poses less constraints on the dependency between Φ j and the attention than a more explicit strategy taken in ( <ref type="bibr" target="#b23">Xu et al., 2015)</ref>. We let the objective associated with the transla- tion quality (i.e., the likelihood) to drive the train- ing, as in Equation 9. This strategy is arguably advantageous, since the attention weight on a hid- den state h j cannot be interpreted as the propor- tion of the corresponding word being translated in the target sentence. For one thing, the hidden state h j , after the transformation from encoding RNN, bears the contextual information from other parts of the source sentence, and thus loses the rigid cor- respondence with the corresponding word. There- fore, penalizing the discrepancy between the sum of alignment probabilities and the expected fertil- ity does not hold in this scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>We carry out experiments on a Chinese-English translation task. Our training data for the trans- lation task consists of 1.25M sentence pairs ex- tracted from LDC corpora 4 , with 27.9M Chinese words and 34.5M English words respectively. We choose NIST 2002 dataset as our development set, and the <ref type="bibr">NIST 2005</ref><ref type="bibr">NIST , 2006</ref> and 2008 datasets as our test sets. We carry out experiments of the align- ment task on the evaluation dataset from ( <ref type="bibr" target="#b15">Liu and Sun, 2015)</ref>, which contains 900 manually aligned Chinese-English sentence pairs. We use the case- insensitive 4-gram NIST BLEU score ( <ref type="bibr" target="#b19">Papineni et al., 2002</ref>) for the translation task, and the align- ment error rate (AER) <ref type="bibr" target="#b17">(Och and Ney, 2003)</ref> for the alignment task. To better estimate the qual- ity of the soft alignment probabilities generated by NMT, we propose a variant of AER, naming SAER:</p><formula xml:id="formula_13">SAER = 1 − |M A × M S | + |M A × M P | |M A | + |M S |</formula><p>where A is a candidate alignment, and S and P are the sets of sure and possible links in the ref- erence alignment respectively (S ⊆ P ). M de- notes alignment matrix, and for both M S and M P we assign the elements that correspond to the ex- isting links in S and P with probabilities 1 while assign the other elements with probabilities 0. In this way, we are able to better evaluate the quality of the soft alignments produced by attention-based NMT. We use sign-test <ref type="bibr" target="#b9">(Collins et al., 2005</ref>) for statistical significance test. For efficient training of the neural networks, we limit the source and target vocabularies to the most frequent 30K words in Chinese and English, cov- ering approximately 97.7% and 99.3% of the two corpora respectively. All the out-of-vocabulary words are mapped to a special token UNK. We set N = 2 for the fertility model in the linguistic cov- erages. We train each model with the sentences of length up to 80 words in the training data. The word embedding dimension is 620 and the size of a hidden layer is 1000. All the other settings are the same as in ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>  <ref type="table">Table 1</ref>: Evaluation of translation quality. d denotes the dimension of NN-based coverages, and † and ‡ indicate statistically significant difference (p &lt; 0.01) from GroundHog and Moses, respectively. "+" is on top of the baseline system GroundHog.</p><p>We compare our method with two state-of-the- art models of SMT and NMT 5 :</p><p>• Moses ( <ref type="bibr" target="#b13">Koehn et al., 2007)</ref>: an open source phrase-based translation system with default configuration and a 4-gram language model trained on the target portion of training data.</p><p>• GroundHog ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>: an attention-based NMT system. <ref type="table">Table 1</ref> shows the translation performances mea- sured in BLEU score. Clearly the proposed NMT- COVERAGE significantly improves the translation quality in all cases, although there are still consid- erable differences among different variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Translation Quality</head><p>Parameters Coverage model introduces few pa- rameters. The baseline model (i.e., GroundHog) has 84.3M parameters. The linguistic coverage using fertility introduces 3K parameters (2K for fertility model), and the NN-based coverage with gating introduces 10K×d parameters (6K×d for gating), where d is the dimension of the coverage vector. In this work, the most complex coverage model only introduces 0.1M additional parame- ters, which is quite small compared to the number of parameters in the existing model (i.e., 84.3M).</p><p>Speed Introducing the coverage model slows down the training speed, but not significantly. When running on a single GPU device Tesla K80, the speed of the baseline model is 960 target words per second. System 4 ("+Linguistic coverage with fertility") has a speed of 870 words per second, while System 7 ("+NN-based coverage (d=10)") achieves a speed of 800 words per second.</p><p>Linguistic Coverages (Rows 3 and 4): Two observations can be made. First, the simplest linguistic coverage (Row 3) already significantly improves translation performance by 1.1 BLEU points, indicating that coverage information is very important to the attention model. Second, in- corporating fertility model boosts the performance by better estimating the covered ratios of source words.</p><p>NN-based Coverages (Rows 5-7): (1) Gating (Rows 5 and 6): Both variants of NN-based cover- ages outperform GroundHog with averaged gains of 0.8 and 1.3 BLEU points, respectively. In- troducing gating activation function improves the performance of coverage models, which is consis- tent with the results in other tasks ( <ref type="bibr" target="#b7">Chung et al., 2014)</ref>. <ref type="formula" target="#formula_1">(2)</ref> Coverage dimensions (Rows 6 and 7): Increasing the dimension of coverage models fur- ther improves the translation performance by 0.6 point in BLEU score, at the cost of introducing more parameters (e.g., from 10K to 100K). <ref type="bibr">6</ref>  <ref type="table">Table 2</ref> lists the alignment performances. We find that coverage information improves atten- tion model as expected by maintaining an annota- tion summarizing attention history on each source word. More specifically, linguistic coverage with fertility significantly reduces alignment errors un- der both metrics, in which fertility plays an impor- tant role. NN-based coverages, however, does not significantly reduce alignment errors until increas- ing the coverage dimension from 1 to 10. It in- dicates that NN-based models need slightly more   <ref type="bibr">(d = 10)</ref> 64.25 50.50 <ref type="table">Table 2</ref>: Evaluation of alignment quality. The lower the score, the better the alignment quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Alignment Quality</head><p>dimensions to encode the coverage information. <ref type="figure" target="#fig_4">Figure 5</ref> shows an example. The coverage mechanism does meet the expectation: the align- ments are more concentrated and most impor- tantly, translated source words are less likely to get involved in generation of the target words next. For example, the first four Chinese words are as- signed lower alignment probabilities (i.e., darker color) after the corresponding translation "roma- nia reinforces old buildings" is produced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effects on Long Sentences</head><p>Following <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>, we group sen- tences of similar lengths together and compute BLEU score and averaged length of translation for each group, as shown in <ref type="figure" target="#fig_5">Figure 6</ref>. <ref type="bibr" target="#b5">Cho et al. (2014a)</ref> show that the performance of Ground- hog drops rapidly when the length of input sen- tence increases. Our results confirm these find- ings. One main reason is that Groundhog pro- duces much shorter translations on longer sen- tences (e.g., &gt; 40, see right panel in <ref type="figure" target="#fig_5">Figure 6</ref>), and thus faces a serious under-translation prob- lem. NMT-COVERAGE alleviates this problem by incorporating coverage information into the atten- tion model, which in general pushes the attention to untranslated parts of the source sentence and implicitly discourages early stop of decoding. It is worthy to emphasize that both NN-based cov- erages (with gating, d = 10) and linguistic cover- ages (with fertility) achieve similar performances on long sentences, reconfirming our claim that the two variants improve the attention model in their own ways.</p><p>As an example, consider this source sentence in the test set: qiáod¯ an běn s` aij`aij`ı píngj¯ un déf¯ en 24.3f¯ en , t¯ a z` ai s¯ an zh¯ ou qián ji¯ eshòu shˇoushùshˇoushù ,qiúdù ı z` ai cˇıcˇı q¯ ıji¯ an 4 shèng 8 f` u .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Groundhog translates this sentence into:</head><p>jordan achieved an average score of eight weeks ahead with a surgical oper- ation three weeks ago .</p><p>in which the sub-sentence ",qiúdù ı z` ai cˇıcˇı q¯ ıji¯ an 4 shèng 8 f` u" is under-translated. With the (NN- based) coverage mechanism, NMT-COVERAGE translates it into:</p><p>jordan 's average score points to UNK this year . he received surgery before three weeks , with a team in the period of 4 to 8 . in which the under-translation is rectified.</p><p>The quantitative and qualitative results show that the coverage models indeed help to allevi- ate under-translation, especially for long sentences consisting of several sub-sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Our work is inspired by recent works on im- proving attention-based NMT with techniques that have been successfully applied to SMT. Follow- ing the success of Minimum Risk Training (MRT) in SMT ,  proposed MRT for end-to-end NMT to optimize model pa- rameters directly with respect to evaluation met- rics. Based on the observation that attention- based NMT only captures partial aspects of atten- tional regularities,  proposed agreement-based learning ( <ref type="bibr" target="#b14">Liang et al., 2006</ref>) to encourage bidirectional attention models to agree on parameterized alignment matrices. Along the same direction, inspired by the coverage mecha- nism in SMT, we propose a coverage-based ap- proach to NMT to alleviate the over-translation and under-translation problems.</p><p>Independent from our work, <ref type="bibr" target="#b8">Cohn et al. (2016) and</ref><ref type="bibr" target="#b10">Feng et al. (2016)</ref> made use of the concept of "fertility" for the attention model, which is sim- ilar in spirit to our method for building the lin- guistically inspired coverage with fertility. <ref type="bibr" target="#b8">Cohn et al. (2016)</ref> introduced a feature-based fertility that includes the total alignment scores for the sur- rounding source words. In contrast, we make pre- diction of fertility before decoding, which works as a normalizer to better estimate the coverage ra- tio of each source word. <ref type="bibr" target="#b10">Feng et al. (2016)</ref> used the previous attentional context to represent im- plicit fertility and passed it to the attention model, which is in essence similar to the input-feed method proposed in ( <ref type="bibr" target="#b16">Luong et al., 2015)</ref>. Compar- atively, we predict explicit fertility for each source word based on its encoding annotation, and incor- porate it into the linguistic-inspired coverage for attention model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented an approach for enhancing NMT, which maintains and utilizes a coverage vector to indicate whether each source word is translated or not. By encouraging NMT to pay less attention to translated words and more attention to untranslated words, our approach alleviates the se- rious over-translation and under-translation prob- lems that traditional attention-based NMT suffers from. We propose two variants of coverage mod- els: linguistic coverage that leverages more lin- guistic information and NN-based coverage that resorts to the flexibility of neural network approx- imation . Experimental results show that both variants achieve significant improvements in terms of translation quality and alignment quality over NMT without coverage.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example translations of (a) NMT without coverage, and (b) NMT with coverage. In conventional NMT without coverage, the Chinese word "gu¯ anb`anb`ı" is over translated to "close(d)" twice, while "b` eipò" (means "be forced to") is mistakenly untranslated. Coverage model alleviates these problems by tracking the "coverage" of source words.</figDesc><graphic url="image-3.png" coords="2,314.36,397.01,204.09,128.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture of coverage-based attention model. A coverage vector C i−1 is maintained to keep track of which source words have been translated before time i. Alignment decisions α i are made jointly taking into account past alignment information embedded in C i−1 , which lets the attention model to consider more about untranslated source words.</figDesc><graphic url="image-4.png" coords="3,325.70,62.81,181.41,100.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: NN-based coverage model.</figDesc><graphic url="image-5.png" coords="5,124.44,62.81,113.39,134.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>We next consider Neural Network (NN) based coverage model.</head><label></label><figDesc>When C i,j is a vector (d &gt; 1) and g update (·) is a neural network, we actually have an RNN model for coverage, as illustrated in Fig- ure 4. In this work, we take the following form:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example alignments. Using coverage mechanism, translated source words are less likely to contribute to generation of the target words next (e.g., top-right corner for the first four Chinese words.).</figDesc><graphic url="image-6.png" coords="8,75.17,72.77,204.10,163.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance of the generated translations with respect to the lengths of the input sentences. Coverage models alleviate under-translation by producing longer translations on long sentences.</figDesc><graphic url="image-8.png" coords="9,93.32,62.81,181.41,210.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>System 
SAER AER 
GroundHog 
67.00 54.67 
+ Ling. cov. w/o fertility 
66.75 53.55 
+ Ling. cov. w/ fertility 
64.85 52.13 
+ NN cov. w/o gating (d = 1) 67.10 54.46 
+ NN cov. w/ gating (d = 1) 
66.30 53.51 
+ NN cov. w/ gating </table></figure>

			<note place="foot" n="1"> Our code is publicly available at https://github. com/tuzhaopeng/NMT-Coverage.</note>

			<note place="foot" n="3">.1 Coverage Model Since the coverage vector summarizes the attention record for h j (and therefore for a small neighbor centering at the j th source word), it will discourage further attention to it if it has been heavily attended, and implicitly push the attention to the less attended segments of the source sentence since the attention weights are normalized to one. This can potentially solve both coverage mistakes mentioned above, when modeled and learned properly.</note>

			<note place="foot" n="2"> Fertility in SMT is a random variable with a set of fertility probabilities, n(Φj|xj) = p(Φ&lt;j, x), which depends on the fertilities of previous source words. To simplify the calculation and adapt it to the attention model in NMT, we define the fertility in NMT as a constant number, which is independent of previous fertilities.</note>

			<note place="foot" n="4"> The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.</note>

			<note place="foot" n="5"> There are recent progress on aggregating multiple models or enlarging the vocabulary(e.g., in (Jean et al., 2015)), but here we focus on the generic models.</note>

			<note place="foot" n="6"> In a pilot study, further increasing the coverage dimension only slightly improved the translation performance. One possible reason is that encoding the relatively simple coverage information does not require too many dimensions.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported by China National 973 project 2014CB340301. Yang Liu is supported by the National Natural Science Foundation of China (No. 61522204) and the 863 Program (2015AA011808). We thank the anonymous re-viewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSST</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incorporating Structural Alignment Biases into an Attentional Neural Translation Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Clause restructuring for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Philipp Koehn, and Ivona Kučerová</title>
		<imprint>
			<publisher>Michael Collins</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Implicit distortion and fertility models for attention-based encoder-decoder nmt model. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sepp Hochreiter and Jürgen Schmidhuber</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Moses: open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Alignment by agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Contrastive unsupervised word alignment with nonlocal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun2015] Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Luong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Och and Ney2003</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Papineni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Minimum Risk Training for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
