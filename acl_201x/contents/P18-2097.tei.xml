<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Empirical Study of Building a Strong Baseline for Constituency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0237</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Takase</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0237</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetaka</forename><surname>Kamigaito</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0237</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Morishita</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0237</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0237</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Empirical Study of Building a Strong Baseline for Constituency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="612" to="618"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>612</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper investigates the construction of a strong baseline based on general purpose sequence-to-sequence models for constituency parsing. We incorporate several techniques that were mainly developed in natural language generation tasks, e.g., machine translation and summariza-tion, and demonstrate that the sequence-to-sequence model achieves the current top-notch parsers&apos; performance without requiring explicit task-specific knowledge or architecture of constituent parsing.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequence-to-sequence (Seq2seq) models have successfully improved many well-studied NLP tasks, especially for natural language genera- tion (NLG) tasks, such as machine translation (MT) <ref type="bibr" target="#b22">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Cho et al., 2014</ref>) and abstractive summarization ( <ref type="bibr" target="#b17">Rush et al., 2015</ref>). Seq2seq models have also been applied to con- stituency parsing <ref type="bibr" target="#b23">(Vinyals et al., 2015)</ref> and pro- vided a fairly good result. However one obvi- ous, intuitive drawback of Seq2seq models when they are applied to constituency parsing is that they have no explicit architecture to model latent nested relationships among the words and phrases in constituency parse trees, Thus, models that di- rectly model them, such as RNNG ( <ref type="bibr" target="#b7">Dyer et al., 2016)</ref>, are an intuitively more promising approach. In fact, RNNG and its extensions ( <ref type="bibr" target="#b11">Kuncoro et al., 2017;</ref><ref type="bibr" target="#b8">Fried et al., 2017</ref>) provide the current state- of-the-art performance. Sec2seq models are cur- rently considered a simple baseline of neural- based constituency parsing.</p><p>After the first proposal of an Seq2seq con- stituency parser, many task-independent tech- niques have been developed, mainly in the NLG research area. Our aim is to update the Seq2seq approach proposed in <ref type="bibr" target="#b23">Vinyals et al. (2015)</ref> as a stronger baseline of constituency parsing. Our motivation is basically identical to that described in <ref type="bibr" target="#b5">Denkowski and Neubig (2017)</ref>. A strong base- line is crucial for reporting reliable experimental results. It offers a fair evaluation of promising new techniques if they solve new issues or simply re- solve issues that have already been addressed by current generic technology. More specifically, it might become possible to analyze what types of implicit linguistic structures are easier or harder to capture for neural models by comparing the out- puts of strong Seq2seq models and task-specific models, e.g., RNNG.</p><p>The contributions of this paper are summarized as follows: (1) a strong baseline for constituency parsing based on general purpose Seq2seq mod- els 1 , (2) an empirical investigation of several generic techniques that can (or cannot) contribute to improve the parser performance, (3) empiri- cal evidence that Seq2seq models implicitly learn parse tree structures well without knowing task- specific and explicit tree structure information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Constituency Parsing by Seq2seq</head><p>Our starting point is an RNN-based Seq2seq model with an attention mechanism that was ap- plied to constituency parsing ( <ref type="bibr" target="#b23">Vinyals et al., 2015)</ref>. We omit detailed descriptions due to space limita- tions, but note that our model architecture is iden- tical to the one introduced in <ref type="bibr" target="#b14">Luong et al. (2015a)</ref>  <ref type="bibr">2</ref> .</p><p>A key trick for applying Seq2seq models to constituency parsing is the linearization of parse <ref type="bibr">1</ref> Our code and experimental configurations for reproduc- ing our experiments are publicly available: https://github.com/nttcslab-nlp/strong s2s baseline parser <ref type="bibr">2</ref> More specifically, our Seq2seq model fol- lows the one implemented in seq2seq-attn (https://github.com/harvardnlp/seq2seq-attn), which is the alpha-version of the OpenNMT tool (http://opennmt.net).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original input</head><p>John has a dog . Output: S-exp.</p><p>(S (NP NNP ) (VP VBZ (NP DT NN ) ) . ) Linearized form (S (NP NNP )NP (VP VBZ (NP DT NN )NP )VP . )S w/ POS normalized (S (NP XX )NP (VP XX (NP XX XX )NP )VP . )S <ref type="table">Table 1</ref>: Examples of linearization and POS-tag normalization ( <ref type="bibr" target="#b23">Vinyals et al., 2015)</ref> trees <ref type="bibr" target="#b23">(Vinyals et al., 2015)</ref>. Roughly speaking, a linearized parse tree consists of open, close brack- eting and POS-tags that correspond to a given in- put raw sentence. Since a one-to-one mapping ex- ists between a parse tree and its linearized form (if the linearized form is a valid tree), we can recover parse trees from the predicted linearized parse tree. <ref type="bibr" target="#b23">Vinyals et al. (2015)</ref> also introduced the part-of-speech (POS) tag normalization tech- nique. They substituted each POS tag in a lin- earized parse tree to a single XX-tag 3 , which al- lows Seq2seq models to achieve a more compet- itive performance range than the current state-of- the-art parses <ref type="bibr">4</ref> . <ref type="table">Table 1</ref> shows an example of a parse tree to which linearization and POS-tag nor- malization was applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task-independent Extensions</head><p>This section describes several generic techniques that improve Seq2seq performance <ref type="bibr">5</ref> . <ref type="table" target="#tab_0">Table 2</ref> lists the notations used in this paper for a convenient reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Subword as input features</head><p>Applying subword decomposition has recently be- come a leading technique in NMT literature <ref type="bibr" target="#b18">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b25">Wu et al., 2016)</ref>. Its primary advantage is a significant reduction of the serious out-of-vocabulary (OOV) problem. We incorpo- rated subword information as an additional feature of the original input words. A similar usage of subword features was previously proposed in <ref type="bibr" target="#b0">Bojanowski et al. (2017)</ref>.</p><p>Formally, the encoder embedding vector at en- coder position i, namely, e i , is calculated as fol- lows: <ref type="bibr">3</ref> We did not substitute POS-tags for punctuation symbols such as ".", and ",". <ref type="bibr">4</ref> Several recently developed neural-based constituency parsers ignore POS tags since they are not evaluated in the standard evaluation metric of constituency parsing (Bracket- ing F-measure). <ref type="bibr">5</ref> Figure in the supplementary material shows the brief sketch of the method explained in the following section. D : dimension of the embeddings H : dimension of the hidden states i : index of the (token) position in input sentence j : index of the (token) position in output linearized format of parse tree V (e) : vocabulary of word for input (encoder) side V (s) : vocabulary of subword for input (encoder) side E : encoder embedding matrix for V (e) , where E ∈ R D×|V (e) | F : encoder embedding matrix for V (s) , where F ∈ R D×|V (s) | wi : i-th word (token) in the input sentence, wi ∈ V (e) x k : one-hot vector representation of the k-th word in V (e) s k : one-hot vector representation of the k-th subword in V (s) u : encoder embedding vector of unknown token φ(·) : function that returns the index of given word in the vocabulary V (e) ψ(·) : function that returns a set of indices in the subword vocabulary V <ref type="bibr">(s)</ref> generated from the given word. e.g., k ∈ ψ(wi) ei : encoder embedding vector at position i in encoder V (d) : vocabulary of output with POS-tag normalization V (q) : vocabulary of output without POS-tag normalization</p><formula xml:id="formula_0">e i = Ex k + k ∈ψ(w i ) F s k ,<label>(1)</label></formula><formula xml:id="formula_1">W (o) : decoder output matrix for V (d) , where W (o) ∈ R |V (o) |×H W (q) : decoder output matrix for V (q) , where W (q) ∈ R |V (q) |×H</formula><p>zj : final hidden vector calculated at the decoder position j oj : final decoder output scores at decoder position j qj : output scores of auxiliary task at decoder position j b : additional bias term in the decoder output layer for mask pj : vector format of output probability at decoder position j A : number of models for ensembling C : number of candidates generating for LM-reranking </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unknown token embedding as a bias</head><p>We generally replace rare words, e.g., those ap- pearing less than five times in the training data, with unknown tokens in the Seq2seq approach. However, we suspect that embedding vectors, which correspond to unknown tokens, cannot be trained well for the following reasons: (1) the occurrence of unknown tokens remains relatively small in the training data since they are obvi- ous replacements for rare words, and (2) Seq2seq is relatively ineffective for training infrequent words ( <ref type="bibr" target="#b15">Luong et al., 2015b</ref>). Based on these ob- servations, we utilize the unknown embedding as a bias term b of linear layer (W x + b) when ob- taining every encoder embeddings for overcoming infrequent word problem. Then, we modify Eq. 2 as follows:</p><formula xml:id="formula_2">e i = (Ex k + u) + k ∈ψ(w i ) (F s k + u).<label>(2)</label></formula><p>Note that if w i is unknown token, then Eq. 2 be- comes</p><formula xml:id="formula_3">e i = 2u + k ∈ψ(w i ) (F s k + u).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-task learning</head><p>Several papers on the Seq2seq approach ( <ref type="bibr" target="#b13">Luong et al., 2016)</ref> have reported that the multi-task learning extension often improves the task perfor- mance if we can find effective auxiliary tasks re- lated to the target task. From this general knowl- edge, we re-consider jointly estimating POS-tags by incorporating the linearized forms without the POS-tag normalization as an auxiliary task. In detail, the linearized forms with and without the POS-tag normalization are independently and si- multaneously estimated as o j and q j , respectively, in the decoder output layer by following equation:</p><formula xml:id="formula_4">o j = W (o) z j , and q j = W (q) z j . (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Output length controlling</head><p>As described in <ref type="bibr" target="#b23">Vinyals et al. (2015)</ref>, not all the outputs (predicted linearized parse trees) obtained from the Seq2seq parser are valid (well-formed) as a parse tree. Toward guaranteeing that every out- put is a valid tree, we introduce a simple extension of the method for controlling the Seq2seq output length ( <ref type="bibr" target="#b10">Kikuchi et al., 2016)</ref>. First, we introduce an additional bias term b in the decoder output layer to prevent the selection of certain output words:</p><formula xml:id="formula_5">p j = softmax(o j + b).<label>(4)</label></formula><p>If we set a large negative value at the m-th element in b, namely b m ≈ −∞, then the m-th element in p j becomes approximately 0, namely p j,m ≈ 0, regardless of the value of the k-th element in o j . We refer to this operation to set value −∞ in b as a mask. Since this naive masking approach is harmless to GPU-friendly processing, we can still exploit GPU parallelization.</p><p>We set b to always mask the EOS-tag and change b when at least one of the following con- ditions is satisfied: (1) if the number of open and closed brackets generated so far is the same, then we mask the XX-tags (or the POS-tags) and all the closed brackets. (2) if the number of predicted XX-tags (or POS-tags) is equivalent to that of the words in a given input sentence, then we mask the XX-tags (or all the POS-tags) and all the open brackets. If both conditions (1) and (2) are satis- fied, then the decoding process is finished. The additional cost for controlling the mask is to count the number of XX-tags and the open and closed brackets so far generated in the decoding process.</p><note type="other">Dim. of embedding D 300 Dim. of hidden state H 200 Encoder RNN unit bi-LSTM Num. of layers L 2 Decoder RNN unit LSTM with attention Dropout rate 0.3 Optimizer SGD Gradient clipping G 1.0 Learning rate decay 0</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.9 (after 50 epoch) Initial learning rate 1.0 Mini-batch size M 16 (shuffled at each epoch) Stopping criterion 100 epochs (w/o early stopping)</head><p>Beam size (at Test) B 5 <ref type="table" target="#tab_5">Table 3</ref>: List of model and optimization configu- rations (hyper-parameters) in our experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Pre-trained word embeddings</head><p>The pre-trained word embeddings obtained from a large external corpora often boost the final task performance even if they only initialize the input embedding layer. In constituency parsing, several systems also incorporate pre-trained word embed- dings, such as Vinyals et al. <ref type="formula" target="#formula_0">(2015)</ref>; <ref type="bibr" target="#b6">Durrett and Klein (2015)</ref>. To maintain as much reproducibil- ity of our experiments as possible, we simply ap- plied publicly available pre-trained word embed- dings, i.e., glove.840B.300d 7 , as initial val- ues of the encoder embedding layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Model ensemble</head><p>Ensembling several independently trained models together significantly improves many NLP tasks.</p><p>In the ensembling process, we predict the out- put tokens using the arithmetic mean of predicted probabilities computed by each model:</p><formula xml:id="formula_6">p j = 1 A A a=1 p (a) j ,<label>(5)</label></formula><p>where p (a) j represents the probability distribution at position j predicted by the a-th model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Language model (LM) reranking</head><p>Choe and Charniak (2016) demonstrated that reranking the predicted parser output candidates with an RNN language model (LM) significantly improves performance. We refer to this reranking process as LM-rerank. Following their success, we also trained RNN-LMs on the PTB dataset with their published preprocessing code 8 to reproduce the experiments in Choe and Charniak (2016) for our LM-rerank. We selected the current state- of-the-art LM ( <ref type="bibr" target="#b26">Yang et al., 2018)</ref>  <ref type="bibr">9</ref> as our LM- reranker, which is a much stronger LM than was used in <ref type="bibr" target="#b2">Choe and Charniak (2016</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experiments used the English Penn Treebank data <ref type="bibr" target="#b16">(Marcus et al., 1994)</ref>, which are the most widely used benchmark data in the literature. We used the standard split of training (Sec.02-21), development (Sec.22), and test data (Sec.23) and strictly followed the instructions for the evalua- tion settings explained in <ref type="bibr" target="#b23">Vinyals et al. (2015)</ref>. For data pre-processing, all the parse trees were transformed into linearized forms, which include standard UNK replacement for OOV words and POS-tag normalization by XX-tags. As explained in <ref type="bibr" target="#b23">Vinyals et al. (2015)</ref>, we did not apply any parse tree binarization or special unary treatment, which were used as common techniques in the literature.  <ref type="table">Table 7</ref>: List of bracketing F-measures on test data (PTB Sec.23) reported in recent top-notch sys- tems: scores with bold font represent our scores.</p><p>ments unless otherwise specified. <ref type="table" target="#tab_2">Table 4</ref> shows the main results of our experiments. We reported the Bracketing F-measures (Bra.F) and the complete match scores (CM) evaluated by the EVALB tool <ref type="bibr">10</ref> . The averages (ave), stan- dard deviations (stdev), lowest (min), and high- est (max) scores were calculated from ten inde- pendent runs of each setting trained with different random initialization values. This table empiri- cally reveals the effectiveness of individual tech- niques. Each technique gradually improved the performance, and the best result (j) achieved ap-proximately 3 point gain from the baseline con- ventional Seq2seq model (a) on test data Bra.F. One drawback of Seq2seq approach is that it seems sensitive to initialization. Comparing only with a single result for each setting may produce inaccurate conclusions. Therefore, we should evaluate the performances over several trials to im- prove the evaluation reliability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>The baseline Seq2seq models, (a) and (f), produced the malformed parse trees. We post- processed such malformed parse trees by simple rules introduced in ( <ref type="bibr" target="#b23">Vinyals et al., 2015</ref>). On the other hand, we confirmed that all the results apply- ing the technique explained in Sec. 3.4 produced no malformed parse trees. Ensembling and Reranking: <ref type="table" target="#tab_3">Table 5</ref> shows the results of our models with model ensembling and LM-reranking. For ensemble, we randomly se- lected eight of the ten Seq2seq models reported in <ref type="table" target="#tab_2">Table 4</ref>. For LM-reranking, we first generated 80 candidates by the above eight ensemble models and selected the best parse tree for each input in terms of the LM-reranker. The results in <ref type="table" target="#tab_3">Table 5</ref> were taken from a single-shot evaluation, unlike the averages of ten independent runs in <ref type="table" target="#tab_2">Table 4</ref>. Hyper-parameter selection: We empirically in- vestigated the impact of the hyper-parameter se- lections. <ref type="table" target="#tab_4">Table 6</ref> shows the results. The follow- ing observations appear informative for building strong baseline systems: (1) Smaller mini-batch size M and gradient clipping G provided the bet- ter performance. Such settings lead to slower and longer training, but higher performance. (2) Larger layer size, hidden state dimension, and beam size have little impact on the performance; our setting, L = 2, H = 200, and B = 5 looks adequate in terms of speed/performance trade-off. Input unit selection: As often demonstrated in the NMT literature, using subword split as input token unit instead of standard tokenized word unit has potential to improve the performance. <ref type="table" target="#tab_4">Table 6</ref> (e) shows the results of utilizing subword splits. Clearly, 8K and 16K subword splits as input to- ken units significantly degraded the performance. It seems that the numbers of XX-tags in output and tokens in input should keep consistent for better performance since Seq2seq models look to some- how learn such relationship, and used it during the decoding. Thus, using subword information as features is one promising approach for leveraging subword information into constituency parsing. <ref type="table">Table 7</ref> lists the reported constituency parsing scores on PTB that were recently published in the literature. We split the results into three categories. The first category (top row) contains the results of the methods that were trained only from the pre-defined training data (PTB Sec.02-21), with- out any additional resources. The second category (middle row) consists of the results of methods that were trained from the pre-defined PTB train- ing data as well as those listed in the top row, but incorporating word embeddings obtained from a large-scale external corpus to initialize the encoder embedding layer. The third category (bottom row) shows the performance of the methods that were trained using high-confidence, auto-parsed trees in addition to the pre-defined PTB training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to current top systems</head><p>Our Seq2seq approach successfully achieved the competitive level as the current top-notch methods: RNNG and its variants. Note here that, as described in <ref type="bibr" target="#b7">Dyer et al. (2016)</ref>, RNNG uses Berkeley parser's mapping rules for effectively handling singleton words in the training corpus. In contrast, we demonstrated that Seq2seq models have enough power to achieve a competitive state- of-the-art performance without leveraging such task-dependent knowledge. Moreover, they need no explicit information of parse tree structures, transition states, stacks, (Stanford or Berkeley) mapping rules, or external silver training data dur- ing the model training except general purpose word embeddings as initial values. These obser- vations from our experiments imply that recently developed Seq2seq models have enough ability to implicitly learn parsing structures from linearized parse trees. Our results argue that Seq2seq models can be a strong baseline for constituency parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper investigated how well general purpose Seq2seq models can achieve the higher perfor- mance of constituency parsing as a strong baseline method. We incorporated several generic tech- niques to enhance Seq2seq models, such as incor- porating subword features, and output length con- trolling. We experimentally demonstrated that by applying ensemble and LM-reranking techniques, a general purpose Seq2seq model achieved almost the same performance level as the state-of-the-art constituency parser without any task-specific or explicit tree structure information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>List of notations used in this paper. 

where k = φ(w i ). Note that the second term 
of RHS indicates our additional subword features, 
and the first represents the standard word em-
bedding extraction procedure. Among several 
choices, we used the byte-pair encoding (BPE) ap-
proach proposed in Sennrich et al. (2016) applying 
1,000 merge operations 6 . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>) .</head><label>.</label><figDesc>. ±stdev min / max ave. ±stdev min / max ave. ±stdev ave. ±stdev (dev.max model) (a) Seq2seq w/ attn (+post-proc for valid parse tree) 88.08 ±0.41 87.27 / 88.72 35.80 ±0.78 34.88 / 37.41 88.13 ±0.22 35.05 ±0.79 88.39 35.±0.20 89.85 / 90.48 41.09 ±0.98 39.35 / 42.82 90.38 ±0.28 40.76 ±0.74 90.62 41.39 (f) (a) + Pre-trained emb. ( §3.5) enc. initialization 89.99 ±0.17 89.75 / 90.34 40.69 ±0.83 39.41 / 41.76 90.14 ±0.12 40.40 ±0.44 90.32 40.</figDesc><table>Development Data (PTB Sec.22) 
Test Data (PTB Sec.23) 
Bracketing F1 (Bra.F) 
Complete match (CM) 
Bra.F 
CM 
Bra.F CM 
ID 
Method 
category 
ave97 
(b) (a) + Dec.control ( §3.4) 
dec. mask. 
88.35 ±0.37 87.70 / 88.83 35.89 ±0.80 34.94 / 37.47 
-
-
-
-
(c) (b) + Subword ( §3.1) 
enc. feature 
89.76 ±0.23 89.40 / 90.03 39.79 ±0.79 38.47 / 40.88 
-
-
-
-
(d) (c) + Unk bias ( §3.2) 
enc. featture 
90.10 ±0.24 89.77 / 90.54 40.98 ±0.82 39.59 / 42.18 
-
-
-
-
(e) (d) + Pos ( §3.3) 
dec. multitask 
90.21 89 
(g) (f) + Dec.control ( §3.4) 
dec. mask. 
90.28 ±0.15 90.10 / 90.55 40.78 ±0.84 39.53 / 41.88 
-
-
-
-
(h) (g) + Subword ( §3.1) 
enc. feature 
90.34 ±0.10 90.20 / 90.53 41.19 ±0.64 40.12 / 42.06 
-
-
-
-
(i) (h) + Unk bias ( §3.2) 
enc. feature 
90.92 ±0.17 90.67 / 91.17 43.38 ±0.57 42.47 / 44.29 
-
-
-
-
(j) (i) + Pos ( §3.3) 
dec. multitask 
90.93 ±0.14 90.68 / 91.07 42.76 ±0.38 42.00 / 43.18 91.18 ±0.12 42.39 ±0.68 91.36 43.50 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results on English PTB data: Results were average (ave), worst (min), and best (max) per-
formance of ten models independently trained with distinct random initial values. Test data was only 
evaluated on baseline and our best setting ((a), (e), (f) and (j)) to prevent over-tuning to the test 
data. We confirmed that all our results contained no malformed parse trees. 

Dev. 
Test 
ID 
Method 
Bra.F CM Bra.F CM 
(k) (e) + ensemble A = 8 ( §3.6) 
92.32 45.76 92.18 45.90 
(l) (k) + LM-rerank C = 80 ( §3.7) 94.31 53.59 94.14 52.69 
(m) (j) + ensemble A = 8 ( §3.6) 
92.90 47.85 92.74 47.27 
(n) (m) + LM-rerank C = 80 ( §3.7) 94.30 54.12 94.32 52.81 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Ensembling and reranking results 

(a) Mini-batch size M 
(b) Gradient clipping G 
method 
Bra.F CM 
(j) M = 16 90.93 42.76 
M = 64 89.85 40.94 
M = 256 89.41 40.41 

method 
Bra.F CM 
(j) G = 1 90.93 42.76 
G = 5 87.36 36.71 

(c) Hidden dim H and layer L 
(d) Beam size B 
method 
Bra.F CM 
(j) H = 200, L = 2 90.93 42.76 
H = 200, L = 3 90.75 43.00 
H = 200, L = 4 90.55 42.84 
H = 512, L = 2 90.59 43.38 

method 
Bra.F CM 
B = 1 90.55 42.49 
(j) B = 5 90.93 42.76 
B = 20 90.98 42.76 
B = 50 91.01 42.76 

(e) usage of subword information (feature or split) 
method 
Bra.F CM 
(h) word split with 1K subword feature 90.93 42.76 
8K subword split 
87.39 33.62 
16K subword split 
87.20 31.21 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Impact of hyper-parameter selections. We 
only evaluated the development data (PTB Sec. 
22) to prevent over-tuning to the test data. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 3 summarizes the model configurations and the optimization settings used in our experi-</head><label>3</label><figDesc></figDesc><table>System (Brief description) 
Bra.F 
[Trained (strictly) from PTB only, no additional resources] 
(Kamigaito et al., 2017) Seq2seq, sup.attention 
89.5 
(Cross and Huang, 2016a) Shift-reduce 
89.95 
Ours; Seq2seq 
90.62 
(Watanabe and Sumita, 2015) Shift-reduce 
90.68 
(Shindo et al., 2012) 
91.1 
(Cross and Huang, 2016b) Shift-reduce 
91.3 
(Kamigaito et al., 2017) Seq2seq, sup.attention, ensemble 
91.5 
(Dyer et al., 2016) Shift-reduce, discriminative 
91.7 
(Liu and Zhang, 2017) Shift-reduce 
91.7 
(Stern et al., 2017a) Top-down 
91.79 
Ours; Seq2seq, ensemble 
92.18 
(Shindo et al., 2012) ensemble 
92.4 
(Stern et al., 2017b) Top-down, rerank 
92.56 
(Choe and Charniak, 2016) CKY, LM-rerank 
92.6 
(Dyer et al., 2016) Shift-reduce, generative 
93.3 
(Kuncoro et al., 2017) Shift-reduce, rerank 
93.6 
Ours; Seq2seq, ensemble, LM-rerank(80) 
94.14 
(Fried et al., 2017) Shift-reduce, ensemble, rerank 
94.25 
[PTB only, but utilizing pre-trained emb. from external corpus for init.] 
(Vinyals et al., 2015) Seq2seq 
88.3 
(Vinyals et al., 2015) Seq2seq, ensemble 
90.5 
(Durrett and Klein, 2015) CKY 
91.1 
Ours; Seq2seq 
91.36 
Ours; Seq2seq, ensemble 
92.74 
Ours best; Seq2seq, ensemble, LM-rerank(80) 
94.32 
[Trained from PTB and other external silver data] 
(Choe and Charniak, 2016) CKY, LM-rerank 
93.8 
(Fried et al., 2017) Shift-reduce, ensemble, rerank 
94.66 

</table></figure>

			<note place="foot" n="6"> https://github.com/rsennrich/subword-nmt</note>

			<note place="foot" n="7"> https://nlp.stanford.edu/projects/glove/ 8 https://github.com/cdg720/emnlp2016 9 We used the identical hyper-parameters introduced in their site: https://github.com/zihangdai/mos.</note>

			<note place="foot" n="10"> http://nlp.cs.nyu.edu/evalb/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Parsing as language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kook</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2331" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incremental parsing with minimal features using bi-directional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="32" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stronger baselines for trustable results in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Neural Machine Translation (WNMT)</title>
		<meeting>the 1st Workshop on Neural Machine Translation (WNMT)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural CRF parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL) and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL) and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="302" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2016 North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving neural parsing by disentangling model combination and reranking effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="161" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Supervised attention for sequence-to-sequence constituency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetaka</forename><surname>Kamigaito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Controlling output length in neural encoder-decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What do recurrent neural network grammars learn about syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the 15th European Chapter of the Association for Computational Linguistics (EACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1249" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Shift-reduce constituent parsing with neural lookahead features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="45" to="58" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations</title>
		<meeting>the 4th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL) and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL) and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building a Large Annotated Corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Neural Attention Model for Abstractive Sentence Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bayesian symbol-refined tree substitution grammars for syntactic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="440" to="448" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A minimal span-based neural constituency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="818" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effective inference for generative neural parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1695" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 28th Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grammar as a Foreign Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transitionbased neural constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL) and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL) and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1169" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<meeting><address><addrLine>Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick</addrLine></address></meeting>
		<imprint>
			<publisher>Greg Corrado, Macduff Hughes, and Jeffrey Dean</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck: A high-rank RNN language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
