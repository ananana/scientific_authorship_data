<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Alleviating Poor Context with Background Knowledge for Named Entity Disambiguation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ander</forename><surname>Barrena</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IXA NLP Group UPV</orgName>
								<orgName type="institution">EHU University of the Basque Country Donostia</orgName>
								<address>
									<settlement>Basque Country</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IXA NLP Group UPV</orgName>
								<orgName type="institution">EHU University of the Basque Country Donostia</orgName>
								<address>
									<settlement>Basque Country</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IXA NLP Group UPV</orgName>
								<orgName type="institution">EHU University of the Basque Country Donostia</orgName>
								<address>
									<settlement>Basque Country</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Alleviating Poor Context with Background Knowledge for Named Entity Disambiguation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1903" to="1912"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Named Entity Disambiguation (NED) algorithms disambiguate mentions of named entities with respect to a knowledge-base, but sometimes the context might be poor or misleading. In this paper we introduce the acquisition of two kinds of background information to alleviate that problem: entity similarity and selectional preferences for syntactic positions. We show, using a generative Näive Bayes model for NED, that the additional sources of context are complementary, and improve results in the CoNLL 2003 and TAC KBP DEL 2014 datasets, yielding the third best and the best results, respectively. We provide examples and analysis which show the value of the acquired background information.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of Named Entity Disambiguation (NED) is to link each mention of named entities in a docu- ment to a knowledge-base of instances. The task is also known as Entity Linking or Entity Resolution ( <ref type="bibr" target="#b6">Bunescu and Pasca, 2006;</ref><ref type="bibr" target="#b18">McNamee and Dang, 2009;</ref><ref type="bibr" target="#b10">Hachey et al., 2012</ref>). NED is confounded by the ambiguity of named entity mentions. For in- stance, according to Wikipedia, Liechtenstein can refer to the micro-state, several towns, two cas- tles or a national football team, among other in- stances. Another ambiguous entity is Derbyshire which can refer to a county in England or a cricket team. Most NED research use knowledge-bases derived or closely related to Wikipedia.</p><p>For a given mention in context, NED systems ( <ref type="bibr" target="#b10">Hachey et al., 2012;</ref><ref type="bibr" target="#b15">Lazic et al., 2015</ref>) typically rely on two models: (1) a mention module returns possible entities which can be referred to by the mention, ordered by prior probabilities; (2) a con- <ref type="figure">Figure 1</ref>: Two examples where NED systems fail, motivating our two background models: similar entities (top) and selectional preferences (bottom). The logos correspond to the gold label.</p><p>text model orders the entities according to the con- text of the mention, using features extracted from annotated training data. In addition, some systems check whether the entity is coherent with the rest of entities mentioned in the document, although <ref type="bibr" target="#b15">(Lazic et al., 2015)</ref> shows that the coherence mod- ule is not required for top performance. <ref type="figure">Figure 1</ref> shows two real examples from the de- velopment dataset which contains text from News, where the clues in the context are too weak or mis- leading. In fact, two mentions in those examples (Derbyshire in the first and Liechtenstein in the second) are wrongly disambiguated by a bag-of-words context model.</p><p>In the first example, the context is very poor, and the system returns the county instead of the cricket team. In order to disambiguate it correctly one needs to be aware that Derbyshire, when oc- curring on News, is most notably associated with cricket. This background information can be ac- quired from large News corpora such as Reuters ( <ref type="bibr" target="#b16">Lewis et al., 2004</ref>), using distributional methods to construct a list of closely associated entities ( <ref type="bibr" target="#b19">Mikolov et al., 2013</ref>). <ref type="figure">Figure 1</ref> shows entities which are distributionally similar to Derbyshire, ordered by similarity strength. Although the list might say nothing to someone not acquainted with cricket, all entities in the list are strongly related to cricket: Middlesex used to be a county in the UK that gives name to a cricket club, Nottinghamshire is a county hosting two powerful cricket and foot- ball teams, Edgbaston is a suburban area and a cricket ground, the most notable team to carry the name Glamorgan is Glamorgan County Cricket Club, Trevor Barsby is a cricketer, as are all other people in the distributional context. When using these similar entities as context, our system does return the correct entity for this mention.</p><p>In the second example, the words in the con- text lead the model to return the football team for Liechtenstein, instead of the country, without be- ing aware that the nominal event "visit to" prefers locations arguments. This kind of background in- formation, known as selections preferences, can be easily acquired from corpora <ref type="bibr" target="#b9">(Erk, 2007)</ref>. <ref type="figure">Fig- ure 1</ref> shows the most frequent entities found as ar- guments of "visit to" in the Reuters corpus. When using these filler entities as context, the context model does return the correct entity for this men- tion.</p><p>In this article we explore the addition of two kinds of background information induced from corpora to the usual context of occurrence: (1) given a mention we use distributionally similar en- tities as additional context; (2) given a mention and the syntactic dependencies in the context sen- tence, we use the selectional preferences of those syntactic dependencies as additional context. We test their contribution separately and combined, showing that they introduce complementary infor- mation.</p><p>Our contributions are the following: (1) we in- troduce novel background information to provide additional disambiguation context for NED; <ref type="bibr">(2)</ref> we integrate this information in a Bayesian gen- erative NED model; (3) we show that similar enti- ties are useful when no textual context is present; (4) we show that selectional preferences are use- ful when limited context is present; (5) both kinds of background information help improve results of a NED system, yielding the state-of-the-art in the TAC KBP DEL 2014 dataset and getting the third best results in the CoNLL 2003 dataset; (6) we release both resources for free to facilitate repro- ducibility. <ref type="bibr">1</ref> The paper is structured as follows. We first in- troduce the method to acquire background infor- mation, followed by the NED system. Section 4 presents the evaluation datasets, Section 5 the de- velopment experiments and Section 6 the overall results. They are followed by related work, error analysis and the conclusions section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Acquiring background information</head><p>We built our two background information re- sources from the Reuters corpus ( <ref type="bibr" target="#b16">Lewis et al., 2004</ref>), which comprises 250K documents. We chose this corpus because it is the one used to se- lect the documents annotated in one of our gold standards (cf. Section 4). The documents in this corpus are tagged with categories, which we used to explore the influence of domains.</p><p>The documents were processed using a publicly available NLP pipeline, Ixa-pipes, 2 including to- kenization, lematization, dependency tagging and NERC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Similar entity mentions</head><p>Distributional similarity is known to provide use- ful information regarding words that have similar co-occurrences. We used the popular word2vec 3 tool to produce vector representations for named entities in the Reuters corpus. In order to build a resource that yields similar entity mentions, we took all entity-mentions detected by the NERC tool and, if they were multi word entities, joined them into a single token replacing spaces with un- derscores, and appended a tag to each of them. We run word2vec with default parameters on the pre- processed corpus. We only keep the vectors for named entities, but note that the corpus contains both named entities and other words, as they are needed to properly model co-occurrences.</p><p>Given a named entity mention, we are thus able to retrieve the named entity mentions which are most similar in the distributional vector space. All in all, we built vectors for 95K named entity men- tions. <ref type="figure">Figure 1</ref> shows the ten most similar named entities for Derbyshire according to the vectors learned from the Reuters corpus. These similar mentions can be seen as a way to encode some no- tion of a topic-related most frequent sense prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Selectional Preferences</head><p>Selectional preferences model the intuition that ar- guments of predicates impose semantic constraints (or preferences) on the possible fillers for that ar- gument position <ref type="bibr" target="#b21">(Resnik, 1996)</ref>. In this work, we use the simplest model, where the selectional pref- erence for an argument position is given by the frequency-weighted list of fillers <ref type="bibr" target="#b9">(Erk, 2007)</ref>.</p><p>We extract dependency patterns as follows. Af- ter we parse Reuters with the Mate dependency parser <ref type="bibr" target="#b4">(Bohnet, 2010)</ref>  In addition to triples (single dependency rela- tions) we also extracted tuples involving two de- pendency relations in two flavors: (H</p><formula xml:id="formula_0">D 1 − − → C 1 D 2 − − → C 2 ) and (C 1 D 1 ← − − H D 2 − − → C 2 ).</formula><p>Templates and fillers are defined as done for single dependencies, but, in this case, we extract fillers in any of the three positions and we thus have three different templates for each flavor.</p><p>As dependency parsers work at the word level, we had to post-process the output to identify whether the word involved in the dependency was part of a named entity identified by the NERC al- gorithm. We only keep tuples which involve at least one name entity. Some examples for the three kinds of tuples follow, including the frequency of occurrence, with entities shown in bold:</p><formula xml:id="formula_1">(beat SBJ − −− → Australia) 141 (refugee M OD − −−− → Hutu) 1681 (visit M OD − −−− → to M OD − −−− → United States) 257 (match M OD − −−− → against M OD − −−− → Manchester United) 12 (Spokesman SBJ ← −− − tell OBJ − −− → Reuters) 1378 (The Middle East M OD ← −−− − process M OD − −−− → peace) 1126</formula><p>When disambiguating a mention of a named entity, we check whether the mention occurs on a known dependency template, and we extract the most frequent fillers of that dependency tem- plate. For instance, the bottom example in <ref type="figure">Fig</ref> , and we thus extract the selectional preference for this tem- plate, which includes, in the figure 1, the ten most frequent filler entities.</p><p>We extracted more than 4.3M unique tuples from Reuters, producing 2M templates and their respective fillers. The most frequent depen- dency was MOD, followed by SUBJ and OBJ <ref type="bibr">5</ref> The selectional preferences include 400K differ- ent named entities as fillers.</p><p>Note that selectional preferences are different from dependency path features. Dependency path features refer to features in the immediate context of the entity mention, and are sometimes added as additional features of supervised classifiers. Se- lectional preferences are learnt collecting fillers in the same dependency path, but the fillers occur elsewhere in the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NED system</head><p>Our disambiguation system is a Näive Bayes model as initially introduced by <ref type="bibr" target="#b11">(Han and Sun, 2011a)</ref>, but adapted to integrate the background information extracted from the Reuters corpus. The model is trained using Wikipedia, <ref type="bibr">6</ref> which is also used to generate the entity candidates for each mention.</p><p>Following usual practice, candidate generation is performed off-line by constructing an associa- tion between strings and Wikipedia articles, which we call dictionary. The association is performed using article titles, redirections, disambiguation pages, and textual anchors. Each association is scored with the number of times the string was used to refer to the article ( ). We also use Wikipedia to extract training mention contexts for all possible candidate entities. Men- tion contexts for an entity are built by collecting a window of 50 words surrounding any hyper link pointing to that entity.</p><p>Both training and test instances are pre- processed the same way: occurrence context is to- kenized, multi-words occurring in the dictionary are collapsed as a single token (longest matches are preferred). All occurrences of the same tar- get mention in a document are disambiguated col- lectively, as we merge all contexts of the multiple mentions into one, following the one-entity-per- discourse hypothesis ( <ref type="bibr" target="#b2">Barrena et al., 2014)</ref>.</p><p>The Näive Bayes model is depicted in <ref type="figure" target="#fig_2">Figure 2</ref>. The candidate entity e of a given mention s, which occurs within a context c, is selected according to the following formula: The formula combines evidences taken from five different probabilities: the entity prior p(e), the mention probability p(s|e), the textual context p(c|s), the selectional preferences P (c sp |e, s) and the distributional similarity P (c sim |e, s). This for- mula is also referred to as the "Full model", as we also report results of partial models which use different combinations of the five probability esti- mations.</p><p>Entity prior P (e) represents the popularity of entity e, and is estimated as follows:</p><formula xml:id="formula_2">P (e) ∝ f ( * , e) + 1 f ( * , * ) + N</formula><p>where f ( * , e) is the number of times the entity e is referenced within Wikipedia, f ( * , * ) is the total number of entity mentions and N is the number of distinct entities in Wikipedia. The estimation is smoothed using the add-one method. Mention probability P (s|e) represents the probability of generating the mention s given the entity e, and is estimated as follows:</p><formula xml:id="formula_3">P (s|e) ∝ θ f (s, e) f ( * , e) + (1 − θ) f (s, * ) f ( * , * )</formula><p>where f (s, e) is the number of times mention s is used to refer to entity e and f (s, * ) is the number of times mention s is used as anchor. We set the θ hyper-parameter to 0.9 according to developments experiments in the CoNLL testa dataset (cf. Sec- tion 5.5).</p><p>Textual context P (c|e) is the probability of en- tity e generating the context c = {w 1 , . . . , w n }, and is expressed as:</p><formula xml:id="formula_4">P (c|e) = w∈c P (w|e) 1 n where 1</formula><p>n is a correcting factor that compensates the effect of larger contexts having smaller probabili- ties. P (w|e), the probability of entity e generating word w, is estimated following a bag-of-words ap- proach:</p><formula xml:id="formula_5">P (w|e) ∝ λ c(w, e) c( * , e) + (1 − λ) f (w, * ) f ( * , * )</formula><p>where c(w, e) is the number of times word w ap- pears in the mention contexts of entity e, and c( * , e) is the total number of words in the men- tion contexts. The term in the right is a smooth- ing term, calculated as the likelihood of word w being used as an anchor in Wikipedia. λ is set to 0.9 according to development experiments done in CoNLL testa. Distributional Similarity P (c sim |e, s) is the probability of generating a set of similar entity mentions given an entity mention pair. This prob- ability is calculated and estimated in exactly the same way as the textual context above, but replac- ing the mention context c with the mentions of the 30 most similar entities for s (cf. Section 2.1).</p><p>Selectional Preferences P (c sp |e, s) is the prob- ability of generating a set of fillers c sp given an entity and mention pair. The probability is again analogous to the previous ones, but using the filler entities of the selectional preferences of s instead of the context c (cf. Section 2.2). In our ex- periments, we select the 30 most frequent fillers for each selectional preferences, concatenating the filler list when more than one selectional prefer- ence is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ensemble model</head><p>In addition to the Full model, we created an en- semble system that combines the probabilities de- scribed above using a weighting schema, which we call "Full weighted model". In particular, we add an exponent coefficient to the probabilities, thus allowing to control the contribution of each model.</p><p>arg max e P (e) α P (s|e) β P (c|e) γ P (c sp |e, s) δ P (c sim |e, s) ω</p><p>We performed an exhaustive grid search in the interval (0, 1) for each of the weights, using a step size of 0.05, and discarding the combinations whose sum is not one. Evaluation of each combi- nation was performed in the CoNLL testa devel- opment set, and the best combination was applied in the test sets. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Datasets</head><p>The evaluation has been performed on one of the most popular datasets, the CoNLL 2003 named- entity disambiguation dataset, also know as the AIDA or CoNLL-Yago dataset <ref type="bibr" target="#b13">(Hoffart et al., 2011)</ref>. It is composed of 1393 news documents from Reuters Corpora where named entity men- tions have been manually identified. It is divided in three main parts: train, testa and testb. We used testa for development experiments, and testb for the final results and comparison with the state-of- the-art. We ignored the training part.</p><p>In addition, we also report results in the Text Analysis Conference 2014 Diagnostic Entity Linking task dataset (TAC DEL 2014). <ref type="bibr">8</ref> The gold standard for this task is very similar to the CoNLL dataset, where target named entity mentions have been detected by hand. Through the beginning of the task (2009 to 2013) the TAC datasets were query-driven, that is, the input included a doc- ument and a challenging and sometimes partial target-mention to disambiguate. As this task also involved mention detection and our techniques are sensitive to mention detection errors, we preferred to factor out that variation and focus on the 2014.</p><p>The evaluation measure used in this paper is micro-accuracy, that is, the percentage of link- able mentions that the system disambiguates cor- rectly, as widely used in the CoNLL dataset. Note <ref type="bibr">7</ref> The best combination was α = 0.05, β = 0.1, γ = 0.55 </p><formula xml:id="formula_6">δ = 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Development experiments</head><p>We started to check the contribution of the ac- quired background information in the testa section of the CoNLL dataset. In fact, we decided to fo- cus first on a subset of testa about sports, <ref type="bibr">9</ref> and also acquired background information from the sports sub-collection of the Reuters corpus. <ref type="bibr">10</ref> The ratio- nale was that we wanted to start in a controlled setting, and having assumed that the domain of the test documents and the source of the back- ground information could play a role, we decided to start focusing on the sports domain first. An- other motivation is that we noticed that the ambi- guity between locations and sport clubs (e.g. foot- ball, cricket, rugby, etc.) is challenging, as shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Entity similarity with no context</head><p>In our first controlled experiment, we wanted to test whether the entity similarity resource pro- vided any added value for the cases where the target mentions had to be disambiguated out of context. Our hypothesis was that the background information from the unannotated Reuters collec- tion, entity similarity in this case, should provide improved performance. We thus simulated a cor- pus where mentions have no context, extracting the named entity mentions in the sports subset that Method m-acc P (e)P (s|e) 63.83 P (e)P (s|e)P (c sim |e, s) 70.98 <ref type="table">Table 2</ref>: Results on mentions with no context on the sports subset of testa, limited to 85% of the mentions (cf. Section 5.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>m-acc P (e)P (s|e)</p><p>63.66 P (e)P (s|e)P (c|e) 66.18 P (e)P (s|e)P (c sp |e, s) 67.33 P (e)P (s|e)P (c|e)P (c sp |e, s) 68.78 <ref type="table">Table 3</ref>: Results on mentions with access to lim- ited context on the sports subset of testa, limited to the 45% of mentions (cf. Section 5.2).</p><p>had an entry in the entity similarity resource (cf. Section 2.1), totaling 85% of the 3319 mentions. <ref type="table">Table 2</ref> shows that the entity similarity resource improves the results of the model combining the entity prior and mention probability, similar to the so-called most frequent sense baseline (MFS). Note that the combination of both entity prior and mention probability is a hard-to-beat baseline, as we will see in Section 6. This experiment confirms that entity similarity information is useful when no context is present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Selectional preferences with short context</head><p>In our second controlled experiment, we wanted to test whether the selectional preferences pro- vided any added value for the cases where the target mentions had limited context, that of the dependency template. Our hypothesis was that the background information from the unannotated Reuters collection, selectional preferences in this case, should provide improved performance with respect to the baseline generative model of con- text. We thus simulated a corpus where mentions have only short context, exactly the same as the dependency templates which apply to the exam- ple, constructed extracting the named entity men- tions in the sports subset that contained matching templates in the selectional preference resource (cf. Section 2.2), totaling 45% of the 3319 men- tions. <ref type="table">Table 3</ref> shows that the selectional preference re- source (third row) allows to improve the results with respect to the no-context baseline (first row) and, more importantly, with respect to the base- Method m-acc P (e)P (s|e)P (c|e) 69.54 P (e)P (s|e)P (c|e)P (c sp |e, s) 71.25 P (e)P (s|e)P (c|e)P (c sim |e, s) 72.64 Full 73.94  line generative model (second row). The last row shows that the context model and the selectional preference model are complementary, as they pro- duce the best result in the table. This experiment confirms that selectional preference information is effective when limited context is present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Combinations</head><p>In our third controlled experiments, we combine all three context and background models and eval- uate them in the subset of the sports mentions that have entries in the similarity resource, and also contain matching templates in the selectional pref- erence resource (41% of the sports subset). Note that, in this case, the context model has access to the entire context. <ref type="table" target="#tab_1">Table 4</ref> shows that, effectively, the background information adds up, with best re- sults for the full combined model (cf. Section 3), confirming that both sources of background infor- mation are complementary to the baseline context model and between themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Sports subsection of CoNLL testa</head><p>The previous experiments have been run on a con- trolled setting, limited to the subset where our con- structed resources could be applied. In this sec- tion we report results for the entire sports subset of CoNLL testa. The middle column in <ref type="table" target="#tab_2">Table 5</ref> shows the results for the two baselines, and the improvements when adding the two background models, separately, and in combination. The re- sults show that the improvements reported in the controlled experiments carry over when evaluat- ing to all mentions in the Sport subsection, with an accumulated improvement of 3.5 absolute points over the standard NED system (second row). The experiments so far have tried to factor out domain variation, and thus the results have been produced using the background information ac- quired from the sports subset of the Reuters col- lection. In order to check whether this control of the target domain is necessary, reproduced the same experiment using the full Reuters collection to build the background information, as reported in the rightmost column in <ref type="table" target="#tab_2">Table 5</ref>. The results are very similar, 11 with a small decrease for se- lectional preferences, a small increase for the sim- ilarity resource, and a small increase for the full system. In view of these results, we decided to use the full Reuters collection to acquire the back- ground knowledge for the rest of the experiments, and did not perform further domain-related exper- iments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Results on CoNLL testa</head><p>Finally, <ref type="table" target="#tab_3">Table 6</ref> reports the results on the full de- velopment dataset. The results show that the good results in the sports subsection carry over to the full dataset. The table reports results for the base- line systems (two top rows) and the addition of the background models, including the Full model, which yields the best results.</p><p>In addition, the two rows in the bottom report the results of the ensemble methods (cf. Section 3.1) which learn the weights on the same develop- ment dataset. These results are reported for com- pleteness, as they are an over-estimation, and are over-fit. Note that all hyper-parameters have been tuned on this development dataset, including the ensemble weights, smoothing parameters λ and θ (cf. Section 3), as well as the number of similar entities and the number of fillers in the selectional preferences. The next section will show that the good results are confirmed in unseen test datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Overall Results</head><p>In the previous sections we have seen that the background information is effective improving the results on development. In this section we report <ref type="bibr">11</ref> The two first rows do not use background information, and are thus the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>testa P (e)P (s|e) 73.76 P (e)P (s|e)P (c|e)</p><p>78.98 P (e)P (s|e)P (c|e)P (c sp |e, s) 79.32 P (e)P (s|e)P (c|e)P (c sim |e, s) 81.76 Full 81.90 P (e) α P (s|e) β P (c|e) γ 85.20 Full weighted 86.62  the result of our model in the popular CoNLL testb and TAC2014 DEL datasets, which allow to com- pare to the state-of-the-art in NED. <ref type="table" target="#tab_4">Table 7</ref> reports our results, confirming that both background information resources improve the re- sults over the standard NED generative system, separately, and in combination, for both datasets (Full row). All differences with respect to the stan- dard generative system are statistically significant according to the Wilcoxon test (p-value &lt; 0.05).</p><p>In addition, we checked the contribution of learning the ensemble weights on the development dataset (testa). Both the generative system with and without background information improve con- siderably.</p><p>The error reduction between the weighted model using background information (Full weighted row) and the generative system without background information (previous row) exceeds 10% in both datasets, providing very strong results, and confirming that the improvement due to background information is consistent across both datasets, even when applied on a very strong system. The difference is statistically significant in both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CoNLL TAC14</head><p>Full weighted 88.32 83.46 (  83.61 80.69 ( <ref type="bibr" target="#b15">Lazic et al., 2015)</ref> 86.40 - (Alhelbawy &amp; Gaizauskas,14) *87.60 - ( <ref type="bibr" target="#b8">Chisholm and Hachey, 2015)</ref> 88.70 - ( <ref type="bibr" target="#b20">Pershina et al., 2015)</ref> *91.77 - TAC14 best <ref type="bibr" target="#b14">(Ji et al., 2014</ref>) - 82.70 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Our generative model is based on <ref type="bibr" target="#b12">(Han and Sun, 2011b)</ref>, which is basically the core method used in later work ( <ref type="bibr" target="#b15">Lazic et al., 2015</ref>) with good results. Although the first do not report results on our datasets the other two do. ( ) combines the generative model with a graph-based system yielding strong results in both datasets. ( <ref type="bibr" target="#b15">Lazic et al., 2015</ref>) adds a parameter es- timation method which improved the results using unannotated data. Our work is complementary to those, as we could also introduce additional dis- ambiguation probabilities ( , or apply more sophisticated parameter estimation methods ( <ref type="bibr" target="#b15">Lazic et al., 2015)</ref>. <ref type="table" target="#tab_5">Table 8</ref> includes other high performing or well- known systems, which usually use complex meth- ods to combine features coming from different sources, where our results are only second to those of <ref type="bibr" target="#b8">(Chisholm and Hachey, 2015</ref>) in the CoNLL dataset and best in TAC 2014 DEL. The goal of this paper is not to provide the best performing system, but yet, the results show that our use of background information allows to obtain very good results.</p><p>Alhelbawy and Gaizauskas (2014) combines lo- cal and coherence features by means of a graph ranking scheme, obtaining very good results on the CONLL 2003 dataset. They evaluate on the full dataset, i.e. they test on train, testa and testb (20K, 4.8K and 4.4K mentions respectively). Our results on the same dataset are 84.25 (Full) and 88.07 (Full weighted), but note that we do tune the parameters on testa, so this might be slighly over-estimated. Our system does not use global coherence, and therefore their method is comple- mentary to our NED system. In principle, our pro- posal for enriching context should improve the re- sults of their system. <ref type="bibr" target="#b20">Pershina et al. (2015)</ref> propose a system closely resembling <ref type="bibr" target="#b1">(Alhelbawy and Gaizauskas, 2014</ref>). They report the best known results on CONNL 2003 so far, but unfortunately, their results are not directly comparable to the rest of the state-of-the- art, as they artificially insert the gold standard en- tity in the candidate list. <ref type="bibr">12</ref> In <ref type="bibr" target="#b8">(Chisholm and Hachey, 2015</ref>) the authors ex- plore the use of links gathered from the web as an additional source of information for NED. They present a complex two-staged supervised system that incorporates global coherence features, with large amount of noisy training. Again, using ad- ditional training data seems an interesting future direction complementary to ours.</p><p>We are not aware of other works which try to use additional sources of context or background information as we do. <ref type="bibr" target="#b7">(Cheng and Roth, 2013)</ref> use relational information from Wikipedia to add constraints to the coherence model, and is some- how reminiscent of our use dependency templates, although they focus on recognizing a fixed set of relations between entities (as in information ex- traction) and do not model selectional preferences. ( <ref type="bibr" target="#b2">Barrena et al., 2014</ref>) explored the use of syntac- tic collocations to ensure coherence, but did not model any selectional preferences.</p><p>Previous work on word sense disambiguation using selectional preference includes <ref type="bibr" target="#b17">(McCarthy and Carroll, 2003)</ref> among others, but they re- port low results. <ref type="bibr" target="#b5">(Brown et al., 2011</ref>) applied wordNet hypernyms for disambiguating verbs, but they did not test the improvement of this feature. ( <ref type="bibr" target="#b22">Taghipour and Ng, 2015</ref>) use embeddings as fea- tures which are fed into a supervised classifier, but our method is different, as we use embeddings to find similar words to be fed as additional context. None of the state-of-the-art systems, e.g. ( <ref type="bibr" target="#b23">Zhong and Ng, 2010</ref>), uses any model of selectional pref- erences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>We performed an analysis of the cases where our background models worsened the disambiguation performance. Both distributional similarity and selectional preferences rely on correct mention de- tection in the background corpus. We detected that mentions where missed, which caused some coverage issues. In addition, the small size of the background corpus sometimes produces ar- bitrary contexts. For instance, subject position fillers of "score" include mostly basketball play- ers like Michael Jordan or Karl Malone. A sim- ilar issue was detected in the distributional simi- larity resource. A larger corpus would produce a broader range of entities, and thus use of larger background corpora (e.g. Gigaword) should alle- viate those issues.</p><p>Another issue was that some dependencies do not provide any focused context, as for instance arguments of say or tell. We think that a more so- phisticated combination model should be able to detect which selectional preferences and similar- ity lists provide a focused set of instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions and Future Work</head><p>In this article we introduced two novel kinds of background information induced from corpora to the usual context of occurrence in NED: (1) given a mention we used distributionally similar entities as additional context; (2) given a mention and the syntactic dependencies in the context sentence, we used the selectional preferences of those syntactic dependencies as additional context. We showed that similar entities are specially useful when no textual context is present, and that selectional pref- erences are useful when limited context is present.</p><p>We integrated them in a Bayesian generative NED model which provides very strong results. In fact, when integrating all knowledge resources we yield the state-of-the-art in the TAC KBP DEL 2014 dataset and get the third best results in the CoNLL 2003 dataset. Both resources are freely available for reproducibility. <ref type="bibr">13</ref> The analysis of the acquired information and the error analysis show several avenues for future work. First larger corpora should allow to increase the applicability of the similarity resource, and specially, that of the dependency templates, and also provide better quality resources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>integrated in IxaPipes, we extract (H D − → C) dependency triples, where D is one of the Subject, Object or Modifier dependen- cies 4 (SBJ, OBJ, M OD, respectively), H is the head word and C the dependent word. We extract fillers in both directions, that is, the set of fillers in the dependent position {C : (H D − → C)}, but also the fillers in the head position {H : (H D − → C)}. Each such configuration forms a template, (H D − → * ) and ( * D − → C).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>- ure 1 shows how Liechtenstein occurs as a filler of the template (visit M OD − −−− → to M OD − −−− → *)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Dependencies among variables in our Bayesian network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>e</head><label></label><figDesc>= arg max e P (s, c, c sp , c sim , e) = arg max e P (e)P (s|e)P (c|e)P (c sp |e, s)P (c sim |e, s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results on mentions with limited context 
on the sports subset of testa, limited to the 41% of 
the mentions (cf. Section 5.3) 

Models 
Spor. Reut. 
P (e)P (s|e) 
65.52 65.52 
P (e)P (s|e)P (c|e) 
72.81 72.81 
P (e)P (s|e)P (c|e)P (c sp |e, s) 
73.56 73.06 
P (e)P (s|e)P (c|e)P (c sim |e, s) 75.73 76.62 
Full 
76.30 76.87 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results on the entire sports subset of 
testa: middle column uses the sports subset of 
Reuters to acquire background information, right 
column uses the full Reuters (cf. Section 5.4). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Results on the full testa dataset (cf. Sec-
tion 5.5). 

System 

CoNLL TAC14 

P (e)P (s|e) 
73.07 78.31 
P (e)P (s|e)P (c|e) 
79.98 82.11 
P (e)P (s|e)P (c|e)P (c sp |e, s) 
81.31 82.61 
P (e)P (s|e)P (c|e)P (c sim |e, s) 82.72 83.24 
Full 
82.85 83.21 
P (e) α P (s|e) β P (c|e) γ 
86.44 81.61 
Full weighted 
88.32 83.46 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 7 :</head><label>7</label><figDesc>Overall micro accuracy results on the CoNLL testb and TAC 2014 DEL datasets.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Overall micro accuracy results on the 
CoNLL testb and TAC 2014 DEL datasets, includ-
ing the current state-of-the-art. Starred results are 
not comparable, see text. 

</table></figure>

			<note place="foot" n="1"> http://ixa2.si.ehu.es/anderbarrena/ 2016ACL_files.zip 2 http://ixa2.si.ehu.es/ixa-pipes/ 3 https://code.google.com/archive/p/ word2vec/</note>

			<note place="foot" n="4"> Labels are taken from the Penn Treebank https://www.ling.upenn.edu/courses/Fall_ 2003/ling001/penn_treebank_pos.html</note>

			<note place="foot" n="5"> 1.5M, 0.8M and 0.7M respectively 6 We used a dump from 25-5-2011. This dump is close in time to annotations of the datasets used in the evaluation (c.f. Section 4)</note>

			<note place="foot" n="9"> Including 102 out of the 216 documents in testa, totaling 3319 mentions. 10 Including approx. 35K documents out of the 250K documents in Reuters</note>

			<note place="foot" n="12"> https://github.com/masha-p/PPRforNED/ readme.txt</note>

			<note place="foot" n="13"> http://ixa2.si.ehu.es/anderbarrena/ 2016ACL_files.zip is funded by the Basque Government (A type Research Group). Ander Barrena enjoys an PhD scholarship from the Basque Government.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their suggestions. This work was partially funded by MINECO (TUNER project, TIN2015-65308-C5-1-R). The IXA group</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Studying the wikipedia hyperlink graph for relatedness and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ander</forename><surname>Barrena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
		<idno>abs/1503.01655</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph ranking for collective named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayman</forename><surname>Alhelbawy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="75" to="80" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">one entity per discourse&quot; and &quot;one entity per collocation&quot; improve named-entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ander</forename><surname>Barrena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Cabaleiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland, August</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2260" to="2269" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
	<note>Anselmo Peñas, and Aitor Soroa</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining mention context and hyperlinks from wikipedia for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ander</forename><surname>Barrena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the Fourth Joint Conference on Lexical and Computational Semantics<address><addrLine>Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="101" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Very high accuracy and fast dependency parsing is not a contradiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics, COLING &apos;10</title>
		<meeting>the 23rd International Conference on Computational Linguistics, COLING &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="89" to="97" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Verbnet class assignment as a wsd task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Susan Windisch Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Computational Semantics</title>
		<meeting>the Ninth International Conference on Computational Semantics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using encyclopedic knowledge for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceesings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>eesings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL)<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Relational inference for wikification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Entity disambiguation with web links</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chisholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="145" to="156" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple, similarity-based model for selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluating Entity Linking with Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hachey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="130" to="150" />
			<date type="published" when="2012-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A generative entitymention model for linking entities with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="945" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A generative entitymention model for linking entities with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL HLT</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust Disambiguation of Named Entities in Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="782" to="792" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Overview of tac-kbp2014 entity discovery and linking tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Text Analysis Conference (TAC2014)</title>
		<meeting>Text Analysis Conference (TAC2014)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Plato: A selective context model for entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="503" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>David D Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="639" to="654" />
			<date type="published" when="2003-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Overview of the TAC 2009 Knowledge Base Population track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><surname>Dang</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Personalized page rank for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pershina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="238" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Selectional constraints: An information-theoretic model and its computational realization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="127" to="159" />
			<date type="published" when="1996-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semisupervised word sense disambiguation using word embeddings in general and specific domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Taghipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="314" to="323" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">It makes sense: A wide-coverage word sense disambiguation system for free text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL: System Demonstrations</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
