<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coherence Modeling of Asynchronous Conversations: A Neural Entity Grid Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tasnim</forename><surname>Mohiuddin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanyang Technological University</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanyang Technological University</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien</forename><surname>Dat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanyang Technological University</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
							<email>t.d.nguyen@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanyang Technological University</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Coherence Modeling of Asynchronous Conversations: A Neural Entity Grid Approach</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="558" to="568"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>558</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel coherence model for written asynchronous conversations (e.g., forums, emails), and show its applications in coherence assessment and thread reconstruction tasks. We conduct our research in two steps. First, we propose improvements to the recently proposed neural entity grid model by lexicalizing its entity transitions. Then, we extend the model to asynchronous conversations by incorporating the underlying conversational structure in the entity grid representation and feature computation. Our model achieves state of the art results on standard coherence assessment tasks in monologue and conversations outperforming existing models. We also demonstrate its effectiveness in reconstructing thread structures.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentences in a text or a conversation do not occur independently, rather they are connected to form a coherent discourse that is easy to comprehend. Coherence models are computational models that can distinguish a coherent discourse from incoher- ent ones. It has ranges of applications in text gen- eration, summarization, and coherence scoring.</p><p>Inspired by formal theories of discourse, a number of coherence models have been proposed ( <ref type="bibr" target="#b1">Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b16">Lin et al., 2011;</ref><ref type="bibr" target="#b15">Li and Jurafsky, 2017)</ref>. The entity grid model ( <ref type="bibr" target="#b1">Barzilay and Lapata, 2008</ref>) is one of the most popular coherence models that has received much attention over the years. As exemplified in <ref type="table" target="#tab_0">Table  1</ref>, the model represents a text by a grid that cap- tures how grammatical roles of different discourse entities (e.g., nouns) change from one sentence to  another in the text. The grid is then converted into a feature vector containing probabilities of local entity transitions, enabling machine learning mod- els to measure the degree of coherence. Earlier extensions of this basic model incorporate entity- specific features <ref type="bibr" target="#b4">(Elsner and Charniak, 2011b)</ref>, multiple ranks <ref type="bibr" target="#b5">(Feng and Hirst, 2012)</ref>, and coher- ence relations <ref type="bibr" target="#b6">(Feng et al., 2014)</ref>.</p><p>Recently, <ref type="bibr" target="#b21">Nguyen and Joty (2017)</ref> proposed a neural version of the grid models. Their model first transforms the grammatical roles in a grid into their distributed representations, and employs a convolution operation over it to model entity tran- sitions in the distributed space. The spatially max- pooled features from the convoluted features are used for coherence scoring. This model achieves state-of-the-art results in standard evaluation tasks on the Wall Street Journal (WSJ) corpus.</p><p>Although the neural grid model effectively cap- tures long entity transitions, it is still limited in that it does not consider any lexical information regarding the entities, thereby, fails to distinguish between entity types. Although the extended neu- ral grid considers entity features like named entity and proper mention, it requires an explicit feature extraction step, which can prevent us to transfer the model to a resource-poor language or domain.</p><p>Apart from these limitations, previous research on coherence models has mainly focused on monologic discourse (e.g., news article). The only exception is the work of <ref type="bibr" target="#b3">Elsner and Charniak (2011a)</ref>, who applied coherence models to the task of conversation disentanglement in synchronous conversations like phone and chat conversations. With the emergence of Internet technologies, asynchronous communication media like emails, blogs, and forums have become a commonplace for discussing events and issues, seeking answers, and sharing personal experiences. Participants in these media interact with each other asyn- chronously, by writing at different times. We be- lieve coherence models for asynchronous conver- sations can help many downstream applications in these domains. For example, we will demonstrate later that coherence models can be used to pre- dict the underlying thread structure of a conversa- tion, which provides crucial information for build- ing effective conversation summarization systems <ref type="bibr" target="#b2">(Carenini et al., 2008)</ref> and community question answering systems ( <ref type="bibr" target="#b0">Barron-Cedeno et al., 2015)</ref>.</p><p>To the best of our knowledge, none has stud- ied the problem of coherence modeling in asyn- chronous conversation before. Because of its asynchronous nature, information flow in these conversations is often not sequential as in mono- logue or synchronous conversation. This poses a novel set of challenges for discourse analysis mod- els ( <ref type="bibr" target="#b13">Joty et al., 2013;</ref><ref type="bibr" target="#b17">Louis and Cohen, 2015)</ref>. For example, consider the forum conversation in <ref type="figure" target="#fig_1">Fig- ure 2(a)</ref>. It is not obvious how a coherence model like the entity grid can represent the conversation, and use it in downstream tasks effectively.</p><p>In this paper we aim to remedy the above lim- itations of existing models in two steps. First, we propose improvements to the existing neural grid model by lexicalizing its entity transitions. We propose methods based on word embeddings to achieve better generalization with the lexical- ized model. Second, we adapt the model to asyn- chronous conversations by incorporating the un- derlying conversational structure in the grid rep- resentation and subsequently in feature computa- tion. For this, we propose a novel grid representa- tion for asynchronous conversations, and adapt the convolution layer of the neural model accordingly.</p><p>We evaluate our approach on two discrimination tasks. The first task is the standard one, where we assess the models based on their performance in discriminating an original document from its ran- dom permutation. In our second task, we ask the models to distinguish an original document from its inverse order of the sentences. For our adapted model to asynchronous conversation, we also eval- uate it on thread reconstruction, a task specific to asynchronous conversation. We performed a se- ries of experiments, and our main findings are:</p><p>(a) Our experiments on the WSJ corpus validate the utility of our proposed extension to the ex- isting neural grid model, yielding absolute F 1 improvements of up to 4.2% in the standard task and up to 5.2% in the inverse-order dis- crimination task, setting a new state-of-the-art.</p><p>(b) Our experiments on a forum dataset show that our adapted model that considers the conver- sational structure outperforms the temporal baseline by more than 4% F 1 in the standard task and by about 10% F 1 in the inverse order discrimination task.</p><p>(c) When applied to the thread reconstruction task, our model achieves promising results outperforming several strong baselines.</p><p>We have released our source code and datasets at https://ntunlpsg.github. io/project/coherence/n-coh-acl18/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section we give an overview of existing coherence models. In the interest of coherence, we defer description of the neural grid model <ref type="bibr" target="#b21">(Nguyen and Joty, 2017)</ref> until next section, where we present our extension to this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Traditional Entity Grid Models</head><p>Introduced by <ref type="bibr" target="#b1">Barzilay and Lapata (2008)</ref>, the entity grid model represents a text by a two- dimensional matrix. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the rows correspond to sentences, and the columns corre- spond to entities (noun phrases). Each entry E i,j represents the syntactic role that entity e j plays in sentence s i , which can be one of: subject (S), ob- ject (O), other (X), or absent (-). In cases where an entity appears more than once with different gram- matical roles in the same sentence, the role with the highest rank (S O X) is considered.</p><p>Motivated by the Centering Theory ( <ref type="bibr" target="#b8">Grosz et al., 1995)</ref>, the model considers local entity transitions as the deciding patterns for assessing coherence. A local entity transition of length k is a sequence of {S,O,X,-} k , representing gram- matical roles played by an entity in k consecutive sentences. Each grid is represented by a vector of 4 k transition probabilities computed from the grid. To distinguish between transitions of important entities from unimportant ones, the model con- siders the salience of the entities, which is mea- sured by their occurrence frequency in the docu- ment. With the feature vector representation, co- herence assessment task is formulated as a ranking problem in a SVM preference ranking framework <ref type="bibr" target="#b12">(Joachims, 2002)</ref>. <ref type="bibr" target="#b1">Barzilay and Lapata (2008)</ref> showed significant improvements in two out of three evaluation tasks when a coreference resolver is used to identify coreferent entities in a text. <ref type="bibr" target="#b4">Elsner and Charniak (2011b)</ref> show improve- ments to the grid model by including non-head nouns as entities. Instead of employing a coref- erence resolver, they match the nouns to detect coreferent entities. They demonstrate further im- provements by extending the grid to distinguish between entities of different types. They do so by incorporating entity-specific features like named entity, noun class and modifiers. <ref type="bibr" target="#b16">Lin et al. (2011)</ref> model transitions of discourse roles for entities as opposed to their grammatical roles. They instanti- ate discourse roles by discourse relations in Penn Discourse Treebank ( <ref type="bibr" target="#b24">Prasad et al., 2008)</ref>. In a fol- low up work, <ref type="bibr" target="#b6">Feng et al. (2014)</ref> trained the same model but using relations derived from deep dis- course structures annotated with Rhetorical Struc- ture Theory ( <ref type="bibr" target="#b19">Mann and Thompson, 1988</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Other Existing Models</head><p>Guinaudeau and Strube (2013) proposed a graph- based unsupervised method. They convert an en- tity grid into a bipartite graph consisting of two sets of nodes, representing sentences and enti- ties, respectively. The edges are assigned weights based on the grammatical role of the entities in the respective sentences. They perform one-mode projections to transform the bipartite graph to a di- rected graph containing only sentence nodes. The coherence score of the document is then computed as the average out-degree of sentence nodes.</p><p>Louis and Nenkova (2012) introduced a coher- ence model based on syntactic patterns by as- suming that sentences in a coherent text exhibit certain syntactic regularities. They propose a local coherence model that captures the co-occurrence of structural features in adjacent sentences, and a global model based on a hidden Markov model, which learns the global syntactic patterns from clusters of sentences with similar syntax.</p><p>Li and Hovy <ref type="formula" target="#formula_0">(2014)</ref> proposed a neural frame- work to compute the coherence score of a docu- ment by estimating coherence probability for ev- ery window of three sentences. They encode each sentence in the window using either a recurrent or a recursive neural network. To get a document- level coherence score, they sum up the window- level log probabilities. Li and Jurafsky <ref type="formula" target="#formula_0">(2017)</ref> proposed two encoder-decoder models augmented with latent variables for both coherence evalua- tion and discourse generation. Their first model incorporates global discourse information (topics) by feeding the output of a sentence-level HMM- LDA model ( <ref type="bibr" target="#b9">Gruber et al., 2007)</ref> into the encoder- decoder model. Their second model is trained end-to-end with variational inference.</p><p>In our work, we take an entity-based approach, and extend the neural grid model proposed re- cently by Nguyen and Joty (2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Extending Neural Entity Grid</head><p>In this section we first briefly describe the neu- ral entity grid model proposed by <ref type="bibr" target="#b21">Nguyen and Joty (2017)</ref>. Then, we propose our extension to this model that leads to improved performance. We present our coherence model for asynchronous conversation in the next section. <ref type="figure">Figure 1</ref> depicts the neural grid model of <ref type="bibr" target="#b21">Nguyen and Joty (2017)</ref>. Given an entity grid E, they first transform each entry E i,j (a grammatical role) into a distributed representation of d dimensions by looking up a shared embedding matrix M ∈ R |G|×d , where G is the vocabulary of possible grammatical roles, i.e., G = {S, O, X, −}. For- mally, the look-up operation can be expressed as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural Entity Grid</head><formula xml:id="formula_0">L = M (E 1,1 ) · · · M (E i,j ) · · · M (E I,J )<label>(1)</label></formula><p>where M (E i,j ) refers to the row in M that corre- sponds to grammatical role E i,j , and I and J are <ref type="figure">Figure 1</ref>: Neural entity grid model proposed by <ref type="bibr" target="#b21">Nguyen and Joty (2017)</ref>. The model is trained us- ing a pairwise ranking approach with shared pa- rameters for positive and negative documents.</p><p>the number of rows (sentences) and columns (en- tities) in the entity grid, respectively. The result of the look-up operation is a tensor L ∈ R I×J×d , which is fed to a convolution layer to model local entity transitions in the distributed space. The convolution layer of the neural network composes patches of entity transitions into high- level abstract features by treating entities indepen- dently (i.e., 1D convolution). Formally, it applies a filter w ∈ R m.d to each local entity transition of length m to generate a new abstract feature z i :</p><formula xml:id="formula_1">z i = h(w T L i:i+m,j + b i )<label>(2)</label></formula><p>where L i:i+m,j denotes concatenation of m vec- tors in L for entity e j , b i is a bias term, and h is a nonlinear activation function. Repeated applica- tion of this filter to each possible m-length tran- sitions of different entities in the grid generates a feature map,</p><formula xml:id="formula_2">z i = [z 1 , · · · , z I.J+m−1 ]. This pro- cess is repeated N times with N different filters to get N different feature maps, [z 1 , · · · , z N ].</formula><p>A max-pooling operation is then applied to extract the most salient features from each feature map:</p><formula xml:id="formula_3">p = [µ l (z 1 ), · · · , µ l (z N )]<label>(3)</label></formula><p>where µ l (z i ) refers to the max operation applied to each non-overlapping window of l features in the feature map z i . Finally, the pooled features are used in a linear layer to produce a coherence score:</p><formula xml:id="formula_4">y = u T p + b (4)</formula><p>where u is the weight vector and b is a bias term. The model is trained with a pairwise ranking loss based on ordered training pairs (E i , E j ):</p><formula xml:id="formula_5">L(θ) = max{0, 1 − φ(E i |θ) + φ(E j |θ)} (5)</formula><p>where entity grid E i exhibits a higher degree of coherence than grid E j , and y = φ(E k |θ) denotes the transformation of input grid E k to a coher- ence score y done by the model with parameters θ.</p><p>We will see later that such ordering of documents (grids) can be obtained automatically by permut- ing the original document. Notice that the network shares its parameters (θ) between the positive (E i ) and the negative (E j ) instances in a pair.</p><p>Since entity transitions in the convolution step are modeled in a continuous space, it can effec- tively capture longer transitions compared to tradi- tional grid models. Unlike traditional grid models that compute transition probabilities from a single grid, convolution filters and role embeddings in the neural model are learned from all training in- stances, which helps the model to generalize well.</p><p>Since the abstract features in the feature maps are generated by convolving over role transitions of different entities in a document, the model im- plicitly considers relations between entities in a document, whereas transition probabilities in tra- ditional entity grid models are computed with- out considering any such relation between entities. Convolution over the entire grid also incorporates global information (e.g., topic) of a discourse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Lexicalized Neural Entity Grid</head><p>Despite its effectiveness, the neural grid model presented above has a limitation. It does not con- sider any lexical information regarding the enti- ties, thus, cannot distinguish between transitions of different entities. Although the extended neural grid model proposed in <ref type="bibr" target="#b21">(Nguyen and Joty, 2017)</ref> does incorporate entity features like named entity type and proper mention, it requires an explicit feature extraction step using tools like named en- tity recognizer. This can prevent us in transferring the model to resource-poor languages or domains.</p><p>To address this limitation, we propose to lexi- calize entity transitions. This can be achieved by attaching the entity with the grammatical roles. For example, if an entity e j appears as a sub- ject (S) in sentence s i , the grid entry E i,j will be encoded as e j -S. This way, an entity OBAMA as subject (OBAMA-S) and as object (OBAMA-O) will have separate entries in the embedding ma- trix M . We can initialize the word-role embed- dings randomly, or with pre-trained embeddings for the word (OBAMA). In another variation, we kept word and role embeddings separate and con-Author: barspinboy Post ID: 1 s0: im having troubles since i uninstall some of my apps, then when i checked my system registry bunch of junks were left behind by the apps i already uninstall. s1: is there any way i could clean my registry aside from expensive registry cleaners.</p><p>Author: kees bakker Post ID: 2 s2: use regedit to delete the 'bunch of junks' you found in registry. s3: regedit is free, but depending on which applications it were .. s4: it's somewhat doubtful there will be less crashes and faster setup.</p><p>Author catenated them after the look-up, thus enforcing OBAMA-S and OBAMA-O to share a part of their representations. However, in our experiments, we found the former approach to be more effective.</p><note type="other">: willy Post ID: 3 s5: i tend to use ccleaner (google for it) as a registry cleaner. s6: using its defaults does pretty well. s7: in no way will it cure any hardcore problems as you mentioned. s8: i further suggest, .. Author: caktus Post ID: 4 s9: try regseeker to clean your registry junk. s10: it's free and pretty safe to use automatic. s11: then clean temp files (don't compress any files or use indexing.) s12: if the c drive is compressed, then uncompress it. Author: barspinboy Post ID: 5 s13: thanks guyz, my registry is clean now s14: i tried all those suggestions you mentioned ccleaners regedit de- fragmentation and uninstalling process; it all worked out (a) A forum conversation p1 s0 s1 p2 s2 s3 s4 p3 s5 s6 s7 s8 p4 s9 s10 s11 s12 p5 s13 s14</note><formula xml:id="formula_6">(b) Conversational tree p1 O O p2 O - - p3 O - - - p4 O - - - p5 S - (c) Role transition for 'registry' registry P2 P1 P0 l0 O O O l1 O O O l2 O O O l3 - - - l4 - - - l5 - - φ l6 S φ φ l7 - φ φ (d) Grid representations</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Coherence Models for Asynchronous Conversations</head><p>The main difference between monologue and asynchronous conversation is that information flow in asynchronous conversation is not sequen- tial as in monologue, rather it is often interleaved. For example, consider the forum conversation in <ref type="figure" target="#fig_1">Figure 2</ref>(a). There are three possible subconver- sations, each corresponding to a path from the root node to a leaf node in the conversation graph in <ref type="figure" target="#fig_1">Figure 2</ref>(b). In response to seeking sugges- tions about how to clean system registry, the first path (p 1 ←p 2 ) suggests to use regedit, the second path (p 1 ←p 3 ) suggests ccleaner, and the third one (p 1 ←p 4 ) suggests using regseeker. These discus- sions are interleaved in the chronological order of the posts (p 1 ←p 2 ←p 3 ←p 4 ←p 5 ). Therefore, monologue-based coherence models may not be effective if applied directly to the conversation. We hypothesize that coherence models for asyn- chronous conversation should incorporate the con- versational structure like the tree structure in <ref type="figure" target="#fig_1">Fig- ure 2(b)</ref>, where the nodes represent posts and the edges represent 'reply-to' links between them.</p><p>Since the grid models operate at the sentence level, we construct conversational structure at the sen- tence level. We do this by linking the boundary sentences across posts and by linking sentences in the same post chronologically. Specifically, we connect the first sentence of post p j to the last sen- tence of post p i if p j replies to p i , and sentence s t+1 is linked to s t if both s t and s t+1 are in the same post. 1 Now the question is, how can we rep- resent a conversation tree with an entity grid, and then model entity transitions in the tree? In the fol- lowing, we describe our approach to this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Conversational Entity Grid</head><p>The conversation tree captures how topics flow in an asynchronous conversation. Our key hypothe- sis is that in a coherent conversation entities ex- hibit certain local patterns in the conversation tree in terms of their distribution and syntactic real- ization. <ref type="figure" target="#fig_1">Figure 2(c)</ref> shows how the grammatical roles of entity 'registry' in our example conversa- tion change over the tree. For coherence assess- ment, we wish to model entity transitions along each of the conversation paths (top-to-bottom), and also their spatial relations across the paths (left-to-right). The existing grid representation is insufficient to model the two-dimensional (2D) spatial entity transitions in a conversation tree.</p><p>We propose a three-dimensional (3D) grid for representing entity transitions in an asynchronous conversation. The first dimension in our grid rep- <ref type="bibr">1</ref> The links between sentences are not explicitly shown in  resents entities, while the second and third dimen- sions represent depth and path of the tree, respec- tively. <ref type="figure" target="#fig_1">Figure 2(d)</ref> shows an example representa- tion for an entity 'registry'. Each column in the matrix represents transitions of the entity along a path, whereas each row represents transitions of the entity at a level of the conversation tree.</p><p>Although illustrated with a tree structure, our method is applicable to general graph-structured conversations, where a post can reply to multiple previous posts. Our model relies on paths from the root to the leaf nodes, which can be extracted for any graph as long as we avoid loops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Modeling Entity Transitions</head><p>As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, given a 3D entity grid as input, the look-up layer (Eq. 1) of our neural grid model produces a 4D tensor L∈R I×J×P ×d , where I is the total number of entities in the conversation, J is the depth of the tree, P is the number of paths in the tree, and d is the embedding dimension. The convolution layer then uses a 2D filter w ∈ R m.n.d to convolve local patches of entity transitions</p><formula xml:id="formula_7">z i = h(w T L i,j:j+m,p:p+n + b i )<label>(6)</label></formula><p>where m and n are the height and width of the filter, and L i,j:j+m,p:p+n ∈ R m.n.d denotes a con- catenated vector containing (m × n) embeddings representing a 2D window of entity transitions. As we repeatedly apply the filter to each possible win- dow with stride size 1, we get a 2D feature map Z i of dimensions (I.J +m−1)×(I.P +n−1). Em- ploying N different filters, we get N such 2D fea- ture maps, [Z 1 , · · · , Z N ], based on which the max pooling layer extracts the most salient features:</p><formula xml:id="formula_8">p = [µ l×w (Z 1 ), · · · , µ l×w (Z N )]<label>(7)</label></formula><p>where µ l×w refers to the max operation applied to each non-overlapping 2D window of l×w features in a feature map. The pooled features are then lin- earized and used for coherence scoring in the final layer of the network as described by Equation 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments on Monologue</head><p>To validate our proposed extension to the neural grid model, we first evaluate our lexicalized neural grid model in the standard evaluation setting.</p><p>Evaluation Tasks and Dataset: We evaluate our models on the standard discrimination task ( <ref type="bibr" target="#b1">Barzilay and Lapata, 2008)</ref>, where a coherence model is asked to distinguish an original docu- ment from its incoherent renderings generated by random permutations of its sentences. The model is considered correct if it ranks the original docu- ment higher than the permuted one. We use the same train-test split of the WSJ dataset as used in <ref type="bibr" target="#b21">(Nguyen and Joty, 2017</ref>) and other studies <ref type="bibr" target="#b4">(Elsner and Charniak, 2011b;</ref><ref type="bibr" target="#b6">Feng et al., 2014)</ref>. Following previous studies, we use 20 random permutations of each article for both training and testing, and exclude permutations that match the original article. <ref type="table" target="#tab_2">Table 2</ref> gives some statistics about the dataset along with the number of pairs used for training and testing. <ref type="bibr" target="#b21">Nguyen and Joty (2017)</ref> randomly selected 10% of the training pairs for development purposes, which we also use for tuning hyperparameters in our models.</p><p>In addition to the standard setting, we also eval- uate our models on an inverse-order setting, where we ask the models to distinguish an original doc- ument from the inverse order of its sentences (i.e., from last to first). The transitions of roles in a neg- ative grid are in the reverse order of the original grid. We do not train our models explicitly on this task, rather use the trained model from the stan- dard setting.   <ref type="bibr" target="#b11">and Szegedy, 2015)</ref>, which gave better re- sults than using dropout. Minibatch size, embed- ding size and filter number were fixed to 32, 300 and 150, respectively. We tuned for optimal filter and pooling lengths in {2, · · · , 12}. We train up to 25 epochs, and select the model that performs best on the development set; see supplementary doc- uments for best hyperparameter settings for differ- ent models. We run each experiment five times, each time with a different random seed, and we report the average of the runs to avoid any ran- domness in results. Statistical significance tests are done using an approximate randomization test with SIGF V.2 <ref type="bibr" target="#b22">(Padó, 2006</ref>).</p><p>Results and Discussions: We present our re- sults on the standard discrimination task and the inverse-order task in <ref type="table" target="#tab_4">Table 3</ref>; see Std (F 1 ) and Inv (F 1 ) columns, respectively. For space limi- tations, we only show F 1 scores here, and report both accuracy and F 1 in the supplementary docu- ment. We compare our lexicalized models (group III) with the unlexicalized models (group II) of Nguyen and Joty (2017). <ref type="bibr">3</ref> We also report the re- sults of non-neural entity grid models <ref type="bibr" target="#b4">(Elsner and Charniak, 2011b</ref>) in group I. The extended ver- sions use entity-specific features.</p><p>We experimented with both random and pre- trained initialization for word embeddings in our lexicalized models. As can be noticed in Ta- ble 3, both versions give significant improvements over the unlexicalized models on both the standard and the inverse-order discrimination tasks (2.7 - 4.3% absolute). Our best model with Google pre- trained embeddings ( <ref type="bibr" target="#b20">Mikolov et al., 2013</ref>) yields state-of-the-art results. We also experimented  with Glove ( <ref type="bibr" target="#b23">Pennington et al., 2014)</ref>, which has more vocabulary coverage than word2vec -Glove covers 89.77% of our vocabulary items, whereas word2vec covers 85.66%. However, Glove did not perform well giving F 1 score of 86% in the stan- dard discrimination task. <ref type="bibr" target="#b25">Schnabel et al. (2015)</ref> also report similar results where word2vec was found to be superior to Glove in most evaluation tasks. Our model also outperforms the extended neural grid model that relies on an additional fea- ture extraction step for entity features. These re- sults demonstrate the efficacy of lexicalization in capturing fine-grained entity information without loosing generalizability, thanks to distributed rep- resentation and pre-trained embeddings.</p><note type="other">Model Emb. Std (F 1 ) Inv (F 1 ) I Grid (E&amp;C</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments on Conversation</head><p>We evaluate our coherence models for asyn- chronous conversations on two tasks: discrimina- tion and thread reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation on Discrimination</head><p>The discrimination tasks are applicable to conver- sations also. We first present the dataset we use, then we describe how we create coherent and in- coherent examples to train and test our models.</p><p>Dataset: Our conversational corpus contains discussion threads regarding computer trou- bleshooting from the technology related news site CNET. <ref type="bibr">4</ref> This corpus was originally collected by <ref type="bibr" target="#b17">Louis and Cohen (2015)</ref>, and it contains 13,352 threads. For our experiments, we selected 3,825 threads assuring that each contains at least 3 and at most 15 posts. We use 2,400 threads for train- ing, 750 for testing and 675 for development pur- poses.  Model Settings and Training: To validate the efficacy of our conversational grid model, we com- pare it with the following baseline settings:</p><p>• Temporal: In the temporal setting, we con- struct an entity grid from the chronological order of the sentences in a conversation, and use it with our monologue-based coherence models. Models in this setting thus disregard the structure of the conversation and treat it as a monologue.</p><p>• Path-level: This is a special case of our model, where we consider each path (a column in our conversational grid) in the conversation tree sep- arately. We construct an entity grid for a path and provide as input to our monologue-based models.</p><p>To train the models with pairwise ranking, we create 20 incoherent conversations for each origi- nal conversation by shuffling the sentences in their temporal order. For models involving conversation trees (path-level and our model), the tree struc- ture remains unchanged for original and permuted conversations, only the position of the sentences vary based on the permutation. Since the shuf- fling is done globally at the conversation level, this scheme allows us to compare the three represen- tations (temporal, path-level and tree-level) fairly with the same set of permutations.</p><p>An incoherent conversation may have paths in the tree that match the original paths. We remove those matched paths when training the path-level model. See <ref type="table" target="#tab_5">Table 4</ref> for number of pairs used for training and testing our models. We evaluate path- level models by aggregating correct/wrong deci- sions for the paths -if the model makes more cor- rect decisions for the original conversation than the incoherent one, it is counted as a correct de- cision overall. Aggregating path-level coherence scores (e.g., by averaging or summing) would al- low a coherence model to get awarded for as- signing higher score to an original path (hence, correct) while making wrong decisions for the rest; see supplementary document for an example. Similar to the setting in Monologue, we did not train explicitly on the inverse-order task, rather use the trained model from the standard setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv. Rep Model</head><p>Emb.  Results and Discussions: <ref type="table" target="#tab_8">Table 5</ref> compares the results of our models on the two discrimination tasks. We observe more gains in conversation than in monologue for the lexicalized models -4.9% to 7.3% on the standard task, and 10% to 13.6% on the inverse-order task. Notice especially the huge gains on the inverse-order task. This indicates lex- icalization helps to better adapt to new domains.</p><formula xml:id="formula_9">Std (F 1 ) Inv (F 1 ) Temporal Neural Grid (N&amp;J)</formula><p>A comparison of the results on the standard task across the representations shows that path-level models perform on par with the temporal models, whereas the tree-level models outperform others by a significant margin. The improvements are 2.7% for randomly initialized word vectors and 4% for Google embeddings. Although, the path- level model considers some conversational struc- tures, it observes only a portion of the conversation in its input. The common topics (expressed by en- tities) of a conversation get distributed across mul- tiple conversational paths. This limits the path- level model to learn complex relationships be- tween entities in a conversation. By encoding an entire conversation into a single grid and by mod- eling the spatial relations between the entities, our conversational grid model captures both local and global information (topic) of a conversation.</p><p>Interestingly, the improvements are higher on the inverse-order task for both path-and tree-level models. The inverse order yields more dissimilar- ity at the paths with respect to the original order, thus making them easier to distinguish.</p><p>If we notice the hyperparameter settings for the best models on this task (see supplementary docu- ment), we see they use a filter width of 1. This in- dicates that to find the right order of the sentences in conversations, it is sufficient to consider entity transitions along the conversational paths in a tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation on Thread Reconstruction</head><p>One crucial advantage of our tree-level model over other models is that we can use it to build pre- dictive models to uncover the thread structure of a conversation from its posts. Consider again the thread in <ref type="figure" target="#fig_1">Figure 2</ref>. Our goal is to train a coherence model that can recover the tree structure in <ref type="figure" target="#fig_1">Figure  2</ref>(b) from the sequence of posts (p 1 , p 2 , . . . , p 5 ).</p><p>This task has been addressed previously ( <ref type="bibr" target="#b28">Wang et al., 2008</ref><ref type="bibr" target="#b27">Wang et al., , 2011</ref>). Most methods learn an edge- level classifier to decide for a possible link be- tween two posts using features like distance in po- sition/time, cosine similarity, etc. To our knowl- edge, we are the first to use coherence models for this problem. However, our goal in this paper is not to build a state-of-the-art system for thread re- construction, rather to evaluate coherence models by showing its effectiveness in scoring candidate tree hypotheses. In contrast to previous methods, our approach therefore considers the whole thread structure at once, and computes coherence scores for all possible candidate trees of a conversation. The tree that receives the highest score is predicted as the thread structure of the conversation.</p><p>Training: We train our coherence model for thread reconstruction using pairwise ranking loss as before. For a given sequence of comments in a thread, we construct a set of valid candidate trees; a valid tree is one that respects the chronological order of the comments, i.e., a comment can only reply to a comment that precedes it. The training set contains ordered pairs (T i , T j ), where T i is a true (gold) tree and T j is a valid but false tree.</p><p>Experiments: The number of valid trees grows exponentially with the number of posts in a thread, which makes the inference difficult. As a proof of concept that coherence models are useful for find- ing the right tree, we built a simpler dataset by se- lecting forum threads from the CNET corpus en- suring that a thread contains at most 5 posts. The final dataset contains 1200 threads with an average of 3.8 posts and 27.64 sentences per thread.</p><p>We assess the performance of the models at two levels: (i) thread-level, where we evaluate if the model could identify the entire conversa- tion thread correctly, and (ii) edge-level, where we evaluate if the model could identify individual replies correctly. For comparison, we use a num- ber of simple but well performing baselines:</p><p>• All-previous creates thread structure by linking  a comment to its previous (in time) comment.</p><p>• All-first creates thread structure by linking all the comments to the initial comment.</p><p>• COS-sim creates thread structure by linking a comment to one of the previous comments with which it has the highest cosine similarity. We use TF.IDF representation for the comments. <ref type="table" target="#tab_10">Table 6</ref> compares our best conversational grid model (tree-level with Google vectors) with the baselines. The low thread-level accuracy across all the systems prove that reconstructing an en- tire tree is a difficult task. Models are reasonably accurate at the edge level. Our coherence model shows promising results, yielding substantial im- provements over the baselines. It delivers 2.7% improvements in thread-level and 2.5% in edge- level accuracy over the best baseline (COS-sim).</p><p>Interestingly, our best model for this task uses a filter width of 2 (maximum can be 4 for 5 posts). This indicates that spatial (left-to-right) relations between entity transitions are important to find the right thread structure of a conversation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented a coherence model for asynchronous conversations. We first extended the existing neu- ral grid model by lexicalizing its entity transitions. We then adapt the model to conversational dis- course by incorporating the thread structure in its grid representation and feature computation. We designed a 3D grid representation for capturing spatio-temporal entity transitions in a conversation tree, and employed a 2D convolution to compose high-level features from this representation.</p><p>Our lexicalized grid model yields state of the art results on standard coherence assessment tasks in monologue and conversations. We also show a novel application of our model in forum thread reconstruction. Our future goal is to use the coher- ence model to generate new conversations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>s0</head><label></label><figDesc>− O − − S X − − − − X X − − X − s1 − − O − − X X − − S − − X − − − s2 S − − − X S − − X − − X − − S X s3 − − − O − − − X − − − − − X S −</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) A forum conversation, (b) Thread structure of the conversation, (c) Entity role transition over a conversation tree, and (d) 2D role transition matrix for an entity; φ denotes zero-padding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 (</head><label>2</label><figDesc>b) to avoid visual clutter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Conversational Neural Grid model for assessing coherence in asynchronous conversations.</figDesc><graphic url="image-2.png" coords="6,115.99,62.81,362.82,93.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Entity grid representation (bottom) for a 
document (top) from the WSJ corpus. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>The number of test pairs in this set- ting is same as the number of test documents. Model Settings and Training: We train the neural models with the pairwise ranking loss in Equation 5. For a fair comparison, we use</figDesc><table>Sections # Doc. Avg. # Sen. # Pairs 

Train 
00-13 
1,378 
21.5 
26,422 
Test 
14-24 
1,053 
22.3 
20,411 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Statistics on the WSJ dataset.</head><label>2</label><figDesc></figDesc><table>similar model settings as in (Nguyen and Joty, 
2017) 2 -ReLU as activation functions (h), RM-
Sprop (Tieleman and Hinton, 2012) as the learn-
ing algorithm, Glorot-uniform (Glorot and Ben-
gio, 2010) for initializing weight matrices, and 
uniform U (−0.01, 0.01) for initializing embed-
dings randomly. We applied batch normalization 
(Ioffe </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Discrimination results on the WSJ 
dataset. Superscript  † indicates a lexicalized 
model is significantly superior to the unlexicalized 
Neural Grid (N&amp;J) model with p-value &lt; 0.01. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 shows</head><label>4</label><figDesc></figDesc><table>some basic statistics about 
the resulting dataset. The threads roughly contain 
29 sentences and 6 comments on average. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 : Statistics on the CNET dataset.</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Discrimination results on CNET. Super-
script  † indicates a model is significantly superior 
to its temporal counterpart with p-value &lt; 0.01. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Thread reconstruction results;  † indicates 
significant difference from COS-sim (p&lt; .01). 

</table></figure>

			<note place="foot" n="2"> https://ntunlpsg.github.io/project/coherence/n-coh-acl17 3 Our reproduced results for the neural grid model are slightly lower than their reported results (∼ 1%). We suspect this is due to the randomness in the experimental setup.</note>

			<note place="foot" n="4"> https://www.cnet.com/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Threadlevel information for comment classification in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Barron-Cedeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, ACL&apos;15</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, ACL&apos;15<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="687" to="693" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling local coherence: An entity-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Summarizing emails with conversational cohesion and subjectivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46nd Annual Meeting on Association for Computational Linguistics, ACL&apos;08</title>
		<meeting>the 46nd Annual Meeting on Association for Computational Linguistics, ACL&apos;08</meeting>
		<imprint>
			<publisher>OH. ACL</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="353" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Disentangling chat with local coherence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1179" to="1189" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extending the entity grid with entity-specific features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="125" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extending the entity-based coherence model with multiple ranks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Vanessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL &apos;12</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL &apos;12<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="315" to="324" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The impact of deep hierarchical discourse structures in the evaluation of text coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Wei Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR W&amp;CP: Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2010)</title>
		<meeting><address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Centering: A framework for modeling the local coherence of discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">J</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="225" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hidden topic markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Rosen-Zvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Eleventh International Conference on Artificial Intelligence and Statistics<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="163" to="170" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph-based local coherence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Guinaudeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August 2013</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August 2013<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="93" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>JMLR.org</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;02</title>
		<meeting>the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;02<address><addrLine>Edmonton, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Topic segmentation and labeling in asynchronous conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="521" to="573" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A model of coherence based on distributed sentence representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2039" to="2048" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural net models of open-domain discourse coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="198" to="209" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatically evaluating text coherence using discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="997" to="1006" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Conversation trees: A grammar model for topic structure in forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A coherence model based on syntactic patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1157" to="1168" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rhetorical structure theory: Toward a functional theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A neural local coherence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1320" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">User&apos;s guide to sigf: Significance testing by approximate randomisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The penn discourse treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)</title>
		<meeting>the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluation methods for unsupervised word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="298" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">RMSprop. COURSERA: Neural Networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Predicting thread discourse structure over technical web forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="13" to="25" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recovering implicit thread structure in newsgroup style conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><surname>Ros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Web and Social Media</title>
		<meeting>the Eleventh International Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>ICWSM</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
