<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Language Games through Interaction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Language Games through Interaction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2368" to="2378"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a new language learning setting relevant to building adaptive natural language interfaces. It is inspired by Wittgenstein&apos;s language games: a human wishes to accomplish some task (e.g., achieving a certain configuration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks). The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer&apos;s capabilities. We created a game called SHRDLURN in a blocks world and collected interactions from 100 people playing it. First, we analyze the humans&apos; strategies, showing that using compositionality and avoiding synonyms correlates positively with task performance. Second, we compare computer strategies, showing that modeling prag-matics on a semantic parsing model accelerates learning for more strategic players.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Wittgenstein (1953) famously said that language derives its meaning from use, and introduced the concept of language games to illustrate the fluid- ity and purpose-orientedness of language. He de- scribed how a builder B and an assistant A can use a primitive language consisting of four words- 'block', 'pillar', 'slab', 'beam'-to successfully communicate what block to pass from A to B. This is only one such language; many others would also work for accomplishing the cooperative goal.</p><p>This paper operationalizes and explores the idea of language games in a learning setting, which we call interactive learning through language games The human types in an utterance, and the computer (which does not know the goal state) tries to in- terpret the utterance and perform the correspond- ing action. The computer initially knows nothing about the language, but through the human's feed- back, learns the human's language while making progress towards the game goal.</p><p>(ILLG). In the ILLG setting, the two parties do not initially speak a common language, but nonethe- less need to collaboratively accomplish a goal. Specifically, we created a game called SHRD- LURN, 1 in homage to the seminal work of <ref type="bibr" target="#b28">Winograd (1972)</ref>. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the objective is to transform a start state into a goal state, but the only action the human can take is entering an utterance. The computer parses the utterance and produces a ranked list of possible interpretations according to its current model. The human scrolls through the list and chooses the intended one, si- multaneously advancing the state of the blocks and providing feedback to the computer. Both the hu- man and the computer wish to reach the goal state (only known to the human) with as little scrolling as possible. For the computer to be successful, it has to learn the human's language quickly over the course of the game, so that the human can accom- plish the goal more efficiently. Conversely, the hu- man must also accommodate the computer, at least partially understanding what it can and cannot do.</p><p>We model the computer in the ILLG as a se- mantic parser (Section 3), which maps natural lan- guage utterances (e.g., 'remove red') into logical forms (e.g., remove(with(red))). The seman- tic parser has no seed lexicon and no annotated logical forms, so it just generates many candidate logical forms. Based on the human's feedback, it performs online gradient updates on the parame- ters corresponding to simple lexical features.</p><p>During development, it became evident that while the computer was eventually able to learn the language, it was learning less quickly than one might hope. For example, after learning that 'remove red' maps to remove(with(red)), it would think that 'remove cyan' also mapped to remove(with(red)), whereas a human would likely use mutual exclusivity to rule out that hypothesis <ref type="bibr" target="#b18">(Markman and Wachtel, 1988)</ref>. We therefore introduce a pragmatics model in which the computer explicitly reasons about the human, in the spirit of previous work on pragmatics <ref type="bibr" target="#b10">(Golland et al., 2010;</ref><ref type="bibr" target="#b8">Frank and Goodman, 2012;</ref><ref type="bibr" target="#b23">Smith et al., 2013)</ref>. To make the model suitable for our ILLG setting, we introduce a new online learning algorithm. Empirically, we show that our pragmatic model improves the online accuracy by 8% compared to our best non-pragmatic model on the 10 most successful players (Section 5.3).</p><p>What is special about the ILLG setting is the real-time nature of learning, in which the human also learns and adapts to the computer. While the human can teach the computer any language- English, Arabic, Polish, a custom programming language-a good human player will choose to use utterances that the computer is more likely to learn quickly. In the parlance of communication theory, the human accommodates the computer <ref type="bibr" target="#b9">(Giles, 2008;</ref><ref type="bibr" target="#b13">Ireland et al., 2011</ref>). Using Ama- zon Mechanical Turk, we collected and analyzed around 10k utterances from 100 games of SHRD- LURN. We show that successful players tend to use compositional utterances with a consistent vo- cabulary and syntax, which matches the inductive biases of the computer (Section 5.2). In addition, through this interaction, many players adapt to the computer by becoming more consistent, more pre- cise, and more concise.</p><p>On the practical side, natural language systems are often trained once and deployed, and users must live with their imperfections. We believe that studying the ILLG setting will be integral for creating adaptive and customizable systems, es- pecially for resource-poor languages and new do- mains where starting from close to scratch is un- avoidable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Setting</head><p>We now describe the interactive learning of lan- guage games (ILLG) setting formally. There are two players, the human and the computer. The game proceeds through a fixed number of levels. In each level, both players are presented with a starting state s ∈ Y, but only the human sees the goal state t ∈ Y. (e.g. in SHRDLURN, Y is the set of all configurations of blocks). The human transmits an utterance x (e.g., 'remove red') to the computer. The computer then con- structs a ranked list of candidate actions Z = [z 1 , . . . , z K ] ⊆ Z (e.g., remove(with(red)), add(with(orange)), etc.), where Z is all possible actions. For each z i ∈ Z, it computes y i = z i s , the successor state from executing ac- tion z i on state s. The computer returns to the hu- man the ordered list Y = [y 1 , . . . , y K ] of succes- sor states. The human then chooses y i from the list Y (we say the computer is correct if i = 1). The state then updates to s = y i . The level ends when s = t, and the players advance to the next level.</p><p>Since only the human knows the goal state t and only the computer can perform actions, the only way for the two to play the game successfully is for the human to somehow encode the desired ac- tion in the utterance x. However, we assume the two players do not have a shared language, so the human needs to pick a language and teach it to the computer. As an additional twist, the human does not know the exact set of actions Z (although they might have some preconception of the computer's capabilities). <ref type="bibr">2</ref> Finally, the human only sees the outcomes of the computer's actions, not the actual logical actions themselves.</p><p>We expect the game to proceed as follows: In the beginning, the computer does not understand what the human is saying and performs arbitrary actions. As the computer obtains feedback and learns, the two should become more proficient at communicating and thus playing the game. Herein lies our key design principle: language learning should be necessary for the players to achieve good game performance.</p><p>SHRDLURN. Let us now describe the details of our specific game, SHRDLURN. Each state s ∈ Y consists of stacks of colored blocks ar- ranged in a line <ref type="figure" target="#fig_0">(Figure 1)</ref>, where each stack is a vertical column of blocks. The actions Z are defined compositionally via the gram- mar in <ref type="table">Table 1</ref>. Each action either adds to or removes from a set of stacks, and a set of stacks is computed via various set operations and selecting by color. For example, the action remove(leftmost(with(red))) removes the top block from the leftmost stack whose topmost block is red. The compositionality of the actions gives the computer non-trivial capabilities. Of course, the human must teach a language to har- ness those capabilities, while not quite knowing the exact extent of the capabilities. The actual game proceeds according to a curriculum, where the earlier levels only need simpler actions with fewer predicates.</p><p>We designed SHRDLURN in this way for sev- eral reasons. First, visual block manipulations are intuitive and can be easily crowdsourced, and it can be fun as an actual game that people would play. Second, the action space is designed to be compositional, mirroring the structure of natural language. Third, many actions z lead to the same successor state y = z s ; e.g., the 'leftmost stack' might coincide with the 'stack with red blocks' for some state s and therefore an action involving ei- ther one would result in the same outcome. Since the human only points out the correct y, the com- puter must grapple with this indirect supervision, a reflection of real language learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Semantic parsing model</head><p>Following <ref type="bibr" target="#b31">Zettlemoyer and Collins (2005)</ref> and most recent work on semantic parsing, we use a log-linear model over logical forms (actions) z ∈ Z given an utterance x:</p><formula xml:id="formula_0">p θ (z | x) ∝ exp(θ T φ(x, z)),<label>(1)</label></formula><p>where φ(x, z) ∈ R d is a feature vector and θ ∈ R d is a parameter vector. The denotation y (succes- sor state) is obtained by executing z on a state s; formally, y = z s .</p><p>Features. Our features are n-grams (including skip-grams) conjoined with tree-grams on the log- ical form side. Specifically, on the utterance side (e.g., 'stack red on orange'), we use uni- grams ('stack', * , * ), bigrams ('red', 'on', * ), tri- grams ('red', 'on', 'orange'), and skip-trigrams ('stack', * , 'on'). On the logical form side, fea- tures corresponds to the predicates in the logical forms and their arguments. For each predicate h, let h.i be the i-th argument of h. Then, we de- fine tree-gram features ψ(h, d) for predicate h and depth d = 0, 1, 2, 3 recursively as follows:</p><formula xml:id="formula_1">ψ(h, 0) = {h}, ψ(h, d) = {(h, i, ψ(h.i, d − 1)) | i = 1, 2, 3}.</formula><p>The set of all features is just the cross product of utterance features and logical form features. For example, if x = 'enlever tout' and z = remove(all()), then features include:</p><formula xml:id="formula_2">('enlever', all) ('tout', all) ('enlever', remove) ('tout', remove) ('enlever', (remove, 1, all)) ('tout', (remove, 1, all))</formula><p>Note that we do not model an explicit alignment or derivation compositionally connecting the utter- ance and the logical form, in contrast to most tradi- tional work in semantic parsing <ref type="bibr" target="#b31">(Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b30">Wong and Mooney, 2007;</ref><ref type="bibr" target="#b17">Liang et al., 2011;</ref><ref type="bibr" target="#b16">Kwiatkowski et al., 2010;</ref><ref type="bibr" target="#b1">Berant et al., 2013)</ref>, instead following a looser model of semantics similar to <ref type="bibr" target="#b21">(Pasupat and Liang, 2015)</ref>. Modeling explicit alignments or derivations is only computationally feasible when we are learn- ing from annotated logical forms or have a seed lexicon, since the number of derivations is much larger than the number of logical forms. In the ILLG setting, neither are available.</p><p>Generation/parsing. We generate logical forms from smallest to largest using beam search. Specifically, for each size n = 1, . . . , 8, we con- struct a set of logical forms of size n (with ex- actly n predicates) by combining logical forms of smaller sizes according to the grammar rules in Ta- ble 1. For each n, we keep the 100 logical forms z with the highest score θ T φ(x, z) according to the current model θ. Let Z be the set of logical forms on the final beam, which contains logical forms of all sizes n. During training, due to pruning at</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rule</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantics Description</head><p>Set all() all stacks Color cyan|brown|red|orange primitive color Color → Set with(c) stacks whose top block has color c Set → Set not(s) all stacks except those in s</p><formula xml:id="formula_3">Set → Set leftmost|rightmost(s) leftmost/rightmost stack in s Set Color → Act add(s, c)</formula><p>add block with color c on each stack in s Set → Act remove(s) remove the topmost block of each stack in s <ref type="table">Table 1</ref>: The formal grammar defining the compositional action space Z for SHRDLURN. We use c to denote a Color, and s to denote a Set.</p><p>For example, one action that we have in SHRDLURN is: 'add an orange block to all but the leftmost brown block' → add(not(leftmost(with(brown))),orange).</p><p>intermediate sizes, Z is not guaranteed to contain the logical form that obtains the observed state y.</p><p>To mitigate this effect, we use a curriculum so that only simple actions are needed in the initial levels, giving the human an opportunity to teach the com- puter about basic terms such as colors first before moving to larger composite actions.</p><p>The system executes all of the logical forms on the final beam Z, and orders the resulting denota- tions y by the maximum probability of any logical form that produced it. 3</p><p>Learning. When the human provides feedback in the form of a particular y, the system forms the following loss function:</p><formula xml:id="formula_4">(θ, x, y) = − log p θ (y | x, s) + λ||θ|| 1 , (2) p θ (y | x, s) = z:zs=y p θ (z | x).<label>(3)</label></formula><p>Then it makes a single gradient update using Ada- Grad ( <ref type="bibr" target="#b7">Duchi et al., 2010)</ref>, which maintains a per- feature step size.</p><p>z rm-red = remove(with(red)) as the cor- rect logical form. The computer then performs a gradient update on the loss function (2), up- weighting features such as ('remove', remove) and ('remove', red).</p><p>Next, suppose the human utters 'remove cyan'. Note that z rm-red will score higher than all other formulas since the ('remove', red) feature will fire again. While statistically justified, this be- havior fails to meet our intuitive expectations for a smart language learner. Moreover, this behav- ior is not specific to our model, but applies to any statistical model that simply tries to fit the data without additional prior knowledge about the spe- cific language. While we would not expect the computer to magically guess 'remove cyan' → remove(with(cyan)), it should at least push down the probability of z rm-red because z rm-red intuitively is already well-explained by another ut- terance 'remove red'.</p><p>This phenomenon, mutual exclusivity, was stud- ied by <ref type="bibr" target="#b18">Markman and Wachtel (1988)</ref>. They found that children, during their language acquisition process, reject a second label for an object and treat it instead as a label for a novel object.</p><p>The pragmatic computer. To model mutual ex- clusivity formally, we turn to probabilistic mod- els of pragmatics ( <ref type="bibr" target="#b10">Golland et al., 2010;</ref><ref type="bibr" target="#b8">Frank and Goodman, 2012;</ref><ref type="bibr" target="#b23">Smith et al., 2013;</ref><ref type="bibr" target="#b11">Goodman and Lassiter, 2015)</ref>, which operationalize the ideas of <ref type="bibr" target="#b12">Grice (1975)</ref>. The central idea in these models is to treat language as a cooperative game between a speaker (human) and a listener (computer) as we are doing, but where the listener has an ex- plicit model of the speaker's strategy, which in turn models the listener. Formally, let S(x | z) be the speaker's strategy and L(z | x) be the listener's <ref type="table">Table 2</ref>: Suppose the computer saw one exam- ple of 'remove red' →z rm-red , and then the hu- man utters 'remove cyan'. top: the literal lis- tener, p θ (z | x), mistakingly chooses z rm-red over z rm-cyan . middle: the pragmatic speaker, S(x | z), assigns a higher probability to to 'remove cyan' given z rm-cyan ; bottom: the pragmatic lis- tener, L(z | x) correctly assigns a lower probabil- ity to z rm-red where p(z) is uniform.</p><formula xml:id="formula_5">z rm-red z rm-cyan z 3 , z 4 , . . . p θ (z | x) 'remove red' 0.8 0.1 0.1 'remove cyan' 0.6 0.2 0.2 S(x | z) 'remove red' 0.57 0.33 0.33 'remove cyan' 0.43 0.67 0.67 L(z | x) 'remove red' 0.46 0.27 0.27 'remove cyan' 0.24 0.38 0.38</formula><p>strategy. The speaker takes into account the literal semantic parsing model p θ (z | x) as well as a prior over utterances p(x), while the listener considers the speaker S(x | z) and a prior p(z):</p><formula xml:id="formula_6">S(x | z) ∝ (p θ (z | x)p(x)) β ,<label>(4)</label></formula><formula xml:id="formula_7">L(z | x) ∝ S(x | z)p(z),<label>(5)</label></formula><p>where β ≥ 1 is a hyperparameter that sharpens the distribution ( <ref type="bibr" target="#b23">Smith et al., 2013</ref>). The com- puter would then use L(z | x) to rank candidates rather than p θ . Note that our pragmatic model only affects the ranking of actions returned to the hu- man and does not affect the gradient updates of the model p θ . Let us walk through a simple example to see the effect of modeling pragmatics. <ref type="table">Table 2</ref> shows that the literal listener p θ (z | x) assigns high probabil- ity to z rm-red for both 'remove red' and 'remove cyan'. Assuming a uniform p(x) and β = 1, the pragmatic speaker S(x | z) corresponds to normal- izing each column of p θ . Note that if the pragmatic speaker wanted to convey z rm-cyan , there is a de- cent chance that they would favor 'remove cyan'. Next, assuming a uniform p(z), the pragmatic lis- tener L(z | x) corresponds to normalizing each row of S(x | z). The result is that conditioned on 'remove cyan', z rm-cyan is now more likely than z rm-red , which is the desired effect.</p><p>The pragmatic listener models the speaker as a cooperative agent who behaves in a way to max- imize communicative success. Certain speaker behaviors such as avoiding synonyms (e.g., not 'delete cardinal') and using a consistent word or- dering (e.g, not 'red remove') fall out of the game theory. <ref type="bibr">4</ref> For speakers that do not follow this strat- egy, our pragmatic model is incorrect, but as we get more data through game play, the literal lis- tener p θ (z | x) will sharpen, so that the literal lis- tener and the pragmatic listener will coincide in the limit.</p><formula xml:id="formula_8">∀z, C(z) ← 0 ∀z, Q(z) ← repeat receive utterance x from human L(z | x) ∝ P (z) Q(z) p θ (z | x) β send human a list Y ranked by L(z | x) receive y ∈ Y from human θ ← θ − η θ (θ, x, y) Q(z) ← Q(z) + p θ (z | x) β C(z) ← C(z) + p θ (z | x, z s = y) P (z) ← C(z)+α z :C(z )&gt;0 C(z )+α</formula><p>until game ends Algorithm 1: Online learning algorithm that updates the parameters of the semantic parser θ as well as counts C, Q required to perform pragmatic reasoning.</p><p>Online learning with pragmatics. To imple- ment the pragmatic listener as defined in (5), we need to compute the speaker's normalization con- stant x p θ (z | x)p(x) in order to compute S(x | z) in (4). This requires parsing all utterances x based on p θ (z | x). To avoid this heavy computa- tion in an online setting, we propose Algorithm 1, where some approximations are used for the sake of efficiency. First, to approximate the intractable sum over all utterances x, we only use the exam- ples that are seen to compute the normalization constant</p><formula xml:id="formula_9">x p θ (z | x)p(x) ≈ i p θ (z | x i )</formula><p>. Then, in order to avoid parsing all previous exam- ples again using the current parameters for each new example, we store Q(z) = i p θ i (z | x i ) β , where θ i is the parameter after the model updates on the i th example x i . While θ i is different from the current parameter θ, p θ (z | x i ) ≈ p θ i (z | x i ) for the relevant example x i , which is accounted for by both θ i and θ.</p><p>In Algorithm 1, the pragmatic listener L(z | x) can be interpreted as an importance-weighted ver- sion of the sharpened literal listener p β θ , where it is downweighted by Q(z), which reflects which z's the literal listener prefers, and upweighted by P (z), which is just a smoothed estimate of the ac- tual distribution over logical forms p(z). By con- struction, Algorithm 1 is the same as (4) except that it uses the normalization constant Q based on stale parameters θ i after seeing example, and it uses samples to compute the sum over x. Follow- ing (5), we also need p(z), which is estimated by P (z) using add-α smoothing on the counts C(z). Note that Q(z) and C(z) are updated after the model parameters are updated for the current ex- ample.</p><p>Lastly, there is a small complication due to only observing the denotation y and not the logical form z. We simply give each consistent logical form {z | z s = y} a pseudocount based on the model:</p><formula xml:id="formula_10">C(z) ← C(z) + p θ (z | x, z s = y) where p θ (z | x, z s = y) ∝ exp(θ T φ(x, z)) for z s = y (0 otherwise).</formula><p>Compared to prior work where the setting is specifically designed to require pragmatic infer- ence, pragmatics arises naturally in ILLG. We think that this form of pragmatics is the most im- portant during learning, and becomes less impor- tant if we had more data. Indeed, if we have a lot of data and a small number of possible zs, then L(z|x) ≈ p θ (z|x) as x p θ (z|x)p(x) → p(z) when β = 1. 5 However, for semantic parsing, we would not be in this regime even if we have a large amount of training data. In particular, we are nowhere near that regime in SHRDLURN, and most of our utterances / logical forms are seen only once, and the importance of modeling pragmatics remains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setting</head><p>Data. Using Amazon Mechanical Turk (AMT), we paid 100 workers 3 dollars each to play SHRD- LURN. In total, we have 10223 utterances along with their starting states s. Of these, 8874 ut- terances are labeled with their denotations y; the rest are unlabeled, since the player can try any ut- terance without accepting an action. 100 players completed the entire game under identical settings. <ref type="bibr">5</ref> Technically, we also need p θ to be well-specified.</p><p>We deliberately chose to start from scratch for ev- ery worker, so that we can study the diversity of strategies that different people used in a controlled setting.</p><p>Each game consists of 50 blocks tasks divided into 5 levels of 10 tasks each, in increasing com- plexity. Each level aims to reach an end goal given a start state. Each game took on average 89 utterances to complete. 6 It only took 6 hours to complete these 100 games on AMT and each game took around an hour on average according to AMT's work time tracker (which does not account for multi-tasking players). The players were pro- vided minimal instructions on the game controls. Importantly, we gave no example utterances in or- der to avoid biasing their language use. Around 20 players were confused and told us that the in- structions were not clear and gave us mostly spam utterances. Fortunately, most players understood the setting and some even enjoyed SHRDLURN as reflected by their optional comments:</p><p>• That was probably the most fun thing I have ever done on mTurk.</p><p>• Wow this was one mind bending games <ref type="bibr">[sic]</ref>.</p><p>Metrics. We use the number of scrolls as a mea- sure of game performance for each player. For each example, the number of scrolls is the position in the list Y of the action selected by the player. It was possible to complete this version of SHRD- LURN by scrolling (all actions can be found in the first 125 of Y )-22 of the 100 players failed to teach an actual language, and instead finished the game mostly by scrolling. Let us call them spam players, who usually typed single letters, random words, digits, or random phrases (e.g. 'how are you'). Overall, spam players had to scroll a lot: 21.6 scrolls per utterance versus only 7.4 for the non-spam players.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Human strategies</head><p>Some example utterances can be found in <ref type="table">Table 3</ref>. Most of the players used English, but vary in their adherence to conventions such as use of determin- ers, plurals, and proper word ordering. 5 players invented their own language, which are more pre- cise, more consistent than general English. One player used Polish, and another used Polish nota- tion (bottom of <ref type="table">Table 3</ref>). <ref type="bibr">Most successful players (1st-20th)</ref> rem cy pos 1, stack or blk pos 4, rem blk pos 2 thru 5, rem blk pos 2 thru 4, stack bn blk pos 1 thru 2, fill bn blk, stack or blk pos 2 thru 6, rem cy blk pos 2 fill rd blk <ref type="bibr">(3.01)</ref> remove the brown block, remove all orange blocks, put brown block on orange blocks, put orange blocks on all blocks, put blue block on leftmost blue block in top row (2.78)</p><p>Remove the center block, Remove the red block, Remove all red blocks, Remove the first orange block, Put a brown block on the first brown block, Add blue block on first blue block (2.72) Average players (21th-50th) reinsert pink, take brown, put in pink, remove two pink from second layer, Add two red to second layer in odd intervals, Add five pink to second layer, Remove one blue and one brown from bottom layer (9.17) remove red, remove 1 red, remove 2 4 orange, add 2 red, add 1 2 3 4 blue, emove 1 3 5 orange, add 2 4 orange, add 2 orange, remove 2 3 brown, add 1 2 3 4 5 red, remove 2 3 4 5 6, remove 2, add 1 2 3 4 6 red <ref type="bibr">(8.37)</ref> move second cube, double red with blue, double first red with red, triple second and fourth with orange, add red, remove orange on row two, add blue to column two, add brown on first and third (7.18)</p><p>Least successful players (51th-  <ref type="table">Table 3</ref>: Example utterances, along with the average number of scrolls for that player in parentheses. Success is measured by the number of scrolls, where the more successful players need less scrolls. 1) The 20 most successful players tend to use consistent and concise language whose semantics is similar to our logical language. 2) Average players tend to be slightly more verbose and inconsistent (left and right), or significantly different from our logical langauge (middle). 3) Reasons for being unsuccessful vary. Left: no tokenization, middle: used a coordinate system and many conjunctions; right: confused in the beginning, and used a language very different from our logical language.</p><note type="other">) holdleftmost, holdbrown, holdleftmost, blueonblue, brownonblue1, blueonorange, holdblue, holdorange2, blueonred2 , holdends1, holdrightend, hold2, orangeonorangerightmost (14.15) 'add red cubes on center left, center right, far left and far right', 'remove blue blocks on row two column two, row two column four', remove red blocks in center left and center right on second row (12.6) laugh with me, red blocks with one aqua, aqua red alternate, brown red red orange aqua orange, red brown red brown red brown, space red orange red, second level red space red space red space (</note><p>Overall, we find that many players adapt in ILLG by becoming more consistent, less verbose, and more precise, even if they used standard En- glish at the beginning. For example, some players became more consistent over time (e.g. from us- ing both 'remove' and 'discard' to only using 're- move'). In terms of verbosity, removing function words like determiners as the game progresses is a common adaptation. In each of the following examples from different players, we compare an utterance that appeared early in the game to a sim- ilar utterance that appeared later: 'Remove the red ones' became 'Remove red.'; 'add brown on top of red' became 'add orange on red'; 'add red blocks to all red blocks' became 'add red to red'; 'dark red' became 'red'; one player used 'the' in all of the first 20 utterances, and then never used 'the' in the last 75 utterances.</p><p>Players also vary in precision, ranging from overspecified (e.g. 'remove the orange cube at the left', 'remove red blocks from top row') to under- specified or requiring context (e.g. 'change col- ors', 'add one blue', 'Build more blocus', 'Move the blocks fool','Add two red cubes'). We found that some players became more precise over time, as they gain a better understanding of ILLG.</p><p>Most players use utterances that actually do not match our logical language in <ref type="table">Table 1</ref>, even the successful players. In particular, numbers are of- ten used. While some concepts always have the same effect in our blocks world (e.g. 'first block' means leftmost), most are different. More con- cretely, of the top 10 players, 7 used numbers of some form and only 3 players matched our logical language. Some players who did not match the logical language performed quite well neverthe-less. One possible explanation is because the ac- tion required is somewhat constrained by the logi- cal language and some tokens can have unintended interpretations. For example, the computer can correctly interpret numerical positional references, as long as the player only refers to the leftmost and rightmost positions. So if the player says 'rem blk pos 4' and 'rem blk pos 1', the computer can interpret 'pos' as rightmost and interpret the bigram ('pos', '1') as leftmost. On the other hand, players who deviated significantly by de- scribing the desired state declaratively (e.g. 'red orange red', '246') rather than using actions, or a coordinate system (e.g. 'row two column two') performed poorly. Although players do not have to match our logical language exactly to perform well, being similar is definitely helpful.</p><p>Compositionality. As far as we can tell, all players used a compositional language; no one in- vented unrelated words for each action. Interest- ingly, 3 players did not put spaces between words. Since we assume monomorphemic words sepa- rated by spaces, they had to do a lot of scrolling as a result (e.g., 14.15 with utterances like 'or- angeonorangerightmost').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Computer strategies</head><p>We now present quantitative results on how quickly the computer can learn, where our goal is to achieve high accuracy on new utterances as we make just a single pass over the data. The num- ber of scrolls used to evaluate player is sensitive to outliers and not as intuitive as accuracy. Instead, we consider online accuracy, described as follows. Formally, if a player produced T utterances x (j) and labeled them y (j) , then online accuracy</p><formula xml:id="formula_11">def = 1 T T j=1 I y (j) = z (j) s (j) ,</formula><p>where</p><formula xml:id="formula_12">z (j) = arg max z p θ (j−1) (z|x (j)</formula><p>) is the model prediction based on the previous parame- ter θ (j−1) . Note that the online accuracy is de- fined with respect to the player-reported labels, which only corresponds to the actual accuracy if the player is precise and honest. This is not true for most spam players.</p><p>Compositionality. To study the importance of compositionality, we consider two baselines. First, consider a non-compositional model (mem-   <ref type="table">Table 4</ref>: Average online accuracy under vari- ous settings. memorize: featurize entire utter- ance and logical form non-compositionally; half model: featurize the utterances with unigrams, bi- grams, and skip-grams but conjoin with the entire logical form; full model: the model described in Section 3; +prag: the models above, with our on- line pragmatics algorithm described in Section 4. Both compositionality and pragmatics improve ac- curacy. orize) that just remembers pairs of complete ut- terance and logical forms. We implement this using indicator features on features (x, z), e.g., ('remove all the red blocks', z rm-red ), and use a large learning rate. Second, we consider a model (half ) that treats utterances composition- ally with unigrams, bigrams, and skip-trigrams features, but the logical forms are regarded as non-compositional, so we have features such as ('remove', z rm-red ), ('red', z rm-red ), etc. <ref type="table">Table 4</ref> shows that the full model (Section 3) significantly outperforms both the memorize and half baselines. The learning rate η = 0.1 is se- lected via cross validation, and we used α = 1 and β = 3 following <ref type="bibr" target="#b23">Smith et al. (2013)</ref>.</p><p>Pragmatics. Next, we study the effect of prag- matics on online accuracy. <ref type="figure" target="#fig_2">Figure 2</ref> shows that modeling pragmatics helps successful players (e.g., top 10 by number of scrolls) who use precise and consistent languages. Interestingly, our prag- matics model did not help and can even hurt the less successful players who are less precise and consistent. This is expected behavior: the prag- matics model assumes that the human is coopera- tive and behaving rationally. For the bottom half of the players, this assumption is not true, in which case the pragmatics model is not useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work and Discussion</head><p>Our work connects with a broad body of work on grounded language, in which language is used in some environment as a means towards some goal. Examples include playing games ( <ref type="bibr" target="#b2">Branavan et al., 2009</ref><ref type="bibr" target="#b3">Branavan et al., , 2010</ref><ref type="bibr" target="#b22">Reckman et al., 2010</ref>) interacting with robotics <ref type="bibr" target="#b25">(Tellex et al., 2011</ref><ref type="bibr" target="#b24">(Tellex et al., , 2014</ref>, and following instructions ( <ref type="bibr" target="#b27">Vogel and Jurafsky, 2010;</ref><ref type="bibr" target="#b6">Chen and Mooney, 2011;</ref><ref type="bibr" target="#b0">Artzi and Zettlemoyer, 2013</ref>) Se- mantic parsing utterances to logical forms, which we leverage, plays an important role in these set- tings ( <ref type="bibr" target="#b15">Kollar et al., 2010;</ref><ref type="bibr" target="#b19">Matuszek et al., 2012;</ref><ref type="bibr" target="#b0">Artzi and Zettlemoyer, 2013)</ref>.</p><p>What makes this work unique is our new inter- active learning of language games (ILLG) setting, in which a model has to learn a language from scratch through interaction. While online gradient descent is frequently used, for example in seman- tic parsing <ref type="bibr" target="#b32">(Zettlemoyer and Collins, 2007;</ref><ref type="bibr" target="#b5">Chen, 2012)</ref>, we using it in a truly online setting, taking one pass over the data and measuring online accu- racy <ref type="bibr" target="#b4">(Cesa-Bianchi and Lugosi, 2006</ref>).</p><p>To speed up learning, we leverage computa- tional models of pragmatics <ref type="bibr" target="#b14">(Jäger, 2008;</ref><ref type="bibr" target="#b10">Golland et al., 2010;</ref><ref type="bibr" target="#b8">Frank and Goodman, 2012;</ref><ref type="bibr" target="#b23">Smith et al., 2013;</ref><ref type="bibr" target="#b26">Vogel et al., 2013</ref>). The main differ- ence is these previous works use pragmatics with a trained base model, whereas we learn the model online. <ref type="bibr" target="#b20">Monroe and Potts (2015)</ref> uses learning to improve the pragmatics model. In contrast, we use pragmatics to speed up the learning pro- cess by capturing phenomena like mutual exclu- sivity <ref type="bibr" target="#b18">(Markman and Wachtel, 1988)</ref>. We also dif- fer from prior work in several details. First, we model pragmatics in the online learning setting where we use an online update for the pragmat- ics model. Second, unlikely the reference games where pragmatic effects plays an important role by design, SHRDLURN is not specifically designed to require pragmatics. The improvement we get is mainly due to players trying to be consistent in their language use. Finaly, we treat both the utter- ance and the logical forms as featurized composi- tional objects. <ref type="bibr" target="#b23">Smith et al. (2013)</ref> treats utterances (i.e. words) and logical forms (i.e. objects) as cat- egories; <ref type="bibr" target="#b20">Monroe and Potts (2015)</ref> used features, but also over flat categories.</p><p>Looking forward, we believe that the ILLG set- ting is worth studying and has important implica- tions for natural language interfaces. Today, these systems are trained once and deployed. If these systems could quickly adapt to user feedback in real-time as in this work, then we might be able to more readily create systems for resource-poor languages and new domains, that are customizable and improve through use.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The SHRDLURN game: the objective is to transform the start state into the goal state. The human types in an utterance, and the computer (which does not know the goal state) tries to interpret the utterance and perform the corresponding action. The computer initially knows nothing about the language, but through the human's feedback, learns the human's language while making progress towards the game goal.</figDesc><graphic url="image-1.png" coords="1,307.28,223.54,226.77,178.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>14.32) Spam players (∼ 85th-100) next, hello happy, how are you, move, gold, build goal blocks, 23,house, gabboli, x, run"xav, d, j, xcv, dulicate goal (21.7) Most interesting usu´nusu´n br ˛ azowe klocki, postaw pomara´nczowypomara´nczowy klocek na pierwszym klocku, postaw czerwone klocki na pomara´nczowychpomara´nczowych, usu´nusu´n pomara´nczowepomara´nczowe klocki w górnym rz˛ edzie rm scat + 1 c, + 1 c, rm sh, +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Pragmatics improve online accuracy. In these plots, each marker is a player. red o: players who ranked 1-20 in terms of minimizing number of scrolls, green x: players 20-50; blue +: lower than 50 (includes spam players). Marker sizes correspond to player rank, where better players are depicted with larger markers. 2a: online accuracies with and without pragmatics on the full model; 2b: same for the half model.</figDesc></figure>

			<note place="foot" n="1"> Demo: http://shrdlurn.sidaw.xyz</note>

			<note place="foot" n="2"> This is often the case when we try to interact with a new software system or service before reading the manual.</note>

			<note place="foot" n="4"> Modeling pragmatics In our initial experience with the semantic parsing model described in Section 3, we found that it was able to learn reasonably well, but lacked a reasoning ability that one finds in human learners. To illustrate the point, consider the beginning of a game when θ = 0 in the log-linear model p θ (z | x). Suppose that human utters &apos;remove red&apos; and then identifies 3 We tried ordering based on the sum of the probabilities (which corresponds to marginalizing out the logical form), but this had the degenerate effect of assigning too much probability mass to y being the set of empty stacks, which can result from many actions.</note>

			<note place="foot" n="4"> Of course, synonyms and variable word order occur in real language. We would need a more complex game compared to SHRDLURN to capture this effect.</note>

			<note place="foot" n="6"> This number is not 50 because some block tasks need multiple steps and players are also allowed to explore without reaching the goal.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>DARPA Communicating with Computers (CwC) program under ARO prime contract no. W911NF-15-1-0462. The first author is supported by a NSERC PGS-D fellowship. In addition, we thank Will Monroe, and Chris Potts for their insightful comments and discussions on pragmatics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility</head><p>All code, data, and experiments for this paper are available on the CodaLab platform:</p><p>https://worksheets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>codalab.org/worksheets/ 0x9fe4d080bac944e9a6bd58478cb05e5e</head><p>The client side code is here:</p><p>https://github.com/sidaw/shrdlurn/tree/ acl16-demo and a demo: http://shrdlurn.sidaw.xyz</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from questionanswer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reinforcement learning for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reading between the lines: Learning to map high-level instructions to commands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1268" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Prediction, learning, and games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast online lexicon learning for grounded language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to interpret natural language navigation instructions from observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="859" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory (COLT)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting pragmatic reasoning in language games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">336</biblScope>
			<biblScope unit="page" from="998" to="998" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Communication accommodation theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Giles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Sage Publications, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A gametheoretic approach to generating spatial descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Probabilistic Semantics and Pragmatics: Uncertainty in Language and Thought. The Handbook of Contemporary Semantic Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lassiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>WileyBlackwell</publisher>
		</imprint>
	</monogr>
	<note>2nd Edition</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Logic and conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Grice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Syntax and semantics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="41" to="58" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Language style matching predicts relationship initiation and stability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Ireland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Slatcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Eastwick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Scissors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="44" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Game theory in semantics and pragmatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jäger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>University of Tübingen</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Grounding verbs of motion in natural language commands to robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Experimental Robotics (ISER)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inducing probabilistic CCG grammars from logical form with higherorder unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1223" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Children&apos;s use of mutual exclusivity to constrain the meanings of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Markman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Wachtel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="125" to="157" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A joint model of language and perception for grounded attribute learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1671" to="1678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning in the Rational Speech Acts model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 20th Amsterdam Colloquium</title>
		<meeting>20th Amsterdam Colloquium</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning meanings of words and constructions, grounded in a virtual game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Reckman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Orkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Natural Language Processing</title>
		<imprint>
			<publisher>KONVENS</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning and using language via recursive pragmatic reasoning about other agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Asking for help using inverse semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knepper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Understanding natural language commands for robotic navigation and mobile manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Emergence of gricean maxims from multi-agent decision theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bodoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1072" to="1081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to follow navigational directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="806" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Understanding Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wittgenstein</surname></persName>
		</author>
		<title level="m">Philosophical Investigations</title>
		<meeting><address><addrLine>Blackwell, Oxford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning synchronous grammars for semantic parsing with lambda calculus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="960" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Online learning of relaxed CCG grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
