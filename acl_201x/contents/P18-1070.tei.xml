<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STRUCTVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">STRUCTVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="754" to="765"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>754</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Semantic parsing is the task of transducing natural language (NL) utterances into formal meaning representations (MRs), commonly represented as tree structures. Annotating NL utterances with their corresponding MRs is expensive and time-consuming, and thus the limited availability of labeled data often becomes the bottleneck of data-driven, supervised models. We introduce STRUCTVAE, a vari-ational auto-encoding model for semi-supervised semantic parsing, which learns both from limited amounts of parallel data, and readily-available unlabeled NL utterances. STRUCTVAE models latent MRs not observed in the unlabeled data as tree-structured latent variables. Experiments on semantic parsing on the ATIS domain and Python code generation show that with extra unlabeled data, STRUCTVAE outperforms strong supervised models. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic parsing tackles the task of mapping nat- ural language (NL) utterances into structured for- mal meaning representations (MRs). This in- cludes parsing to general-purpose logical forms such as λ-calculus <ref type="bibr">Collins, 2005, 2007)</ref> and the abstract meaning represen- tation (AMR, <ref type="bibr" target="#b1">Banarescu et al. (2013)</ref>; <ref type="bibr" target="#b29">Misra and Artzi (2016)</ref>), as well as parsing to computer- executable programs to solve problems such as question answering <ref type="bibr" target="#b2">(Berant et al., 2013;</ref><ref type="bibr" target="#b52">Yih et al., 2015;</ref>, or generation of domain- specific (e.g., SQL) or general purpose program- ming languages (e.g., Python) ( <ref type="bibr" target="#b36">Quirk et al., 2015;</ref><ref type="bibr" target="#b54">Yin and Neubig, 2017;</ref><ref type="bibr" target="#b37">Rabinovich et al., 2017</ref>).</p><p>1 Code available at http://pcyin.me/struct vae While these models have a long history ( <ref type="bibr" target="#b56">Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b43">Tang and Mooney, 2001</ref>), re- cent advances are largely attributed to the success of neural network models ( <ref type="bibr" target="#b49">Xiao et al., 2016;</ref><ref type="bibr" target="#b9">Dong and Lapata, 2016;</ref><ref type="bibr" target="#b60">Zhong et al., 2017)</ref>. However, these mod- els are also extremely data hungry: optimization of such models requires large amounts of training data of parallel NL utterances and manually anno- tated MRs, the creation of which can be expensive, cumbersome, and time-consuming. Therefore, the limited availability of parallel data has become the bottleneck of existing, purely supervised-based models. These data requirements can be alleviated with weakly-supervised learning, where the deno- tations (e.g., answers in question answering) of MRs (e.g., logical form queries) are used as indi- rect supervision ( <ref type="bibr" target="#b7">Clarke et al. (2010)</ref>; <ref type="bibr" target="#b24">Liang et al. (2011)</ref>; <ref type="bibr" target="#b2">Berant et al. (2013)</ref>, inter alia), or data- augmentation techniques that automatically gen- erate pseudo-parallel corpora using hand-crafted or induced grammars <ref type="bibr" target="#b15">(Jia and Liang, 2016;</ref><ref type="bibr" target="#b47">Wang et al., 2015)</ref>.</p><p>In this work, we focus on semi-supervised learning, aiming to learn from both limited amounts of parallel NL-MR corpora, and unla- beled but readily-available NL utterances. We draw inspiration from recent success in applying variational auto-encoding (VAE) models in semi- supervised sequence-to-sequence learning , and propose STRUCTVAE -a principled deep gener- ative approach for semi-supervised learning with tree-structured latent variables <ref type="figure" target="#fig_0">(Fig. 1)</ref>. STRUCT- VAE is based on a generative story where the surface NL utterances are generated from tree- structured latent MRs following the standard VAE architecture: (1) an off-the-shelf semantic parser functions as the inference model, parsing an ob- served NL utterance into latent meaning represen- tations ( § 3.2); (2) a reconstruction model decodes the latent MR into the original observed utterance <ref type="bibr">( § 3.1)</ref>. This formulation enables our model to perform both standard supervised learning by op- timizing the inference model (i.e., the parser) us- ing parallel corpora, and unsupervised learning by maximizing the variational lower bound of the likelihood of the unlabeled utterances <ref type="bibr">( § 3.3)</ref>.</p><p>In addition to these contributions to semi- supervised semantic parsing, STRUCTVAE con- tributes to generative model research as a whole, providing a recipe for training VAEs with struc- tured latent variables. Such a structural latent space is contrast to existing VAE research using flat representations, such as continuous distributed representations <ref type="bibr" target="#b19">(Kingma and Welling, 2013)</ref>, dis- crete symbols , or hy- brids of the two ( <ref type="bibr" target="#b61">Zhou and Neubig, 2017)</ref>.</p><p>We apply STRUCTVAE to semantic parsing on the ATIS domain and Python code genera- tion. As an auxiliary contribution, we implement a transition-based semantic parser, which uses Ab- stract Syntax Trees (ASTs, § 3.2) as intermedi- ate MRs and achieves strong results on the two tasks. We then apply this parser as the inference model for semi-supervised learning, and show that with extra unlabeled data, STRUCTVAE outper- forms its supervised counterpart. We also demon- strate that STRUCTVAE is compatible with differ- ent structured latent representations, applying it to a simple sequence-to-sequence parser which uses λ-calculus logical forms as MRs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Semi-supervised Semantic Parsing</head><p>In this section we introduce the objectives for semi-supervised semantic parsing, and present high-level intuition in applying VAEs for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Supervised and Semi-supervised Training</head><p>Formally, semantic parsing is the task of map- ping utterance x to a meaning representation z. As noted above, there are many varieties of MRs that can be represented as either graph structures (e.g., AMR) or tree structures (e.g., λ-calculus and ASTs for programming languages). In this work we specifically focus on tree-structured MRs (see <ref type="figure" target="#fig_3">Fig. 2</ref> for a running example Python AST), although application of a similar framework to graph-structured representations is also feasible.</p><p>Traditionally, purely supervised semantic parsers train a probabilistic model p φ (z|x) using parallel data L of NL utterances and annotated MRs (i.e., L = {{x, z}). As noted in the introduction, one major bottleneck in this ap- proach is the lack of such parallel data. Hence, we turn to semi-supervised learning, where the model additionally has access to a relatively large amount of unlabeled NL utterances U = {x}. Semi-supervised learning then aims to maximize the log-likelihood of examples in both L and U:</p><formula xml:id="formula_0">J = x,z ∈L log p φ (z|x) supervised obj. Js +α x∈U log p(x) unsupervised obj. Ju (1)</formula><p>The joint objective consists of two terms: (1) a supervised objective J s that maximizes the con- ditional likelihood of annotated MRs, as in stan- dard supervised training of semantic parsers; and (2) a unsupervised objective J u , which maximizes the marginal likelihood p(x) of unlabeled NL ut- terances U, controlled by a tuning parameter α. Intuitively, if the modeling of p φ (z|x) and p(x) is coupled (e.g., they share parameters), then op- timizing the marginal likelihood p(x) using the unsupervised objective J u would help the learn- ing of the semantic parser p φ (z|x) ( <ref type="bibr" target="#b62">Zhu, 2005)</ref>. STRUCTVAE uses the variational auto-encoding framework to jointly optimize p φ (z|x) and p(x), as outlined in § 2.2 and detailed in § 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">VAEs for Semi-supervised Learning</head><p>From Eq. (1), our semi-supervised model must be able to calculate the probability p(x) of unlabeled NL utterances. To model p(x), we use VAEs, which provide a principled framework for gener- ative models using neural networks <ref type="bibr" target="#b19">(Kingma and Welling, 2013</ref>). As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, VAEs define a generative story (bold arrows in <ref type="figure" target="#fig_0">Fig. 1</ref>, explained in § 3.1) to model p(x), where a latent MR z is sampled from a prior, and then passed to the recon- struction model to decode into the surface utter- ance x. There is also an inference model q φ (z|x) that allows us to infer the most probable latent MR z given the input x (dashed arrows in <ref type="figure" target="#fig_0">Fig. 1</ref>, ex- plained in § 3.2). In our case, the inference pro- cess is equivalent to the task of semantic parsing if we set q φ (·) p φ (·). VAEs also provide a framework to compute an approximation of p(x) using the inference and reconstruction models, al- lowing us to effectively optimize the unsupervised and supervised objectives in Eq. (1) in a joint fash- ion ( , explained in § 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">STRUCTVAE: VAEs with</head><p>Tree-structured Latent Variables</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generative Story</head><p>STRUCTVAE follows the standard VAE architec- ture, and defines a generative story that explains how an NL utterance is generated: a latent mean- ing representation z is sampled from a prior dis- tribution p(z) over MRs, which encodes the la- tent semantics of the utterance. A reconstruction model p θ (x|z) then decodes the sampled MR z into the observed NL utterance x. Both the prior p(z) and the reconstruction model p(x|z) takes tree-structured MRs as inputs. To model such inputs with rich internal structures, we follow , and model the dis- tribution over a sequential surface representation of z, z s instead. Specifically, we have p(z) p(z s ) and p θ (x|z) p θ (x|z s ) 2 . For code gener- ation, z s is simply the surface source code of the AST z. For semantic parsing, z s is the linearized s-expression of the logical form. Linearization al- lows us to use standard sequence-to-sequence net- works to model p(z) and p θ (x|z). As we will ex- plain in § 4.3, we find these two components per- form well with linearization.</p><p>Specifically, the prior is parameterized by a Long Short-Term Memory (LSTM) language model over z s . The reconstruction model is an attentional sequence-to-sequence network ( <ref type="bibr" target="#b26">Luong et al., 2015)</ref>, augmented with a copying mech- anism ( <ref type="bibr" target="#b11">Gu et al., 2016)</ref>, allowing an out-of- vocabulary (OOV) entity in z s to be copied to x (e.g., the variable name my list in <ref type="figure" target="#fig_0">Fig. 1</ref> and its AST in <ref type="figure" target="#fig_3">Fig. 2)</ref>. We refer readers to Appendix B for details of the neural network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference Model</head><p>STRUCTVAE models the semantic parser p φ (z|x) as the inference model q φ (z|x) in VAE ( § 2.2), which maps NL utterances x into tree-structured meaning representations z. q φ (z|x) can be any trainable semantic parser, with the correspond- ing MRs forming the structured latent semantic space. In this work, we primarily use a seman- tic parser based on the Abstract Syntax Descrip- tion Language (ASDL) framework ( <ref type="bibr" target="#b46">Wang et al., 1997</ref>) as the inference model. The parser en- codes x into ASTs <ref type="figure" target="#fig_3">(Fig. 2)</ref>. ASTs are the native meaning representation scheme of source code in modern programming languages, and can also be adapted to represent other semantic structures, like λ-calculus logical forms (see § 4.2 for details). We remark that STRUCTVAE works with other se- mantic parsers with different meaning representa- tions as well (e.g., using λ-calculus logical forms for semantic parsing on ATIS, explained in § 4.3).</p><p>Our inference model is a transition-based parser inspired by recent work in neural semantic pars- ing and code generation. The transition system is an adaptation of Yin and Neubig (2017) (hereafter YN17), which decomposes the generation process of an AST into sequential applications of tree- construction actions following the ASDL gram- mar, thus ensuring the syntactic well-formedness of generated ASTs. Different from YN17, where ASTs are represented as a Context Free Grammar learned from a parsed corpus, we follow <ref type="bibr" target="#b37">Rabinovich et al. (2017)</ref> and use ASTs defined under the ASDL formalism ( § 3.2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Generating ASTs with ASDL Grammar</head><p>First, we present a brief introduction to ASDL. An AST can be generated by applying typed construc- tors in an ASDL grammar, such as those in <ref type="figure" target="#fig_4">Fig. 3</ref> for the Python ASDL grammar. Each constructor specifies a language construct, and is assigned to a particular composite type. For example, the con- structor Call has type expr (expression), and it denotes function calls. Constructors are associated with multiple fields. For instance, the Call con- structor and has three fields: func, args and key- words. Like constructors, fields are also strongly typed. For example, the func field of Call has expr type. Fields with composite types are in- stantiated by constructors of the same type, while fields with primitive types store values (e.g., iden- tifier names or string literals). Each field also has  </p><formula xml:id="formula_1">t Frontier Field Action t 1 stmt root Expr(expr value) t 2</formula><p>expr value Call(expr func, expr* args, keyword* keywords) t <ref type="bibr">3</ref> expr func Name(identifier id)</p><formula xml:id="formula_2">t 4 identifier id GENTOKEN[sorted] t 5</formula><p>expr* args Name(identifier id) t <ref type="bibr">6</ref> identifier id GENTOKEN[my list] t <ref type="bibr">7</ref> expr* args REDUCE (close the frontier field) t 8 keyword* keywords keyword(identifier arg, expr value) t <ref type="bibr">9</ref> identifier arg GENTOKEN[reverse] t 10 expr value Name(identifier id) t 11 identifier id GENTOKEN[T rue] t 12 keyword* keywords REDUCE (close the frontier field)  Each node in an AST corresponds to a typed field in a constructor (except for the root node). Depending on the cardinality of the field, an AST node can be instantiated with one or multiple con- structors. For instance, the func field in the ex- ample AST has single cardinality, and is instan- tiated with a Name constructor; while the args field with sequential cardinality could have multi- ple constructors (only one shown in this example).</p><p>Our parser employs a transition system to gen- erate an AST using three types of actions. <ref type="figure" target="#fig_3">Fig. 2</ref> (Right) lists the sequence of actions used to gen- erate the example AST. The generation process starts from an initial derivation with only a root node of type stmt (statement), and proceeds ac- cording to the top-down, left-to-right traversal of the AST. At each time step, the parser applies an action to the frontier field of the derivation:</p><p>APPLYCONSTR[c] actions apply a constructor c to the frontier composite field, expanding the derivation using the fields of c. For fields with sin- gle or optional cardinality, an APPLYCONSTR ac- tion instantiates the empty frontier field using the constructor, while for fields with sequential car- dinality, it appends the constructor to the frontier field. For example, at t 2 the Call constructor is applied to the value field of Expr, and the deriva- tion is expanded using its three child fields.</p><p>REDUCE actions complete generation of a field with optional or multiple cardinalities. For in- stance, the args field is instantiated by Name at t 5 , and then closed by a REDUCE action at t 7 .</p><p>GENTOKEN <ref type="bibr">[v]</ref> actions populate an empty primitive frontier field with token v. A primitive field whose value is a single token (e.g., identi- fier fields) can be populated with a single GEN- TOKEN action. Fields of string type can be in- stantiated using multiple such actions, with a final GENTOKEN[&lt;/f&gt;] action to terminate the genera- tion of field values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Modeling q φ (z|x)</head><p>The probability of generating an AST z is natu- rally decomposed into the probabilities of the ac- tions {a t } used to construct z:</p><formula xml:id="formula_3">q φ (z|x) = t p(a t |a &lt;t , x).</formula><p>Following YN17, we parameterize q φ (z|x) using a sequence-to-sequence network with auxiliary re- current connections following the topology of the AST. Interested readers are referred to <ref type="bibr">Appendix B and Yin and Neubig (2017)</ref> for details of the neu- ral network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Semi-supervised Learning</head><p>In this section we explain how to optimize the semi-supervised learning objective Eq. (1) in STRUCTVAE.</p><p>Supervised Learning For the supervised learn- ing objective, we modify J s , and use the labeled data to optimize both the inference model (the se-mantic parser) and the reconstruction model:</p><formula xml:id="formula_4">J s (x,z)∈L log q φ (z|x) + log p θ (x|z)<label>(2)</label></formula><p>Unsupervised Learning To optimize the unsu- pervised learning objective J u in Eq. (1), we max- imize the variational lower-bound of log p(x):</p><formula xml:id="formula_5">log p(x) ≥ E z∼q φ (z|x) log p θ (x|z) − λ · KL[q φ (z|x)||p(z)] = L (3)</formula><p>where To optimize the parameters of our model in the face of non-differentiable discrete latent variables, we follow , and ap- proximate ∂L ∂φ using the score function estimator (a.k.a. REINFORCE, Williams (1992)):</p><formula xml:id="formula_6">∂L ∂φ = ∂ ∂φ E z∼q φ (z|x) log p θ (x|z) − λ log q φ (z|x) − log p(z) learning signal = ∂ ∂φ E z∼q φ (z|x) l (x, z) ≈ 1 |S(x)| z i ∈S(x) l (x, z i ) ∂ log q φ (z i |x) ∂φ<label>(4)</label></formula><p>where we approximate the gradient using a set of samples S(x) drawn from q φ (·|x). To ensure the quality of sampled latent MRs, we follow <ref type="bibr" target="#b12">Guu et al. (2017)</ref> and use beam search. The term l (x, z) is defined as the learning signal . The learning signal weights the gradient for each latent sample z. In REIN- FORCE, to cope with the high variance of the learning signal, it is common to use a baseline b(x) to stabilize learning, and re-define the learn- ing signal as</p><formula xml:id="formula_7">l(x, z) l (x, z) − b(x).<label>(5)</label></formula><p>Specifically, in STRUCTVAE, we define</p><formula xml:id="formula_8">b(x) = a · log p(x) + c,<label>(6)</label></formula><p>where log p(x) is a pre-trained LSTM language model. This is motivated by the empirical obser- vation that log p(x) correlates well with the recon- struction score log p θ (x|z), hence with l (x, z).</p><p>Finally, for the reconstruction model, its gradi- ent can be easily computed:</p><formula xml:id="formula_9">∂L ∂θ ≈ 1 |S(x)| z i ∈S(x) ∂ log p θ (x|z i ) ∂θ .</formula><p>Discussion Perhaps the most intriguing question here is why semi-supervised learning could im- prove semantic parsing performance. While the underlying theoretical exposition still remains an active research problem ( <ref type="bibr" target="#b39">Singh et al., 2008)</ref>, in this paper we try to empirically test some likely hypotheses. In Eq. (4), the gradient received by the inference model from each latent sample z is weighed by the learning signal l(x, z). l(x, z) can be viewed as the reward function in REINFORCE learning. It can also be viewed as weights associ- ated with pseudo-training examples {{x, z : z ∈ S(x)} sampled from the inference model. Intu- itively, a sample z with higher rewards should:</p><p>(1) have z adequately encode the input, leading to high reconstruction score log p θ (x|z); and <ref type="formula" target="#formula_4">(2)</ref> have z be succinct and natural, yielding high prior probability. Let z * denote the gold-standard MR of x. Consider the ideal case where z * ∈ S(x) and l(x, z * ) is positive, while l(x, z ) is negative for other imperfect samples z ∈ S(x), z = z * . In this ideal case, x, z * would serve as a positive training example and other samples x, z would be treated as negative examples. Therefore, the in- ference model would receive informative gradient updates, and learn to discriminate between gold and imperfect MRs. This intuition is similar in spirit to recent efforts in interpreting gradient up- date rules in reinforcement learning ( <ref type="bibr" target="#b12">Guu et al., 2017</ref>). We will present more empirical statistics and observations in § 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>In our semi-supervised semantic parsing experi- ments, it is of interest how STRUCTVAE could further improve upon a supervised parser with ex- tra unlabeled data. We evaluate on two datasets:</p><p>Semantic Parsing We use the ATIS dataset, a collection of 5,410 telephone inquiries of flight booking (e.g., "Show me flights from ci0 to ci1").</p><p>The target MRs are defined using λ-calculus log- ical forms (e.g., "lambda $0 e (and (flight $0) (from $ci0) (to $ci1))"). We use the pre-processed dataset released by <ref type="bibr" target="#b9">Dong and Lapata (2016)</ref>, where entities (e.g., cities) are canoni- calized using typed slots (e.g., ci0). To predict λ-calculus logical forms using our transition-based parser, we use the ASDL grammar defined by <ref type="bibr" target="#b37">Rabinovich et al. (2017)</ref> to convert between logical forms and ASTs (see Appendix C for details).  <ref type="bibr" target="#b54">Yin and Neubig (2017)</ref> and use the astor package to con- vert ASDL ASTs into Python source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Setup</head><p>Labeled and Unlabeled Data STRUCTVAE re- quires access to extra unlabeled NL utterances for semi-supervised learning. However, the datasets we use do not accompany with such data. We therefore simulate the semi-supervised learning scenario by randomly sub-sampling K examples from the training split of each dataset as the la- beled set L. To make the most use of the NL ut- terances in the dataset, we construct the unlabeled set U using all NL utterances in the training set 3,4 .</p><p>Training Procedure Optimizing the unsuper- vised learning objective Eq. (3) requires sampling structured MRs from the inference model q φ (z|x). Due to the complexity of the semantic parsing problem, we cannot expect any valid samples from randomly initialized q φ (z|x). We therefore pre-train the inference and reconstruction mod- els using the supervised objective Eq. (2) until convergence, and then optimize using the semi- supervised learning objective Eq. (1). Throughout all experiments we set α (Eq. <ref type="formula">(1)</ref>) and λ (Eq. <ref type="formula">(3)</ref>) to 0.1. The sample size |S(x)| is 5. We observe that the variance of the learning signal could still be high when low-quality samples are drawn from the inference model q φ (z|x). We therefore clip <ref type="bibr">3</ref> We also tried constructing U using the disjoint portion of the NL utterances not presented in the labeled set L, but found this yields slightly worse performance, probably due to lacking enough unlabeled data. Interpreting these results would be an interesting avenue for future work. <ref type="bibr">4</ref> While it might be relatively easy to acquire additional unlabeled utterances in practical settings (e.g., through query logs of a search engine), unfortunately most academic seman- tic parsing datasets, like the ones used in this work, do not feature large sets of in-domain unlabeled data. We therefore perform simulated experiments instead.  all learning signals lower than k = −20.0. Early- stopping is used to avoid over-fitting. We also pre- train the prior p(z) ( § 3.3) and the baseline func- tion Eq. (6). Readers are referred to Appendix D for more detail of the configurations.</p><p>Metric As standard in semantic parsing re- search, we evaluate by exact-match accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>Tab. 1 and Tab. 2 list the results on ATIS and DJANGO, resp, with varying amounts of labeled data L. We also present results of training the transition-based parser using only the supervised objective (SUP., Eq. <ref type="formula" target="#formula_4">(2)</ref>). We also compare STRUCTVAE with self-training (SELFTRAIN), a semi-supervised learning baseline which uses the supervised parser to predict MRs for unlabeled utterances in U − L, and adds the predicted ex- amples to the training set to fine-tune the super- vised model. Results for STRUCTVAE are aver- aged over four runs to account for the additional fluctuation caused by REINFORCE training.</p><p>Supervised System Comparison First, to high- light the effectiveness of our transition parser based on ASDL grammar (hence the reliability of our supervised baseline), we compare the super- vised version of our parser with existing parsing models. On ATIS, our supervised parser trained on the full data is competitive with existing neural network based models, surpassing the SEQ2TREE model, and on par with the Abstract Syntax Net- work (ASN) without using extra supervision. On DJANGO, our model significantly outperforms the YN17 system, probably because the transition sys- tem used by our parser is defined natively to con- struct ASDL ASTs, reducing the number of ac- tions for generating each example. On DJANGO, the average number of actions is 14.3, compared with 20.3 reported in YN17.</p><formula xml:id="formula_10">−30 −20 −10 0 10 20 0.0 0.1 0.2 z * (ˆ µ = 2.59, ˆ σ = 30.80) z (ˆ µ = −5.12, ˆ σ = 214.62)<label>(a)</label></formula><p>Semi-supervised Learning Next, we discuss our main comparison between STRUCTVAE with the supervised version of the parser (recall that the supervised parser is used as the inference model in STRUCTVAE, § 3.2). First, comparing our proposed STRUCTVAE with the supervised parser when there are extra unlabeled data (i.e., |L| &lt; 4, 434 for ATIS and |L| &lt; 16, 000 for DJANGO), semi-supervised learning with STRUCTVAE con- sistently achieves better performance. Notably, on DJANGO, our model registers results as compet- itive as previous state-of-the-art method (YN17) using only half the training data (71.5 when |L| = 8000 v.s. 71.6 for YN17). This demonstrates that STRUCTVAE is capable of learning from un- labeled NL utterances by inferring high quality, structurally rich latent meaning representations, further improving the performance of its super- vised counterpart that is already competitive. Sec- ond, comparing STRUCTVAE with self-training, we find STRUCTVAE outperforms SELFTRAIN in eight out of ten settings, while SELFTRAIN  under-performs the supervised parser in four out of ten settings. This shows self-training does not necessarily yield stable gains while STRUCTVAE does. Intuitively, STRUCTVAE would perform better since it benefits from the additional signal of the quality of MRs from the reconstruction model ( § 3.3), for which we present more analysis in our next set of experiments.</p><p>For the sake of completeness, we also report the results of STRUCTVAE when L is the full train- ing set. Note that in this scenario there is no extra unlabeled data disjoint with the labeled set, and not surprisingly, STRUCTVAE does not outper- form the supervised parser. In addition to the su- pervised objective Eq. (2) used by the supervised parser, STRUCTVAE has the extra unsupervised objective Eq. (3), which uses sampled (probably incorrect) MRs to update the model. When there is no extra unlabeled data, those sampled (incor- rect) MRs add noise to the optimization process, causing STRUCTVAE to under-perform.</p><p>Study of Learning Signals As discussed in § 3.3, in semi-supervised learning, the gradient received by the inference model from each sam- pled latent MR is weighted by the learning signal. Empirically, we would expect that on average, the learning signals of gold-standard samples z * , l(x, z * ), are positive, larger than those of other (imperfect) samples z , l(x, z ). We therefore study the statistics of l(x, z * ) and l(x, z ) for all utterances x ∈ U − L, i.e., the set of utterances which are not included in the labeled set. <ref type="bibr">5</ref> The statistics are obtained by performing inference using trained models. <ref type="figure" target="#fig_5">Figures 4a and 4b</ref> depict the histograms of learning signals on DJANGO and ATIS, resp. We observe that the learning signals for gold samples concentrate on positive intervals. We also show the mean and variance of the learning signals. On average, we have l(x, z * ) being positive and l(x, z) negative. Also note that the distribution of l(x, z * ) has smaller variance and is more concentrated. Therefore the inference model receives informative gradient up- dates to discriminate between gold and imperfect NL join p and cmd into a file path, substitute it for f z s 1 f = os.path.join(p, cmd) log q(z|x) = −1.00 log p(x|z) = −2.00 log p(z) = −24.33 l(x, z) = 9.14 z s  samples. Next, we plot the distribution of the rank of l(x, z * ), among the learning signals of all samples of x, {l(x, z i ) : z i ∈ S(x)}. Results are shown in <ref type="figure" target="#fig_7">Fig. 5</ref>. We observe that the gold samples z * have the largest learning signals in around 80% cases. We also find that when z * has the largest learning signal, its average difference with the learning signal of the highest-scoring incorrect sample is 1.27 and 0.96 on DJANGO and ATIS, respectively.</p><p>Finally, to study the relative contribution of the reconstruction score log p(x|z) and the prior log p(z) to the learning signal, we present ex- amples of inferred latent MRs during training (Tab. 3). Examples 1&amp;2 show that the reconstruc- tion score serves as an informative quality measure of the latent MR, assigning the correct samples z s 1 with high log p(x|z), leading to positive learning signals. This is in line with our assumption that a good latent MR should adequately encode the se- mantics of the utterance. Example 3 shows that the prior is also effective in identifying "unnatu- ral" MRs (e.g., it is rare to add a function and a string literal, as in z s 2 ). These results also sug- gest that the prior and the reconstruction model perform well with linearization of MRs. Finally, note that in Examples 2&amp;3 the learning signals for the correct samples z s 1 are positive even if their in- ference scores q(z|x) are lower than those of z s 2 .</p><p>|L| SUPERVISED STRUCTVAE-SEQ 500 47.3 55.6 1,000 62.5 73.1 2,000 73.9 74.8 3,000</p><p>80.6 81.3 4,434 (All) 84.6 84.2  This result further demonstrates that learning sig- nals provide informative gradient weights for op- timizing the inference model.</p><p>Generalizing to Other Latent MRs Our main results are obtained using a strong AST-based se- mantic parser as the inference model, with copy- augmented reconstruction model and an LSTM language model as the prior. However, there are many other ways to represent and infer structure in semantic parsing <ref type="bibr" target="#b4">(Carpenter, 1998;</ref><ref type="bibr" target="#b41">Steedman, 2000)</ref>, and thus it is of interest whether our ba- sic STRUCTVAE framework generalizes to other semantic representations. To examine this, we test STRUCTVAE using λ-calculus logical forms as latent MRs for semantic parsing on the ATIS domain. We use standard sequence-to-sequence networks with attention ( <ref type="bibr" target="#b26">Luong et al., 2015</ref>) as inference and reconstruction models. The infer- ence model is trained to construct a tree-structured logical form using the transition actions defined in <ref type="bibr" target="#b5">Cheng et al. (2017)</ref>. We use a classical tri-gram Kneser-Ney language model as the prior. Tab. 4</p><p>lists the results for this STRUCTVAE-SEQ model.</p><p>We can see that even with this very different model structure STRUCTVAE still provides signif- icant gains, demonstrating its compatibility with different inference/reconstruction networks and priors. Interestingly, compared with the results in Tab. 1, we found that the gains are especially larger with few labeled examples -STRUCT- VAE-SEQ achieves improvements of 8-10 points when |L| &lt; 1000. These results suggest that semi-supervision is especially useful in improving a mediocre parser in low resource settings.   <ref type="formula" target="#formula_6">(4)</ref>) to stabilize learning, which is based on a language model (LM) over utterances (Eq. <ref type="formula" target="#formula_8">(6)</ref>). We com- pare this baseline with a commonly used one in REINFORCE training: the multi-layer perceptron (MLP). The MLP takes as input the last hidden state of the utterance given by the encoding LSTM of the inference model. Tab. 5 lists the results over sampled settings. We found that although STRUCTVAE with the MLP baseline sometimes registers better performance on ATIS, in most set- tings it is worse than our LM baseline, and could be even worse than the supervised parser. On the other hand, our LM baseline correlates well with the learning signal, yielding stable improvements over the supervised parser. This suggests the im- portance of using carefully designed baselines in REINFORCE learning, especially when the re- ward signal has large range (e.g., log-likelihoods).</p><p>Impact of the Prior p(z) <ref type="figure">Fig. 6</ref> depicts the per- formance of STRUCTVAE as a function of the KL term weight λ in Eq. (3). When STRUCTVAE degenerates to a vanilla auto-encoder without the prior distribution (i.e., λ = 0), it under-performs the supervised baseline. This is in line with our observation in Tab. 3 showing that the prior helps identify unnatural samples. The performance of the model also drops when λ &gt; 0.1, suggesting that empirically controlling the influence of the prior to the inference model is important.</p><p>Impact of Unlabeled Data Size <ref type="figure">Fig. 7</ref> illus- trates the accuracies w.r.t. the size of unlabeled data. STRUCTVAE yields consistent gains as the size of the unlabeled data increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Works</head><p>Semi-supervised Learning for NLP Semi- supervised learning comes with a long his- tory ( <ref type="bibr" target="#b62">Zhu, 2005)</ref>, with applications in NLP from early work of self-training <ref type="bibr" target="#b51">(Yarowsky, 1995)</ref>, and graph-based methods ( <ref type="bibr" target="#b8">Das and Smith, 2011)</ref>, to recent advances in auto-encoders ( <ref type="bibr" target="#b6">Cheng et al., 2016;</ref><ref type="bibr" target="#b40">Socher et al., 2011;</ref><ref type="bibr" target="#b59">Zhang et al., 2017)</ref> and deep generative methods ( <ref type="bibr" target="#b50">Xu et al., 2017)</ref>. Our work follows the line of neural variational infer- ence for text processing ( , and resembles , which uses VAEs to model summaries as discrete latent vari- ables for semi-supervised summarization, while we extend the VAE architecture for more complex, tree-structured latent variables.</p><p>Semantic Parsing Most existing works allevi- ate issues of limited parallel data through weakly- supervised learning, using the denotations of MRs as indirect supervision ( <ref type="bibr" target="#b38">Reddy et al., 2014;</ref><ref type="bibr" target="#b22">Krishnamurthy et al., 2016;</ref><ref type="bibr" target="#b30">Neelakantan et al., 2016;</ref><ref type="bibr" target="#b32">Pasupat and Liang, 2015;</ref><ref type="bibr" target="#b53">Yin et al., 2016</ref>). For semi-supervised learning of semantic pars- ing, <ref type="bibr" target="#b16">Kate and Mooney (2007)</ref> first explore us- ing transductive SVMs to learn from a semantic parser's predictions.  ap- ply self-training to bootstrap an existing parser for AMR parsing. Kocisk´y  em- ploy VAEs for semantic parsing, but in con- trast to STRUCTVAE's structured representation of MRs, they model NL utterances as flat la- tent variables, and learn from unlabeled MR data. There have also been efforts in unsupervised se- mantic parsing, which exploits external linguis- tic analysis of utterances (e.g., dependency trees) and the schema of target knowledge bases to infer the latent MRs ( <ref type="bibr" target="#b34">Poon and Domingos, 2009;</ref><ref type="bibr" target="#b33">Poon, 2013)</ref>. Another line of research is domain adap- tation, which seeks to transfer a semantic parser learned from a source domain to the target domain of interest, therefore alleviating the need of paral- lel data from the target domain ( <ref type="bibr" target="#b42">Su and Yan, 2017;</ref><ref type="bibr" target="#b10">Fan et al., 2017;</ref><ref type="bibr" target="#b13">Herzig and Berant, 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose STRUCTVAE, a deep generative model with tree-structured latent variables for semi-supervised semantic parsing. We apply STRUCTVAE to semantic parsing and code gen- eration tasks, and show it outperforms a strong su- pervised parser using extra unlabeled data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical Representation of STRUCTVAE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left An example ASDL AST with its surface source code. Field names are labeled on upper arcs. Blue squares denote fields with sequential cardinality. Grey nodes denote primitive identifier fields, with annotated values. Fields are labeled with time steps at which they are generated. Right Action sequences used to construct the example AST. Frontier fields are denoted by their signature (type name). Each constructor in the Action column refers to an APPLYCONSTR action.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Excerpt of the python abstract syntax grammar (Python Software Foundation, 2016)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Histograms of learning signals on DJANGO (|L| = 5000) and ATIS (|L| = 2000). Difference in sample means is statistically significant (p &lt; 0.05).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Distribution of the rank of l(x, z * ) in sampled set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>2 p</head><label>2</label><figDesc>= path.join(p, cmd) log q(z|x) = −8.12 log p(x|z) = −20.96 log p(z) = −27.89 l(x, z) = −9.47 NL append i-th element of existing to child loggers z s 1 child loggers.append(existing[i]) log q(z|x) = −2.38 log p(x|z) = −9.66 log p(z) = −13.52 l(x, z) = 1.32 z s 2 child loggers.append(existing[existing]) log q(z|x) = −1.83 log p(x|z) = −16.11 log p(z) = −12.43 l(x, z) = −5.08 NL split string pks by ',', substitute the result for pri- mary keys z s 1 primary keys = pks.split(',') log q(z|x) = −2.38 log p(x|z) = −11.39 log p(z) = −10.24 l(x, z) = 2.05 z s 2 primary keys = pks.split + ',' log q(z|x) = −0.84 log p(x|z) = −14.87 log p(z) = −20.41 l(x, z) = −2.60</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Performance on DJANGO (|L| = 5000) w.r.t. the KL weight λ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance on DJANGO w.r.t. the size of labeled 
training data L 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Inferred latent MRs on DJANGO (|L| = 5000). For 
simplicity we show the surface representation of MRs (z s , 
source code) instead. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance of the STRUCTVAE-SEQ on ATIS 
w.r.t. the size of labeled training data L 

ATIS 
DJANGO 
|L| 
SUP. MLP 

LM 

|L| 
SUP. MLP 

LM 

500 
63.2 
61.5  † 66.0 1,000 
49.9 
47.0  † 52.0 
1,000 74.6 
76.3 
75.7 5,000 
63.2 
62.5  † 65.6 
2,000 80.4 
82.9 
82.4 8,000 
70.3 
67.6  † 71.5 
3,000 82.8 
81.4  † 83.6 12,000 71.1 
71.6 
72.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison of STRUCTVAE with different base-
line functions b(x), italic  † : semi-supervised learning with 
the MLP baseline is worse than supervised results. 

</table></figure>

			<note place="foot" n="2"> Linearizion is used by the prior and the reconstruction model only, and not by the inference model.</note>

			<note place="foot" n="5"> We focus on cases where z * is in the sample set S(x).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LAW-ID@ACL</title>
		<meeting>LAW-ID@ACL<address><addrLine>Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGNLL</title>
		<meeting>the SIGNLL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Type-logical Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Carpenter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning structured natural language representations for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Saraswat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semisupervised learning for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the world&apos;s response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">Roth</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semisupervised frame-semantic parsing for unknown predicates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT</title>
		<meeting>HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transfer learning for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lambert</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From language to programs: Bridging reinforcement learning and maximum marginal likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">Zheran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Decoupling structure and lexicon for zero-shot semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07918</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning a neural semantic parser from user feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semisupervised learning for semantic parsing using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACLHLT</title>
		<meeting>NAACLHLT</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic parsing with semi-supervised sequential autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kocisk´ykocisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl Moritz</forename><surname>Hermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural amr: Sequence-to-sequence models for parsing and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic parsing to probabilistic programs for situated question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">D</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Latent predictor networks for code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kocisk´ykocisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Language as a latent variable: Discrete generative models for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural shiftreduce CCG semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dipendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural programmer: Inducing latent programs with gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to generate pseudo-code from source code using statistical machine translation (T)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Fudaba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ASE</title>
		<meeting>ASE</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Grounded unsupervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Python Software Foundation</title>
		<ptr target="https://docs.python.org/2/library/ast.html" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Python abstract grammar</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Language to code: Learning semantic parsers for if-this-then-that recipes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Abstract syntax networks for code generation and semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing without questionanswer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unlabeled data: Now it helps, now it doesn&apos;t</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The Syntactic Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cross-domain semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Using multiple clause constructors in inductive logic programming for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lappoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML</title>
		<meeting>ECML</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Morpho-syntactic lexical generalization for ccg semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrienne</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The zephyr abstract syntax description language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">S</forename><surname>Korn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DSL</title>
		<meeting>DSL</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Building a semantic parser overnight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sequence-based structured prediction for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Variational autoencoder for semi-supervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoze</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Neural enquirer: Learning to query tables in natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Online learning of relaxed ccg grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Semi-supervised structured prediction with neural crf autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00103</idno>
		<title level="m">Seq2sql: Generating structured queries from natural language using reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multispace variational encoder-decoders for semisupervised labeled sequence transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<idno>1530</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Computer Sciences, University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
