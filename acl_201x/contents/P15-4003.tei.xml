<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">In-tool Learning for Selective Manual Annotation in Large Corpora</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015. c 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik-Lân</forename><surname>Do</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><forename type="middle">†</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Eckart De Castilho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">In-tool Learning for Selective Manual Annotation in Large Corpora</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL-IJCNLP 2015 System Demonstrations</title>
						<meeting>ACL-IJCNLP 2015 System Demonstrations <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="13" to="18"/>
							<date type="published">July 26-31, 2015. c 2015</date>
						</imprint>
					</monogr>
					<note>‡ Ubiquitous Knowledge Processing Lab (UKP-DIPF) German Institute for Educational Research and Educational Information</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a novel approach to the selective annotation of large corpora through the use of machine learning. Linguistic search engines used to locate potential instances of an infrequent phenomenon do not support ranking the search results. This favors the use of high-precision queries that return only a few results over broader queries that have a higher recall. Our approach introduces a classifier used to rank the search results and thus helping the annotator focus on those results with the highest potential of being an instance of the phenomenon in question, even in low-precision queries. The clas-sifier is trained in an in-tool fashion, except for preprocessing relying only on the manual annotations done by the users in the querying tool itself. To implement this approach, we build upon CSniper 1 , a web-based multiuser search and annotation tool.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapidly growing body of digitally avail- able language data, it becomes possible to investi- gate phenomena of the language system that man- ifest themselves infrequently in corpus data, e.g. non-canonical constructions. To pinpoint occur- rences of such phenomena and to annotate them requires a new kind of annotation tool, since man- ual, sequential annotation is not feasible anymore for large amounts of texts.</p><p>An annotation-by-query approach to identify such phenomena in large corpora is implemented To enable a selective manual annotation pro- cess, a linguistic search engine is used, allowing the creation of queries which single out potential instances of the phenomenon in question. Those potential instances are then displayed to the user, who annotates each one as being an instance of the phenomenon or not. This process of search- ing and annotating can be performed by multiple users concurrently; the annotations are stored for each user separately. In a subsequent evaluation step, a user can review the annotations of all users, e.g. to discard a query if it yields unsatisfying re- sults. Finally, the annotations of multiple users can be merged into a gold standard. This approach relieves the annotator from hav- ing to read through the corpus from the beginning to the end to look for instances of a phenomenon. However, the search may yield many results that may superficially appear to be an instance of the desired phenomenon, but due to ambiguities or due to a broadly defined query only a small sub- set may be actual instances. This still leaves the annotator with the tedious task of clicking through the search results to mark the true instances.</p><p>To reduce the time and effort required, we present an extension of the annotation-by-query approach <ref type="figure" target="#fig_1">(Figure 1</ref>) that introduces a ranking of the query results (Section 2) by means of machine learning; we order the results by confidence of the used classifier.</p><p>To obtain a model for the classifier, we employ an in-tool learning approach, where we learn from the annotations that are made by users in the tool itself. This makes our ranking approach useful for highly specific tasks, since no pre-trained models are needed.</p><p>Finally we demonstrate the viability of our con- cept by the example task of finding non-canonical constructions in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Ranking linguistic query results</head><p>Our approach employs machine learning to facili- tate -but not to completely replace -the man- ual annotation of query results. A query expresses the intention of the user to find a specific linguis- tic phenomenon (information need). An infor- mation retrieval search engine would provide the user with a list of results that are ranked accord- ing to their relevance, fulfilling the information need. However, linguistic search engines such as CQP (Evert and Hardie, 2011) -which is used by CSniper -are basically pattern-matching en- gines, operating on different lexical or morpho- syntactic features like part-of-speech (POS) tags and lemmata and do not have a concept of rele- vance. Thus, if the query provided by the user overgeneralizes, relevant results are hidden among many irrelevant ones, ultimately failing to satisfy the user's information need.</p><p>To tackle this problem, we use the annotations already created by users on the search results to train a classifier. Unannotated query results are then fed to the classifier whose output values are then used as relevance ratings by which the results are ranked. The classifier producing the ranking can be invoked by the user at any time; it can be configured in certain characteristics, e.g. the an- notations of which users should be used as train- ing data, or how many of the selected users have to agree on an annotation for it to be included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Workflow and ranking process in CSniper</head><p>Currently, we train the classifier on features de- rived from the constituency parse tree, which makes it useful for tasks such as locating sen- tences containing infrequent ambiguous grammat- ical constructions (cf. Section 3). Since parsing the query results is too time-intensive to be done during runtime, we parsed the corpora in advance and stored the parse trees in a database. To train the classifier, we employed SVM-light-tk <ref type="bibr" target="#b12">(Moschitti, 2006</ref>; Joachims, 1999), a support vector ma- chine implementation which uses a tree kernel to integrate all sub-trees of the parse tree as features.</p><p>Consider the following typical scenario incor- porating the ranking: A user constructs a query based on various features, such as POS tags or lemmata, which are used to search for matching sentences, e.g.</p><formula xml:id="formula_0">"It" [lemma="be"] [pos="AT0"]? [pos="NN.*"] 2</formula><p>The result is a list of sentences presented in a keywords-in-context (KWIC) view, along with an annotation field ( <ref type="figure" target="#fig_2">Figure 2</ref>).</p><p>Then the user starts to annotate these sentences as Correct or Wrong, depending whether they truly represent instances of the phenomenon in question. Clicking on the Rank results button <ref type="figure" target="#fig_2">(Figure 2</ref>) invokes our ranking process: The SVM- light-tk classifier is trained using the parse trees of the sentences which the user previously annotated. The resulting model is then used to classify the re- maining sentences in the query results. We rank the sentences according to the output value of the decision function of the classifier (which we in- terpret as a relevance/confidence rating) and tran- siently label a sentence as either (Correct) (output value &gt; 0) or (Wrong) (output value ≤ 0). The re- sults in the KWIC view are then reordered accord- ing to the rank, showing the highest-ranking result first. Repeatedly annotating those highest-ranking results and re-ranking allows for quickly annotat- ing instances of the phenomenon, while also im- proving the classifier accuracy at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Find mode</head><p>After annotating instances based on simple queries and ML-supported ranked queries, we considered the natural next step to be searching automatically for the phenomenon in question utilizing machine learning, using arbitrary sentences from the cor- pus as input for the classifier instead of only the results returned by a query. Such an automatic search could address two concerns: 1) it removes the need for the user to design new queries, al- lowing users less experienced in the query lan- guage to annotate more effectively side-by-side with advanced users; 2) it could optimally gener- alize over all the queries that users have already made and potentially locate instances that had not been found by individual high-precision queries.</p><p>To support this, we implemented the Find mode, to locate instances of a phenomenon while abstracting from the queries. In this mode, the SVM is first trained from all previously (manu- ally) labeled instances for a given phenomenon, without taking the queries into account that were used to find those instances. Then the corpus is partitioned into smaller parts containing a prede- fined amount of sentences (we used 500). One of these partitions is chosen at random, and the sentences therein are ranked using the SVM. This step is repeated, until a previously defined num- ber of sentences have been classified as Correct.</p><p>Those sentences are then shown to the user, who now can either confirm a sentence as containing the phenomenon or label it Wrong otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Related work</head><p>Existing annotation tools include automation functionality for annotation tasks, ranging from rule-based tagging to more complex, machine- learning-based approaches.</p><p>Such functionalities can be found in the anno- tation software WordFreak ( <ref type="bibr" target="#b11">Morton and LaCivita, 2003)</ref>, where a plug-in architecture allows for a variety of different taggers and classifiers to be in- tegrated, for example part-of-speech taggers or co- reference resolution engines. Those require pre- trained models, which limits the applicability of the automation capabilities of WordFreak to tasks for which such models are actually available. In addition to assigning annotations a single label, WordFreak allows plugins to rank labels for each annotation based on the confidence of the used classifier. Note that this is different to our rank- ing approach, where we instead perform a ranking of the search results which shall be annotated.</p><p>Another tool incorporating machine learning is WebAnno ( <ref type="bibr" target="#b15">Yimam et al., 2014</ref>), which imple- ments features such as custom labels and anno- tation types. In addition, WebAnno supports au- tomatic annotation similar to our approach, also employing machine learning to build models from the data annotated by users. Those models are then used to annotate the remainder of the documents. To accomplish this, WebAnno uses a split-pane view, showing automatic suggestions in one pane and manually entered annotations in another. The user can accept a suggested annotation, which is transferred to the manual pane. Lacking the search capability, WebAnno lists automatic annotations in the running corpus text, which makes it unsuited for selective annotation in large corpora. The ap- proach that we implemented on top of CSniper in- stead ranks the search results for a given query by confidence of the classifier.</p><p>Yet another form of in-tool learning is active learning, as is implemented, e.g., in Dualist (Set- tles, 2011). In an active learning scenario the sys- tem aims to efficiently train an accurate classifier (i.e. with as little training data as possible) and thus repeatedly asks the user to annotate instances from which it can learn the most. Such an ap- proach can work well for reducing the amount of training data needed to produce a model which achieves high accuracy, as has been -amongst others -shown by <ref type="bibr" target="#b7">Hachey et al. (2005)</ref>. How- ever, they also learn in their experiments that those highly informative instances are often harder to annotate and increase required time and effort of annotators. Our approach is different from active learning as our goal is not to improve the training efficiency of the classifier but rather to allow the user to interactively find and label as many true instances of a phenomenon as possible in a large corpus. Thus, the items presented to the user are not determined by the expected information gain for the classifier but rather by the confidence of the classifier, presenting the user with those instances first which are most likely to be occurrences of the phenomenon in question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Case study: Finding non-canonical constructions</head><p>We demonstrate our novel approach on the task of locating non-canonical constructions (NCC) and conduct an intrinsic evaluation of the accuracy of the system augmented with machine learning output on the data annotated by expert linguists. The linguists annotated sentences for occurrences of certain NCC subtypes: information-packaging constructions <ref type="bibr" target="#b8">(Huddleston and Pullum, 2002</ref>, pp. 1365ff.), which present information in a differ- ent way from their canonical counterparts without changing truth conditions; specifically It-clefts ("It was Peter who made lunch."), NP-preposing ("A treasure, he was searching."), and PP-inversion ("To his left lay the forest.") clauses.</p><p>For our experiments, we used the British Na- tional <ref type="bibr">Corpus (2007)</ref>, comprising 100 million words in multiple domains <ref type="bibr">3</ref> . Constituency pars- ing was conducted using the factored variant of the Stanford Parser ( <ref type="bibr" target="#b10">Klein and Manning, 2003)</ref>, incorporated into a UIMA pipeline using DKPro Core .</p><p>As a baseline we use queries representing the experts' intuition about the realization of the NCCs in terms of POS tags and lemmata. We show that our system improves the precision of the query results even with little training data. Also we present run times for our ranking system un- der real-world conditions for different training set sizes. Further, we compare Krippendorff's α co- efficient as an inter-annotator agreement measure among only annotators to the α which treats our system as one additional annotator.</p><p>We conducted the experiments based on the manually assigned labels of up to five annota- tors. If a sentence has been annotated by multiple users, we use the label that has been assigned by the majority; in case of a tie, we ignore the sen- tence. These so created gold standard annotations were used in an iterative cross-validation setting: for each query and the corresponding annotated sentences we ran nine cross-validation configura- tions, ranging from a 10/90 split between training and testing data to a 90/10 split, to investigate the reliability of the classifier as well as its ability to achieve usable results with little training data.</p><p>For It-clefts, we observe that elaborate queries already have a high precision, on which the SVM improves only marginally. The query</p><formula xml:id="formula_1">"It" /VCC[] [pos="NP0"]+ /RC[] 4 (it17)</formula><p>already yields a precision of 0.9598, which does not increase using our method (using 10% as train- ing data, comparing the precision for the remain- ing 90%). However, while broader queries yield lower precision, the gain by using the SVM be- comes significant <ref type="table">(Table 1)</ref>, as exemplified by the precision improvement from 0.4919 to 0.7782 for the following It-cleft query, even at a 10/90 split.</p><formula xml:id="formula_2">"It" /VCC[] /NP[] /RC[] 5 (it2)</formula><p>For other inspected types of NCC, even elaborate queries yield a low baseline precision, which our approach can improve significantly. This effect can be observed for example in the following NP- preposing query, where precision can be improved from 0.3946 to 0.5871. We conducted a cursory, "real-world" test re- garding the speed of the ranking system. 7 Training the SVM on differently sized subsets of the 449 sentences returned by a test query, we measured the time from clicking the Rank results button until the process was complete and the GUI had updated to reorder the sentences (i.e. including database queries, training, classifying, GUI update). The process times averaged over five "runs" for each training set size (20%, 50% and 90%) amount to 5 seconds, 7 seconds, and 14 seconds respectively. This leaves us with the preliminary impression that our system is fast enough for small to medium it2 it17 it33 np34 np55 np76 pp42 pp99 pp103</p><p>Baseline 0.4919 0.9598 0.7076 0.4715 0.3946 0.4985 0.7893 0.4349 0.2365 SVM, 10/90 0.7782 0.9598 0.7572 0.5744 0.5871 0.5274 0.8152 0.8357 0.8469 SVM, 50/50 0.8517 0.9608 0.8954 0.6410 0.6872 0.6193 0.8657 0.8769 0.8720 SVM, 90/10 0.8634 0.9646 0.9261 0.6822 0.7723 0.6806 0.8865 0.8820 0.8796 <ref type="table">Table 1</ref>: Precision for various NCC queries (Baseline) and for using the SVM with 10%, 50% and 90% training data. sized training sets; as the last result suggests, for larger sets it would be desirable for our system to be faster overall. One way to achieve this is to pre-compute the feature vectors used in the train- ing phase once -this could be done at the same time with the parsing of the sentences, i.e. at the setup time of the system.</p><p>Krippendorff's α, an inter-annotator agreement (IAA) measure which usually assumes values be- tween 0 (no reliable agreement) and 1 (perfect agreement), amounts to 0.8207 averaged over all manually created It-cleft annotations. If we inter- pret the SVM as an additional annotator (α svm ), the IAA drops to 0.5903. At first glance this seems quite low, however upon closer inspection this can be explained by an overfitting of the clas- sifier. This effect occurs for the already precise baseline queries, where in some cases less than 5% of the query results were labeled as Wrong. The same holds for NP-preposing (α: 0.6574, α svm : 0.3835) and PP-inversion (α: 0.9410, α svm : 0.6964). We interpret this as the classifier being successful in helping the annotators after a brief training phase identifying additional occur- rences of particular variants of a phenomenon as covered by the queries, but not easily generalizing to variants substantially different from those cov- ered by the queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>With automatic ranking we introduced an exten- sion to the annotation-by-query workflow which facilitates manual, selective annotation of large corpora. We explained the benefits of in-tool learning to this task and our extension of an open- source tool to incorporate this functionality. Fi- nally, we showed the applicability of the concept and its implementation to the task of finding non- canonical constructions.</p><p>For future work, we plan to speed up the learn- ing process (e.g. by saving feature vectors instead of re-calculating them), and also add the ability for users to configure the features used to train the classifier, e.g. incorporating lemmata or named entities instead of only using the parse tree. In- tegrating such configuration options in an easily understandable and user-friendly fashion may not be trivial but can help to generalize the approach to support additional kinds of sentence level anno- tation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>https://dkpro.github.io/dkpro-csniper in the recently published open-source tool CSniper (Eckart de Castilho et al., 2012).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Annotation-by-query workflow extended with a ranking step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A screenshot showing the results table after the ranking process, with sentences sorted by confidence of the classifier (Score). The results are shown in a keywords-in-context (KWIC) view, separating left context, query match and right context (within a range of one sentence). Clicking on (Correct) changes the label to Correct.</figDesc><graphic url="image-1.png" coords="3,72.00,62.81,453.55,86.01" type="bitmap" /></figure>

			<note place="foot" n="2"> It-cleft example query: &quot;It&quot;, followed by a form of &quot;to be&quot;, an optional determiner and a common noun.</note>

			<note place="foot" n="3"> CSniper and the used SVM implementation are language independent, which allowed us to also run additional preliminary tests using German data.</note>

			<note place="foot" n="4"> &quot;It&quot;, verb clause, one or more proper nouns, relative clause. VCC, NC, and RC are macros we defined in CQP, see Table 2. 5 &quot;It&quot;, verb clause, noun phrase, relative clause. 6 One to two nouns, personal pronoun other than &quot;I&quot;, verb. 7 System configuration: Intel i5 2,4 GHz, 2GB RAM, SSD 3GB/s, Linux in a VM</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Pia Gerhard, Sabine Bartsch, Gert Webelhuth, and Janina Rado for an-notating and testing. Furthermore we would like to thank Janina Rado for creating the CQP macros used in the tests.</p><p>This work has been supported by the Ger-man Federal Ministry of Education and Re-search (BMBF) under the promotional reference 01UG1416B (CEDIFOR), by the German Insti-tute for Educational Research (DIPF) as part of the graduate program "Knowledge Discovery in Scientific Literature" (KDSL), and by the Volk-swagen Foundation as part of the Lichtenberg-Professorship Program under grant No. I/82806.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A broad-coverage collection of portable NLP components for building shareable analysis pipelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Eckart De Castilho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on OIAF4HLT at COLING 2014</title>
		<meeting>the Workshop on OIAF4HLT at COLING 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CSniper: Annotationby-query for Non-canonical Constructions in Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Eckart De Castilho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Bartsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2012</title>
		<meeting>ACL 2012<address><addrLine>Stroudsburg, PA, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="85" to="90" />
		</imprint>
	</monogr>
	<note>System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Twenty-first century corpus workbench: Updating a query architecture for the new millennium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Evert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CL2011</title>
		<meeting>CL2011<address><addrLine>Birmingham, UK. Shortcut Expansion VCC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>pos=&quot;VBB&quot; | pos=&quot;VBD&quot; | pos=&quot;VBZ&quot;]* [lemma=&quot;be&quot;]) | ([pos=&quot;V.*&quot;]* [pos=&quot;VBG&quot; | pos=&quot;VBI&quot; | pos=&quot;VBN&quot;]* [lemma=&quot;be</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">NP</title>
		<imprint/>
	</monogr>
	<note>pos=&quot;AT0&quot;]? []? [pos=&quot;AJ.*&quot;]* [pos=&quot;N.*&quot;</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rc</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>pos=&quot;DTQ&quot; | pos=&quot;PNQ&quot; | pos=&quot;CJT&quot;] /VCF[] []?) | ([pos=&quot;CJT&quot;]? /NP[] /VCF[] []?) | ([pos=&quot;PR.*&quot;]* [pos=&quot;.Q&quot;] /NP[] /VCF</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vcf [pos=&amp;quot;v.?b&amp;quot; | Pos=&amp;quot;v.?d&amp;quot; | Pos=</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>pos=&quot;VM0&quot;] [pos=&quot;V.*&quot;]*</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Table 2: CQP macro expansions for self-defined macros</title>
		<ptr target="http://www.natcorp.ox.ac.uk/docs/c5spec.html" />
	</analytic>
	<monogr>
		<title level="m">BNC uses the CLAWS5 tagset for POS tags</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Investigating the Effects of Selective Sampling on the Annotation Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><forename type="middle">Alex</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL 2005</title>
		<meeting>CoNLL 2005<address><addrLine>Stroudsburg, PA, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The Cambridge Grammar of the English Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rodney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">K</forename><surname>Huddleston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pullum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Making large-scale support vector machine learning practical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods</title>
		<editor>Bernhard Schölkopf, Christopher J. C. Burges, and Alexander J. Smola</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="169" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate Unlexicalized Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2003</title>
		<meeting>ACL 2003<address><addrLine>Stroudsburg, PA, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">WordFreak: An open tool for linguistic annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Lacivita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL HLT 2003, NAACLDemonstrations</title>
		<meeting>NAACL HLT 2003, NAACLDemonstrations<address><addrLine>Stroudsburg, PA, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="17" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Making Tree Kernels Practical for Natural Language Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL 2006</title>
		<meeting>EACL 2006<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Closing the Loop: Fast, Interactive Semi-supervised Annotation with Queries on Features and Instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2011</title>
		<meeting>EMNLP 2011<address><addrLine>Stroudsburg, PA, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1467" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The British National Corpus, version 3 (BNC XML Edition)</title>
		<ptr target="http://www.natcorp.ox.ac.uk/" />
	</analytic>
	<monogr>
		<title level="m">Distributed by Oxford University Computing Services on behalf of the BNC Consortium</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic Annotation Suggestions and Custom Annotation Layers in WebAnno</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Seid Muhie Yimam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Eckart De Castilho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2014, System Demonstrations</title>
		<meeting>ACL 2014, System Demonstrations<address><addrLine>Stroudsburg, PA, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="91" to="96" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
