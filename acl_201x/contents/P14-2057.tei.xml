<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tri-Training for Authorship Attribution with Limited Training Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 23-25</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyun</forename><surname>Qian</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Software Eng</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Hubei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
							<email>liub@cs.uic.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Dept. of Computer Sci-ence, Univ. of Illinois at Chicago IL</orgName>
								<address>
									<postCode>60607</postCode>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer School</orgName>
								<orgName type="laboratory">State Key Laboratory of Software Eng</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Hubei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Peng</surname></persName>
							<email>peng@whu.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Hubei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tri-Training for Authorship Attribution with Limited Training Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="345" to="351"/>
							<date type="published">June 23-25</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Authorship attribution (AA) aims to identify the authors of a set of documents. Traditional studies in this area often assume that there are a large set of labeled documents available for training. However, in the real life, it is often difficult or expensive to collect a large set of labeled data. For example, in the online review domain, most reviewers (authors) only write a few reviews, which are not enough to serve as the training data for accurate classification. In this paper, we present a novel three-view tri-training method to iteratively identify authors of unlabeled data to augment the training set. The key idea is to first represent each document in three distinct views, and then perform tri-training to exploit the large amount of un-labeled documents. Starting from 10 training documents per author, we systematically evaluate the effectiveness of the proposed tri-training method for AA. Experimental results show that the proposed approach outperforms the state-of-the-art semi-supervised method CNG+SVM and other baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Existing approaches to authorship attribution (AA) are mainly based on supervised classifica- tion <ref type="bibr" target="#b27">(Stamatatos, 2009</ref><ref type="bibr" target="#b14">, Kim et al., 2011</ref><ref type="bibr" target="#b24">, Seroussi et al., 2012</ref>. Although this is an effective ap- proach, it has a major weakness, i.e., for each author a large number of his/her articles are needed as the training data. This is possible if the author has written a large number of articles, but will be difficult if he/she has not. For example, in the online review domain, most authors (review- ers) only write a few reviews (documents). It was shown that on average each reviewer only has 2.72 reviews in amazon.com, and only 8% of the reviewers have at least 5 reviews (Jindal and <ref type="bibr" target="#b12">Liu, 2008)</ref>. The small number of labeled documents makes it extremely challenging for supervised learning to train an accurate classifier.</p><p>In this paper, we consider AA with only a few labeled examples. By exploiting the redundancy in human languages, we tackle the problem using a new three-view tri-training algorithm (TTA). Specifically, we first represent each document in three distinct views, and then tri-train three clas- sifiers in these views. The predictions of two classifiers on unlabeled examples are used to augment the training set for the third classifier. This process repeats until a termination condition is met. The enlarged labeled sets are finally used to train classifiers to classify the test data.</p><p>To our knowledge, no existing work has ad- dressed AA in a tri-training framework. The AA problem with limited training data was attempted in <ref type="bibr" target="#b26">(Stamatatos, 2007;</ref><ref type="bibr" target="#b18">Luyckx and Daelemans, 2008)</ref>. However, neither of them used a semi- supervised approach to augment the training set with additional documents. Kourtis and Stama- tatos (2011) introduced a variant of the self- training method in <ref type="bibr" target="#b20">(Nigam and Ghani, 2000</ref>). Note that the original self-training uses one clas- sifier on one view. However, the self-training method in ( <ref type="bibr" target="#b16">Kourtis and Stamatatos, 2011</ref>) uses two classifiers (CNG and SVM) on one view. Both the self-training and tri-training are semi- supervised learning methods. However, the pro- posed approach is not a simple extension of the self-training method CNG+SVM of ( <ref type="bibr" target="#b16">Kourtis and Stamatatos, 2011</ref>). There are key differences.</p><p>First, in their experimental setting, about 115 and 129 documents per author on average are used for two experimental corpora. This number of labeled documents is still very large. We con- sider a much more realistic problem, where the size of the training set is very small. Only 10 samples per author are used in training.</p><p>Second, CNG+SVM uses two learning methods on a single character n-gram view. In contrast, besides the character n-gram view, we also make use of the lexical and syntactic views. That is, three distinct views are used for building classi- fiers. The redundant information in human lan- guage is combined in the tri-training procedure.</p><p>Third, in each round of self-training in CNG+SVM, each classifier is refined by the same newly labeled examples. However, in the pro- posed tri-training method (TTA), the examples labeled by the classifiers of every two views are added to the third view. By doing so, each classi- fier can borrow information from the other two views. And the predictions made by two classifi- ers are more reliable than those by one classifier.</p><p>The main contribution of this paper is thus the proposed three-view tri-training scheme which has a much better generalization ability by ex- ploiting three different views of the same docu- ment. Experimental results on the IMDb review dataset show that the proposed method dramati- cally improves the CNG+SVM method. It also outperforms the co-training method <ref type="bibr" target="#b1">(Blum and Mitchell, 1998</ref>) based on our proposed views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Existing AA methods either focused on finding suitable features or on developing effective techniques. Example features include function words ( <ref type="bibr" target="#b0">Argamon et al., 2007</ref>), richness features (Gamon 2004), punctuation frequencies ( <ref type="bibr" target="#b7">Graham et al., 2005</ref>), character <ref type="bibr" target="#b8">(Grieve, 2007)</ref>, word <ref type="bibr" target="#b2">(Burrows, 1992)</ref> and POS n-grams <ref type="bibr" target="#b6">(Gamon, 2004;</ref><ref type="bibr" target="#b11">Hirst and Feiguina, 2007)</ref>, rewrite rules ( <ref type="bibr" target="#b9">Halteren et al., 1996)</ref>, and similarities (Qian and Liu, 2013). On developing effective learning techniques, supervised classification has been the dominant approach, e.g., neural networks ( <ref type="bibr" target="#b7">Graham et al., 2005;</ref>), decision tree (Uzuner and <ref type="bibr" target="#b28">Katz, 2005;</ref><ref type="bibr" target="#b29">Zhao and Zobel, 2005</ref>), logistic regression ( <ref type="bibr" target="#b19">Madigan et al., 2005</ref>), SVM ( <ref type="bibr" target="#b5">Diederich et al., 2000;</ref><ref type="bibr" target="#b6">Gamon 2004;</ref><ref type="bibr" target="#b14">Kim et al., 2011</ref>), etc.</p><p>The main problem in the traditional research is the unrealistic size of the training set. A size of about 10,000 words per author is regarded as a reasonable training set size ( <ref type="bibr" target="#b0">Argamon et al., 2007</ref><ref type="bibr">, Burrows, 2003</ref>). When no long documents are available, tens or hundreds of short texts are used <ref type="bibr" target="#b10">(Halteren, 2007;</ref><ref type="bibr" target="#b11">Hirst and Feiguina, 2007;</ref><ref type="bibr" target="#b23">Schwartz et al., 2013)</ref>.</p><p>Apart from the existing works dealing with limited data discussed in the introduction, our preliminary study in ( <ref type="bibr" target="#b22">Qian et al., 2014</ref>) used one learning method on two views, but it is inferior to the proposed method in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Tri-Training Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Framework</head><p>We represent each document in three feature views: the character view, the lexical view and the syntactic view. Each view consists of a set of features in the respective type. A classifier can be learned from any of these views. We propose a three-view training algorithm to deal with the problem of limited training data. Logistic regression (LR) is used as the learner. The overall framework is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Given the labeled, unlabeled, and test sets L, U, and T, step 1 extracts the character, lexical, and syntactic views from L, U, and T, respectively. Steps 2-13 iteratively tri-train three classifiers by adding the data which are assigned the same label by two classifiers into the training set of the third classifier. The algorithm first randomly selects u unlabeled documents from U to create a pool U' of examples. Note that we can directly select from the large unlabeled set U. However, it is shown in <ref type="bibr">(Blum and Mitchell 2008</ref>) that a smaller pool can force the classifiers to select instances that are more representative of the underlying distribution that generates U. Hence we set the parameter u to a size of about 1% of the whole unlabeled set, which allows us to observe the effects of different number of iterations. It then iterates over the following steps. First, use character, lexical and syntactic views on the current labeled set to train three classifiers C 1 , C 2 , and C 3 . See Steps 4-9. Second, Input: A small set of labeled documents L = {l1,…, lr}, a large set of unlabeled documents U = {u1,…, us}, and a set of test documents T = {t1,…, tt}, Parameters: the number of iterations k, the size of selected un- labeled documents u Output: tk's class assignment 1 Extract views Lc, Ll, Ls, Uc, Ul, Us, Tc, Tl, Ts from L, U, T 2 Loop for k iterations: 3</p><p>Randomly select u unlabeled documents U' from U; 4</p><p>Learn the first view classifier C1 from L1 (L1=Lc, Ll, or Ls); 5</p><p>Use C1 to label docs in U' based on U1(U1=Uc, Ul, or Us) 6</p><p>Learn the second view classifier C2 from L2 (L2L1) 7</p><p>Use C2 to label documents in U' based on U2 (U2U1); 8</p><p>Learn the third view classifier C3 from L3 (L2L1, L2) 9</p><p>Use C3 to label documents in U' based on U3 (U2U1, U2);  allow two of these three classifiers to classify the unlabeled set U' and choose p documents with agreed labels. See Steps 10-12. The selected documents are then added to the third labeled set for the label assigned (a label is an author here), and the u documents are removed from the unlabeled pool U' (line 13). We call this way of augmenting the training sets InterAdding. The one used in ( <ref type="bibr" target="#b16">Kourtis and Stamatatos, 2011</ref>) is called SelfAdding as it uses only a single view and adds to the same training set. Steps 14-15 assign the test document to a category (author) using the classifier learned from the three views in the augmented labeled data, respectively.</p><formula xml:id="formula_0">10 Up1 = {u | u U', u.label by C2 = u.label by C3}; 11 Up2 = {u | u U', u.label by C1 = u.label by C3}; 12 Up3 = {u | u U', u.label by C1 = u.label by C2}; 13 U = U -U', Li = Li  Upi (i=1</formula><p>Step 16 aggregates the results from three classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Character View</head><p>The features in the character view are the character n-grams of a document. Character n- grams are simple and easily available for any natural language. For a fair comparison with the previous work in ( <ref type="bibr" target="#b16">Kourtis and Stamatatos, 2011)</ref>, we extract frequencies of 3-grams at the character-level. The vocabulary size for character 3-grams in our experiment is 28584.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Lexical View</head><p>The lexical view consists of word unigrams of a document. We represent each article by a vector of word frequencies. The vocabulary size for unigrams in our experiment is 195274.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Syntactic View</head><p>The syntactic view consists of the syntactic features of a document. We use four content- independent structures including n-grams of POS tags (n = 1..3) and rewrite rules <ref type="bibr" target="#b14">(Kim et al., 2011</ref>). The vocabulary sizes for POS 1-grams, POS 2-grams, POS 3-grams, and rewrite rules in our experiment are <ref type="bibr">63, 1917, 21950, and 19240</ref>, respectively. These four types of syntactic structures are merged into a single vector. Hence the syntactic view of a document is represented as a vector of 43140 components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Aggregating Results from Three Views</head><p>In testing, once we obtain the prediction values from three classifiers for a test document t k , an additional algorithm is used to decide the final author attribution. One simple method is voting. However, this method is weaker than the three methods below. It is also hard to compare with the self-training method CNG+SVM in ( <ref type="bibr" target="#b16">Kourtis and Stamatatos, 2011</ref>) as it only has two classifi- ers. Hence we present three other strategies to further aggregate the results from the three views. These methods require the classifier to produce a numeric score to reflect the positive or negative certainty. Many classification algo- rithms give such scores, e.g., SVM and logistic regression. The three methods are as follows: 1) ScoreSum: The learned model first classifies all test cases in T. Then for each test case t k , this method sums up all scores of positive classifications from the three views. It then assigns t k to the author with the highest score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) ScoreSqSum: This method works similarly to</head><p>ScoreSum above except that it sums up the squared scores of positive classifications. 3) ScoreMax: This method works similarly to the ScoreSum method as well except that it finds the maximum classification score for each test document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>We now evaluate the proposed method. We use logistic regression (LR) with L2 regularization <ref type="bibr" target="#b4">(Fan et al., 2008</ref>) and the SVM multiclass (SVM) system (Joachims, 2007) with its default settings as the classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>We conduct experiments on the IMDb dataset ( <ref type="bibr" target="#b25">Seroussi et al., 2010</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline methods</head><p>We use six self-training baselines and three co- training baselines. Self-training in ( <ref type="bibr" target="#b16">Kourtis and Stamatatos, 2011</ref>) uses two different classifiers on one view, and co-training uses one classifier on two views. All baselines except CNG+SVM on the character view are our extensions. Self-training using CNG+SVM on character, lexical and syntactic views respectively: This gives three baselines. It self-trains two classifi- ers from the character 3-gram, lexical, and syn- tactic views using CNG and SVM classifiers ( <ref type="bibr" target="#b16">Kourtis and Stamatatos, 2011)</ref>. CNG is a pro- file-based method which represents the author as the N most frequent character n-grams of all his/her training texts. The original method ap- plied only CNG and SVM on the character n- gram view. Since our results show that its per- formance is extremely poor, we are curious what the reason is. Can this be due to the clas- sifier or to the view? In order to differentiate the effects of views and classifiers, we present two additional types of baselines. The first type is to extend CNG+SVM method to lexical and syntactic views as well. The second type is to extend CNG+SVM method by replacing CNG with LR to show a fair comparison with our framework. Self-training using LR+SVM on character, lexi- cal, and syntactic views: This is the second type extension. It also gives us three baselines. It again uses the character, lexical and syntac- tic view and SVM as one of the two classifiers. The other classifier uses LR rather than CNG. Co-training using LR on Char+Lex, Char+Syn, and Lex+Syn views: This also gives us three baselines. Each baseline co-trains two classifi- ers from every two views of the character 3- gram, lexical, and syntactic views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and analysis (1) Effects of learning algorithms</head><p>We first evaluate the effects of learning algo- rithms on tri-training. We use SVM and LR as the learners as they are among the best methods. The effects of SVM and LR on tri-training are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. For the aggregation results, we draw the curves for ScoreSum. The results for other two stratigies are similar. It is clear that LR outperforms SVM by a large margin for tri- training when the number of iterations (k) is small. One possible reason is that LR is more tolerant to over-fitting caused by the small number of training samples. Hence, we use LR for tri-training in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(2) Effects of aggregation strategies</head><p>We show the effects of the three proposed aggregation strategies. <ref type="table" target="#tab_2">Table 1</ref> indicates that ScoreSum (SS) is the best. We also observe that both ScoreSum and ScoreSqSum (SQ) perform better than ScoreMax (SM) and all single view cases. This suggests that the decision made from a number of scores is much more reliable than that made from only one score. ScoreSum is our default strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(3) Effects of data augmenting strategies</head><p>We now see the effects of data adding methods to augment the labeled set in <ref type="figure">Fig. 3</ref>. <ref type="figure">Figure 3</ref>. Effects of data augmenting methods on tri-training We use two strategies. One is our InterAdding approach and the other is the SelfAdding approach in ( <ref type="bibr" target="#b16">Kourtis and Stamatatos, 2011</ref>), as introduced in Section 3.1. We can see that by adding newly classified samples by two classifiers to the third view, tri-training gets better and better results rapidly. For example, the accuracy for k = 10 iterations grows from 61.24 for SelfAdding to 78.82 for InterAdding, an absolute increase of 17.58%. This implies that by integrating more information from other views, learning can improve greatly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(4) Comparison with self-training baselines</head><p>We show the results of CNG+SVM in classify any test case. Its accuracy is only 1.26% at the start. This directly leads to the failure of the self-training. The reason is that the other classifier SVM can augment nearly 0 documents from the unlabeled set. We also tuned the param- eter N for CNG, but it makes little difference.    <ref type="table" target="#tab_6">Table 3</ref>, we can also see that our tri- training approach outperforms all self-training baselines by a large margin. For example, the accuracy for LR+SVM on the lexical view is 89.31%.Although this is the best for self-training, it is worse than 93.15% of tri-training.</p><p>The reason that self-training does not work well in general is the following: When the train- ing set is small, the available data may not reflect the true distribution of the whole data. Then clas- sifiers will be biased and their classifications will be biased too. In testing, the biased classifiers will not have good accuracy. However, in tri- training, and co-training, each individual view may be biased but the views are independent. Then each view is more likely to produce ran- dom samples for the other views and thus reduce the bias of each view as the iterations progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(5) Comparison with co-training baselines</head><p>We now compare tri-training with co-training <ref type="bibr" target="#b1">(Blum and Mitchell, 1998</ref>) in <ref type="table" target="#tab_8">Table 4</ref>. Again, tri- training beats co-training consistently. The best performance of co-training is 92.81% achieved on the character and lexical views after 60 itera- tions. However, the accuracy is worse than that of tri-training. The key reason is that tri-training considers three views, while co-training uses on- ly two. Also, the predictions by two classifiers are more reliable than those by one classifier.  In <ref type="bibr" target="#b22">(Qian, et al., 2014</ref>), we systematically inves- tigated the effects of learning methods and views using a special co-training approach with two views. Learning was applied on two views but the data augmentation method was like that in self-training. The best result there was 91.23%, worse than 92.81% of co-training here in <ref type="table" target="#tab_8">Table 4</ref>, which is worse than 93.15% of Tri-Training.</p><p>Overall, Tri-training performs the best and co- training is better than self-training and co-self- training. This indicates that learning on different views can better exploit the redundancy in texts to achieve superior classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we investigated the problem of au- thorship attribution with very few labeled exam- ples. A novel three-view tri-training method was proposed to utilize natural views of human lan- guages, i.e., the character, lexical and syntactic views, for classification. We evaluated the pro- posed method and compared it with state-of-the- art baselines. Results showed that the proposed method outperformed all baseline methods.</p><p>Our future work will extend the work by in- cluding more views such as the stylistic and vo- cabulary richness views. Additional experiments will also be conducted to determine the general behavior of the tri-training approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The tri-training algorithm (TTA)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Effects of SVM and LR on tri-training</figDesc><graphic url="image-2.png" coords="4,76.35,573.20,203.80,91.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>C2, C3 from L1, L2, L3; 15 Use Ci to label tk in Ti (i=1..3); 16 Aggregate results from three views</head><label></label><figDesc></figDesc><table>..3); 
14 Learn three classifiers C1, </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 . Effects of three aggregation strategies:</head><label>1</label><figDesc></figDesc><table>ScoreMax(SM), ScoreSum(SS), and ScoreSq-Sum(SQ) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>It 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 . Results for the CNG+SVM baselinewe replace CNG with LR and apply LR+SVM to</head><label>2</label><figDesc>all three views. We only show their best results in Table 3, either on a single view or aggregation. The details are omitted due to space limitations. We can see significant improvements over their corresponding results of CNG+SVM. This demonstrates that the learning methods are critical to self-training as well.</figDesc><table>To distinguish the effects of views from classi-
fiers, we conduct two more types of experiments. 
First, we apply CNG+SVM to the lexical and 
syntactic views. The results are even worse. Its 
accuracy drops to 0.58% and 1.21%, respectively. 
Next, k 
Tri 
Train 

SelfTrain:CNG+SVM SelfTrain:LR+SVM 
Char lex 
Syn Char Lex 
Syn 
0 
46.85 33.22 45.44 34.50 33.22 45.75 34.48 
10 78.82 32.47 45.44 34.50 62.56 73.78 51.94 
20 86.19 32.47 45.44 34.09 71.21 81.44 59.88 
30 89.69 32.47 45.44 34.09 75.21 84.68 63.70 
40 91.52 33.69 45.44 34.09 77.46 88.25 65.74 
50 92.58 33.69 45.44 34.09 78.64 88.25 67.45 
60 93.15 33.69 45.44 34.09 79.54 89.31 68.37 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 . Self-training variations</head><label>3</label><figDesc></figDesc><table>From </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>Co-training vs. tri-training 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the NSFC projects <ref type="bibr">(61272275,</ref><ref type="bibr">61232002,</ref><ref type="bibr">61379044)</ref>, and the 111 project (B07037).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stylistic text classification using functional lexical features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Whitelaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Hota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levitan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JASIST</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="802" to="822" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT. pp</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Not unless you ask nicely: The interpretative nexus between analysis and information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burrows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Literary and Linguistic Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="91" to="109" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">All the way through: Testing for authorship in different frequency data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burrows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LLC</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="27" to="47" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K-W.</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X-R.</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Authorship attribution with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diederich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kindermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Leopold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Paass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Informationstechnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D-S.</forename><surname>Augustin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="109" to="123" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Linguistic correlates of style: authorship classification with deep linguistic analysis features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segmenting documents by stylistic character</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="397" to="415" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quantitative authorship attribution: An evaluation of techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grieve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LLC</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="251" to="270" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Outside the cave of shadows: using syntactic annotation to enhance authorship attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tweedie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Baayen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Literary and Linguistic Computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="121" to="132" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Author verification by linguistic profiling: An exploration of the parameter space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TSLP</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bigrams of syntactic labels for authorship discrimination of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Feiguina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LLC</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="405" to="417" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Opinion spam and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM. pp</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="29" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<ptr target="www.cs.cornell.edu/people/tj/svmlight/old/svmmulticlassv2.12.html" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Authorship classification: a discriminative syntactic tree mining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="455" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. pp</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Author identification using semi-supervised learning. In: Notebook for PAN at CLEF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kourtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">From fingerprint to writeprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="76" to="82" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Authorship attribution and verification with many authors and limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luyckx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING. pp</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Author Identification on the Large Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Genkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fradkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CSNA</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analyzing the effectiveness and applicability of co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ghani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CIKM</title>
		<meeting>of CIKM</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="86" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Identifying Multiple Userids of the Same Author</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1124" to="1135" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>He</surname></persName>
		</author>
		<title level="m">Co-Training on Authorship Attribution with Very Few Labeled Examples: Methods. vs. Views. In SIGIR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Authorship Attribution of Micro-Messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rappoport</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1880" to="1891" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Authorship attribution with author-aware topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Seroussi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bohnert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zukerman</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="264" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Collaborative inference of sentiments from texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Seroussi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zukerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bohnert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Author identification using imbalanced and limited training texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TIR. pp</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="237" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey of modern authorship attribution methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JASIST</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="538" to="556" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A comparative study of language models for book and author recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ö</forename><surname>Uzuner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd IJCNLP</title>
		<meeting>of the 2nd IJCNLP</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="969" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Effective and scalable authorship attribution using function words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Information Retrival Technology</title>
		<meeting>of Information Retrival Technology</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="174" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A framework for authorship identification of online messages: Writing style features and classification techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JASIST</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="378" to="393" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
