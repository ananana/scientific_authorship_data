<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Extraction of Pseudo-Parallel Sentences from Raw Monolingual Data Using Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Marie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Fujita</surname></persName>
						</author>
						<title level="a" type="main">Efficient Extraction of Pseudo-Parallel Sentences from Raw Monolingual Data Using Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="392" to="398"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2062</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a new method for extracting pseudo-parallel sentences from a pair of large monolingual corpora, without relying on any document-level information. Our method first exploits word embed-dings in order to efficiently evaluate trillions of candidate sentence pairs and then a classifier to find the most reliable ones. We report significant improvements in domain adaptation for statistical machine translation when using a translation model trained on the sentence pairs extracted from in-domain monolingual corpora.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Parallel corpus is an indispensable resource for statistical and neural machine translation. Gener- ally, using more sentence pairs to train a transla- tion system makes it able to produce better trans- lations. However, for most language pairs and do- mains, parallel corpora remain scarce due mainly to the cost of their creation <ref type="bibr" target="#b10">(Germann, 2001</ref>).</p><p>In the last two decades, numerous methods have been proposed to extract parallel sentences from comparable corpora. In addition to compa- rable corpora in large quantity, to the best of our knowledge, all previous methods heavily rely on document-level information and/or lexical trans- lation models, such as those for statistical ma- chine translation (SMT) systems <ref type="bibr" target="#b26">(Zhao and Vogel, 2002;</ref><ref type="bibr" target="#b9">Fung and Cheung, 2004;</ref><ref type="bibr" target="#b20">Munteanu and Marcu, 2005;</ref><ref type="bibr" target="#b23">Tillmann and Xu, 2009</ref>) and manually-created bilingual lexicon <ref type="bibr" target="#b24">(Utiyama and Isahara, 2003)</ref>. The most successful approaches use cross-lingual information retrieval techniques <ref type="bibr" target="#b0">(Abdul Rauf and Schwenk, 2011;</ref><ref type="bibr" target="#b22">S , tef˘ anescu et al., 2012</ref>) to extract sentence pairs from com- parable documents. Using such document pairs has the strong advantage that it drastically reduces the search space; we need to consider only sen- tence pairs in each document pair instead of scor- ing all sentence pairs in the two monolingual cor- pora. However, in many cases, we do not have access to document-level information. Only Till- mann and <ref type="bibr" target="#b23">Xu (2009)</ref> have explored this scenario using efficient caching strategies to extract use- ful sentence pairs from nearly one trillion candi- dates in comparable data. Yet, their approach is tightly related to the exploitation of accurate lex- ical translation models and does not allow us to introduce other features. The reliance on lexical translation models implies that we must have al- ready access to parallel data sufficiently large for obtaining accurate estimates. Nevertheless, the most useful sentence pairs for SMT are actually the ones that contain infrequent or even unseen to- kens in these parallel data. Relying only on lexical translation models thus seems rather inadequate to extract sentence pairs containing numerous infre- quent or unseen tokens, and may actually be more prone to extract sentence pairs that contain words and phrases for which we already have accurate translation probability estimates.</p><p>This paper proposes a new method that exploits word embeddings to efficiently extract pseudo- parallel sentences 1 from raw monolingual data without using any document-level information. We report significant improvements of translation quality in a domain adaptation scenario for SMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Sentence pair extraction</head><p>During the sentence pair extraction, we do not assume an access to document-level information. Our method thus has to be efficient in evaluating trillions of sentence pairs hypothesized from two monolingual corpora, each containing millions of sentences. To achieve this computationally chal- lenging task, we need a fast way to compute some similarity between the source and target sentences, without relying on large lexical translation models that may not be available or accurate enough in some low-resourced conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Step 1: Filtering with sentence embeddings</head><p>Assuming the availability of large-scale mono- lingual data, our method exploits word embed- dings ( <ref type="bibr" target="#b19">Mikolov et al., 2013b</ref>) that are fast to es- timate. First, word embeddings for each language are learned from the given monolingual data. This enables us to evaluate arbitrary sentence pair given all the words it contains, which is not fully guaran- teed by a lexical translation model as some tokens may be out-of-vocabulary (OOV). We then pro- ceed to the projection of all the source word em- beddings to the target embedding space, following <ref type="bibr" target="#b18">Mikolov et al. (2013a)</ref>, 2 in order to represent both source and target words in the same space.</p><p>To compute the similarity between arbitrary sentence pairs, we represent each sentence by av- eraging the embeddings of its constituent words. <ref type="bibr">3</ref> As a result of this first step, our method keeps for each source sentence the n closest target sen- tences (n being small, for instance with a value of 100) according to the similarity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Step 2: Refining with a classifier</head><p>Given a far smaller search space, this second step evaluates and re-ranks the remaining sentence pairs, incorporating more complex features to train a classifier. We use a total of five features.</p><p>For each sentence pair, we use the score com- puted in the first step and a more accurate sim- ilarity score based on alignments between word embeddings, following the work in Kajiwara and 2 Despite the availability of more accurate methods <ref type="bibr" target="#b6">(Coulmance et al., 2015;</ref><ref type="bibr" target="#b7">Duong et al., 2016)</ref> we choose this method considering its low computational cost and its reasonable need of external resources to estimate the translation matrix, i.e., only a small bilingual dictionary.</p><p>3 As shown by <ref type="bibr" target="#b1">Adi et al. (2016)</ref>, this can be effective to en- code sentence-level information such as content and length, while being computationally more efficient than other meth- ods, such as inducing paragraph vectors ( <ref type="bibr" target="#b15">Le and Mikolov, 2014</ref>) and using LSTM auto-encoders ( <ref type="bibr" target="#b16">Li et al., 2015)</ref>. Our decision also relies on the promising accuracy of linear pro- jection of word (not sentence) embeddings across different languages ( <ref type="bibr" target="#b18">Mikolov et al., 2013a</ref>). <ref type="bibr" target="#b13">Komachi (2016)</ref>. They found out that the average of the cosine similarity between all the best word pairs, for each source word, taken from the sen- tence pair, shown in Eq. <ref type="formula">(1)</ref>, was a good indicator of similarity between two sentences.</p><formula xml:id="formula_0">S(x, y) = 1 |x| |x| i=1 max j φ(x emb i , y emb j ) (1)</formula><p>where x and y are respectively the source and tar- get sentences, |x| the length of x, and φ the co- sine similarity between the embeddings in the tar- get language space of the i-th word in x, i.e., x emb i , and the j-th word in y, i.e., y emb j . The computa- tion of this score can be highly costly, depending on the sentence length and the number of dimen- sions of the word embeddings. Thus, we compute this score only for the source to target direction, unlike <ref type="bibr" target="#b13">Kajiwara and Komachi (2016)</ref>.</p><p>In many situations, we may also have an access to a lexical translation model trained on some par- allel data. We therefore incorporate the scores pro- posed by <ref type="bibr" target="#b23">Tillmann and Xu (2009)</ref>, but considering one probability for each translation direction, in- stead of summing them up, so that our classifier can optimize their weight separately.</p><formula xml:id="formula_1">P (x|y) = |x| i=1 1 |x| log( 1 |y| |y| j=1 p(x tok i |y tok j )) (2) P (y|x) = |y| j=1 1 |y| log( 1 |x| |x| i=1 p(y tok j |x tok i )) (3)</formula><p>where x tok i is the i-th token in x, y tok j the j-th to- ken in y and p the probability given by an already estimated lexical translation model.</p><p>Our last feature is the length ratio of the source and target sentences (Munteanu and <ref type="bibr" target="#b20">Marcu, 2005</ref>).</p><p>To assign a real-valued score to each sentence pair in order to filter and rank them, we train a Maximum Entropy (ME) classifier, following <ref type="bibr" target="#b20">Munteanu and Marcu (2005)</ref>. ME classifier suits particularly well our situation, since we deal with a small number of dense features and have hun- dred millions of sentence pairs to classify quickly.</p><p>Positive examples for training the classifier can be obtained straightforwardly: we use true sen- tence pairs sampled from parallel data, different from the one used to train the lexical transla- tion model. As for negative examples, Munteanu and Marcu (2005) randomly paired sentences from their parallel data using two constraints: a length ratio not greater than two, and a coverage con- straint that considers a negative example only if more than half of the words of the source sentence has a translation in the given target sentence ac- cording to some bilingual lexicon. However, from a large parallel corpus, one can easily retrieve an- other target sentence, almost identical, containing most of the words that the true target sentence also contains. In this case, the negative example will be almost as semantically close as the positive one, weakening the discriminative power of the fea- tures based on word embeddings. To circumvent this problem, we generate negative examples, as many as positive examples, without using this cov- erage constraint.</p><p>Having assigned a score for each sentence pair, we make a pseudo-parallel corpus selecting the target sentence with the best score for each source sentence and retaining only the sentence pairs with a score above some threshold, th. This pseudo- parallel corpus can then be used to train a new phrase table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We evaluated our method in a scenario of domain adaptation for phrase-based SMT (PBSMT). In this scenario, we assumed a lot of general-domain parallel data to train a general-domain phrase ta- ble and a lot of in-domain monolingual data as our source of in-domain pseudo-parallel sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data and SMT system</head><p>We experimented with the French-English lan- guage pair, both translation directions, on the med- ical domain. We used Moses ( <ref type="bibr" target="#b14">Koehn et al., 2007)</ref> to train, tune, and test our PBSMT systems. The general-domain phrase table was trained on Eu- roparl V7 <ref type="bibr">4</ref> (1.99M sentences). The in-domain monolingual data were prepared by applying the NLTK 5 sentence segmenter to the concatenation of all the monolingual corpora provided for the WMT'14 medical translation task. <ref type="bibr">6</ref> As the source of extracting in-domain sentence pairs, we ran- domly sampled 1M sentences (33M tokens) from the French data and 5M sentences (164M tokens) from the English data. Given pseudo-parallel sen- tences extracted by our method from these data (see Section 3.2), we trained an in-domain phrase table. Moses exploits the two phrase tables, i.e., general-domain and in-domain ones, with its mul- tiple decoding path ability. The PBSMT systems used one language model trained on the entire tar- get in-domain monolingual data concatenated to the target side of Europarl and News Crawl data provided by WMT'15. <ref type="bibr">7</ref> The development and test data used to tune and evaluate the PBSMT sys- tems were excerpts of the EMEA parallel corpus ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameters for sentence pair extraction</head><p>We used word2vec 8 to learn word embeddings with the parameters -cbow 1 -window 10 -negative 15 -sample 1e-4 -iter 15 -min-count 1, specifying 800 and 300 dimensions for the source and target languages, respectively, 9 on the same data used to train the language models. The translation matrix used to project the source word embeddings to the target embedding space was trained on a bilingual lexicon containing the 5k 10 most frequent French tokens, <ref type="bibr">11</ref> from Europarl, and their most probable single token in English given by the Europarl phrase table. The first step of our method eval- uated five trillion (1M×5M) sentence pairs and retained the 100 closest target sentences for each source sentence.</p><p>The second step then dealt with only 100M (1M×100) sentence pairs. The lexical translation probabilities used to compute our features were given by the Europarl lexical translation models. We used Scikit-learn 12 to train the ME clas- sifier, with default parameters, on 5k positive and 5k negative examples 13 randomly generated from the MultiUn corpus. <ref type="bibr">14</ref> According to the classi- fier's score, only the 1-best target sentence for each source sentence was retained. We discarded sentence pairs having a score lower than a thresh- 7 http://statmt.org/wmt15/ 8 http://word2vec.googlecode.com/ 9 <ref type="bibr" target="#b18">Mikolov et al. (2013a)</ref> observed that a more accurate pro- jection is obtained when using a greater number of dimen- sions on the source side than that for the target side. <ref type="bibr">10</ref> Vuli´c <ref type="bibr" target="#b25">Vuli´c and Korhonen (2016)</ref> demonstrated that 5k word pairs is enough to train a useful translation matrix. <ref type="bibr">11</ref> We extracted sentence pairs regarding French and En- glish as source and target languages, respectively, but used the resulted parallel corpus for both translation directions. <ref type="bibr">12</ref>   <ref type="bibr" target="#b21">Papineni et al., 2002</ref>) averaged over 3 tuning runs, obtained when added an in-domain phrase table to the system, created either by the baseline method or by our work with or without the coverage constraint activated (denoted "w/ cov. constraint"). Bold scores indicate statistical significance (p &lt; 0.01) of the score over the baseline system, measured by approximate randomization using <ref type="bibr">MultEval (Clark et al., 2011</ref>). We also present the number of OOV tokens in the test set and the number of sentence pairs actually used to train the in-domain phrase table. The speed of the method to evaluate sentence pairs from monolingual data was measured with 100 CPU threads (Xeon E5-2600) on 1 trillion sentence pairs randomly sampled.</p><p>old value. We examined {0.5, 0.6, 0.7, 0.8, 0.9} as the threshold value through tuning PBSMT sys- tems, and determined 0.7 to be optimal.</p><p>We regarded the method proposed by <ref type="bibr" target="#b23">Tillmann and Xu (2009)</ref> as a baseline, because it does not rely on document-level information, as ours. Un- like our method, in addition to the constraint based on length ratio, this method also used the cover- age constraint. As discussed in Section 2.2, this constraint speeds up the extraction, but sacrifices source sentences with numerous OOV due to its heavy reliance on a bilingual lexicon learned from parallel data. To measure the effect of the cov- erage constraint, we also activated it in some of our experiments using our method. Then, as for our method, we discarded sentence pairs having a score lower than a threshold value and found the threshold value of -10 to be the best among {-15, -12, -10, -7}. <ref type="table">Table 1</ref> presents the results. Both the baseline and our methods outperformed the system using only the general-domain phrase table in both translation directions. This may be explained by the pres- ence of highly parallel sentences in the in-domain monolingual data, from Wikipedia articles for in- stance, that can be retrieved by both methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Our method significantly outperformed the baseline, with 1.4 and 1.7 BLEU points gains re- spectively for Fr→En and En→Fr. Our method, with the optimal threshold of 0.7, extracted 361k sentence pairs from the in-domain monolingual data, while the baseline method extracted only 121k sentence pairs due presumably to the use of the coverage constraint that might remove source sentences with a high OOV ratio. Less OOV to- kens remained with the system using our method, highlighting the positive effect of exploiting word embeddings in addition to lexical translation mod- els. Activating the coverage constraint on our method was harmful and was significantly worse than the baseline. This constraint excludes can- didate sentence pairs by relying only on general- domain lexical translation models, while our clas- sifier is trained to use word embeddings that are more robust but unhelpful to discriminate the remaining candidates. Therefore, the optimal threshold value allowed the extraction of only 11k sentence pairs. In contrast, without this constraint, even with a high threshold value of 0.955 that retrieved as many sentence pairs as the baseline method, the extracted sentence pairs resulted in a significantly higher BLEU score than the base- line method, with a slightly better lexical cover- age. Last but not least, our method is 11.9 times faster than the baseline method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Feature contribution</head><p>To evaluate the impact of the features used during classification, we performed a feature ablation ex- periment. The results for the EMEA translation   <ref type="table">Table 2</ref>: Results (BLEU) obtained without us- ing some of the features during the classification (see Section 2.2). The features removed, indepen- dently, are the following: averaged word embed- dings (avg. emb.), maximum alignment between embeddings (max. al. emb.), lexical translation probabilities (lex. prob.) and the length ratio of the source and target sentences (length). The "th" column indicates the threshold value for the clas- sifier's score above which we retain the sentence pairs. This value was selected among the val- ues {0.5,0.6,0.7,0.8,0.9} with respect to the BLEU score on the development data, through the tuning of the PBSMT system, for each configuration.</p><p>task are reported in <ref type="table">Table 2</ref>. For both translation directions, the features that have the most important were the ones based on lexical translaiton probabilities and alignments be- tween embeddings. For instance, in En→Fr trans- lation, removing them led to a significant drop of 0.4 and 0.8 BLEU points, respectively.</p><p>For the Fr→En translation direction, surpris- ingly, we observed improvements on the test set for all configurations, except when removing ei- ther of the above two types of features. However, we did not observe such improvements for the En→Fr translation direction; removing any fea- ture(s) consistently led to a lower or equal BLEU score. Feature ablation did not improve the perfor- mance on the development set for both translation directions, neither.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Classifier accuracy</head><p>To better understand the performance of our method, we also evaluated the accuracy of the classifier used in step 2 (see Section 2.2). Note that this evaluation does not intend to show how well the classifier retrieves useful pseudo-parallel sentences. We cannot directly evaluate it, as we do not have an evaluation data set that contains gold pseudo-parallel sentences at hand.</p><p>A set of in-domain truly parallel sentences was used for our evaluation. We selected the 50k first source sentences from the held-out in-domain EMEA parallel corpus, <ref type="bibr">15</ref> and used each one of them to make two sentence pairs in order to obtain a positive and a negative example. For the positive example, the source sentence is associated to its correct translation from the EMEA corpus, while for the negative example, we associated the source sentence with a target sentence randomly extracted from the EMEA corpus. The classifier has then to decide if the sentence pair is correct or incorrect.</p><p>The classifier is the same one that was presented in Section 3.2 and trained on the MultiUn parallel data. On our EMEA evaluation data set, this clas- sifier achieves an accuracy of 85.98%. This high accuracy highlights the potential of our method in retrieving highly, or truly, parallel sentences if such kinds of sentence pairs exist in the monolin- gual data exploited by our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>We presented a method for extracting pseudo- parallel sentences from a pair of large monolingual corpora, without relying on any document-level information. Our domain adaptation experiments showed that our method outperformed the state-of- the-art method by more efficiently extracting more useful sentence pairs from in-domain monolingual data. In addition to the improved BLEU scores, our method provides a better handling of OOV, ig- nored by other methods that strongly rely on al- ready trained lexical translation models.</p><p>Our method can further be speeded up by some approximation, such as local sensitive hashing, or by using a smaller number of dimensions for word embeddings. We leave the study of their impact to our future work. We believe that our work is also useful for other downstream tasks that need comparable or pseudo-parallel sentences, such as parallel phrase extraction <ref type="bibr" target="#b12">(Hewavitharana and Vogel, 2016</ref>) and adaptation of neural machine trans- lation systems ( <ref type="bibr" target="#b17">Luong and Manning, 2015;</ref><ref type="bibr" target="#b8">Freitag and Al-Onaizan, 2016</ref>).</p></div>
			<note place="foot" n="1"> As in previous work, we regard the sentence pairs extracted by our method as &quot;pseudo-parallel&quot; because they are not necessarily parallel. As shown by Goutte et al. (2012), even very noisy parallel corpora may be useful for SMT.</note>

			<note place="foot" n="4"> http://statmt.org/europarl/ 5 http://www.nltk.org/ 6 http://statmt.org/wmt14/medical-task/</note>

			<note place="foot" n="15"> We used the EMEA training data provided by the same workshop on domain adaptation (Carpuat et al., 2012) that released the development and test data used in our experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank all the reviewers for their valuable comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Parallel sentence generation from comparable corpora for improved smt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdul</forename><surname>Sadaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Rauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwenk</surname></persName>
		</author>
		<idno type="doi">10.1007/s10590-011-9114-9</idno>
		<ptr target="https://doi.org/10.1007/s10590-011-9114-9" />
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="375" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fine-grained analysis of sentence embeddings using auxiliary prediction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosshi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Kermany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1608.04207v3.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2017</title>
		<meeting>ICLR 2017</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Domain adaptation in machine translation: Final report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabienne</forename><surname>Braune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagadeesh</forename><surname>Jagarlamudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Razmara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Tamchyna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharine</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<ptr target="http://hal3.name/damt/" />
	</analytic>
	<monogr>
		<title level="j">Johns Hopkins Summer Workshop Final Report</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oregon</forename><surname>Portland</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P11-2031" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transgram, fast cross-lingual word-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Coulmance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amine</forename><surname>Benhalloum</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/D15-1131</idno>
		<ptr target="https://doi.org/10.18653/v1/D15-1131" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning crosslingual word embeddings without bilingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D16-1136" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast domain adaptation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<idno>CoRR abs/1612.06897</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mining very-non-parallel corpora: Parallel sentence and lexicon extraction via bootstrapping and em</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Cheung</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W04-3208" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Building a statistical machine translation system from scratch: How much bang for the buck can we expect?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W01-1409" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Data-Driven Methods in Machine Translation</title>
		<meeting>the ACL Workshop on Data-Driven Methods in Machine Translation<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The impact of sentence alignment errors on phrase-based machine translation performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<ptr target="http://www.mt-archive.info/AMTA-2012-Goutte.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of AMTA</title>
		<meeting>AMTA<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extracting parallel phrases from comparable data for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjika</forename><surname>Hewavitharana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<idno type="doi">10.1017/S1351324916000139</idno>
		<ptr target="https://doi.org/10.1017/S1351324916000139" />
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="549" to="573" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building a monolingual parallel corpus for text simplification using sentence similarity based on alignment between word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoyuki</forename><surname>Kajiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C16-1109" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P07-2045" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL. Prague, Czech Republic</title>
		<meeting>ACL. Prague, Czech Republic</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
		<idno>CoRR abs/1405.4053</idno>
		<ptr target="https://arxiv.org/abs/1405.4053" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno>CoRR abs/1506.01057</idno>
		<ptr target="https://arxiv.org/abs/1506.01057" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stanford neural machine translation systems for spoken language domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.mt-archive.info/15/IWSLT-2015-luong.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWSLT. Da Nang</title>
		<meeting>IWSLT. Da Nang<address><addrLine>Vietnam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno>CoRR abs/1309.4168</idno>
		<ptr target="http://arxiv.org/abs/1309.4168" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving machine translation performance by exploiting non-parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Dragos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Munteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="477" to="504" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P02-1040" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Philadelphia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hybrid parallel sentence mining from comparable corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Ion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Hunsicker</surname></persName>
		</author>
		<ptr target="http://www.mt-archive.info/EAMT-2012-Stefanescu.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of EAMT</title>
		<meeting>EAMT<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A simple sentence-level extraction algorithm for comparable data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Ming</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N09-2024" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL. Boulder</title>
		<meeting>HLT-NAACL. Boulder</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reliable measures for aligning japanese-english news articles and sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P03-1010" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the role of seed lexicons in learning bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P16-1024" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive parallel sentences mining from web bilingual news collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=844380.844785" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE ICDM</title>
		<meeting>IEEE ICDM<address><addrLine>Maebashi, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
