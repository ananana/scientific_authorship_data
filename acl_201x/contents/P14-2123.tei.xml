<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EM Decipherment for Large Vocabularies</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Nuhn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology and Pattern Recognition Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology and Pattern Recognition Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EM Decipherment for Large Vocabularies</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="759" to="764"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper addresses the problem of EM-based decipherment for large vocabularies. Here, decipherment is essentially a tagging problem: Every cipher token is tagged with some plaintext type. As with other tagging problems, this one can be treated as a Hidden Markov Model (HMM), only here, the vocabularies are large, so the usual O(N V 2) exact EM approach is infeasible. When faced with this situation, many people turn to sampling. However, we propose to use a type of approximate EM and show that it works well. The basic idea is to collect fractional counts only over a small subset of links in the forward-backward lattice. The subset is different for each iteration of EM. One option is to use beam search to do the subsetting. The second method restricts the successor words that are looked at, for each hypothesis. It does this by consulting pre-computed tables of likely n-grams and likely substitutions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The decipherment of probabilistic substitution ci- phers (ciphers in which each plaintext token can be substituted by any cipher token, following a distribution p(f |e), cf. <ref type="table">Table 2</ref>) can be seen as an important step towards decipherment for MT. This problem has not been studied explicitly be- fore. Scaling to larger vocabularies for proba- bilistic substitution ciphers decipherment is a dif- ficult problem: The algorithms for 1:1 or homo- phonic substitution ciphers are not applicable, and standard algorithms like EM training become in- tractable when vocabulary sizes go beyond a few hundred words. In this paper we present an effi- cient EM based training procedure for probabilis- tic substitution ciphers which provides high deci- pherment accuracies while having low computa- tional requirements. The proposed approach al- lows using high order n-gram language models, and is scalable to large vocabulary sizes. We show improvements in decipherment accuracy in a va- riety of experiments (including MT) while being computationally more efficient than previous pub- lished work on EM-based decipherment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several methods exist for deciphering 1:1 substi- tution ciphers: <ref type="bibr" target="#b8">Ravi and Knight (2008)</ref> solve 1:1 substitution ciphers by formulating the decipher- ment problem as an integer linear program. <ref type="bibr" target="#b0">Corlett and Penn (2010)</ref> solve the same problem us- ing A * search. <ref type="bibr" target="#b6">Nuhn et al. (2013)</ref> present a beam search approach that scales to large vocabulary and high order language models. Even though be- ing successful, these algorithms are not applicable to probabilistic substitution ciphers, or any of its extensions as they occur in decipherment for ma- chine translation.</p><p>EM training for probabilistic ciphers was first covered in <ref type="bibr" target="#b9">Ravi and Knight (2011)</ref>. <ref type="bibr" target="#b5">Nuhn et al. (2012)</ref> have given an approximation to exact EM training using context vectors, allowing to train- ing models even for larger vocabulary sizes. <ref type="bibr" target="#b10">Ravi (2013)</ref> report results on the OPUS subtitle corpus using an elaborate hash sampling technique, based on n-gram language models and context vectors, that is computationally very efficient.</p><p>Conventional beam search is a well studied topic: <ref type="bibr" target="#b2">Huang et al. (1992)</ref> present beam search for automatic speech recognition, using fine-grained pruning procedures. Similarly, <ref type="bibr" target="#b13">Young and Young (1994)</ref> present an HMM toolkit, including pruned forward-backward EM training. <ref type="bibr" target="#b7">Pal et al. (2006)</ref> use beam search for training of CRFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Publications Complexity EM Full ( <ref type="bibr" target="#b3">Knight et al., 2006</ref>), ( <ref type="bibr" target="#b9">Ravi and Knight, 2011</ref>) <ref type="table">Table 1</ref>: Different approximations to exact EM training for decipherment. N is the cipher sequence length, V the size of the target vocabulary, and n the order of the language model.</p><formula xml:id="formula_0">O(N V n ) EM Fixed Candidates (Nuhn et al., 2012) O(N ) EM Beam This Work O(N V ) EM Lookahead This Work O(N )</formula><p>The main contribution of this work is the pre- selection beam search that-to the best of our knowledge-was not known in literature before, and serves as an important step to applying EM training to the large vocabulary decipherment problem. <ref type="table">Table 1</ref> gives an overview of the EM based methods. More details are given in Sec- tion 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Probabilistic Substitution Ciphers</head><p>We define probabilistic substitutions ciphers us- ing the following generative story for ciphertext sequences f N 1 : 1. Stochastically generate a plaintext sequence e N 1 according to a bigram 1 language model. 2. For each plaintext token e n choose a substi- tution f n with probability P (f n |e n , ϑ).</p><p>This generative story corresponds to the model</p><formula xml:id="formula_1">p(e N 1 , f N 1 , ϑ) = p(e N 1 ) · p(f N 1 |e N 1 , ϑ) ,<label>(1)</label></formula><p>with the zero-order membership model</p><formula xml:id="formula_2">p(f N 1 |e N 1 , ϑ) = N n=1 p lex (f n |e n , ϑ)<label>(2)</label></formula><p>with parameters p(f |e, ϑ) ≡ ϑ f |e and normaliza- tion constraints ∀e f ϑ f |e = 1, and first-order plaintext sequence model</p><formula xml:id="formula_3">P (e N 1 ) = N n=1 p LM (e n |e n−1 ) .<label>(3)</label></formula><p>Thus, the probabilistic substitution cipher can be seen as a Hidden Markov Model. <ref type="table">Table 2</ref> gives an overview over the model. We want to find those parameters ϑ that maximize the marginal distribu- tion p(f N 1 |ϑ):</p><formula xml:id="formula_4">ϑ = arg max ϑ    [e N 1 ] p(f N 1 , e N 1 |ϑ )    (4) 1 as the Viterbi decoding arg max e N 1 p(e N 1 |f N 1 , ϑ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Exact EM training</head><p>In the decipherment setting, we are given the ob- served ciphertext f N 1 and the model p(f N 1 |e N 1 , ϑ) that explains how the observed ciphertext has been generated given a latent plaintext e N 1 . Marginaliz- ing the unknown e N 1 , we would like to obtain the maximum likelihood estimate of ϑ as specified in Equation 4. We iteratively compute the maximum likelihood estimate by applying the EM algorithm <ref type="bibr">(Dempster et al., 1977)</ref>:</p><formula xml:id="formula_5">˜ ϑ f |e = n:fn=f p n (e|f N 1 , ϑ) f n:fn=f p n (e|f N 1 , ϑ)<label>(5)</label></formula><p>with</p><formula xml:id="formula_6">p n (e|f N 1 , ϑ) = [e N 1 :en=e] p(e N 1 |f N 1 , ϑ) (6)</formula><p>being the posterior probability of observing the plaintext symbol e at position n given the cipher- text sequence f N 1 and the current parameters ϑ. p n (e|f N 1 , ϑ) can be efficiently computed using the forward-backward algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Approximations to EM-Training</head><p>The computational complexity of EM training stems from the sum</p><formula xml:id="formula_7">[e N 1 :en=e] contained in the posterior p n (e|f N 1 , ϑ).</formula><p>However, we can approx- imate this sum (and hope that the EM training procedure is still working) by only evaluating the dominating terms, i.e. we only evaluate the sum for sequences e N 1 that have the largest contribu- tions to</p><formula xml:id="formula_8">[e N Sequence of cipher tokens : f N 1 = f 1 , . . . , f N Sequence of plaintext tokens : e N 1 = e 1 , . . . , e N Joint probability : p(f N 1 , e N 1 |ϑ) = p(e N 1 ) · p(f N 1 |e N 1 , ϑ) Language model : p(e N 1 ) = N n=1 p LM (e n |e n−1 ) Membership probabilities : p(f N 1 |e N 1 , ϑ) = N n=1 p lex (f n |e n , ϑ)</formula><p>Paramater Set : ϑ = {ϑ f |e }, p(f |e, ϑ) = ϑ f |e Normalization : ∀e : <ref type="table">Table 2</ref>: Definition of the probabilistic substitution cipher model. In contrast to simple or homophonic substitution ciphers, each plaintext token can be substituted by multiple cipher text tokens. The parameter ϑ f |e represents the probability of substituting token e with token f .</p><formula xml:id="formula_9">f ϑ f |e = 1 Probability of cipher sequence : p(f N 1 |ϑ) = [e N 1 ] p(f N 1 , e N 1 |ϑ)</formula><p>acquiring zero probability in some early iteration. In order to allow the lexicon to recover from these zeros, we use a smoothed lexiconˆplexiconˆ lexiconˆp lex (f |e) = λp lex (f |e) + (1 − λ)/|V f | with λ = 0.9 when conducting the E-Step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Beam Search</head><p>Instead of evaluating the sum for terms with the exact largest contributions, we restrict ourselves to terms that are likely to have a large contribution to the sum, dropping any guarantees about the actual contribution of these terms. Beam search is a well known algorithm related to this idea: We build up sequences e c 1 with grow- ing cardinality c. For each cardinality, only a set of the B most promising hypotheses is kept. Then for each active hypothesis of cardinality c, all pos- sible extensions with substitutions f c+1 → e c+1 are explored. Then in turn only the best B out of the resulting B · V e many hypotheses are kept and the algorithm continues with the next cardinality. Reaching the full cardinality N , the algorithm ex- plored B · N · V e many hypotheses, resulting in a complexity of O <ref type="figure">(BN V e )</ref>.</p><p>Even though EM training using beam search works well, it still suffers from exploring all V e possible extensions for each active hypothesis, and thus scaling linearly with the vocabulary size. Due to that, standard beam search EM training is too slow to be used in the decipherment setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Preselection Search</head><p>Instead of evaluating all substitutions f c+1 → e c+1 ∈ V e , this algorithm only expands a fixed number of candidates: For a hypothesis ending in a language model state σ, we only look at B LM many successor words e c+1 with the highest LM probability p LM (e c+1 |σ) and at B lex many suc- cessor words e c+1 with the highest lexical prob- ability p lex (f c+1 |e c+1 ). Altogether, for each hy- pothesis we only look at (B LM + B lex ) many suc- cessor states. Then, just like in the standard beam search approach, we prune all explored new hy- potheses and continue with the pruned set of B many hypotheses. Thus, for a cipher of length N we only explore N · B · (B LM + B lex ) many hy- potheses. <ref type="bibr">2</ref> Intuitively speaking, our approach solves the EM training problem for decipherment using large vocabularies by focusing only on those substitu- tions that either seem likely due to the language model ("What word is likely to follow the cur- rent partial decipherment?") or due to the lexicon model ("Based on my knowledge about the cur- rent cipher token, what is the most likely substitu- tion?").</p><p>In order to efficiently find the maximizing e for p LM (e|σ) and p lex (f |e), we build a lookup ta- ble that contains for each language model state σ the B LM best successor words e, and a separate lookup  <ref type="figure">Figure 1</ref>: Illustration of the search space explored by full search, beam search, and preselection search. Full search keeps all possible hypotheses at cardinality c and explores all possible substitutions at (c+1). Beam search only keeps the B most promising hypotheses and then selects the best new hypotheses for cardinality (c + 1) from all possible substitutions. Preselection search keeps only the B best hypotheses for every cardinality c and only looks at the (B lex + B LM ) most promising substitutions for cardinality (c + 1) based on the current lexicon (B lex dashed lines) and language model (B LM solid lines).  ble do not form a practical problem of our ap- proach. <ref type="figure">Figure 1</ref> illustrates full search, beam search, and our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>We first show experiments for data in which the underlying model is an actual 1:1 substitution ci- pher. In this case, we report the word accuracy of the final decipherment. We then show experi- ments for a simple machine translation task. Here we report translation quality in BLEU. The cor- pora used in this paper are shown in <ref type="table" target="#tab_2">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simple Substitution Ciphers</head><p>In this set of experiments, we compare the exact EM training to the approximations presented in this paper. We use the English side of the German- English VERBMOBIL corpus <ref type="bibr">(Wahlster, 2000</ref>) to construct a word substitution cipher, by substitut- ing every word type with a unique number. In or- der to have a non-parallel setup, we train language  <ref type="table">Table 4</ref>: Results for simple substitution ciphers based on the VERBMOBIL corpus using exact, beam, and preselection EM. Exact EM is not tractable for vocabulary sizes above 200. models of order 2, 3 and 4 on the first half of the corpus and use the second half as ciphertext. <ref type="table">Ta- ble 4</ref> shows the results of our experiments. Since exact EM is not tractable for vocabulary sizes beyond 200 words, we train word classes on the whole corpus and map the words to classes (consistent along the first and second half of the corpus). By doing this, we create new simple sub- stitution ciphers with smaller vocabularies of size 200 and 500. For the smallest setup, we can di- rectly compare all three EM variants. We also in- clude experiments on the original corpus with vo- cabulary size of 3661. When comparing exact EM training with beam-and preselection EM training, the first thing we notice is that it takes about 20 times longer to run the exact EM training than training with beam EM, and about 50 times longer than the preselection EM training. Interestingly,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU [%] Runtime</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2-gram Exact EM(Ravi and Knight, 2011)</head><p>15.3 850.0h whole segment lm Exact EM( <ref type="bibr" target="#b9">Ravi and Knight, 2011)</ref> 19.3 850.0h</p><p>2-gram Preselection EM (This work) 15.7 1.8h 3-gram Preselection EM (This work) 19.5 1.9h <ref type="table">Table 5</ref>: Comparison of MT performance (BLEU scores) and efficiency (running time in CPU hours) on the Spanish/English OPUS corpus using only non-parallel corpora for training.</p><p>the accuracy of the approximations to exact EM training is better than that of the exact EM train- ing. Even though this needs further investigation, it is clear that the pruned versions of EM training find sparser distributions p lex (f |e): This is desir- able in this set of experiments, and could be the reason for improved performance. For larger vocabularies, exact EM training is not tractable anymore. We thus constrain ourselves to running experiments with beam and preselection EM training only. Here we can see that the runtime of the preselection search is roughly the same as when running on a smaller vocabulary, while the beam search runtime scales almost linearly with the vocabulary size. For the full vocabulary of 3661 words, preselection EM using a 4-gram LM needs less than 7% of the time of beam EM with a 3-gram LM and performs by 1% better in symbol accuracy.</p><p>To summarize: Beam search EM is an or- der of magnitude faster than exact EM training while even increasing decipherment accuracy. Our new preselection search method is in turn or- ders of magnitudes faster than beam search EM while even being able to outperform exact EM and beam EM by using higher order language mod- els. We were thus able to scale the EM deci- pherment to larger vocabularies of several thou- sand words. The runtime behavior is also consis- tent with the computational complexity discussed in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Machine Translation</head><p>We show that our algorithm is directly applicable to the decipherment problem for machine transla- tion. We use the same simplified translation model as presented by <ref type="bibr" target="#b9">Ravi and Knight (2011)</ref>. Because this translation model allows insertions and dele- tions, hypotheses of different cardinalities coex- ist during search. We extend our search approach such that pruning is done for each cardinality sep- arately. Other than that, we use the same pres- election search procedure as used for the simple substitution cipher task.</p><p>We run experiments on the opus corpus as pre- sented in <ref type="bibr" target="#b11">(Tiedemann, 2009)</ref>. <ref type="table">Table 5</ref> shows pre- viously published results using EM together with the results of our new method:</p><p>( <ref type="bibr" target="#b9">Ravi and Knight, 2011</ref>) is the only publication that reports results using exact EM training and only n-gram language models on the target side: It has an estimated runtime of 850h. All other published results (using EM training and Bayesian inference) use context vectors as an additional source of information: This might be an explana- tion why <ref type="bibr" target="#b5">Nuhn et al. (2012)</ref> and <ref type="bibr" target="#b10">Ravi (2013)</ref> are able to outperform exact EM training as reported by <ref type="bibr" target="#b9">Ravi and Knight (2011)</ref>. <ref type="bibr" target="#b10">(Ravi, 2013)</ref> reports the most efficient method so far: It only consumes about 3h of computation time. However, as men- tioned before, those results are not directly compa- rable to our work, since they use additional context information on the target side.</p><p>Our algorithm clearly outperforms the exact EM training in run time, and even slighlty im- proves performance in BLEU. Similar to the sim- ple substitution case, the improved performance might be caused by inferring a sparser distribution p lex (f |e). However, this requires further investi- gation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have shown a conceptually consistent and easy to implement EM based training method for deci- pherment that outperforms exact and beam search EM training for simple substitution ciphers and decipherment for machine translation, while re- ducing training time to a fraction of exact and beam EM. We also point out that the preselection method presented in this paper is not restricted to word based translation models and can also be ap- plied to phrase based translation models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>table that contains</head><label>that</label><figDesc>for each source word f the B lex highest scoring tokens e. The language model lookup table remains constant during all it- erations, while the lexicon lookup table needs to be updated between each iteration. Note that the size of the LM lookup table scales linearly with the number of language model states. Thus the memory requirements for the lookup ta-</figDesc><table>f 1 

f 2 
f 3 
f 4 
f 5 

e 5 
e 4 
e 3 
e 2 
e 1 

Beam Search 
Preselection Search 
Full Search 

f 6 

... 

start 

Vocab 

Sentence 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Statistics of the copora used in this pa-
per: The VERBMOBIL corpus is used to conduct 
experiments on simple substitution ciphers, while 
the OPUS corpus is used in our Machine Transla-
tion experiments. 

</table></figure>

			<note place="foot" n="1"> This can be generalized to n-gram language models. After we obtained the parameters ϑ we can obtain e N</note>

			<note place="foot" n="1"> :en=e]. Note that due to this approximation, the new parameter estimates in Equation 5 can become zero. This is a critical issue, since pairs (e, f ) with p(f |e) = 0 cannot recover from</note>

			<note place="foot" n="2"> We always use B = 100, B lex = 5, and BLM = 50.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An exact A* method for deciphering letter-substitution ciphers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Corlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="1040" to="1047" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The sphinx-ii speech recognition system: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fileno</forename><surname>Alleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Hsiao Wuen Hon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer, Speech and Language</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="137" to="148" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised Analysis for Decipherment Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishit</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<title level="m">COLING/ACL on Main conference poster sessions, COLING-ACL &apos;06</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="499" to="506" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deciphering foreign language by combining language models and context vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Nuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Jeju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="156" to="164" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beam search for solving substitution ciphers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Nuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schamper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Assoc. for Computational Linguistics</title>
		<meeting><address><addrLine>Sofia, Bulgaria, August</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1569" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sparse forward-backward using minimum divergence beams for fast training of conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attacking decipherment problems optimally with low-order ngram models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="812" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deciphering foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACLHLT)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACLHLT)<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scalable decipherment for machine translation via hash sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Sofia, Bulgaria, August</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="362" to="371" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">News from OPUS-A collection of multilingual parallel corpora with tools and interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Natural Language Processing</title>
		<editor>N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov</editor>
		<meeting><address><addrLine>Borovets, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="237" to="248" />
		</imprint>
	</monogr>
	<note>John Benjamins, Amsterdam/Philadelphia</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Verbmobil: Foundations of speech-to-speech translations</title>
		<editor>Wolfgang Wahlster</editor>
		<imprint>
			<date type="published" when="2000" />
			<publisher>SpringerVerlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The htk hidden markov model toolkit: Design and philosophy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sj</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ltd</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2" to="44" />
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Entropic Cambridge Research Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
