<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data Recombination for Neural Semantic Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
							<email>robinjia@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<email>pliang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Data Recombination for Neural Semantic Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="12" to="22"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Modeling crisp logical regularities is crucial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results. In this paper, we introduce data recom-bination, a novel framework for injecting such prior knowledge into a model. From the training data, we induce a high-precision synchronous context-free grammar , which captures important conditional independence properties commonly found in semantic parsing. We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sampled from this grammar, thereby teaching the model about these structural properties. Data recombination improves the accuracy of our RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic parsing-the precise translation of nat- ural language utterances into logical forms-has many applications, including question answer- ing ( <ref type="bibr" target="#b34">Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b35">Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b36">Zettlemoyer and Collins, 2007;</ref><ref type="bibr" target="#b19">Liang et al., 2011;</ref><ref type="bibr" target="#b3">Berant et al., 2013)</ref>, instruc- tion following <ref type="bibr" target="#b1">(Artzi and Zettlemoyer, 2013b)</ref>, and regular expression generation <ref type="bibr" target="#b16">(Kushman and Barzilay, 2013)</ref>. Modern semantic parsers <ref type="bibr" target="#b0">(Artzi and Zettlemoyer, 2013a;</ref><ref type="bibr" target="#b3">Berant et al., 2013)</ref> are complex pieces of software, requiring hand- crafted features, lexicons, and grammars.</p><p>Meanwhile, recurrent neural networks <ref type="bibr">(RNNs)</ref> what are the major cities in utah ? what states border maine ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train Model</head><p>Sequence-to-sequence RNN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample New Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synchronous CFG</head><p>Induce Grammar what are the major cities in [states border <ref type="bibr">[maine]</ref>] ? what are the major cities in [states border <ref type="bibr">[utah]</ref>] ? what states border [states border <ref type="bibr">[maine]</ref>] ? what states border [states border <ref type="bibr">[utah]</ref>] ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recombinant Examples</head><p>Figure 1: An overview of our system. Given a dataset, we induce a high-precision synchronous context-free grammar. We then sample from this grammar to generate new "recombinant" exam- ples, which we use to train a sequence-to-sequence RNN.</p><p>have made swift inroads into many structured pre- diction tasks in NLP, including machine trans- lation ( <ref type="bibr" target="#b26">Sutskever et al., 2014;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2014</ref>) and syntactic parsing ( <ref type="bibr" target="#b7">Dyer et al., 2015)</ref>. Because RNNs make very few domain-specific assumptions, they have the poten- tial to succeed at a wide variety of tasks with min- imal feature engineering. However, this flexibil- ity also puts RNNs at a disadvantage compared to standard semantic parsers, which can generalize naturally by leveraging their built-in awareness of logical compositionality.</p><p>In this paper, we introduce data recombina- tion, a generic framework for declaratively inject- x: "what is the population of iowa ?" y: _answer ( NV , ( _population ( NV , V1 ) , _const ( V0 , _stateid ( iowa ) ) ) ) ATIS x: "can you list all flights from chicago to milwaukee" y: ( _lambda $0 e ( _and ( _flight $0 ) ( _from $0 chicago : _ci ) ( _to $0 milwaukee : _ci ) ) ) Overnight x: "when is the weekly standup" y: ( call listValue ( call getProperty meeting.weekly_standup ( string start_time ) ) )</p><p>Figure 2: One example from each of our domains. We tokenize logical forms as shown, thereby cast- ing semantic parsing as a sequence-to-sequence task.</p><p>ing prior knowledge into a domain-general struc- tured prediction model. In data recombination, prior knowledge about a task is used to build a high-precision generative model that expands the empirical distribution by allowing fragments of different examples to be combined in particular ways. Samples from this generative model are then used to train a domain-general model. In the case of semantic parsing, we construct a genera- tive model by inducing a synchronous context-free grammar (SCFG), creating new examples such as those shown in <ref type="figure">Figure 1</ref>; our domain-general model is a sequence-to-sequence RNN with a novel attention-based copying mechanism. Data recombination boosts the accuracy of our RNN model on three semantic parsing datasets. On the GEO dataset, data recombination improves test ac- curacy by 4.3 percentage points over our baseline RNN, leading to new state-of-the-art results for models that do not use a seed lexicon for predi- cates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem statement</head><p>We cast semantic parsing as a sequence-to- sequence task. The input utterance x is a sequence of words x 1 , . . . , x m ∈ V (in) , the input vocabulary; similarly, the output logical form y is a sequence of tokens y 1 , . . . , y n ∈ V (out) , the output vocab- ulary. A linear sequence of tokens might appear to lose the hierarchical structure of a logical form, but there is precedent for this choice:  showed that an RNN can reliably predict tree-structured outputs in a linear fashion. We evaluate our system on three existing se- mantic parsing datasets. <ref type="figure">Figure 2</ref> shows sample input-output pairs from each of these datasets.</p><p>• GeoQuery (GEO) contains natural language questions about US geography paired with corresponding Prolog database queries. We use the standard split of 600 training exam- ples and 280 test examples introduced by <ref type="bibr" target="#b35">Zettlemoyer and Collins (2005)</ref>. We prepro- cess the logical forms to De Brujin index no- tation to standardize variable naming.</p><p>• ATIS (ATIS) contains natural language queries for a flights database paired with corresponding database queries written in lambda calculus. We train on 4473 examples and evaluate on the 448 test examples used by <ref type="bibr" target="#b36">Zettlemoyer and Collins (2007)</ref>.</p><p>• Overnight (OVERNIGHT) contains logical forms paired with natural language para- phrases across eight varied subdomains.  constructed the dataset by generating all possible logical forms up to some depth threshold, then getting multiple natural language paraphrases for each logi- cal form from workers on Amazon Mechan- ical Turk. We evaluate on the same train/test splits as .</p><p>In this paper, we only explore learning from log- ical forms. In the last few years, there has an emergence of semantic parsers learned from de- notations ( <ref type="bibr" target="#b5">Clarke et al., 2010;</ref><ref type="bibr" target="#b19">Liang et al., 2011;</ref><ref type="bibr" target="#b3">Berant et al., 2013;</ref><ref type="bibr" target="#b1">Artzi and Zettlemoyer, 2013b</ref>). While our system cannot directly learn from deno- tations, it could be used to rerank candidate deriva- tions generated by one of these other systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sequence-to-sequence RNN Model</head><p>Our sequence-to-sequence RNN model is based on existing attention-based neural machine trans- lation models ( <ref type="bibr" target="#b2">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b21">Luong et al., 2015a</ref>), but also includes a novel attention-based copying mechanism. Similar copying mechanisms have been explored in parallel by <ref type="bibr" target="#b9">Gu et al. (2016)</ref> and <ref type="bibr" target="#b10">Gulcehre et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Model</head><p>Encoder. The encoder converts the input se- quence x 1 , . . . , x m into a sequence of context-sensitive embeddings b 1 , . . . , b m using a bidirec- tional RNN ( <ref type="bibr" target="#b2">Bahdanau et al., 2014</ref>). First, a word embedding function φ (in) maps each word x i to a fixed-dimensional vector. These vectors are fed as input to two RNNs: a forward RNN and a back- ward RNN. The forward RNN starts with an initial hidden state h F 0 , and generates a sequence of hid- den states h F 1 , . . . , h F m by repeatedly applying the recurrence</p><formula xml:id="formula_0">h F i = LSTM(φ (in) (x i ), h F i−1 ).<label>(1)</label></formula><p>The recurrence takes the form of an LSTM <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997</ref> Decoder. The decoder is an attention-based model ( <ref type="bibr" target="#b2">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b21">Luong et al., 2015a</ref>) that generates the output sequence y 1 , . . . , y n one token at a time. At each time step j, it writes y j based on the current hidden state s j , then up- dates the hidden state to s j+1 based on s j and y j . Formally, the decoder is defined by the following equations:</p><formula xml:id="formula_1">s 1 = tanh(W (s) [h F m , h B 1 ]).<label>(2)</label></formula><formula xml:id="formula_2">e ji = s j W (a) b i .<label>(3)</label></formula><formula xml:id="formula_3">α ji = exp(e ji ) m i =1 exp(e ji ) .<label>(4)</label></formula><formula xml:id="formula_4">c j = m i=1 α ji b i .<label>(5)</label></formula><formula xml:id="formula_5">P (y j = w | x, y 1:j−1 ) ∝ exp(U w [s j , c j ]). (6) s j+1 = LSTM([φ (out) (y j ), c j ], s j ).<label>(7)</label></formula><p>When not specified, i ranges over {1, . . . , m} and j ranges over {1, . . . , n}. Intuitively, the α ji 's de- fine a probability distribution over the input words, describing what words in the input the decoder is focusing on at time j. They are computed from the unnormalized attention scores e ji . The matri- ces W (s) , W (a) , and U , as well as the embedding function φ (out) , are parameters of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attention-based Copying</head><p>In the basic model of the previous section, the next output word y j is chosen via a simple softmax over all words in the output vocabulary. However, this model has difficulty generalizing to the long tail of entity names commonly found in semantic parsing datasets. Conveniently, entity names in the input often correspond directly to tokens in the output (e.g., "iowa" becomes iowa in <ref type="figure">Figure 2</ref>). <ref type="bibr">1</ref> To capture this intuition, we introduce a new attention-based copying mechanism. At each time step j, the decoder generates one of two types of actions. As before, it can write any word in the output vocabulary. In addition, it can copy any in- put word x i directly to the output, where the prob- ability with which we copy x i is determined by the attention score on x i . Formally, we define a latent action a j that is either Write[w] for some w ∈ V (out) or Copy[i] for some i ∈ {1, . . . , m}. We then have</p><formula xml:id="formula_6">P (a j = Write[w] | x, y 1:j−1 ) ∝ exp(U w [s j , c j ]),<label>(8)</label></formula><formula xml:id="formula_7">P (a j = Copy[i] | x, y 1:j−1 ) ∝ exp(e ji ).<label>(9)</label></formula><p>The decoder chooses a j with a softmax over all these possible actions; y j is then a deterministic function of a j and x. During training, we maxi- mize the log-likelihood of y, marginalizing out a.</p><p>Attention-based copying can be seen as a com- bination of a standard softmax output layer of an attention-based model ( <ref type="bibr" target="#b2">Bahdanau et al., 2014</ref>) and a Pointer Network ( <ref type="bibr" target="#b27">Vinyals et al., 2015a)</ref>; in a Pointer Network, the only way to generate output is to copy a symbol from the input.</p><p>Examples ("what states border texas ?", answer(NV, (state(V0), next_to(V0, NV), const(V0, stateid(texas))))) ("what is the highest mountain in ohio ?", answer(NV, highest(V0, (mountain(V0), loc(V0, NV), const(V0, stateid(ohio)))))) Rules created by ABSENTITIES ROOT → "what states border STATEID ?", answer(NV, (state(V0), next_to(V0, NV), const(V0, stateid(STATEID )))) STATEID → "texas", texas ROOT → "what is the highest mountain in STATEID ?", answer(NV, highest(V0, (mountain(V0), loc(V0, NV), const(V0, stateid(STATEID ))))) STATEID → "ohio", ohio Rules created by ABSWHOLEPHRASES ROOT → "what states border STATE ?", answer(NV, (state(V0), next_to(V0, NV), STATE )) STATE → "states border texas", state(V0), next_to(V0, NV), const(V0, stateid(texas)) ROOT → "what is the highest mountain in STATE ?", answer(NV, highest(V0, (mountain(V0), loc(V0, NV), STATE ))) Rules created by CONCAT-2 ROOT → SENT1 &lt;/s&gt; SENT2, SENT1 &lt;/s&gt; SENT2 SENT → "what states border texas ?", answer(NV, (state(V0), next_to(V0, NV), const(V0, stateid(texas)))) SENT → "what is the highest mountain in ohio ?", answer(NV, highest(V0, (mountain(V0), loc(V0, NV), const(V0, stateid(ohio)))))</p><p>Figure 3: Various grammar induction strategies illustrated on GEO. Each strategy converts the rules of an input grammar into rules of an output grammar. This figure shows the base case where the input grammar has rules ROOT → x, y for each (x, y) pair in the training dataset.</p><p>Our approach generalizes data augmentation, which is commonly employed to inject prior knowledge into a model. Data augmenta- tion techniques focus on modeling invariances- transformations like translating an image or adding noise that alter the inputs x, but do not change the output y. These techniques have proven effective in areas like computer vision ( ) and speech recognition <ref type="bibr" target="#b14">(Jaitly and Hinton, 2013</ref>).</p><p>In semantic parsing, however, we would like to capture more than just invariance properties. Con- sider an example with the utterance "what states border texas ?". Given this example, it should be easy to generalize to questions where "texas" is replaced by the name of any other state: simply replace the mention of Texas in the logical form with the name of the new state. Underlying this phenomenon is a strong conditional independence principle: the meaning of the rest of the sentence is independent of the name of the state in ques- tion. Standard data augmentation is not sufficient to model such phenomena: instead of holding y fixed, we would like to apply simultaneous trans- formations to x and y such that the new x still maps to the new y. Data recombination addresses this need.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">General Setting</head><p>In the general setting of data recombination, we start with a training set D of (x, y) pairs, which defines the empirical distributionˆpdistributionˆ distributionˆp(x, y). We then fit a generative model˜pmodel˜ model˜p(x, y) tô p which gener- alizes beyond the support ofˆpofˆ ofˆp, for example by splicing together fragments of different examples. We refer to examples in the support of˜pof˜ of˜p as re- combinant examples. Finally, to train our actual model p θ (y | x), we maximize the expected value of log p θ (y | x), where (x, y) is drawn from˜pfrom˜ from˜p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SCFGs for Semantic Parsing</head><p>For semantic parsing, we induce a synchronous context-free grammar (SCFG) to serve as the backbone of our generative model˜pmodel˜ model˜p. An SCFG consists of a set of production rules X → α, β, where X is a category (non-terminal), and α and β are sequences of terminal and non-terminal sym- bols. Any non-terminal symbols in α must be aligned to the same non-terminal symbol in β, and vice versa. Therefore, an SCFG defines a set of joint derivations of aligned pairs of strings. In our case, we use an SCFG to represent joint deriva-tions of utterances x and logical forms y (which for us is just a sequence of tokens). After we induce an SCFG G from D, the corresponding generative model˜pmodel˜ model˜p(x, y) is the distribution over pairs (x, y) defined by sampling from G, where we choose production rules to apply uniformly at random.</p><p>It is instructive to compare our SCFG-based data recombination with WASP ( <ref type="bibr" target="#b32">Wong and Mooney, 2006;</ref><ref type="bibr" target="#b33">Wong and Mooney, 2007)</ref>, which uses an SCFG as the actual semantic parsing model. The grammar induced by WASP must have good coverage in order to generalize to new in- puts at test time. WASP also requires the imple- mentation of an efficient algorithm for computing the conditional probability p(y | x). In contrast, our SCFG is only used to convey prior knowl- edge about conditional independence structure, so it only needs to have high precision; our RNN model is responsible for boosting recall over the entire input space. We also only need to forward sample from the SCFG, which is considerably eas- ier to implement than conditional inference.</p><p>Below, we examine various strategies for induc- ing a grammar G from a dataset D. We first en- code D as an initial grammar with rules ROOT → x, y for each (x, y) ∈ D. Next, we will define each grammar induction strategy as a map- ping from an input grammar G in to a new gram- mar G out . This formulation allows us to compose grammar induction strategies (Section 4.3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Abstracting Entities</head><p>Our first grammar induction strategy, ABSENTI- TIES, simply abstracts entities with their types. We assume that each entity e (e.g., texas) has a corresponding type e.t (e.g., state), which we infer based on the presence of certain predicates in the logical form (e.g. stateid). For each grammar rule X → α, β in G in , where α con- tains a token (e.g., "texas") that string matches an entity (e.g., texas) in β, we add two rules to G out : (i) a rule where both occurrences are re- placed with the type of the entity (e.g., state), and (ii) a new rule that maps the type to the en- tity (e.g., STATEID → "texas", texas; we re- serve the category name STATE for the next sec- tion). Thus, G out generates recombinant examples that fuse most of one example with an entity found in a second example. A concrete example from the GEO domain is given in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Abstracting Whole Phrases</head><p>Our second grammar induction strategy, ABSW- HOLEPHRASES, abstracts both entities and whole phrases with their types. For each grammar rule X → α, β in G in , we add up to two rules to G out . First, if α contains tokens that string match to an entity in β, we replace both occurrences with the type of the entity, similarly to rule (i) from AB- SENTITIES. Second, if we can infer that the entire expression β evaluates to a set of a particular type (e.g. state) we create a rule that maps the type to α, β. In practice, we also use some simple rules to strip question identifiers from α, so that the resulting examples are more natural. Again, refer to <ref type="figure">Figure 3</ref> for a concrete example.</p><p>This strategy works because of a more general conditional independence property: the meaning of any semantically coherent phrase is condition- ally independent of the rest of the sentence, the cornerstone of compositional semantics. Note that this assumption is not always correct in general: for example, phenomena like anaphora that in- volve long-range context dependence violate this assumption. However, this property holds in most existing semantic parsing datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Concatenation</head><p>The final grammar induction strategy is a surpris- ingly simple approach we tried that turns out to work. For any k ≥ 2, we define the CONCAT-k strategy, which creates two types of rules. First, we create a single rule that has ROOT going to a sequence of k SENT's. Then, for each root- level rule ROOT → α, β in G in , we add the rule SENT → α, β to G out . See <ref type="figure">Figure 3</ref> for an ex- ample.</p><p>Unlike ABSENTITIES and ABSWHOLE- PHRASES, concatenation is very general, and can be applied to any sequence transduction problem. Of course, it also does not introduce additional information about compositionality or indepen- dence properties present in semantic parsing. However, it does generate harder examples for the attention-based RNN, since the model must learn to attend to the correct parts of the now-longer input sequence. Related work has shown that training a model on more difficult examples can improve generalization, the most canonical case being dropout ( <ref type="bibr" target="#b29">Wager et al., 2013)</ref>.</p><formula xml:id="formula_8">function TRAIN(dataset D, number of epochs T ,</formula><p>number of examples to sample n) Induce grammar G from D Initialize RNN parameters θ randomly for each iteration t = 1, . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. , T do</head><p>Compute current learning rate ηt Initialize current dataset Dt to D for i = 1, . . . , n do Sample new example (x , y ) from G Add (x , y ) to Dt end for Shuffle Dt for each example (x, y) in Dt do θ ← θ + ηt log p θ (y | x) end for end for end function <ref type="figure">Figure 4</ref>: The training procedure with data recom- bination. We first induce an SCFG, then sample new recombinant examples from it at each epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Composition</head><p>We note that grammar induction strategies can be composed, yielding more complex grammars. Given any two grammar induction strategies f 1 and f 2 , the composition f 1 • f 2 is the grammar induction strategy that takes in G in and returns f 1 (f 2 (G in )). For the strategies we have defined, we can perform this operation symbolically on the grammar rules, without having to sample from the intermediate grammar f 2 (G in ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our system on three domains: GEO, ATIS, and OVERNIGHT. For ATIS, we report logical form exact match accuracy. For GEO and OVERNIGHT, we determine correctness based on denotation match, as in <ref type="bibr" target="#b19">Liang et al. (2011)</ref> and , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Choice of Grammar Induction Strategy</head><p>We note that not all grammar induction strate- gies make sense for all domains. In particular, we only apply ABSWHOLEPHRASES to GEO and OVERNIGHT. We do not apply ABSWHOLE- PHRASES to ATIS, as the dataset has little nesting structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>We tokenize logical forms in a domain-specific manner, based on the syntax of the formal lan- guage being used. On GEO and ATIS, we dis- allow copying of predicate names to ensure a fair comparison to previous work, as string matching between input words and predicate names is not commonly used. We prevent copying by prepend- ing underscores to predicate tokens; see <ref type="figure">Figure 2</ref> for examples.</p><p>On ATIS alone, when doing attention-based copying and data recombination, we leverage an external lexicon that maps natural language phrases (e.g., "kennedy airport") to entities (e.g., jfk:ap). When we copy a word that is part of a phrase in the lexicon, we write the entity asso- ciated with that lexicon entry. When performing data recombination, we identify entity alignments based on matching phrases and entities from the lexicon.</p><p>We run all experiments with 200 hidden units and 100-dimensional word vectors. We initial- ize all parameters uniformly at random within the interval [−0.1, 0.1]. We maximize the log- likelihood of the correct logical form using stochastic gradient descent. We train the model for a total of 30 epochs with an initial learning rate of 0.1, and halve the learning rate every 5 epochs, starting after epoch 15. We replace word vectors for words that occur only once in the training set with a universal &lt;unk&gt; word vector. Our model is implemented in Theano ( <ref type="bibr" target="#b4">Bergstra et al., 2010)</ref>.</p><p>When performing data recombination, we sam- ple a new round of recombinant examples from our grammar at each epoch. We add these ex- amples to the original training dataset, randomly shuffle all examples, and train the model for the epoch. <ref type="figure">Figure 4</ref> gives pseudocode for this training procedure. One important hyperparameter is how many examples to sample at each epoch: we found that a good rule of thumb is to sample as many re- combinant examples as there are examples in the training dataset, so that half of the examples the model sees at each epoch are recombinant.</p><p>At test time, we use beam search with beam size 5. We automatically balance missing right paren- theses by adding them at the end. On GEO and OVERNIGHT, we then pick the highest-scoring logical form that does not yield an executor error when the corresponding denotation is computed. On ATIS, we just pick the top prediction on the beam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Impact of the Copying Mechanism</head><p>First, we measure the contribution of the attention- based copying mechanism to the model's overall <ref type="bibr">GEO</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ATIS OVERNIGHT No Copying</head><p>74.6 69.9 76.7 With Copying 85.0 76.3 75.8  performance. On each task, we train and evalu- ate two models: one with the copying mechanism, and one without. Training is done without data re- combination. The results are shown in <ref type="table" target="#tab_1">Table 1</ref>. On GEO and ATIS, the copying mechanism helps significantly: it improves test accuracy by 10.4 percentage points on GEO and 6.4 points on ATIS. However, on OVERNIGHT, adding the copying mechanism actually makes our model perform slightly worse. This result is somewhat expected, as the OVERNIGHT dataset contains a very small number of distinct entities. It is also notable that both systems surpass the previous best system on OVERNIGHT by a wide margin.</p><p>We choose to use the copying mechanism in all subsequent experiments, as it has a large advan- tage in realistic settings where there are many dis- tinct entities in the world. The concurrent work of <ref type="bibr" target="#b9">Gu et al. (2016)</ref> and <ref type="bibr" target="#b10">Gulcehre et al. (2016)</ref>, both of whom propose similar copying mechanisms, pro- vides additional evidence for the utility of copying on a wide range of NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The method of Liang et al. (2011) is not comparable to</head><p>For our main results, we train our model with a va- riety of data recombination strategies on all three datasets. These results are summarized in <ref type="table" target="#tab_2">Tables 2  and 3</ref>. We compare our system to the baseline of not using any data recombination, as well as to state-of-the-art systems on all three datasets.</p><p>We find that data recombination consistently improves accuracy across the three domains we evaluated on, and that the strongest results come from composing multiple strategies. Combin- ing ABSWHOLEPHRASES, ABSENTITIES, and CONCAT-2 yields a 4.3 percentage point improve- ment over the baseline without data recombina- tion on GEO, and an average of 1.7 percentage points on OVERNIGHT. In fact, on GEO, we achieve test accuracy of 89.3%, which surpasses the previous state-of-the-art, excluding <ref type="bibr" target="#b19">Liang et al. (2011)</ref>, which used a seed lexicon for predicates. On ATIS, we experiment with concatenating more than 2 examples, to make up for the fact that we cannot apply ABSWHOLEPHRASES, which gen- erates longer examples. We obtain a test accu- racy of 83.3 with ABSENTITIES composed with CONCAT-3, which beats the baseline by 7 percent- age points and is competitive with the state-of-the- art.</p><p>Data recombination without copying. For completeness, we also investigated the effects of data recombination on the model without attention-based copying. We found that recom- bination helped significantly on GEO and ATIS, but hurt the model slightly on OVERNIGHT. On GEO, the best data recombination strategy yielded test accuracy of 82.9%, for a gain of 8.3 percent- age points over the baseline with no copying and no recombination; on ATIS, data recombination gives test accuracies as high as 74.6%, a 4.7 point gain over the same baseline. However, no data re- combination strategy improved average test accu- racy on OVERNIGHT; the best one resulted in a 0.3 percentage point decrease in test accuracy. We hypothesize that data recombination helps less on OVERNIGHT in general because the space of pos- sible logical forms is very limited, making it more like a large multiclass classification task. There- fore, it is less important for the model to learn good compositional representations that general- ize to new logical forms at test time.</p><p>ours, as they as they used a seed lexicon mapping words to predicates. We explicitly avoid using such prior knowledge in our system.  46   We see that the model learns more from longer examples than from same-length examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BASKETBALL BLOCKS CALENDAR HOUSING PUBLICATIONS RECIPES RESTAURANTS SOCIAL Avg. Previous Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Effect of Longer Examples</head><p>Interestingly, strategies like ABSWHOLE- PHRASES and CONCAT-2 help the model even though the resulting recombinant examples are generally not in the support of the test distribution.</p><p>In particular, these recombinant examples are on average longer than those in the actual dataset, which makes them harder for the attention-based model. Indeed, for every domain, our best accuracy numbers involved some form of concate- nation, and often involved ABSWHOLEPHRASES as well. In comparison, applying ABSENTITIES alone, which generates examples of the same length as those in the original dataset, was generally less effective. We conducted additional experiments on artifi- cial data to investigate the importance of adding longer, harder examples. We experimented with adding new examples via data recombination, as well as adding new independent examples (e.g. to simulate the acquisition of more training data). We constructed a simple world containing a set of enti- ties and a set of binary relations. For any n, we can generate a set of depth-n examples, which involve the composition of n relations applied to a single entity. Example data points are shown in <ref type="figure">Figure 5</ref>. We train our model on various datasets, then test it on a set of 500 randomly chosen depth-2 exam- ples. The model always has access to a small seed training set of 100 depth-2 examples. We then add one of four types of examples to the training set:</p><p>• Same length, independent: New randomly chosen depth-2 examples. 3</p><p>• Longer, independent: Randomly chosen depth-4 examples.</p><p>• Same length, recombinant: Depth-2 exam- ples sampled from the grammar induced by applying ABSENTITIES to the seed dataset.</p><p>• Longer, recombinant: Depth-4 examples sampled from the grammar induced by apply- ing ABSWHOLEPHRASES followed by AB- SENTITIES to the seed dataset.</p><p>To maintain consistency between the independent and recombinant experiments, we fix the recombi- nant examples across all epochs, instead of resam- pling at every epoch. In <ref type="figure">Figure 6</ref>, we plot accu- racy on the test set versus the number of additional examples added of each of these four types. As expected, independent examples are more help- ful than the recombinant ones, but both help the model improve considerably. In addition, we see that even though the test dataset only has short ex- amples, adding longer examples helps the model more than adding shorter ones, in both the inde- pendent and recombinant cases. These results un- derscore the importance training on longer, harder examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this paper, we have presented a novel frame- work we term data recombination, in which we generate new training examples from a high- precision generative model induced from the orig- inal training dataset. We have demonstrated its effectiveness in improving the accuracy of a sequence-to-sequence RNN model on three se- mantic parsing datasets, using a synchronous context-free grammar as our generative model. There has been growing interest in applying neural networks to semantic parsing and related tasks. <ref type="bibr" target="#b6">Dong and Lapata (2016)</ref> concurrently de- veloped an attention-based RNN model for se- mantic parsing, although they did not use data re- combination. <ref type="bibr" target="#b8">Grefenstette et al. (2014)</ref> proposed a non-recurrent neural model for semantic pars- ing, though they did not run experiments. <ref type="bibr" target="#b23">Mei et al. (2016)</ref> use an RNN model to perform a related task of instruction following.</p><p>Our proposed attention-based copying mech- anism bears a strong resemblance to two mod- els that were developed independently by other groups. <ref type="bibr" target="#b9">Gu et al. (2016)</ref> apply a very similar copy- ing mechanism to text summarization and single- turn dialogue generation. <ref type="bibr" target="#b10">Gulcehre et al. (2016)</ref> propose a model that decides at each step whether to write from a "shortlist" vocabulary or copy from the input, and report improvements on machine translation and text summarization. Another piece of related work is <ref type="bibr" target="#b22">Luong et al. (2015b)</ref>, who train a neural machine translation system to copy rare words, relying on an external system to generate alignments.</p><p>Prior work has explored using paraphrasing for data augmentation on NLP tasks. <ref type="bibr" target="#b37">Zhang et al. (2015)</ref> augment their data by swapping out words for synonyms from WordNet. <ref type="bibr" target="#b30">Wang and Yang (2015)</ref> use a similar strategy, but identify similar words and phrases based on cosine distance be- tween vector space embeddings. Unlike our data recombination strategies, these techniques only change inputs x, while keeping the labels y fixed. Additionally, these paraphrasing-based transfor- mations can be described in terms of grammar induction, so they can be incorporated into our framework.</p><p>In data recombination, data generated by a high- precision generative model is used to train a sec- ond, domain-general model. Generative oversam- pling ( <ref type="bibr" target="#b20">Liu et al., 2007</ref>) learns a generative model in a multiclass classification setting, then uses it to generate additional examples from rare classes in order to combat label imbalance. Uptraining ( <ref type="bibr" target="#b24">Petrov et al., 2010)</ref> uses data labeled by an ac- curate but slow model to train a computationally cheaper second model.  gen- erate a large dataset of constituency parse trees by taking sentences that multiple existing systems parse in the same way, and train a neural model on this dataset.</p><p>Some of our induced grammars generate ex- amples that are not in the test distribution, but nonetheless aid in generalization. Related work has also explored the idea of training on altered or out-of-domain data, often interpreting it as a form of regularization. Dropout training has been shown to be a form of adaptive regularization <ref type="bibr" target="#b29">Wager et al., 2013)</ref>. <ref type="bibr" target="#b11">Guu et al. (2015)</ref> showed that encouraging a knowledge base completion model to handle longer path queries acts as a form of structural regularization.</p><p>Language is a blend of crisp regularities and soft relationships. Our work takes RNNs, which excel at modeling soft phenomena, and uses a highly structured tool-synchronous context free grammars-to infuse them with an understanding of crisp structure. We believe this paradigm for si- multaneously modeling the soft and hard aspects of language should have broader applicability be- yond semantic parsing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>GEO</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Depth- 2 (Figure 5 :Figure 6 :</head><label>256</label><figDesc>Figure 5: A sample of our artificial data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 : Test accuracy on GEO, ATIS, and OVERNIGHT, both with and without copying. On OVERNIGHT, we average across all eight domains.</head><label>1</label><figDesc></figDesc><table>GEO ATIS 
Previous Work 
Zettlemoyer and Collins (2007) 
84.6 
Kwiatkowski et al. (2010) 
88.9 
Liang et al. (2011) 2 
91.1 
Kwiatkowski et al. (2011) 
88.6 
82.8 
Poon (2013) 
83.5 
Zhao and Huang (2015) 
88.9 
84.2 
Our Model 
No Recombination 
85.0 
76.3 
ABSENTITIES 
85.4 
79.9 
ABSWHOLEPHRASES 
87.5 
CONCAT-2 
84.6 
79.0 
CONCAT-3 
77.5 
AWP + AE 
88.9 
AE + C2 
78.8 
AWP + AE + C2 
89.3 
AE + C3 
83.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Test accuracy using different data recom-
bination strategies on GEO and ATIS. AE is AB-
SENTITIES, AWP is ABSWHOLEPHRASES, C2 is 
CONCAT-2, and C3 is CONCAT-3. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Test accuracy using different data recombination strategies on the OVERNIGHT tasks. 

</table></figure>

			<note place="foot" n="4"> Data Recombination 4.1 Motivation The main contribution of this paper is a novel data recombination framework that injects important prior knowledge into our oblivious sequence-tosequence RNN. In this framework, we induce a high-precision generative model from the training data, then sample from it to generate new training examples. The process of inducing this generative model can leverage any available prior knowledge, which is transmitted through the generated examples to the RNN model. A key advantage of our two-stage approach is that it allows us to declare desired properties of the task which might be hard to capture in the model architecture. 1 On GEO and ATIS, we make a point not to rely on orthography for non-entities such as &quot;state&quot; to _state, since this leverages information not available to previous models (Zettlemoyer and Collins, 2005) and is much less languageindependent.</note>

			<note place="foot" n="3"> Technically, these are not completely independent, as we sample these new examples without replacement. The same applies to the longer &quot;independent&quot; examples.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">UW SPF: The University of Washington semantic parsing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.3011</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Python for Scientific Computing Conference</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the world&apos;s response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A deep architecture for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop on Semantic Parsing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="22" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vocal tract length perturbation (vtlp) improves speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using semantic unification to generate regular expressions from natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="826" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inducing probabilistic CCG grammars from logical form with higher-order unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1223" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lexical generalization in CCG grammar induction for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1512" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative oversampling for mining imbalanced datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining (DMIN)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Listen, attend, and walk: Neural mapping of navigational instructions to action sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Uptraining for accurate deterministic question parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alshawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Grounded unsupervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2674" to="2682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2755" to="2763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dropout training as adaptive regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">That&apos;s so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using #petpeeve tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Building a semantic parser overnight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning for semantic parsing with statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="439" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning synchronous grammars for semantic parsing with lambda calculus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="960" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Online learning of relaxed CCG grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Characterlevel convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Type-driven incremental semantic parsing with polymorphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
