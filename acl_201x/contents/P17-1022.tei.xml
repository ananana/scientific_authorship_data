<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Translating Neuralese</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anca</forename><surname>Dragan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
						</author>
						<title level="a" type="main">Translating Neuralese</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="232" to="242"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1022</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Several approaches have recently been proposed for learning decentralized deep mul-tiagent policies that coordinate via a dif-ferentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents&apos; messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmat-ics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Several recent papers have described approaches for learning deep communicating policies (DCPs): decentralized representations of behavior that en- able multiple agents to communicate via a differ- entiable channel that can be formulated as a recur- rent neural network. DCPs have been shown to solve a variety of coordination problems, including reference games ( <ref type="bibr" target="#b15">Lazaridou et al., 2016b</ref>), logic puzzles ( <ref type="bibr" target="#b5">Foerster et al., 2016)</ref>, and simple control ( <ref type="bibr" target="#b24">Sukhbaatar et al., 2016)</ref>. Appealingly, the agents' communication protocol can be learned via direct</p><formula xml:id="formula_0">z (1) a z (2) a z (1) b z (2) b</formula><p>Figure 1: Example interaction between a pair of agents in a deep communicating policy. Both cars are attempting to cross the intersection, but cannot see each other. By exchanging message vectors z (t) , the agents are able to coordinate and avoid a collision. This paper presents an approach for under- standing the contents of these message vectors by translating them into natural language. backpropagation through the communication chan- nel, avoiding many of the challenging inference problems associated with learning in classical de- centralized decision processes ( <ref type="bibr" target="#b22">Roth et al., 2005</ref>).</p><p>But analysis of the strategies induced by DCPs has remained a challenge. As an example, <ref type="figure">Figure 1</ref> depicts a driving game in which two cars, which are unable to see each other, must both cross an intersection without colliding. In order to ensure success, it is clear that the cars must communi- cate with each other. But a number of successful communication strategies are possible-for exam- ple, they might report their exact (x, y) coordinates at every timestep, or they might simply announce whenever they are entering and leaving the inter- section. If these messages were communicated in natural language, it would be straightforward to determine which strategy was being employed. However, DCP agents instead communicate with an automatically induced protocol of unstructured, real-valued recurrent state vectors-an artificial language we might call "neuralese," which superfi- cially bears little resemblance to natural language, and thus frustrates attempts at direct interpretation.</p><p>We propose to understand neuralese messages by translating them. In this work, we present a sim- ple technique for inducing a dictionary that maps between neuralese message vectors and short natu- ral language strings, given only examples of DCP agents interacting with other agents, and humans interacting with other humans. Natural language already provides a rich set of tools for describing beliefs, observations, and plans-our thesis is that these tools provide a useful complement to the visu- alization and ablation techniques used in previous work on understanding complex models <ref type="bibr" target="#b23">(Strobelt et al., 2016;</ref><ref type="bibr" target="#b21">Ribeiro et al., 2016)</ref>.</p><p>While structurally quite similar to the task of machine translation between pairs of human lan- guages, interpretation of neuralese poses a number of novel challenges. First, there is no natural source of parallel data: there are no bilingual "speakers" of both neuralese and natural language. Second, there may not be a direct correspondence between the strategy employed by humans and DCP agents: even if it were constrained to communicate using natural language, an automated agent might choose to produce a different message from humans in a given state. We tackle both of these challenges by appealing to the grounding of messages in game- play. Our approach is based on one of the core insights in natural language semantics: messages (whether in neuralese or natural language) have similar meanings when they induce similar beliefs about the state of the world.</p><p>Based on this intuition, we introduce a transla- tion criterion that matches neuralese messages with natural language strings by minimizing statistical distance in a common representation space of dis- tributions over speaker states. We explore several related questions:</p><p>• What makes a good translation, and under what conditions is translation possible at all? (Section 4)</p><p>• How can we build a model to translate between neuralese and natural language? (Section 5)</p><p>• What kinds of theoretical guarantees can we provide about the behavior of agents communicating via this translation model? (Section 6)</p><p>Our translation model and analysis are general, and in fact apply equally to human-computer and Figure 2: Overview of our approach-best-scoring transla- tions generated for a reference game involving images of birds. The speaking agent's goal is to send a message that uniquely identifies the bird on the left. From these translations it can be seen that the learned model appears to discriminate based on coarse attributes like size and color.</p><p>human-human translation problems grounded in gameplay. In this paper, we focus our experiments specifically on the problem of interpreting commu- nication in deep policies, and apply our approach to the driving game in <ref type="figure">Figure 1</ref> and two reference games of the kind shown in <ref type="figure">Figure 2</ref>. We find that this approach outperforms a more conventional ma- chine translation criterion both when attempting to interoperate with neuralese speakers and when predicting their state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>A variety of approaches for learning deep policies with communication were proposed essentially si- multaneously in the past year. We have broadly labeled these as "deep communicating policies"; concrete examples include <ref type="bibr" target="#b15">Lazaridou et al. (2016b)</ref>, <ref type="bibr" target="#b5">Foerster et al. (2016)</ref>, and <ref type="bibr" target="#b24">Sukhbaatar et al. (2016)</ref>. The policy representation we employ in this paper is similar to the latter two of these, although the general framework is agnostic to low-level model- ing details and could be straightforwardly applied to other architectures. Analysis of communication strategies in all these papers has been largely ad- hoc, obtained by clustering states from which simi- lar messages are emitted and attempting to manu- ally assign semantics to these clusters. The present work aims at developing tools for performing this analysis automatically. Most closely related to our approach is that of <ref type="bibr" target="#b14">Lazaridou et al. (2016a)</ref>, who also develop a model for assigning natural language interpretations to learned messages; however, this approach relies on supervised cluster labels and is targeted specif- ically towards referring expression games. Here we attempt to develop an approach that can handle general multiagent interactions without assuming a prior discrete structure in space of observations. The literature on learning decentralized multi- agent policies in general is considerably larger ( <ref type="bibr" target="#b1">Bernstein et al., 2002;</ref><ref type="bibr" target="#b3">Dibangoye et al., 2016)</ref>. This includes work focused on communication in multiagent settings ( <ref type="bibr" target="#b22">Roth et al., 2005</ref>) and even communication using natural language messages ( <ref type="bibr" target="#b28">Vogel et al., 2013b</ref>). All of these approaches em- ploy structured communication schemes with man- ually engineered messaging protocols; these are, in some sense, automatically interpretable, but at the cost of introducing considerable complexity into both training and inference.</p><p>Our evaluation in this paper investigates com- munication strategies that arise in a number of dif- ferent games, including reference games and an extended-horizon driving game. Communication strategies for reference games were previously ex- plored by <ref type="bibr" target="#b27">Vogel et al. (2013a)</ref>, <ref type="bibr" target="#b0">Andreas and Klein (2016)</ref> and <ref type="bibr" target="#b12">Kazemzadeh et al. (2014)</ref>, and refer- ence games specifically featuring end-to-end com- munication protocols by <ref type="bibr" target="#b31">Yu et al. (2016)</ref>. On the control side, a long line of work considers nonver- bal communication strategies in multiagent policies <ref type="bibr" target="#b4">(Dragan and Srinivasa, 2013)</ref>.</p><p>Another group of related approaches focuses on the development of more general machinery for interpreting deep models in which messages have no explicit semantics. This includes both visualiza- tion techniques <ref type="bibr" target="#b32">(Zeiler and Fergus, 2014;</ref><ref type="bibr" target="#b23">Strobelt et al., 2016)</ref>, and approaches focused on generat- ing explanations in the form of natural language <ref type="bibr" target="#b11">(Hendricks et al., 2016;</ref><ref type="bibr" target="#b26">Vedantam et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem formulation</head><p>Games Consider a cooperative game with two players a and b of the form given in <ref type="figure" target="#fig_2">Figure 3</ref>. At every step t of this game, player a makes an ob- servation x The distributions p(z|x) and p(u|x, z) may also be viewed as defining a language: they specify how a speaker will generate messages based on world states, and how a listener will respond to these mes-</p><formula xml:id="formula_1">a b x (1) a x (1) b x (2) b u (1) a u (2) a u (2) b u (1) b z (1) a z (2) a z (1) b z (2) b a b x (2)</formula><p>a 0.3: stop 0.5: forward 0.1: left 0.1: right observations actions messages At every timestep t, players a and b make an observation x (t) and receive a message z (t−1) , then produce an action u (t) and a new message z (t) .</p><p>sages. Our goal in this work is to learn to translate between pairs of languages generated by different policies. Specifically, we assume that we have ac- cess to two policies for the same game: a "robot policy" π r and a "human policy" π h . We would like to use the representation of π h , the behavior of which is transparent to human users, in order to un- derstand the behavior of π r (which is in general an uninterpretable learned model); we will do this by inducing bilingual dictionaries that map message vectors z r of π r to natural language strings z h of π h and vice-versa.</p><p>Learned agents π r Our goal is to present tools for interpretation of learned messages that are ag- nostic to the details of the underlying algorithm for acquiring them. We use a generic DCP model as a basis for the techniques developed in this paper.</p><p>Here each agent policy is represented as a deep recurrent Q network <ref type="bibr" target="#b10">(Hausknecht and Stone, 2015)</ref>. This network is built from communicating cells of the kind depicted in <ref type="figure" target="#fig_3">Figure 4</ref>. At every timestep, this agent receives three pieces of information: an  <ref type="formula" target="#formula_4">(2016)</ref>). MLP denotes a multilayer perceptron; GRU denotes a gated recurrent unit ( <ref type="bibr" target="#b2">Cho et al., 2014</ref>). Dashed lines represent recurrent connections. observation of the current state of the world, the agent's memory vector from the previous timestep, and a message from the other player. It then pro- duces three outputs: a predicted Q value for every possible action, a new memory vector for the next timestep, and a message to send to the other agent. <ref type="bibr" target="#b24">Sukhbaatar et al. (2016)</ref> observe that models of this form may be viewed as specifying a single RNN in which weight matrices have a particular block structure. Such models may thus be trained using the standard recurrent Q-learning objective, with communication protocol learned end-to-end.</p><formula xml:id="formula_2">x (t) a z (t1) b h (t1) a h (t) a u (t) a z (t) a MLP GRU</formula><p>Human agents π h The translation model we de- velop requires a representation of the distribution over messages p(z a |x a ) employed by human speak- ers (without assuming that humans and agents pro- duce equivalent messages in equivalent contexts). We model the human message generation process as categorical, and fit a simple multilayer percep- tron model to map from observations to words and phrases used during human gameplay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">What's in a translation?</head><p>What does it mean for a message z h to be a "trans- lation" of a message z r ? In standard machine trans- lation problems, the answer is that z h is likely to co-occur in parallel data with z r ; that is, p(z h |z r ) is large. Here we have no parallel data: even if we could observe natural language and neuralese messages produced by agents in the same state, we would have no guarantee that these messages ac- tually served the same function. Our answer must instead appeal to the fact that both natural language and neuralese messages are grounded in a common environment. For a given neuralese message z r , we will first compute a grounded representation of that message's meaning; to translate, we find a natural-language message whose meaning is most similar. The key question is then what form this grounded meaning representation should take. The existing literature suggests two broad approaches:</p><p>Semantic representation The meaning of a mes- sage z a is given by its denotations: that is, by the set of world states of which z a may be felicitously predicated, given the existing context available to a listener. In probabilistic terms, this says that the meaning of a message z a is represented by the dis- tribution p(x a |z a , x b ) it induces over speaker states. Examples of this approach include <ref type="bibr" target="#b9">Guerin and Pitt (2001)</ref> and <ref type="bibr" target="#b19">Pasupat and Liang (2016)</ref>.</p><p>Pragmatic representation The meaning of a message z a is given by the behavior it induces in a listener. In probabilistic terms, this says that the meaning of a message z a is represented by the dis- tribution p(u b |z a , x b ) it induces over actions given the listener's observation x b . Examples of this ap- proach include <ref type="bibr" target="#b27">Vogel et al. (2013a)</ref> and <ref type="bibr" target="#b8">Gauthier and Mordatch (2016)</ref>.</p><p>These two approaches can give rise to rather dif- ferent behaviors. Consider the following example:</p><p>square hexagon circle few many many</p><p>The top language (in blue) has a unique name for every kind of shape, while the bottom language (in red) only distinguishes between shapes with few sides and shapes with many sides. Now imagine a simple reference game with the following form: player a is covertly assigned one of these three shapes as a reference target, and communicates that reference to b; b must then pull a lever labeled large or small depending on the size of the target shape. Blue language speakers can achieve perfect success at this game, while red language speakers can succeed at best two out of three times. How should we translate the blue word hexagon into the red language? The semantic approach sug- gests that we should translate hexagon as many: while many does not uniquely identify the hexagon, it produces a distribution over shapes that is clos- est to the truth. The pragmatic approach instead suggests that we should translate hexagon as few, as this is the only message that guarantees that the listener will pull the correct lever large. So in order to produce a correct listener action, the trans- lator might have to "lie" and produce a maximally inaccurate listener belief.</p><p>If we were exclusively concerned with building a translation layer that allowed humans and DCP agents to interoperate as effectively as possible, it would be natural to adopt a pragmatic representa- tion strategy. But our goals here are broader: we also want to facilitate understanding, and specif- ically to help users of learned systems form true beliefs about the systems' computational processes and representational abstractions. The example above demonstrates that "pragmatically" optimiz- ing directly for task performance can sometimes lead to translations that produce inaccurate beliefs.</p><p>We instead build our approach around seman- tic representations of meaning. By preserving se- mantics, we allow listeners to reason accurately about the content and interpretation of messages. We might worry that by adopting a semantics-first view, we have given up all guarantees of effective interoperation between humans and agents using a translation layer. Fortunately, this is not so: as we will see in Section 6, it is possible to show that players communicating via a semantic translator perform only boundedly worse (and sometimes bet- ter!) than pairs of players with a common language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Translation models</head><p>In this section, we build on the intuition that mes- sages should be translated via their semantics to define a concrete translation model-a procedure for constructing a natural language ↔ neuralese dictionary given agent and human interactions.</p><p>We understand the meaning of a message z a to be represented by the distribution p(x a |z a , x b ) it induces over speaker states given listener context. We can formalize this by defining the belief distribution β for a message z and context x b as:</p><formula xml:id="formula_3">β(z a , x b ) = p(x a |z a , x b ) = p(z a |x a )p(x b |x a ) x a p(z a |x a )p(x b |x a )</formula><p>Here we have modeled the listener as performing a single step of Bayesian inference, using the lis- tener state and the message generation model (by assumption shared between players) to compute the posterior over speaker states. While in gen- eral neither humans nor DCP agents compute ex- plicit representations of this posterior, past work has found that both humans and suitably-trained neural networks can be modeled as Bayesian rea- soners ( <ref type="bibr" target="#b6">Frank et al., 2009;</ref><ref type="bibr" target="#b18">Paige and Wood, 2016)</ref>.</p><p>This provides a context-specific representation of belief, but for messages z and z to have the same semantics, they must induce the same belief over all contexts in which they occur. In our probabilis- tic formulation, this introduces an outer expectation over contexts, providing a final measure q of the quality of a translation from z to z :</p><formula xml:id="formula_4">q(z, z ) = E D KL (β(z, X b ) || β(z , X b )) | z, z = xa,x b p(x a , x b |z, z )D KL (β(z, x b ) || β(z , x b )) ∝ xa,x b p(x a , x b ) · p(z|x a ) · p(z |x a ) · D KL (β(z, x b ) || β(z , x b ));<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Translating messages</head><p>given: a phrase inventory L function TRANSLATE(z) return arg min z ∈Lˆq∈Lˆ ∈Lˆq(z, z ) functionˆqfunctionˆ functionˆq(z, z ) // sample contexts and distractors</p><formula xml:id="formula_5">x ai , x bi ∼ p(X a , X b ) for i = 1..n x ai ∼ p(X a |x bi ) // compute context weights˜w weights˜ weights˜w i ← p(z|x ai ) · p(z |x ai ) w i ← ˜ w i / j ˜ w j // compute divergences k i ← x∈{xa,x a } p(z|x) log p(z|x) p(z |x) return i w i k i</formula><p>recalling that in this setting</p><formula xml:id="formula_6">D KL (β || β ) = xa p(x a |z, x b ) log p(x a |z, x b ) p(x a |z , x b ) ∝ xa p(x a |x b )p(z|x a ) log p(z|x a ) p(z |x a )<label>(2)</label></formula><p>which is zero when the messages z and z give rise to identical belief distributions and increases as they grow more dissimilar. To translate, we would like to compute tr(z r ) = arg min z h q(z r , z h ) and tr(z h ) = arg min zr q(z h , z r ). Intuitively, Equa- tion 1 says that we will measure the quality of a proposed translation z → z by asking the follow- ing question: in contexts where z is likely to be used, how frequently does z induce the same belief about speaker states as z? While this translation criterion directly encodes the semantic notion of meaning described in Sec- tion 4, it is doubly intractable: the KL divergence and outer expectation involve a sum over all obser- vations x a and x b respectively; these sums are not in general possible to compute efficiently. To avoid this, we approximate Equation 1 by sampling. We draw a collection of samples (x a , x b ) from the prior over world states, and then generate for each sam- ple a sequence of distractors (x a , x b ) from p(x a |x b ) (we assume access to both of these distributions from the problem representation). The KL term in Equation 1 is computed over each true sample and its distractors, which are then normalized and averaged to compute the final score.</p><p>Sampling accounts for the outer p(x a , x b ) in Equation 1 and the inner p(x a |x b ) in Equation 2. The only quantities remaining are of the form p(z|x a ). In the case of neuralese, this distribu- tion already is part of the definition of the agent policy π r and can be reused directly. For natural language, we use transcripts of human interactions to fit a model that maps from world states to a dis- tribution over frequent utterances as discussed in Section 3. Details of these model implementations are provided in Appendix B, and the full translation procedure is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Belief and behavior</head><p>The translation criterion in the previous section makes no reference to listener actions at all. The shapes example in Section 4 shows that some model performance might be lost under translation. It is thus reasonable to ask whether this transla- tion model of Section 5 can make any guarantees about the effect of translation on behavior. In this section we explore the relationship between belief- preserving translations and the behaviors they pro- duce, by examining the effect of belief accuracy and strategy mismatch on the reward obtained by cooperating agents.</p><p>To facilitate this analysis, we consider a sim- plified family of communication games with the structure depicted in <ref type="figure" target="#fig_4">Figure 5</ref>. These games can be viewed as a subset of the family depicted in <ref type="figure" target="#fig_2">Fig- ure 3</ref>; and consist of two steps: a listener makes an observation x a and sends a single message z to a speaker, which makes its own observation x b , takes a single action u, and receives a reward. We emphasize that the results in this section concern the theoretical properties of idealized games, and are presented to provide intuition about high-level properties of our approach. Section 8 investigates empirical behavior of this approach on real-world tasks where these ideal conditions do not hold.</p><p>Our first result is that translations that minimize semantic dissimilarity q cause the listener to take near-optimal actions: 2 Proposition 1. Semantic translations reward rational listeners. Define a rational listener as one that chooses the best action in expectation over the speaker's state:</p><formula xml:id="formula_7">U (z, x b ) = arg max u xa p(x a |x b , z)r(x a , x b , u)</formula><p>for a reward function r ∈ <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> that depends only on the two observations and the action. <ref type="bibr">3</ref> Now let a be a speaker of a language r, b be a listener of the same language r, and b be a listener of a different language h. Suppose that we wish for a and b to interact via the translator tr : z r → z h (so that a produces a message z r , and b takes an action U (z h = tr(z r ), x b )). If tr respects the semantics of z r , then the bilingual pair a and b achieves only boundedly worse reward than the monolingual pair a and b. Specifically, if q(z r , z h ) ≤ D, then</p><formula xml:id="formula_8">Er(X a , X b , U (tr(Z)) ≥ Er(X a , X b , U (Z)) − √ 2D<label>(3)</label></formula><p>So as discussed in Section 4, even by committing to a semantic approach to meaning representation, we have still succeeded in (approximately) captur- ing the nice properties of the pragmatic approach.</p><p>Section 4 examined the consequences of a mis- match between the set of primitives available in two languages. In general we would like some measure of our approach's robustness to the lack of an exact correspondence between two languages. In the case of humans in particular we expect that a variety of different strategies will be employed, many of which will not correspond to the behavior of the learned agent. It is natural to want some as- surance that we can identify the DCP's strategy as long as some human strategy mirrors it. Our second observation is that it is possible to exactly recover a translation of a DCP strategy from a mixture of humans playing different strategies:</p><formula xml:id="formula_9">Proposition 2.</formula><p>Semantic translations find hidden correspondences. Consider a fixed robot policy π r and a set of human policies { π h1 , π h2 , . . . } (recalling from Section 3 that each π is defined by distributions p <ref type="figure">(z |x a ) and p(u|z , x b )</ref>). Suppose further that the messages employed by these human strate- gies are disjoint; that is, if p h i (z |x a ) &gt; 0, then p h j (z |x a ) = 0 for all j = i. Now suppose that all q(z r , z h ) = 0 for all messages in the support of some p h i (z |x a ) and &gt; 0 for all j = i. Then every message z r is translated into a message pro- duced by π h i , and messages from other strategies are ignored.</p><p>This observation follows immediately from the definition of q(z r , z h ), but demonstrates one of the key distinctions between our approach and a conventional machine translation criterion. Maxi- mizing p(z h |z r ) will produce the natural language message most often produced in contexts where z r is observed, regardless of whether that message is useful or informative. By contrast, minimizing q(z h , z r ) will find the z h that corresponds most closely to z r even when z h is rarely used.</p><p>The disjointness condition, while seemingly quite strong, in fact arises naturally in many circumstances-for example, players in the driving game reporting their spatial locations in absolute vs. relative coordinates, or speakers in a color refer- ence game ( <ref type="figure" target="#fig_5">Figure 6</ref>) discriminating based on light- ness vs. hue. It is also possible to relax the above condition to require that strategies be only locally disjoint (i.e. with the disjointness condition holding for each fixed x a ), in which case overlapping hu- man strategies are allowed, and the recovered robot strategy is a context-weighted mixture of these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Tasks</head><p>In the remainder of the paper, we evaluate the em- pirical behavior of our approach to translation. Our evaluation considers two kinds of tasks: reference games and navigation games. In a reference game (e.g. <ref type="figure" target="#fig_5">Figure 6a</ref>), both players observe a pair of can- didate referents. A speaker is assigned a target ref- erent; it must communicate this target to a listener, who then performs a choice action corresponding to its belief about the true target. In this paper we consider two variants on the reference game: a sim- ple color-naming task, and a more complex task involving natural images of birds. For examples of human communication strategies for these tasks, we obtain the XKCD color dataset <ref type="bibr" target="#b16">(McMahan and Stone, 2015;</ref><ref type="bibr" target="#b17">Monroe et al., 2016</ref>) and the Caltech Birds dataset <ref type="bibr" target="#b29">(Welinder et al., 2010</ref>) with accom- panying natural language descriptions ( <ref type="bibr" target="#b20">Reed et al., 2016)</ref>. We use standard train / validation / test splits for both of these datasets. The final task we consider is the driving task <ref type="figure" target="#fig_5">(Figure 6c</ref>) first discussed in the introduction. In this task, two cars, invisible to each other, must each navigate between randomly assigned start and goal positions without colliding. This task takes a number of steps to complete, and potentially in- volves a much broader range of communication strategies. To obtain human annotations for this task, we recorded both actions and messages gener- ated by pairs of human Amazon Mechanical Turk workers playing the driving game with each other. We collected close to 400 games, with a total of more than 2000 messages exchanged, from which we held out 100 game traces as a test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Metrics</head><p>A mechanism for understanding the behavior of a learned model should allow a human user both to correctly infer its beliefs and to successfully interoperate with it; we accordingly report results of both "belief" and "behavior" evaluations.</p><p>To support easy reproduction and comparison (and in keeping with standard practice in machine translation), we focus on developing automatic measures of system performance. We use the avail- able training data to develop simulated models of human decisions; by first showing that these mod- els track well with human judgments, we can be confident that their use in evaluations will corre- late with human understanding. We employ the following two metrics:</p><p>Belief evaluation This evaluation focuses on the denotational perspective in semantics that moti- vated the initial development of our model. We have successfully understood the semantics of a message z r if, after translating z r → z h , a human listener can form a correct belief about the state in which z r was produced. We construct a simple state-guessing game where the listener is presented with a translated message and two state observa- tions, and must guess which state the speaker was in when the message was emitted.</p><p>When translating from natural language to neu- ralese, we use the learned agent model to directly guess the hidden state. For neuralese to natural language we must first construct a "model human listener" to map from strings back to state repre- sentations; we do this by using the training data to fit a simple regression model that scores (state, sen- tence) pairs using a bag-of-words sentence repre- sentation. We find that our "model human" matches the judgments of real humans 83% of the time on the colors task, 77% of the time on the birds task, and 77% of the time on the driving task. This gives us confidence that the model human gives a reason- ably accurate proxy for human interpretation.</p><p>Behavior evaluation This evaluation focuses on the cooperative aspects of interpretability: we mea- sure the extent to which learned models are able to interoperate with each other by way of a transla- tion layer. In the case of reference games, the goal of this semantic evaluation is identical to the goal of the game itself (to identify the hidden state of the speaker), so we perform this additional prag- matic evaluation only for the driving game. We found that the most data-efficient and reliable way to make use of human game traces was to construct a "deaf" model human. The evaluation selects a full game trace from a human player, and replays both the human's actions and messages exactly (dis- regarding any incoming messages); the evaluation measures the quality of the natural-language-to- neuralese translator, and the extent to which the  learned agent model can accommodate a (real) hu- man given translations of the human's messages.</p><p>Baselines We compare our approach to two base- lines: a random baseline that chooses a translation of each input uniformly from messages observed during training, and a direct baseline that directly maximizes p(z |z) (by analogy to a conventional machine translation system). This is accomplished by sampling from a DCP speaker in training states labeled with natural language strings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Results</head><p>In all below, "R" indicates a DCP agent, "H" in- dicates a real human, and "H*" indicates a model human player.</p><p>Reference games Results for the two reference games are shown in   Scores are presented in the form "reward / completion rate". While less accurate than either humans or DCPs with a shared language, the models that employ a translation layer obtain higher reward and a greater overall success rate than baselines.</p><p>cases, while a model trained to communicate in natural language achieves somewhat lower perfor- mance. Regardless of whether the speaker is a DCP and the listener a model human or vice-versa, translation based on the belief-matching criterion in Section 5 achieves the best performance; indeed, when translating neuralese color names to natural language, the listener is able to achieve a slightly higher score than it is natively. This suggests that the automated agent has discovered a more effec- tive strategy than the one demonstrated by humans in the dataset, and that the effectiveness of this strategy is preserved by translation. Example trans- lations from the reference games are depicted in <ref type="figure">Figure 2</ref> and <ref type="figure">Figure 7</ref>.</p><p>Driving game Behavior evaluation of the driving game is shown in <ref type="table" target="#tab_3">Table 3</ref>, and belief evaluation is shown in <ref type="table" target="#tab_2">Table 2</ref>. Translation of messages in the driving game is considerably more challenging than in the reference games, and scores are uniformly lower; however, a clear benefit from the belief- matching model is still visible. Belief matching leads to higher scores on the belief evaluation in both directions, and allows agents to obtain a higher reward on average (though task completion rates remain roughly the same across all agents). Some example translations of driving game messages are shown in <ref type="figure" target="#fig_7">Figure 8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We have investigated the problem of interpreting message vectors from deep networks by translat- ing them. After introducing a translation criterion based on matching listener beliefs about speaker states, we presented both theoretical and empirical evidence that this criterion outperforms a conven- tional machine translation approach at recovering the content of message vectors and facilitating col- laboration between humans and learned agents. While our evaluation has focused on under- standing the behavior of deep communicating poli- cies, the framework proposed in this paper could be much more generally applied. Any encoder- decoder model <ref type="bibr" target="#b25">(Sutskever et al., 2014</ref>) can be thought of as a kind of communication game played between the encoder and the decoder, so we can analogously imagine computing and translating "beliefs" induced by the encoding to explain what features of the input are being transmitted. The cur- rent work has focused on learning a purely categor- ical model of the translation process, supported by an unstructured inventory of translation candidates, and future work could explore the compositional structure of messages, and attempt to synthesize novel natural language or neuralese messages from scratch. More broadly, the work here shows that the denotational perspective from formal seman- tics provides a framework for precisely framing the demands of interpretable machine learning <ref type="bibr" target="#b30">(Wilson et al., 2016)</ref>, and particularly for ensuring that human users without prior exposure to a learned model are able to interoperate with it, predict its behavior, and diagnose its errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>240</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>large bird black wings black crown agent translator agent translator small brown light brown dark brown</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>It then takes an action u (t) a and sends a message z (t) a to b. (The process is symmetric for b.) The distributions p(u a |x a , z b ) and p(z a |x a ) together define a policy π which we assume is shared by both players, i.e. p(u a |x a , z b ) = p(u b |x b , z a ) and p(z a |x a ) = p(z b |x b ). As in a standard Markov decision process, the actions (u (t) a , u (t) b ) alter the world state, generating new observations for both players and a reward shared by both.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Schematic representation of communication games. At every timestep t, players a and b make an observation x (t) and receive a message z (t−1) , then produce an action u (t) and a new message z (t) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Cell implementing a single step of agent communication (compare with Sukhbaatar et al. (2016) and Foerster et al. (2016)). MLP denotes a multilayer perceptron; GRU denotes a gated recurrent unit (Cho et al., 2014). Dashed lines represent recurrent connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Simplified game representation used for analysis in Section 6. A speaker agent sends a message to a listener agent, which takes a single action and receives a reward.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Tasks used to evaluate the translation model. (a-b) Reference games: both players observe a pair of reference candidates (colors or images); Player a is assigned a target (marked with a star), which player b must guess based on a message from a. (c) Driving game: each car attempts to navigate to its goal (marked with a star). The cars cannot see each other, and must communicate to avoid a collision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Best-scoring translations generated for driving task generated from the given speaker state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>The end-to-end trained 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Belief evaluation results for the driving game. Driving 
states are challenging to identify based on messages alone (as 
evidenced by the comparatively low scores obtained by single-
language pairs) . Translation based on belief achieves the best 
overall performance in both directions. 

R / R 
H / H 
R / H 

1.93 / 0.71 
-/ 0.77 

1.35 / 0.64 random 
1.49 / 0.67 direct 
1.54 / 0.67 belief (ours) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Behavior evaluation results for the driving game.</figDesc><table></table></figure>

			<note place="foot" n="1"> We have released code and data at http://github. com/jacobandreas/neuralese.</note>

			<note place="foot" n="2"> Proof is provided in Appendix A.</note>

			<note place="foot" n="3"> This notion of rationality is a fairly weak one: it permits many suboptimal communication strategies, and requires only that the listener do as well as possible given a fixed speakera first-order optimality criterion likely to be satisfied by any richly-parameterized model trained via gradient descent.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>JA is supported by a Facebook Graduate Fellowship and a Berkeley AI / Huawei Fellowship. We are grateful to Lisa Anne Hendricks for assistance with the Caltech Birds dataset.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reasoning about pragmatics with neural listeners and speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The complexity of decentralized control of Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Daniel S Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Givan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Immerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zilberstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of operations research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="819" to="840" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimally solving Dec-POMDPs as continuous-state MDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Jilles Steeve Dibangoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Buffet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charpillet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="443" to="497" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating legible motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anca</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to communicate with deep multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2137" to="2145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Informative communication in word production and word learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Noah D Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual conference of the cognitive science society</title>
		<meeting>the 31st annual conference of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1228" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A paradigm for situated and goal-driven language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.03585</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Denotational semantics for agent communication language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Guerin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Pitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth international conference on Autonomous agents</title>
		<meeting>the fifth international conference on Autonomous agents</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06527</idno>
		<title level="m">Deep recurrent q-learning for partially observable mdps</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generating visual explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ReferItGame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multi-agent cooperation and the emergence of (natural) language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Peysakhovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07182</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Towards multi-agent communication-based language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07133</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Bayesian model of grounded color semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="103" to="115" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning to generate compositional color descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Noah D Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03821</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Inference networks for sequential monte carlo in graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooks</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Inferring logical forms from denotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06900</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Why should I trust you?: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reasoning about joint beliefs for executiontime communication decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems</title>
		<meeting>the fourth international joint conference on Autonomous agents and multiagent systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="786" to="793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07461</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Context-aware captions from context-agnostic supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Emergence of Gricean maxims from multi-agent decision theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bodoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1072" to="1081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Implicatures and nested beliefs in approximate Decentralized-POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">and William Herlands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Been</forename><surname>Andrew Gordon Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09139</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of nips 2016 workshop on interpretable machine learning for complex systems</title>
		<meeting>nips 2016 workshop on interpretable machine learning for complex systems</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A joint speaker-listener-reinforcer model for referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.09542</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
