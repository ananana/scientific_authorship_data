<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1715" to="1725"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural machine translation (NMT) models typically operate with a fixed vocabulary , but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper , we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via com-positional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm , and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU, respectively.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation has recently shown impressive results <ref type="bibr" target="#b7">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b26">Sutskever et al., 2014;</ref><ref type="bibr">Bahdanau et al., 2015)</ref>. However, the translation of rare words is an open problem. The vocabulary of neu- ral models is typically limited to 30 000-50 000 words, but translation is an open-vocabulary prob-</p><p>The research presented in this publication was conducted in cooperation with Samsung Electronics Polska sp. z o.o. - Samsung R&amp;D Institute Poland. lem, and especially for languages with produc- tive word formation processes such as aggluti- nation and compounding, translation models re- quire mechanisms that go below the word level. As an example, consider compounds such as the German Abwasser|behandlungs|anlange 'sewage water treatment plant', for which a segmented, variable-length representation is intuitively more appealing than encoding the word as a fixed-length vector.</p><p>For word-level NMT models, the translation of out-of-vocabulary words has been addressed through a back-off to a dictionary look-up ( <ref type="bibr" target="#b6">Jean et al., 2015;</ref><ref type="bibr" target="#b16">Luong et al., 2015b</ref>). We note that such techniques make assumptions that often do not hold true in practice. For instance, there is not al- ways a 1-to-1 correspondence between source and target words because of variance in the degree of morphological synthesis between languages, like in our introductory compounding example. Also, word-level models are unable to translate or gen- erate unseen words. Copying unknown words into the target text, as done by <ref type="bibr" target="#b6">(Jean et al., 2015;</ref><ref type="bibr" target="#b16">Luong et al., 2015b)</ref>, is a reasonable strategy for names, but morphological changes and transliteration is often required, especially if alphabets differ.</p><p>We investigate NMT models that operate on the level of subword units. Our main goal is to model open-vocabulary translation in the NMT network itself, without requiring a back-off model for rare words. In addition to making the translation pro- cess simpler, we also find that the subword models achieve better accuracy for the translation of rare words than large-vocabulary models and back-off dictionaries, and are able to productively generate new words that were not seen at training time. Our analysis shows that the neural networks are able to learn compounding and transliteration from sub- word representations.</p><p>This paper has two main contributions:</p><p>• We show that open-vocabulary neural ma-chine translation is possible by encoding (rare) words via subword units. We find our architecture simpler and more effective than using large vocabularies and back-off dictio- naries ( <ref type="bibr" target="#b6">Jean et al., 2015;</ref><ref type="bibr" target="#b16">Luong et al., 2015b</ref>).</p><p>• We adapt byte pair encoding (BPE) <ref type="bibr" target="#b4">(Gage, 1994)</ref>, a compression algorithm, to the task of word segmentation. BPE allows for the representation of an open vocabulary through a fixed-size vocabulary of variable-length character sequences, making it a very suit- able word segmentation strategy for neural network models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation</head><p>We follow the neural machine translation archi- tecture by <ref type="bibr">Bahdanau et al. (2015)</ref>, which we will briefly summarize here. However, we note that our approach is not specific to this architecture. The neural machine translation system is imple- mented as an encoder-decoder network with recur- rent neural networks.</p><p>The encoder is a bidirectional neural network with gated recurrent units ( <ref type="bibr" target="#b0">Cho et al., 2014</ref>) that reads an input sequence x = (x 1 , ..., x m ) and calculates a forward sequence of hidden states (</p><formula xml:id="formula_0">− → h 1 , ..., − → h m )</formula><p>, and a backward sequence</p><formula xml:id="formula_1">( ← − h 1 , ..., ← − h m ).</formula><p>The hidden states − → h j and ← − h j are concatenated to obtain the annotation vector h j .</p><p>The decoder is a recurrent neural network that predicts a target sequence y = (y 1 , ..., y n ). Each word y i is predicted based on a recurrent hidden state s i , the previously predicted word y i−1 , and a context vector c i . c i is computed as a weighted sum of the annotations h j . The weight of each annotation h j is computed through an alignment model α ij , which models the probability that y i is aligned to x j . The alignment model is a single- layer feedforward neural network that is learned jointly with the rest of the network through back- propagation.</p><p>A detailed description can be found in ( <ref type="bibr">Bahdanau et al., 2015)</ref>. Training is performed on a parallel corpus with stochastic gradient descent. For translation, a beam search with small beam size is employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Subword Translation</head><p>The main motivation behind this paper is that the translation of some words is transparent in that they are translatable by a competent transla- tor even if they are novel to him or her, based on a translation of known subword units such as morphemes or phonemes. Word categories whose translation is potentially transparent include:</p><p>• named entities. Between languages that share an alphabet, names can often be copied from source to target text. Transcription or translit- eration may be required, especially if the al- phabets or syllabaries differ. Example: Barack Obama (English; German)</p><p>Áàðàê Îáàìà (Russian) (ba-ra-ku o-ba-ma) (Japanese) • cognates and loanwords. Cognates and loan- words with a common origin can differ in regular ways between languages, so that character-level translation rules are sufficient <ref type="bibr" target="#b29">(Tiedemann, 2012</ref> In an analysis of 100 rare tokens (not among the 50 000 most frequent types) in our German training data 1 , the majority of tokens are poten- tially translatable from English through smaller units.</p><p>We find 56 compounds, 21 names, 6 loanwords with a common origin (emanci- pate→emanzipieren), 5 cases of transparent affix- ation (sweetish 'sweet' + '-ish' → süßlich 'süß' + '-lich'), 1 number and 1 computer language iden- tifier.</p><p>Our hypothesis is that a segmentation of rare words into appropriate subword units is suffi- cient to allow for the neural translation network to learn transparent translations, and to general- ize this knowledge to translate and produce unseen words. <ref type="bibr">2</ref> We provide empirical support for this hy-pothesis in Sections 4 and 5. First, we discuss dif- ferent subword representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Related Work</head><p>For Statistical Machine Translation (SMT), the translation of unknown words has been the subject of intensive research.</p><p>A large proportion of unknown words are names, which can just be copied into the tar- get text if both languages share an alphabet. If alphabets differ, transliteration is required <ref type="bibr" target="#b2">(Durrani et al., 2014</ref>). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely re- lated languages <ref type="bibr" target="#b30">(Vilar et al., 2007;</ref><ref type="bibr" target="#b27">Tiedemann, 2009;</ref><ref type="bibr" target="#b18">Neubig et al., 2012)</ref>.</p><p>The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmen- tation have been investigated <ref type="bibr" target="#b19">(Nießen and Ney, 2000;</ref><ref type="bibr" target="#b9">Koehn and Knight, 2003;</ref><ref type="bibr" target="#b31">Virpioja et al., 2007;</ref><ref type="bibr" target="#b24">Stallard et al., 2012)</ref>. Segmentation al- gorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries.</p><p>The best choice of subword units may be task- specific. For speech recognition, phone-level lan- guage models have been used <ref type="bibr">(Bazzi and Glass, 2000</ref>). <ref type="bibr" target="#b17">Mikolov et al. (2012)</ref> investigate subword language models, and propose to use syllables. For multilingual segmentation tasks, multilingual algorithms have been proposed <ref type="bibr" target="#b23">(Snyder and Barzilay, 2008</ref>). We find these intriguing, but inapplica- ble at test time.</p><p>Various techniques have been proposed to pro- duce fixed-length continuous word vectors based on characters or morphemes ( <ref type="bibr" target="#b14">Luong et al., 2013;</ref><ref type="bibr">Botha and Blunsom, 2014;</ref><ref type="bibr" target="#b12">Ling et al., 2015a;</ref><ref type="bibr" target="#b8">Kim et al., 2015</ref>). An effort to apply such techniques to NMT, parallel to ours, has found no significant improvement over word-based approaches ( <ref type="bibr" target="#b13">Ling et al., 2015b</ref>). One technical difference from our work is that the attention mechanism still oper- ates on the level of words in the model by <ref type="bibr" target="#b13">Ling et al. (2015b)</ref>, and that the representation of each word is fixed-length. We expect that the attention mechanism benefits from our variable-length rep- resentation: the network can learn to place atten- tion on different subword units at each step. Re- call our introductory example Abwasserbehand- lungsanlange, for which a subword segmentation avoids the information bottleneck of a fixed-length representation.</p><p>Neural machine translation differs from phrase- based methods in that there are strong incentives to minimize the vocabulary size of neural models to increase time and space efficiency, and to allow for translation without back-off models. At the same time, we also want a compact representation of the text itself, since an increase in text length reduces efficiency and increases the distances over which neural models need to pass information.</p><p>A simple method to manipulate the trade-off be- tween vocabulary size and text size is to use short- lists of unsegmented words, using subword units only for rare words. As an alternative, we pro- pose a segmentation algorithm based on byte pair encoding (BPE), which lets us learn a vocabulary that provides a good compression rate of the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Byte Pair Encoding (BPE)</head><p>Byte Pair Encoding (BPE) <ref type="bibr" target="#b4">(Gage, 1994</ref>) is a sim- ple data compression technique that iteratively re- places the most frequent pair of bytes in a se- quence with a single, unused byte. We adapt this algorithm for word segmentation. Instead of merg- ing frequent pairs of bytes, we merge characters or character sequences.</p><p>Firstly, we initialize the symbol vocabulary with the character vocabulary, and represent each word as a sequence of characters, plus a special end-of- word symbol '·', which allows us to restore the original tokenization after translation. We itera- tively count all symbol pairs and replace each oc- currence of the most frequent pair ('A', 'B') with a new symbol 'AB'. Each merge operation pro- duces a new symbol which represents a charac- ter n-gram. Frequent character n-grams (or whole words) are eventually merged into a single sym- bol, thus BPE requires no shortlist. The final sym- bol vocabulary size is equal to the size of the initial vocabulary, plus the number of merge operations -the latter is the only hyperparameter of the algo- rithm.</p><p>For efficiency, we do not consider pairs that cross word boundaries. The algorithm can thus be run on the dictionary extracted from a text, with each word being weighted by its frequency. A minimal Python implementation is shown in Al- The main difference to other compression al- gorithms, such as Huffman encoding, which have been proposed to produce a variable-length en- coding of words for NMT ( <ref type="bibr">Chitnis and DeNero, 2015)</ref>, is that our symbol sequences are still in- terpretable as subword units, and that the network can generalize to translate and produce new words (unseen at training time) on the basis of these sub- word units. <ref type="figure" target="#fig_0">Figure 1</ref> shows a toy example of learned BPE operations. At test time, we first split words into sequences of characters, then apply the learned op- erations to merge the characters into larger, known symbols. This is applicable to any word, and allows for open-vocabulary networks with fixed symbol vocabularies. <ref type="bibr">3</ref> In our example, the OOV 'lower' would be segmented into 'low er·'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Learn BPE operations</head><p>We evaluate two methods of applying BPE: learning two independent encodings, one for the source, one for the target vocabulary, or learning the encoding on the union of the two vocabular- ies (which we call joint BPE). <ref type="bibr">4</ref> The former has the advantage of being more compact in terms of text and vocabulary size, and having stronger guaran- tees that each subword unit has been seen in the training text of the respective language, whereas the latter improves consistency between the source and the target segmentation. If we apply BPE in- dependently, the same name may be segmented differently in the two languages, which makes it harder for the neural models to learn a mapping between the subword units. To increase the con- sistency between English and Russian segmenta- tion despite the differing alphabets, we transliter- ate the Russian vocabulary into Latin characters with ISO-9 to learn the joint BPE encoding, then transliterate the BPE merge operations back into Cyrillic to apply them to the Russian training text. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We aim to answer the following empirical ques- tions:</p><p>• Can we improve the translation of rare and unseen words in neural machine translation by representing them via subword units?</p><p>• Which segmentation into subword units per- forms best in terms of vocabulary size, text size, and translation quality?</p><p>We perform experiments on data from the shared translation task of WMT 2015. For English→German, our training set consists of 4.2 million sentence pairs, or approximately 100 mil- lion tokens. For English→Russian, the training set consists of 2.6 million sentence pairs, or approx- imately 50 million tokens. We tokenize and true- case the data with the scripts provided in Moses ( <ref type="bibr" target="#b10">Koehn et al., 2007</ref>). We use newstest2013 as de- velopment set, and report results on newstest2014 and newstest2015.</p><p>We report results with BLEU (mteval-v13a.pl), and CHRF3 <ref type="bibr" target="#b21">(Popovi´cPopovi´c, 2015</ref>), a character n-gram F 3 score which was found to correlate well with human judgments, especially for translations out of English <ref type="bibr" target="#b25">(Stanojevi´cStanojevi´c et al., 2015)</ref>. Since our main claim is concerned with the translation of rare and unseen words, we report separate statis- tics for these. We measure these through unigram F 1 , which we calculate as the harmonic mean of clipped unigram precision and recall. <ref type="bibr">6</ref> We perform all experiments with Groundhog 7 ( <ref type="bibr">Bahdanau et al., 2015)</ref>. We generally follow set- tings by previous work ( <ref type="bibr">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b6">Jean et al., 2015</ref></p><note type="other">). All networks have a hidden layer size of 1000, and an embedding layer size of 620. Following Jean et al. (2015), we only keep a shortlist of τ = 30000 words in memory.</note><p>During training, we use Adadelta <ref type="figure" target="#fig_0">(Zeiler, 2012)</ref>, a minibatch size of 80, and reshuffle the train- ing set between epochs. We train a network for approximately 7 days, then take the last 4 saved models (models being saved every 12 hours), and continue training each with a fixed embedding layer (as suggested by <ref type="bibr" target="#b6">(Jean et al., 2015)</ref>) for 12 hours. We perform two independent training runs for each models, once with cut-off for gradient clipping ( <ref type="bibr" target="#b20">Pascanu et al., 2013</ref>) of 5.0, once with a cut-off of 1.0 -the latter produced better single models for most settings. We report results of the system that performed best on our development set (newstest2013), and of an ensemble of all 8 mod- els.</p><p>We use a beam size of 12 for beam search, with probabilities normalized by sentence length. We use a bilingual dictionary based on fast-align ( <ref type="bibr" target="#b3">Dyer et al., 2013</ref>). For our baseline, this serves as back-off dictionary for rare words. We also use the dictionary to speed up translation for all ex- periments, only performing the softmax over a fil- tered list of candidate translations (like <ref type="bibr" target="#b6">Jean et al. (2015)</ref>, we use K = 30000; K = 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Subword statistics</head><p>Apart from translation quality, which we will ver- ify empirically, our main objective is to represent an open vocabulary through a compact fixed-size subword vocabulary, and allow for efficient train- ing and decoding. <ref type="bibr">8</ref> Statistics for different segmentations of the Ger-man side of the parallel data are shown in <ref type="table">Table  1</ref>. A simple baseline is the segmentation of words into character n-grams. <ref type="bibr">9</ref> Character n-grams allow for different trade-offs between sequence length (# tokens) and vocabulary size (# types), depend- ing on the choice of n. The increase in sequence length is substantial; one way to reduce sequence length is to leave a shortlist of the k most frequent word types unsegmented. Only the unigram repre- sentation is truly open-vocabulary. However, the unigram representation performed poorly in pre- liminary experiments, and we report translation re- sults with a bigram representation, which is empir- ically better, but unable to produce some tokens in the test set with the training set vocabulary.</p><p>We report statistics for several word segmenta- tion techniques that have proven useful in previous SMT research, including frequency-based com- pound splitting <ref type="bibr" target="#b9">(Koehn and Knight, 2003)</ref>, rule- based hyphenation <ref type="bibr" target="#b11">(Liang, 1983)</ref>, and Morfessor ( <ref type="bibr" target="#b1">Creutz and Lagus, 2002</ref>). We find that they only moderately reduce vocabulary size, and do not solve the unknown word problem, and we thus find them unsuitable for our goal of open-vocabulary translation without back-off dictionary. BPE meets our goal of being open-vocabulary, and the learned merge operations can be applied to the test set to obtain a segmentation with no unknown symbols. <ref type="bibr">10</ref> Its main difference from the character-level model is that the more com- pact representation of BPE allows for shorter se- quences, and that the attention model operates on variable-length units. 11 <ref type="table">Table 1</ref> shows BPE with 59 500 merge operations, and joint BPE with 89 500 operations.</p><p>In practice, we did not include infrequent sub- word units in the NMT network vocabulary, since there is noise in the subword symbol sets, e.g. because of characters from foreign alphabets. Hence, our network vocabularies in <ref type="table" target="#tab_1">Table 2</ref> are typically slightly smaller than the number of types in <ref type="table">Table 1. vocabulary   BLEU   CHRF3</ref> unigram F1 (%) name segmentation shortlist source target single ens-8 single ens-8 all rare OOV syntax-based ( <ref type="bibr" target="#b22">Sennrich and Haddow, 2015)</ref> 24.4 -55.3 -59.1 46.0 37.7 WUnk - -300 000 500 000 20.6 22.8 47.2 48.9 56.7 20.4 0.0 WDict - -300 000 500 000 22.0 24.2 50.5 52.4 58.1 36.8 36.8 C2-50k char-bigram 50 000 60 000 60 000 22.8 25.3 51.9 53.5 58.4 40.5 30.9 BPE-60k BPE -60 000 60 000 21.5 24.5 52.0 53.9 58.4 40.9 29.3 BPE-J90k BPE (joint) -90 000 90 000 22.8 24.7 51.7 54.1 58.5 41.8 33.6 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Translation experiments</head><p>English→German translation results are shown in <ref type="table" target="#tab_1">Table 2</ref>; English→Russian results in <ref type="table">Table 3</ref>.</p><p>Our baseline WDict is a word-level model with a back-off dictionary. It differs from WUnk in that the latter uses no back-off dictionary, and just rep- resents out-of-vocabulary words as UNK <ref type="bibr">12</ref> . The back-off dictionary improves unigram F 1 for rare and unseen words, although the improvement is smaller for English→Russian, since the back-off dictionary is incapable of transliterating names.</p><p>All subword systems operate without a back-off dictionary. We first focus on unigram F 1 , where all systems improve over the baseline, especially for rare words (36.8%→41.8% for EN→DE; 26.5%→29.7% for EN→RU). For OOVs, the baseline strategy of copying unknown words works well for English→German. However, when alphabets differ, like in English→Russian, the subword models do much better.</p><p>Unigram F 1 scores indicate that learning the BPE symbols on the vocabulary union (BPE- J90k) is more effective than learning them sep- arately (BPE-60k), and more effective than using character bigrams with a shortlist of 50 000 unseg- mented words (C2-50k), but all reported subword segmentations are viable choices and outperform the back-off dictionary baseline.</p><p>Our subword representations cause big im- provements in the translation of rare and unseen words, but these only constitute 9-11% of the test sets. Since rare words tend to carry central in- formation in a sentence, we suspect that BLEU and CHRF3 underestimate their effect on transla- tion quality. Still, we also see improvements over the baseline in total unigram F 1 , as well as BLEU and CHRF3, and the subword ensembles outper- form the WDict baseline by 0.3-1.3 BLEU and 0.6-2 CHRF3. There is some inconsistency be- tween BLEU and CHRF3, which we attribute to the fact that BLEU has a precision bias, and CHRF3 a recall bias.</p><p>For English→German, we observe the best BLEU score of 25.3 with C2-50k, but the best CHRF3 score of 54.1 with BPE-J90k. For com- parison to the (to our knowledge) best non-neural MT system on this data set, we report syntax- based SMT results ( <ref type="bibr" target="#b22">Sennrich and Haddow, 2015)</ref>. We observe that our best systems outperform the syntax-based system in terms of BLEU, but not in terms of CHRF3. Regarding other neural sys- tems, <ref type="bibr" target="#b15">Luong et al. (2015a)</ref> report a BLEU score of 25.9 on newstest2015, but we note that they use an ensemble of 8 independently trained models, and also report strong improvements from applying dropout, which we did not use. We are confident that our improvements to the translation of rare words are orthogonal to improvements achievable through other improvements in the network archi-tecture, training algorithm, or better ensembles.</p><p>For English→Russian, the state of the art is the phrase-based system by . It outperforms our WDict baseline by 1.5 BLEU. The subword models are a step towards closing this gap, and BPE-J90k yields an improvement of 1.3 BLEU, and 2.0 CHRF3, over WDict.</p><p>As a further comment on our translation results, we want to emphasize that performance variabil- ity is still an open problem with NMT. On our de- velopment set, we observe differences of up to 1 BLEU between different models. For single sys- tems, we report the results of the model that per- forms best on dev (out of 8), which has a stabi- lizing effect, but how to control for randomness deserves further attention in future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Unigram accuracy</head><p>Our main claims are that the translation of rare and unknown words is poor in word-level NMT mod- els, and that subword models improve the trans- lation of these word types. To further illustrate the effect of different subword segmentations on the translation of rare and unseen words, we plot target-side words sorted by their frequency in the training set. <ref type="bibr">13</ref> To analyze the effect of vocabulary size, we also include the system C2-3/500k, which is a system with the same vocabulary size as the WDict baseline, and character bigrams to repre- sent unseen words. <ref type="figure" target="#fig_2">Figure 2</ref> shows results for the English-German ensemble systems on newstest2015. Unigram F 1 of all systems tends to decrease for lower- frequency words. The baseline system has a spike in F 1 for OOVs, i.e. words that do not occur in the training text. This is because a high propor- tion of OOVs are names, for which a copy from the source to the target text is a good strategy for English→German.</p><p>The systems with a target vocabulary of 500 000 words mostly differ in how well they translate words with rank &gt; 500 000. A back-off dictionary is an obvious improvement over producing UNK, but the subword system C2-3/500k achieves better performance. Note that all OOVs that the back- off dictionary produces are words that are copied from the source, usually names, while the subword <ref type="bibr">13</ref> We perform binning of words with the same training set frequency, and apply bezier smoothing to the graph. systems can productively form new words such as compounds.</p><p>For the 50 000 most frequent words, the repre- sentation is the same for all neural networks, and all neural networks achieve comparable unigram F 1 for this category. For the interval between fre- quency rank 50 000 and 500 000, the comparison between C2-3/500k and C2-50k unveils an inter- esting difference. The two systems only differ in the size of the shortlist, with C2-3/500k represent- ing words in this interval as single units, and C2- 50k via subword units. We find that the perfor- mance of C2-3/500k degrades heavily up to fre- quency rank 500 000, at which point the model switches to a subword representation and perfor- mance recovers. The performance of C2-50k re- mains more stable. We attribute this to the fact that subword units are less sparse than words. In our training set, the frequency rank 50 000 corre- sponds to a frequency of 60 in the training data; the frequency rank 500 000 to a frequency of 2. Because subword representations are less sparse, reducing the size of the network vocabulary, and representing more words via subword units, can lead to better performance.</p><p>The F 1 numbers hide some qualitative differ- ences between systems. For English→German, WDict produces few OOVs (26.5% recall), but with high precision (60.6%) , whereas the subword systems achieve higher recall, but lower precision. We note that the character bigram model C2-50k produces the most OOV words, and achieves rel- atively low precision of 29.1% for this category. However, it outperforms the back-off dictionary in recall (33.0%). BPE-60k, which suffers from transliteration (or copy) errors due to segmenta- tion inconsistencies, obtains a slightly better pre- cision (32.4%), but a worse recall (26.6%). In con- trast to BPE-60k, the joint BPE encoding of BPE- J90k improves both precision (38.6%) and recall (29.8%).</p><p>For English→Russian, unknown names can only rarely be copied, and usually require translit- eration. Consequently, the WDict baseline per- forms more poorly for OOVs (9.2% precision; 5.2% recall), and the subword models improve both precision and recall (21.9% precision and 15.6% recall for BPE-J90k). The full unigram F 1 plot is shown in <ref type="figure" target="#fig_3">Figure 3</ref>. unigram F1 (%) name segmentation shortlist source target single ens-8 single ens-8 all rare OOV phrase-based (  24.3 -53.8 -56.0 31.3 16.5 WUnk - -300 000 500 000 18.8 22.4 46.5 49.9 54.2 25.2 0.0 WDict - -300 000 500 000 19.1 22.8 47.5 51.0 54.8 26.5 6.6 C2-50k char-bigram 50 000 60 000 60 000 20.9 24.1 49.0 51.6 55.2 27.8 17.4 BPE-60k BPE -60 000 60 000 20.5 23.6 49.8 52.7 55.3 29.7 15.6 BPE-J90k BPE (joint) -90 000 100 000 20.4 24.1 49.7 53.0 55.8 29.7 18.3 <ref type="table">Table 3</ref>: English→Russian translation performance (BLEU, CHRF3 and unigram F 1 ) on newstest2015. Ens-8: ensemble of 8 models. Best NMT system in bold. Unigram F 1 (with ensembles) is computed for all words (n = 55654), rare words (not among top 50 000 in training set; n = 5442), and OOVs (not in training set; n = 851).   <ref type="table" target="#tab_2">Table 4</ref> shows two translation examples for the translation direction English→German, Ta- ble 5 for English→Russian. The baseline sys- tem fails for all of the examples, either by delet- ing content (health), or by copying source words that should be translated or transliterated. The subword translations of health research insti- tutes show that the subword systems are capa- ble of learning translations when oversplitting (re- search→Fo|rs|ch|un|g), or when the segmentation does not match morpheme boundaries: the seg- mentation Forschungs|instituten would be linguis- tically more plausible, and simpler to align to the English research institutes, than the segmentation Forsch|ungsinstitu|ten in the BPE-60k system, but still, a correct translation is produced. If the sys- tems have failed to learn a translation due to data sparseness, like for asinine, which should be trans- lated as dumm, we see translations that are wrong, but could be plausible for (partial) loanwords (asi- nine Situation→Asinin-Situation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Manual Analysis</head><p>The English→Russian examples show that the subword systems are capable of translitera- tion. However, transliteration errors do occur, either due to ambiguous transliterations, or be- cause of non-consistent segmentations between source and target text which make it hard for the system to learn a transliteration mapping. Note that the BPE-60k system encodes Mirza- yeva inconsistently for the two language pairs (Mirz|ayeva→Ìèð|çà|åâà Mir|za|eva). This ex- ample is still translated correctly, but we observe spurious insertions and deletions of characters in the BPE-60k system. An example is the translit- eration of rakfisk, where a ï is inserted and a ê is deleted. We trace this error back to transla- tion pairs in the training data with inconsistent segmentations, such as <ref type="table" target="#tab_1">(p|rak|ri|ti→ïðà|êðèò|è system   sentence  source  health research institutes  reference  Gesundheitsforschungsinstitute  WDict  Forschungsinstitute  C2-50k  Fo|rs|ch|un|gs|in|st|it|ut|io|ne|n  BPE-60k  Gesundheits|forsch|ungsinstitu|ten  BPE-J90k  Gesundheits|forsch|ungsin|stitute  source</ref> asinine situation reference dumme Situation WDict asinine situation → UNK → asinine C2-50k as|in|in|e situation → As|in|en|si|tu|at|io|n BPE-60k as|in|ine situation → A|in|line-|Situation BPE-J90K as|in|ine situation → As|in|in-|Situation  (pra|krit|i)), from which the translation (rak→ïðà) is erroneously learned. The segmentation of the joint BPE system (BPE-J90k) is more consistent (pra|krit|i→ïðà|êðèò|è (pra|krit|i)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The main contribution of this paper is that we show that neural machine translation systems are capable of open-vocabulary translation by repre- senting rare and unseen words as a sequence of subword units. 14 This is both simpler and more effective than using a back-off translation model. We introduce a variant of byte pair encoding for word segmentation, which is capable of encod- ing open vocabularies with a compact symbol vo- cabulary of variable-length subword units. We show performance gains over the baseline with both BPE segmentation, and a simple character bi- gram segmentation.</p><p>Our analysis shows that not only out-of- vocabulary words, but also rare in-vocabulary words are translated poorly by our baseline NMT system, and that reducing the vocabulary size of subword models can actually improve perfor- mance. In this work, our choice of vocabulary size is somewhat arbitrary, and mainly motivated by comparison to prior work. One avenue of future research is to learn the optimal vocabulary size for a translation task, which we expect to depend on the language pair and amount of training data, au- tomatically. We also believe there is further po- tential in bilingually informed segmentation algo- rithms to create more alignable subword units, al- though the segmentation algorithm cannot rely on the target text at runtime.</p><p>While the relative effectiveness will depend on language-specific factors such as vocabulary size, we believe that subword segmentations are suit- able for most language pairs, eliminating the need for large NMT vocabularies or back-off models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: BPE merge operations learned from dictionary {'low', 'lowest', 'newer', 'wider'}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: English→German unigram F 1 on newstest2015 plotted by training set frequency rank for different NMT systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: English→Russian unigram F 1 on newstest2015 plotted by training set frequency rank for different NMT systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>English→German translation performance (BLEU, CHRF3 and unigram F 1 ) on newstest2015. 
Ens-8: ensemble of 8 models. Best NMT system in bold. Unigram F 1 (with ensembles) is computed for 
all words (n = 44085), rare words (not among top 50 000 in training set; n = 2900), and OOVs (not in 
training set; n = 1168). 

segmentation 
# tokens 
# types # UNK 
none 
100 m 1 750 000 
1079 
characters 
550 m 
3000 
0 
character bigrams 
306 m 
20 000 
34 
character trigrams 
214 m 
120 000 
59 
compound splitting 
102 m 1 100 000 
643 
morfessor* 
109 m 
544 000 
237 
hyphenation 
186 m 
404 000 
230 
BPE 
112 m 
63 000 
0 
BPE (joint) 
111 m 
82 000 
32 
character bigrams 
129 m 
69 000 
34 
(shortlist: 50 000) 

Table 1: Corpus statistics for German training 
corpus with different word segmentation tech-
niques. #UNK: number of unknown tokens in 
newstest2013. : (Koehn and Knight, 2003); *: 
(Creutz and Lagus, 2002); : (Liang, 1983). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>English→German translation example. 
"|" marks subword boundaries. 

system 
sentence 
source 
Mirzayeva 
reference 
Ìèðçàåâà (Mirzaeva) 
WDict 
Mirzayeva → UNK → Mirzayeva 
C2-50k 
Mi|rz|ay|ev|a → Ìè|ðç|àå|âà (Mi|rz|ae|va) 
BPE-60k 
Mirz|ayeva → Ìèð|çà|åâà (Mir|za|eva) 
BPE-J90k Mir|za|yeva → Ìèð|çà|åâà (Mir|za|eva) 
source 
rakfisk 
reference 
ðàêôèñêà (rakfiska) 
WDict 
rakfisk → UNK → rakfisk 
C2-50k 
ra|kf|is|k → ðà|êô|èñ|ê (ra|kf|is|k) 
BPE-60k 
rak|f|isk → ïðà|ô|èñê (pra|f|isk) 
BPE-J90k rak|f|isk → ðàê|ô|èñêà (rak|f|iska) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>English→Russian translation examples. 
"|" marks subword boundaries. 

</table></figure>

			<note place="foot" n="1"> Primarily parliamentary proceedings and web crawl data. 2 Not every segmentation we produce is transparent. While we expect no performance benefit from opaque segmentations, i.e. segmentations where the units cannot be translated independently, our NMT models show robustness towards oversplitting.</note>

			<note place="foot" n="3"> The only symbols that will be unknown at test time are unknown characters, or symbols of which all occurrences in the training text have been merged into larger symbols, like &apos;safeguar&apos;, which has all occurrences in our training text merged into &apos;safeguard&apos;. We observed no such symbols at test time, but the issue could be easily solved by recursively reversing specific merges until all symbols are known.</note>

			<note place="foot" n="4"> In practice, we simply concatenate the source and target side of the training set to learn joint BPE. 5 Since the Russian training text also contains words that use the Latin alphabet, we also apply the Latin BPE operations.</note>

			<note place="foot" n="6"> Clipped unigram precision is essentially 1-gram BLEU without brevity penalty. 7 github.com/sebastien-j/LV_groundhog 8 The time complexity of encoder-decoder architectures is at least linear to sequence length, and oversplitting harms efficiency.</note>

			<note place="foot" n="9"> Our character n-grams do not cross word boundaries. We mark whether a subword is word-final or not with a special character, which allows us to restore the original tokenization. 10 Joint BPE can produce segments that are unknown because they only occur in the English training text, but these are rare (0.05% of test tokens). 11 We highlighted the limitations of word-level attention in section 3.1. At the other end of the spectrum, the character level is suboptimal for alignment (Tiedemann, 2009).</note>

			<note place="foot" n="12"> We use UNK for words that are outside the model vocabulary, and OOV for those that do not occur in the training text.</note>

			<note place="foot" n="14"> The source code of the segmentation algorithms is available at https://github.com/rsennrich/ subword-nmt.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Maja Popovi´cPopovi´c for her implementa-tion of CHRF, with which we verified our re-implementation. The research presented in this publication was conducted in cooperation with Samsung Electronics Polska sp. z o.o.-Sam-sung R&amp;D Institute Poland. This project received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement 645452 (QT21). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised Discovery of Morphemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><surname>Lagus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Workshop on Morphological and Phonological Learning</title>
		<meeting>the ACL-02 Workshop on Morphological and Phonological Learning</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Integrating an Unsupervised Transliteration Model into Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter</title>
		<meeting>the 14th Conference of the European Chapter<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="148" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Simple, Fast, and Effective Reparameterization of IBM Model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
	<note>Atlanta</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A New Algorithm for Data Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Gage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">C Users J</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1994-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Edinburgh/JHU Phrase-based Machine Translation Systems for WMT 2015</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Bogoychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="126" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On Using Very Large Target Vocabulary for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent Continuous Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno>abs/1508.06615</idno>
		<title level="m">Character-Aware Neural Language Models</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Empirical Methods for Compound Splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL &apos;03: Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="187" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Moses: Open Source Toolkit for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-2007 Demo and Poster Sessions</title>
		<meeting>the ACL-2007 Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Word hy-phen-a-tion by com-put-er</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franklin</forename><forename type="middle">M</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Stanford University, Department of Linguistics</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fermandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Character-based Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Better Word Representations with Recursive Neural Networks for Morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08-08" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attentionbased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Addressing the Rare Word Problem in Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Subword Language Modeling with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haison</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Unpublished</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Machine Translation without Words through Substring Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference</title>
		<meeting><address><addrLine>Jeju Island</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2012-07-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving SMT quality with morpho-syntactic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonja</forename><surname>Nießen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th Int. Conf. on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1081" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML 2013</title>
		<meeting>the 30th International Conference on Machine Learning, ICML 2013<address><addrLine>Atlanta, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">chrF: character n-gram F-score for automatic MT evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Popovi´cpopovi´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="392" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Joint Dependency Model of Morphological and Syntactic Structure for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2081" to="2087" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised Multilingual Learning for Morphological Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="737" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised Morphology Rivals Supervised Morphology for Arabic MT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stallard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kayser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoong Keok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2012-07-08" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="322" to="327" />
		</imprint>
	</monogr>
	<note>Proceedings of the Conference</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Results of the WMT15 Metrics Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloš</forename><surname>Stanojevi´cstanojevi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="256" to="273" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Character-based PSMT for Closely Related Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 13th</title>
		<meeting>13th</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Annual Conference of the European Association for Machine Translation (EAMT&apos;09)</title>
		<imprint>
			<biblScope unit="page" from="12" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Character-Based Pivot Translation for Under-Resourced Languages and Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="141" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Can We Translate Letters?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Thorsten</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Workshop on Statistical Machine Translation</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="33" to="39" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Morphology-Aware Statistical Machine Translation Based on Morphs Induced in an Unsupervised Manner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Virpioja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><forename type="middle">J</forename><surname>Väyrynen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Sadeniemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Machine Translation Summit XI</title>
		<meeting>the Machine Translation Summit XI<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="491" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">ADADELTA: An Adaptive Learning Rate Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
