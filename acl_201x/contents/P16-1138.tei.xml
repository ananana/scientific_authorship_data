<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simpler Context-Dependent Logical Forms via Model Projections</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reginald</forename><surname>Long</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
							<email>ppasupat@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<email>pliang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Simpler Context-Dependent Logical Forms via Model Projections</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1456" to="1465"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We consider the task of learning a context-dependent mapping from utterances to de-notations. With only denotations at training time, we must search over a combina-torially large space of logical forms, which is even larger with context-dependent utterances. To cope with this challenge, we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms. Though less expressive, we find that these simpler models are much faster and can be surprisingly effective. Moreover, they can be used to bootstrap the full model. Finally, we collected three new context-dependent semantic parsing datasets, and develop a new left-to-right parser.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Suppose we are only told that a piece of text (a command) in some context (state of the world) has some denotation (the effect of the command)-see <ref type="figure" target="#fig_2">Figure 1</ref> for an example. How can we build a sys- tem to learn from examples like these with no ini- tial knowledge about what any of the words mean?</p><p>We start with the classic paradigm of training semantic parsers that map utterances to logical forms, which are executed to produce the deno- tation ( <ref type="bibr" target="#b26">Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b27">Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b24">Wong and Mooney, 2007;</ref><ref type="bibr" target="#b29">Zettlemoyer and Collins, 2009;</ref><ref type="bibr" target="#b11">Kwiatkowski et al., 2010)</ref>. More recent work learns directly from de- notations ( <ref type="bibr" target="#b7">Clarke et al., 2010;</ref><ref type="bibr" target="#b14">Liang, 2013;</ref><ref type="bibr" target="#b1">Berant et al., 2013;</ref><ref type="bibr" target="#b0">Artzi and Zettlemoyer, 2013</ref>), but in this setting, a constant struggle is to con- tain the exponential explosion of possible logical forms. With no initial lexicon and longer context- dependent texts, our situation is exacerbated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context:</head><p>Text: Pour the last green beaker into beaker 2. Then into the first beaker.</p><p>Mix it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Denotation:</head><p>Figure 1: Our task is to learn to map a piece of text in some context to a denotation. An exam- ple from the ALCHEMY dataset is shown. In this paper, we ask: what intermediate logical form is suitable for modeling this mapping?</p><p>In this paper, we propose projecting a full se- mantic parsing model onto simpler models over equivalence classes of logical form derivations. As illustrated in <ref type="figure">Figure 2</ref>, we consider the following sequence of models:</p><p>• Model A: our full model that derives logi- cal forms (e.g., in <ref type="figure" target="#fig_2">Figure 1</ref>, the last utter- ance maps to mix(args <ref type="bibr">[1]</ref>[1])) compo- sitionally from the text so that spans of the ut- terance (e.g., "it") align to parts of the logical form (e.g., args <ref type="bibr">[1]</ref>[1], which retrieves an argument from a previous logical form). This is based on standard semantic parsing (e.g., <ref type="bibr" target="#b27">Zettlemoyer and Collins (2005)</ref>).</p><p>• Model B: collapse all derivations with the same logical form; we map utterances to full logical forms, but without an alignment be- tween the utterance and logical forms. This "floating" approach was used in <ref type="bibr" target="#b18">Pasupat and Liang (2015)</ref> and <ref type="bibr" target="#b22">Wang et al. (2015)</ref>.</p><p>• Model C: further collapse all logical forms whose top-level arguments have the same de- notation. In other words, we map utterances Mix it</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model C</head><p>Mix it mix args <ref type="bibr">[1]</ref> <ref type="bibr">[1]</ref> mix(args <ref type="bibr">[1]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mix it</head><p>Figure 2: Derivations generated for the last ut- terance in <ref type="figure" target="#fig_2">Figure 1</ref>. All derivations above ex- ecute to mix(beaker2). Model A generates anchored logical forms (derivations) where words are aligned to predicates, which leads to multiple derivations with the same logical form. Model B discards these alignments, and Model C collapses the arguments of the logical forms to denotations.</p><p>to flat logical forms (e.g., mix(beaker2)), where the arguments of the top-level predi- cate are objects in the world. This model is in the spirit of <ref type="bibr" target="#b25">Yao et al. (2014)</ref> and <ref type="bibr" target="#b2">Bordes et al. (2014)</ref>, who directly predicted concrete paths in a knowledge graph for question an- swering.</p><p>Model A excels at credit assignment: the latent derivation explains how parts of the logical form are triggered by parts of the utterance. The price is an unmanageably large search space, given that we do not have a seed lexicon. At the other end, Model C only considers a small set of logical forms, but the mapping from text to the correct logical form is more complex and harder to model. We collected three new context-dependent se- mantic parsing datasets using Amazon Mechanical Turk: ALCHEMY <ref type="figure" target="#fig_2">(Figure 1</ref>), SCENE <ref type="figure">(Figure 3)</ref>, and TANGRAMS <ref type="figure">(Figure 4)</ref>. Along the way, we develop a new parser which processes utterances left-to-right but can construct logical forms with- out an explicit alignment.</p><p>Our empirical findings are as follows: First, Model C is surprisingly effective, mostly surpass- ing the other two given bounded computational re- sources (a fixed beam size). Second, on a synthetic dataset, with infinite beam, Model A outperforms the other two models. Third, we can bootstrap up to Model A from the projected models with finite beam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context:</head><p>Text: A man in a red shirt and orange hat leaves to the right, leaving behind a man in a blue shirt in the middle. He takes a step to the left.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Denotation:</head><p>Figure 3: SCENE dataset: Each person has a shirt of some color and a hat of some color. They enter, leave, move around on a stage, and trade hats. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task</head><p>In this section, we formalize the task and describe the new datasets we created for the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Setup</head><p>First, we will define the context-dependent seman- tic parsing task. Define w 0 as the initial world state, which consists of a set of entities (beakers in ALCHEMY) and properties (location, color(s), and amount filled). The text x is a sequence of utterances x 1 , . . . , x L . For each utterance x i (e.g., "mix"), we have a latent logical form z i (e.g., mix(args <ref type="bibr">[1]</ref>[2])). Define the context c i = (w 0 , z 1:i−1 ) to include the initial world state w 0 and the history of past logical forms z 1:i−1 . Each logical form z i is executed on the context c i to produce the next state:</p><formula xml:id="formula_0">w i = Exec(c i , z i ) for each i = 1, . . . , L. Overloading notation, we write w L = Exec(w 0 , z), where z = (z 1 , . . . , z L ).</formula><p>The learning problem is: given a set of training examples {(w 0 , x, w L )}, learn a mapping from the text x to logical forms z = (z 1 , . . . , z L ) that pro- duces the correct final state (w L = Exec(w 0 , z)).</p><p>Dataset # examples # train # test words/example utterances SCENE 4402 3363 1039 56.2 "then one more", "he moves back" ALCHEMY 4560 3661 899 39.9 "mix", "throw the rest out" TANGRAMS 4989 4189 800 27.2 "undo", "replace it", "take it away" <ref type="table">Table 1</ref>: We collected three datasets. The number of examples, train/test split, number of tokens per example, along with interesting phenomena are shown for each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Datasets</head><p>We created three new context-dependent datasets, ALCHEMY, SCENE, and TANGRAMS (see <ref type="table">Table 1</ref> for a summary), which aim to capture a diverse set of context-dependent linguistic phenomena such as ellipsis (e.g., "mix" in ALCHEMY), anaphora on entities (e.g., "he" in SCENE), and anaphora on actions (e.g., "repeat step 3", "bring it back" in TANGRAMS).</p><p>For each dataset, we have a set of properties and actions. In ALCHEMY, properties are color, and amount; actions are pour, drain, and mix. In SCENE, properties are hat-color and shirt-color; actions are enter, leave, move, and trade-hats. In TANGRAMS, there is one property (shape), and actions are add, remove, and swap. In addition, we include the position property (pos) in each dataset. Each ex- ample has L = 5 utterances, each denoting some transformation of the world state.</p><p>Our datasets are unique in that they are grounded to a world state and have rich linguis- tic context-dependence. In the context-dependent ATIS dataset ( <ref type="bibr" target="#b8">Dahl et al., 1994)</ref> used by <ref type="bibr" target="#b29">Zettlemoyer and Collins (2009)</ref>, logical forms of utter- ances depend on previous logical forms, though there is no world state and the linguistic phenom- ena is limited to nominal references. In the map navigation dataset <ref type="bibr" target="#b5">(Chen and Mooney, 2011</ref>), used by <ref type="bibr" target="#b0">Artzi and Zettlemoyer (2013)</ref>, utterances only reference the current world state. <ref type="bibr" target="#b21">Vlachos and Clark (2014)</ref> released a corpus of annotated di- alogues, which has interesting linguistic context- dependence, but there is no world state.</p><p>Data collection. Our strategy was to automati- cally generate sequences of world states and ask Amazon Mechanical Turk (AMT) workers to de- scribe the successive transformations. Specifi- cally, we started with a random world state w 0 . For each i = 1, . . . , L, we sample a valid action and argument (e.g., pour(beaker1, beaker2)). To encourage context-dependent descriptions, we upweight recently used ac- tions and arguments (e.g., the next action is more like to be drain(beaker2) rather than drain(beaker5)). Next, we presented an AMT worker with states w 0 , . . . , w L and asked the worker to write a description in between each pair of successive states.</p><p>In initial experiments, we found it rather non- trivial to obtain interesting linguistic context- dependence in these micro-domains: often a context-independent utterance such as "beaker 2" is just clearer and not much longer than a possi- bly ambiguous "it". We modified the domains to encourage more context. For example, in SCENE, we removed any visual indication of absolute posi- tion and allowed people to only move next to other people. This way, workers would say "to the left of the man in the red hat" rather than "to position 2".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We now describe Model A, our full context- dependent semantic parsing model. First, let Z denote the set of candidate logical forms (e.g., pour(color(green),color(red))). Each logical form consists of a top-level action with arguments, which are either primitive values (green, 3, etc.), or composed via selection and superlative operations. See <ref type="table" target="#tab_1">Table 2</ref> for a full description. One notable feature of the logical forms is the context dependency: for example, given some context (w 0 , z 1:4 ), the predicate actions <ref type="bibr">[2]</ref> refers to the action of z 2 and args <ref type="bibr">[2]</ref>[1] refers to first argument of z 2 . <ref type="bibr">1</ref> We use the term anchored logical forms (a.k.a. derivations) to refer to logical forms augmented with alignments between sub-logical forms of z i and spans of the utterance x i . In the example above, color(green) might align with "green beaker" from <ref type="figure" target="#fig_2">Figure 1</ref>; see <ref type="figure">Figure 2</ref> for another example.</p><formula xml:id="formula_1">Property[p] Value[v] ⇒ Set[p(v)] all entities whose property p is v Set[s] Property[p] ⇒ Value[argmin/argmax(s, p)] element in s with smallest/largest p Set[s] Int[i] ⇒ Value[s[i]] i-th element of s Action[a] Value[v1] Value[v2] ⇒ Root[a(v1, v2)]</formula><p>top-level action applied to arguments v1, v2  <ref type="bibr">[j]</ref> (for all i ∈ {1, . . . , L} and j ∈ {1, 2}) that refer to the j-th ar- gument used in the i-th logical form. Actions include the fixed domain-specific set plus special tokens actions <ref type="bibr">[i]</ref> (for all i ∈ {1, . . . , L}), which refers to the i-th action in the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Derivation condition Example</head><p>(F1) zi contains predicate r (zi contains predicate pour, "pour") (F2) property p of zi.bj is y (color of arg 1 is green, "green") (F3) action zi.a is a and property p of zi.yj is y (action is pour and pos of arg 2 is 2, "pour, 2") (F4) properties p of zi.v1 is y and p of zi.v2 is y (color of arg 1 is green and pos of arg 2 is 2, "first green, 2") (F5) arg zi.vj is one of zi−1's args (arg reused, "it") (F6) action zi.a = zi−1.a (action reused, "pour") (F7) properties p of zi.yj is y and p of zi−1.y k is y (pos of arg 1 is 2 and pos of prev. arg 2 is 2, "then") (F8) t1 &lt; s2 spans don't overlap <ref type="table">Table 3</ref>: Features φ(x i , c i , z i ) for Model A: The left hand side describes conditions under which the system fires indicator features, and right hand side shows sample features for each condition. For each derivation condition (F1)-(F7), we conjoin the condition with the span of the utterance that the referenced actions and arguments align to. For condition (F8), we just fire the indicator by itself.</p><p>Log-linear model. We place a conditional dis- tribution over anchored logical forms z i ∈ Z given an utterance x i and context c i = (w 0 , z 1:i−1 ), which consists of the initial world state w 0 and the history of past logical forms z 1:i−1 . We use a standard log-linear model:</p><formula xml:id="formula_2">p θ (z i | x i , c i ) ∝ exp(φ(x i , c i , z i ) · θ), (1)</formula><p>where φ is the feature mapping and θ is the param- eter vector (to be learned). Chaining these distri- butions together, we get a distribution over a se- quence of logical forms z = (z 1 , . . . , z L ) given the whole text x:</p><formula xml:id="formula_3">p θ (z | x, w 0 ) = L i=1 p θ (z i | x i , (w 0 , z 1:i−1 )). (2)</formula><p>Features. Our feature mapping φ consists of two types of indicators:</p><p>1. For each derivation, we fire features based on the structure of the logical form/spans.</p><p>2. For each span s (e.g., "green beaker") aligned to a sub-logical form z (e.g., color(green)), we fire features on uni- grams, bigrams, and trigrams inside s con- joined with various conditions of z.</p><p>The exact features given in <ref type="table">Table 3</ref>, references the first two utterances of <ref type="figure" target="#fig_2">Figure 1</ref> and the associated logical forms below:</p><p>x 1 = "Pour the last green beaker into beaker 2." x 2 = "Then into the first beaker."</p><formula xml:id="formula_4">z 2 = actions[1](args[1][2],pos(3)).</formula><p>We describe the notation we use for <ref type="table">Table 3</ref>, re- stricting our discussion to actions that have two or fewer arguments. Our featurization scheme, however, generalizes to an arbitrary number of arguments. Given a logical form z i , let z i .a be its action and (z i .b 1 , z i .b 2 ) be its arguments (e.g., color(green)). The first and second argu- ments are anchored over spans [s 1 , t 1 ] and [s 2 , t 2 ], respectively. Each argument z i .b j has a corre- sponding value z i .v j (e.g., beaker1), obtained by executing z i .b j on the context c i . Finally, let j, k ∈ {1, 2} be indices of the arguments. For ex- ample, we would label the constituent parts of z 1 (defined above) as follows:</p><formula xml:id="formula_5">• z 1 .a = pour • z 1 .b 1 = argmin(color(green),pos) • z 1 .v 1 = beaker3 • z 1 .b 2 = pos(2) • z 1 .v 2 = beaker2 4 Left-to-right parsing</formula><p>We describe a new parser suitable for learning from denotations in the context-dependent setting. Like a shift-reduce parser, we proceed left to right, but each shift operation advances an entire utter- ance rather than one word. We then sit on the utterance for a while, performing a sequence of build operations, which either combine two logi- cal forms on the stack (like the reduce operation) or generate fresh logical forms, similar to what is done in the floating parser of <ref type="bibr" target="#b18">Pasupat and Liang (2015)</ref>.</p><p>Our parser has two desirable properties: First, proceeding left-to-right allows us to build and score logical forms z i that depend on the world state w i−1 , which is a function of the previous log- ical forms. Note that w i−1 is a random variable in our setting, whereas it is fixed in <ref type="bibr" target="#b29">Zettlemoyer and Collins (2009)</ref>. Second, the build operation allows us the flexibility to handle ellipsis (e.g., "Mix.") and anaphora on full logical forms (e.g., "Do it again."), where there's not a clear alignment be- tween the words and the predicates generated.</p><p>The parser transitions through a sequence of hy- potheses. Each hypothesis is h = (i, b, σ), where i is the index of the current utterance, where b is the number of predicates constructed on utterance x i , and σ is a stack (list) of logical forms. The stack includes both the previous logical forms z 1:i−1 and fragments of logical forms built on the current ut- terance. When processing a particular hypothesis, the parser can choose to perform either the shift or build operation:</p><p>Shift: The parser moves to the next utterance by incrementing the utterance index i and resetting b, which transitions a hypothesis from (i, b, σ) to (i + 1, 0, σ).</p><p>Build: The parser creates a new logical form by combining zero or more logical forms on the stack. There are four types of build operations:</p><p>1. Create a predicate out of thin air (e.g.,</p><p>args <ref type="bibr">[1]</ref>[1] in <ref type="figure" target="#fig_7">Figure 5</ref>). This is useful when the utterance does not explicitly refer- ence the arguments or action. For example, in <ref type="figure" target="#fig_7">Figure 5</ref>, we are able to generate the log- ical form args[1] <ref type="bibr">[1]</ref> in the presence of ellipsis.</p><p>2. Create a predicate anchored to some span of the utterance (e.g., actions <ref type="bibr">[1]</ref> anchored to "Repeat"). This allows us to do credit as- signment and capture which part of the utter- ance explains which part of the logical form.</p><p>3. Pop z from the stack σ and push z onto σ, where z is created by applying a rule in <ref type="table" target="#tab_1">Ta- ble 2</ref> to z.</p><p>4. Pop z, z from the stack σ and push z onto σ, where z is created by applying a rule in <ref type="table" target="#tab_1">Ta- ble 2</ref> to z, z (e.g., actions <ref type="bibr">[1]</ref>(args <ref type="bibr">[1]</ref> [1]) by the top-level root rule).</p><p>The build step stops once a maximum number of predicates B have been constructed or when the top-level rule is applied. We have so far described the search space over logical forms. In practice, we keep a beam of the K hypotheses with the highest score under the cur- rent log-linear model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Projections</head><p>Model A is ambitious, as it tries to learn from scratch how each word aligns to part of the log- ical form. For example, when Model A parses "Mix it", one derivation will correctly align "Mix" to mix, but others will align "Mix" to args <ref type="bibr">[1]</ref> [1], "Mix" to pos(2), and so on ( <ref type="figure">Figure 2)</ref>.</p><p>As we do not assume a seed lexicon that could map "Mix" to mix, the set of anchored logical forms is exponentially large. For example, parsing just the first sentence of <ref type="figure" target="#fig_2">Figure 1</ref> would generate 1,216,140 intermediate anchored logical forms.</p><p>How can we reduce the search space? The key is that the space of logical forms is much smaller than the space of anchored logical forms. Even though both grow exponentially, dealing directly with logical forms allows us to generate pour without the combinatorial choice over alignments. We thus define Model B over the space of these logical forms. <ref type="figure">Figure 2</ref> shows that the two an- chored logical forms, which are treated differently in Model A are collapsed in Model B. This dra- matically reduces the search space; parsing the first sentence of <ref type="figure" target="#fig_2">Figure 1</ref> generates 7,047 interme- diate logical forms.</p><p>We can go further and notice that many compositional logical forms reduce to the same flat logical form if we evaluate all the argu- ments. For example, in <ref type="figure">Figure 2</ref>, mix(args <ref type="bibr">[1]</ref> [1]) and mix(pos(2)) are equivalent to mix(beaker2). We define Model C to be the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>delete(pos(2)) actions[1](args[1][1]) actions[1] args[1][1]</head><p>Delete the second figure. Repeat. Delete the second figure. Repeat. pos <ref type="formula">(2)</ref> actions <ref type="bibr">[1]</ref> delete(pos(2)) delete(pos <ref type="formula">(2))</ref> actions <ref type="bibr">[1]</ref> Delete the second figure. Repeat.  <ref type="formula">(2)</ref>) for "Delete the second <ref type="figure">figure.</ref>" Continuing, we shift the utterance "Repeat". Then, we build action <ref type="bibr">[1]</ref> aligned to the word "Repeat." followed by args <ref type="bibr">[1]</ref>[1], which is unaligned. Finally, we combine the two logical forms.</p><p>space of these flat logical forms which consist of a top-level action plus primitive arguments. Us- ing Model C, parsing the first sentence of <ref type="figure" target="#fig_2">Figure 1</ref> generates only 349 intermediate logical forms.</p><p>Note that in the context-dependent setting, the number of flat logical forms (Model C) still in- creases exponentially with the number of utter- ances, but it is an overwhelming improvement over Model A. Furthermore, unlike other forms of relaxation, we are still generating logical forms that can express any denotation as before. The gains from Model B to Model C hinge on the fact that in our world, the number of denotations is much smaller than the number of logical forms.</p><p>Projecting the features. While we have defined the space over logical forms for Models B and C, we still need to define a distribution over these spaces to to complete the picture. To do this, we propose projecting the features of the log-linear model (1). Define Π A→B to be a map from a anchored logical form z A (e.g., mix(pos(2) ) aligned to "mix") to an unanchored one z B (e.g., mix(pos(2))), and define Π B→C to be a map from z B to the flat logical form z C (e.g., mix(beaker2)).</p><p>We construct a log-linear model for Model B by constructing features φ(z B ) (omitting the de- pendence on x i , c i for convenience) based on the Model A features φ(z A ). Specifically, φ(z B ) is the component-wise maximum of φ(z A ) over all z A that project down to z B ; φ(z C ) is defined simi- larly:</p><formula xml:id="formula_6">φ(z B ) def = max{φ(z A ) : Π A→B (z A ) = z B }, (3) φ(z C ) def = max{φ(z B ) : Π B→C (z B ) = z C }. (4)</formula><p>Concretely, Model B's features include indica- tor features over LF conditions in <ref type="table">Table 3</ref> con- joined with every n-gram of the entire utterance, as there is no alignment. This is similar to the model of <ref type="bibr" target="#b18">Pasupat and Liang (2015)</ref>. Note that most of the derivation conditions (F2)-(F7) al- ready depend on properties of the denotations of the arguments, so in Model C, we can directly rea- son over the space of flat logical forms z C (e.g., mix(beaker2)) rather than explicitly comput- ing the max over more complex logical forms z B (e.g., mix(color(red))).</p><p>Expressivity. In going from Model A to Model C, we gain in computational efficiency, but we lose in modeling expressivity. For example, for "sec- ond green beaker" in <ref type="figure" target="#fig_2">Figure 1</ref>, instead of predict- ing color(green) <ref type="bibr">[2]</ref>, we would have to pre- dict beaker3, which is not easily explained by the words "second green beaker" using the simple features in <ref type="table">Table 3</ref>.</p><p>At the same time, we found that simple fea- tures can actually simulate some logical forms. For example, color(green) can be explained by the feature that looks at the color property of beaker3. Nailing color(green) <ref type="bibr">[2]</ref>, how- ever, is not easy. Surprisingly, Model C can use a conjunction of features to express superlatives (e.g., argmax(color(red),pos)) by using one feature that places more mass on selecting ob- jects that are red and another feature that places more mass on objects that have a greater position value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Our experiments aim to explore the computation- expressivity tradeoff in going from Model A to Model B to Model C. We would expect that un- der the computational constraint of a finite beam size, Model A will be hurt the most, but with an  <ref type="table">Table 4</ref>: Test set accuracy and oracle accuracy for examples containing L = 3 and L = 5 utterances. Model C surpasses Model B in both accuracy and oracle on ALCHEMY and SCENE, whereas Model B does better in TANGRAMS.</p><note type="other">Dataset Model 3-acc 3-ora 5-acc 5-</note><p>infinite beam, Model A should perform better.</p><p>We evaluate all models on accuracy, the frac- tion of examples that a model predicts correctly. A predicted logical form z is deemed to be correct for an example (w 0 , x, w L ) if the predicted logi- cal form z executes to the correct final world state w L . We also measure the oracle accuracy, which is the fraction of examples where at least one z on the beam executes to w L . All experiments train for 6 iterations using AdaGrad ( <ref type="bibr" target="#b9">Duchi et al., 2010)</ref> and L 1 regularization with a coefficient of 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Real data experiments</head><p>Setup. We use a beam size of 500 within each utterance, and prune to the top 5 between utter- ances. For the first two iterations, Models B and C train on only the first utterance of each example (L = 1). In the remaining iterations, the models train on two utterance examples. We then evaluate on examples with L = 1, . . . , 5, which tests our models ability to extrapolate to longer texts.</p><p>Accuracy with finite beam. We compare mod- els B and C on the three real datasets for both L = 3 and L = 5 utterances (Model A was too ex- pensive to use). <ref type="table">Table 4</ref> shows that on 5 utterance examples, the flatter Model C achieves an average accuracy of 20% higher than the more composi- tional Model B. Similarly, the average oracle accu- racy is 39% higher. This suggests that (i) the cor- rect logical form often falls off the beam for Model B due to a larger search space, and (ii) the expres- sivity of Model C is sufficient in many cases.</p><p>On the other hand, Model B outperforms Model C on the TANGRAMS dataset. This happens for two reasons. The TANGRAMS dataset has the smallest search space, since all of the utterances refer to objects using position only. Addition- ally, many utterances reference logical forms that Model C is unable to express, such as "repeat the first step", or "add it back". <ref type="figure" target="#fig_8">Figure 6</ref> shows how the models perform as the number of utterances per example varies. When the search space is small (fewer number of ut- terances), Model B outperforms or is competitive with Model C. However, as the search space in- creases (tighter computational constraints), Model C does increasingly better.</p><p>Overall, both models perform worse as L in- creases, since to predict the final world state w L correctly, a model essentially needs to predict an entire sequence of logical forms z 1 , . . . , z L , and errors cascade. Furthermore, for larger L, the ut- terances tend to have richer context-dependence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Artificial data experiments</head><p>Setup. Due to the large search space, running model A on real data is impractical. In order feasi- bly evaluate Model A, we constructed an artificial dataset. The worlds are created using the proce- dure described in Section 2.2. We use a simple template to generate utterances (e.g., "drain 1 from the 2 green beaker").</p><p>To reduce the search space for Model A, we only allow actions (e.g., drain) to align to verbs and property values (e.g., green) to align to ad- jectives. Using these linguistic constraints pro- vides a slightly optimistic assessment of Model A's performance.</p><p>We train on a dataset of 500 training examples and evaluate on 500 test examples. We repeat this procedure for varying beam sizes, from 40 to 260. The model only uses features (F1) through (F3).</p><p>Accuracy under infinite beam. Since Model A is more expressive, we would expect it to be more powerful when we have no computational con- straints. <ref type="figure" target="#fig_9">Figure 7</ref> shows that this is indeed the case: When the beam size is greater than 250, all models attain an oracle of 1, and Model A out- performs Model B, which performs similarly to Model C. This is because the alignments provide a powerful signal for constructing the logical forms. Without alignments, Models B and C learn noisier features, and accuracy suffers accordingly.</p><p>Bootstrapping. Model A performs the best with unconstrained computation, and Model C per- forms the best with constrained computation. Is there some way to bridge the two? Even though  Model A is unable to learn anything with beam size &lt; 240. However, for beam sizes larger than 240, Model A attains 100% accuracy. Model C does better than Models A and B when the beam size is small &lt; 40, but otherwise performs compa- rably to Model B. Bootstrapping Model A using Model C parameters outperforms all of the other models and attains 100% even with smaller beams.</p><p>Model C has limited expressivity, it can still learn to associate words like "green" with their corre- sponding predicate green. These should be use- ful for Model A too.</p><p>To operationalize this, we first train Model C and use the parameters to initialize model A. Then we train Model A. <ref type="figure" target="#fig_9">Figure 7</ref> shows that although Model A and C predict different logical forms, the initialization allows Model C to A to perform well in constrained beam settings. This bootstrapping   works here because Model C is a projection of Model A, and thus they share the same features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Error Analysis</head><p>We randomly sampled 20 incorrect predictions on 3 utterance examples from each of the three real datasets for Model B and Model C. We catego- rized each prediction error into one of the follow- ing categories: (i) logical forms falling off the beam; (ii) choosing the wrong action (e.g., map- ping "drain" to pour); (iii) choosing the wrong argument due to misunderstanding the description (e.g., mapping "third beaker" to pos(1)); (iv) choosing the wrong action or argument due to mis- understanding of context (see <ref type="figure" target="#fig_11">Figure 8)</ref>; (v) noise in the dataset. <ref type="table" target="#tab_3">Table 5</ref> shows the fraction of each error category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work and Discussion</head><p>Context-dependent semantic parsing. Utter- ances can depend on either linguistic context or world state context. <ref type="bibr" target="#b29">Zettlemoyer and Collins (2009)</ref> developed a model that handles references to previous logical forms; Artzi and Zettlemoyer (2013) developed a model that handles references to the current world state. Our system considers both types of context, handling linguistic phenom- ena such as ellipsis and anaphora that reference both previous world states and logical forms.</p><p>Logical form generation. Traditional semantic parsers generate logical forms by aligning each part of the logical form to the utterance ( <ref type="bibr" target="#b26">Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b24">Wong and Mooney, 2007;</ref><ref type="bibr" target="#b28">Zettlemoyer and Collins, 2007;</ref><ref type="bibr" target="#b12">Kwiatkowski et al., 2011</ref>). In general, such systems rely on a lexicon, which can be hand-engineered, extracted <ref type="bibr" target="#b4">(Cai and Yates, 2013;</ref><ref type="bibr" target="#b1">Berant et al., 2013)</ref>, or au- tomatically learned from annotated logical forms ( <ref type="bibr" target="#b11">Kwiatkowski et al., 2010;</ref><ref type="bibr" target="#b6">Chen, 2012)</ref>. Recent work on learning from denotations has moved away from anchored logical forms. Pa- supat and Liang (2014) and <ref type="bibr" target="#b22">Wang et al. (2015)</ref> proposed generating logical forms without align- ments, similar to our Model B. <ref type="bibr" target="#b25">Yao et al. (2014)</ref> and <ref type="bibr" target="#b2">Bordes et al. (2014)</ref> have explored predicting paths in a knowledge graph directly, which is sim- ilar to the flat logical forms of Model C.</p><p>Relaxation and bootstrapping. The idea of first training a simpler model in order to work up to a more complex one has been explored other con- texts. In the unsupervised learning of generative models, bootstrapping can help escape local op- tima and provide helpful regularization <ref type="bibr" target="#b16">(Och and Ney, 2003;</ref><ref type="bibr" target="#b13">Liang et al., 2009)</ref>. When it is difficult to even find one logical form that reaches the de- notation, one can use the relaxation technique of <ref type="bibr" target="#b20">Steinhardt and Liang (2015)</ref>.</p><p>Recall that projecting from Model A to C cre- ates a more computationally tractable model at the cost of expressivity. However, this is because Model C used a linear model. One might imag- ine that a non-linear model would be able to re- cuperate some of the loss of expressivity. In- deed, <ref type="bibr" target="#b15">Neelakantan et al. (2016)</ref> use recurrent neu- ral networks attempt to perform logical operations. One could go one step further and bypass log- ical forms altogether, performing all the logical reasoning in a continuous space <ref type="bibr" target="#b3">(Bowman et al., 2014;</ref><ref type="bibr" target="#b23">Weston et al., 2015;</ref><ref type="bibr" target="#b10">Guu et al., 2015;</ref><ref type="bibr" target="#b19">Reed and de Freitas, 2016)</ref>. This certainly avoids the combinatorial explosion of logical forms in Model A, but could also present additional optimization challenges. It would be worth exploring this av- enue to completely understand the computation- expressivity tradeoff. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 4: TANGRAMS dataset: One can add figures, remove figures, and swap the position of figures. All the figures slide to the left.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>z 1 =</head><label>1</label><figDesc>pour(argmin(color(green),pos),pos(2))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Delete the second figure. Repeat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Suppose we have already constructed delete(pos(2)) for "Delete the second figure." Continuing, we shift the utterance "Repeat". Then, we build action[1] aligned to the word "Repeat." followed by args[1][1], which is unaligned. Finally, we combine the two logical forms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Test results on our three datasets as we vary the number of utterances. The solid lines are the accuracy, and the dashed line are the oracles: With finite beam, Model C significantly outperforms Model B on ALCHEMY and SCENE, but is slightly worse on TANGRAMS.</figDesc><graphic url="image-10.png" coords="8,69.97,261.87,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Test results on our artificial dataset with varying beam sizes. The solid lines are the accuracies, and the dashed line are the oracle accuracies. Model A is unable to learn anything with beam size &lt; 240. However, for beam sizes larger than 240, Model A attains 100% accuracy. Model C does better than Models A and B when the beam size is small &lt; 40, but otherwise performs comparably to Model B. Bootstrapping Model A using Model C parameters outperforms all of the other models and attains 100% even with smaller beams.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Predicted logical forms for this text: The logical form add takes a figure and position as input. Model B predicts the correct logical form. Model C does not understand that "back" refers to position 5, and adds the cat figure to position 1. model beam action argument context noise B 0.47 0.03 0.17 0.23 0.04 C 0.15 0.03 0.25 0.5 0.07</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>[1]) Mix it mix pos(2) mix(pos(2))</head><label></label><figDesc></figDesc><table>Mix it 

mix 
pos(2) 

mix(pos(2)) 

mix(pos(2)) 

Mix it 

mix(beaker2) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Grammar that defines the space of candidate logical forms. Values include numbers, colors, as well as special tokens args[i]</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Percentage of errors for Model B and C: 
Model B suffers predominantly from computation 
constraints, while Model C suffers predominantly 
from a lack of expressivity. 

</table></figure>

			<note place="foot">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1456-1465, Berlin, Germany, August 7-12, 2016. c 2016 Association for Computational Linguistics Simpler Context-Dependent Logical Forms via Model Projections Reginald Long Stanford University reggylong@cs.stanford.edu Panupong Pasupat</note>

			<note place="foot" n="1"> These special predicates play the role of references in Zettlemoyer and Collins (2009). They perform contextindependent parsing and resolve references, whereas we resolve them jointly while parsing.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their con-structive feedback. The third author is supported by a Microsoft Research Faculty Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Can recursive neural tensor networks learn logical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing via schema matching and lexicon extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to interpret natural language navigation instructions from observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="859" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast online lexicon learning for grounded language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the world&apos;s response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Expanding the scope of the ATIS task: The ATIS-3 corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hunicke-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudnicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shriberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Human Language Technology</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory (COLT)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inducing probabilistic CCG grammars from logical form with higher-order unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1223" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lexical generalization in CCG grammar induction for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1512" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning semantic correspondences with less supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<title level="m">Lambda dependency-based compositional semantics. arXiv</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural programmer: Inducing latent programs with gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Zero-shot entity extraction from web pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural programmerinterpreters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning with relaxed supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A new corpus and imitation learning framework for context-dependent semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="547" to="559" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Building a semantic parser overnight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Towards AI-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning synchronous grammars for semantic parsing with lambda calculus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="960" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Freebase QA: Information extraction or semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van-Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Semantic parsing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Online learning of relaxed CCG grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning context-dependent mappings from sentences to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
