<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entity Hierarchy Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poyao</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingkai</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Entity Hierarchy Embedding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1292" to="1300"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Existing distributed representations are limited in utilizing structured knowledge to improve semantic relatedness modeling. We propose a principled framework of embedding entities that integrates hierarchical information from large-scale knowledge bases. The novel embedding model associates each category node of the hierarchy with a distance metric. To capture structured semantics, the entity similarity of context prediction are measured under the aggregated metrics of relevant categories along all inter-entity paths. We show that both the entity vectors and category distance metrics encode meaningful semantics. Experiments in entity linking and entity search show superiority of the proposed method.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has been a growing interest in distributed representation that learns compact vectors (a.k.a embedding) for words ( <ref type="bibr" target="#b14">Mikolov et al., 2013a</ref>), phrases ( <ref type="bibr" target="#b16">Passos et al., 2014)</ref>, and concepts <ref type="bibr" target="#b8">(Hill and Korhonen, 2014</ref>), etc. The induced vectors are expected to capture semantic relatedness of the linguistic items, and are widely used in senti- ment analysis <ref type="bibr" target="#b19">(Tang et al., 2014</ref>), machine transla- tion ( , and information retrieval <ref type="bibr" target="#b5">(Clinchant and Perronnin, 2013)</ref>, to name a few.</p><p>Despite the impressive success, existing work is still limited in utilizing structured knowledge to enhance the representation. For instance, word and phrase embeddings are largely induced from plain text. Though recent knowledge graph em- beddings ( <ref type="bibr" target="#b13">Lin et al., 2015;</ref><ref type="bibr" target="#b22">Wang et al., 2014</ref>) inte- grate the relational structure among entities, they primarily target at link prediction and lack an ex- plicit relatedness measure.</p><p>In this paper, we propose to improve the dis- tributed representations of entities by integrating hierarchical information from large-scale knowl- edge bases (KBs). An entity hierarchy groups en- tities into categories which are further organized to form a taxonomy. It provides rich structured knowledge on entity relatedness <ref type="bibr" target="#b18">(Resnik, 1995)</ref>. Our work goes beyond the previous heuristic use of entity hierarchy which relies on hand-crafted features <ref type="bibr" target="#b9">(Kaptein and Kamps, 2013;</ref><ref type="bibr" target="#b17">Ponzetto and Strube, 2007)</ref>, and develops a principled optimization-based framework. We learn a dis- tance metric for each category node, and mea- sure entity-context similarity under the aggregat- ed metrics of all relevant categories. The met- ric aggregation encodes the hierarchical property that nearby entities tend to share common seman- tic features. We further provide a highly-efficient implementation in order to handle large complex hierarchies.</p><p>We train a distributed representation for the w- hole entity hierarchy of Wikipedia. Both the entity vectors and the category distance metrics capture meaningful semantics. We deploy the embedding in both entity linking <ref type="bibr" target="#b7">(Han and Sun, 2012</ref>) and entity search <ref type="bibr" target="#b6">(Demartini et al., 2010)</ref> tasks. Hi- erarchy embedding significantly outperforms that without structural knowledge. Our methods also show superiority over existing competitors.</p><p>To the best of our knowledge, this is the first work to learn distributed representations that in- corporates hierarchical knowledge in a principled framework. Our model that encodes hierarchy by distance metric learning and aggregation provides a potentially important and general scheme for u- tilizing hierarchical knowledge.</p><p>The rest of the paper is organized as follows: §2 describes the proposed embedding model; §3 presents the application of the learned embed- ding; §4 evaluates the approach; §5 reviews related work; and finally, §6 concludes the paper. Dist. metrics í µí± í µí± 1 ,í µí± 2 = í µí± ℎ 2</p><p>Figure 1: The model architecture. The text context of an entity is based on its KB encyclopedia article. The entity hierarchical structure is incorporated through distance metric learning and aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Entity Hierarchy Embedding</head><p>The objective of the embedding model is to find a representation for each entity that is useful for predicting other entities occurring in its contex- t. We build entity's context upon KB encyclope- dia articles, where entity annotations are readily available. We further incorporate the entity hierar- chical structure in the context prediction through distance metric learning and aggregation, which encodes the rich structured knowledge in the in- duced representations. Our method is flexible and efficient to model large complex DAG-structured hierarchies. <ref type="figure">Figure 1</ref> shows an overview of the model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Architecture</head><p>Our architecture builds on the skip-gram word embedding framework ( <ref type="bibr" target="#b15">Mikolov et al., 2013b</ref>).</p><p>In the skip-gram model, a set of (target, con- text) word pairs are extracted by sliding a fixed- length context window over a text corpus, and the word vectors are learned such that the similarity of the target-and context-word vectors is maxi- mized. We generalize both the context definition and the similarity measure for entity hierarchy em- bedding. Unlike words that can be directly extracted from plain text, entities are hidden semantics underly- ing their surface forms. In order to avoid manual annotation cost, we exploit the text corpora from KBs where the referent entities of surface text are readily annotated. Moreover, since a KB encyclo- pedia article typically focuses on describing one entity, we naturally extend the entity's context as its whole article, and obtain a set of entity pairs D = {(e T , e C )}, where e T denotes the target- entity and e C denotes the context-entity occurring in entity e T 's context.</p><p>Let E be the set of entities. For each entity e ∈ E, the model learns both a "target vector" v e ∈ R n and "context vector" ¯ v e ∈ R n , by maximizing the training objective</p><formula xml:id="formula_0">L = 1 |D| (e T ,e C )∈D log p(eC |eT ),<label>(1)</label></formula><p>where the prediction probability is defined as a softmax:</p><formula xml:id="formula_1">p(eC |eT ) = exp {−d (eT , eC )} e∈E exp {−d (eT , e)} .<label>(2)</label></formula><p>Here d(e, e ) is the distance between the target vector of e (i.e., v e ) and the context vector of e (i.e., ¯ v e ). We present the design in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hierarchical Extension</head><p>An entity hierarchy takes entities as leaf nodes and categories as internal nodes, which provides key knowledge sources on semantic relatedness that 1) far-away entities in the hierarchy tend to be se- mantically distant, and 2) nearby entities tend to share common semantic features. We aim to en- code this knowledge in our representations. As K- B hierarchies are large complex DAG structures, we develop a highly-efficient scheme to enable practical training. Specifically, we associate a separate distance metric M h ∈ R n×n with each category h in the hierarchy. A distance metric is a positive semidef- inite (PSD) matrix. We then measure the distance between two entities under some aggregated dis- tance metric as detailed below. The local metrics thus not only serve to capture the characteristics of individual categories, but also make it possible to share the representation across entities through metric aggregation of relevant categories.</p><p>Metric aggregation Given two entities e and e , let P e,e be the path between them. One obvious way to define the aggregated metric M e,e ∈ R n×n is through a combination of the metrics on the Figure 2: Paths in a DAG-structured hierarchy. A path P is defined as a sequence of non-duplicated nodes with the property that there exists a turning node t ∈ P such that any two consecutive nodes before t are (child, parent) pairs, while consecu- tive nodes after t are (parent, child) pairs. Thus a turning node is necessarily a common ancestor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>path:</head><p>h∈P e,e M h , leading to a nice property that the more nodes a path has, the more distant the en- tities tend to be (as M h is PSD). This simple strat- egy, however, can be problematic when the hier- archy has a complex DAG structure, in that there can be multiple paths between two entities <ref type="figure">(Fig- ure 2)</ref>. Though the shortest path can be selected, it ignores other related category nodes and loses rich information. In contrast, an ideal scheme should not only mirror the distance in the hierarchy, but also take into account all possible paths in order to capture the full aspects of relatedness.</p><p>However, hierarchies in large KBs can be com- plex and contains combinationally many paths be- tween any two entities. We propose an efficien- t approach that avoids enumerating paths and in- stead models the underlying nodes directly. In par- ticular, we extend P e,e as the set of all category nodes included in any of the e → e paths, and define the aggregate metric as</p><formula xml:id="formula_2">M e,e = γ e,e h∈P e,e π ee ,h M h ,<label>(3)</label></formula><p>where {π ee ,h } are the relative weights of the cat- egories such that h∈P e,e π ee ,h = 1. This serves to balance the size of P across different entity pairs. We set π ee ,h ∝ ( 1</p><formula xml:id="formula_3">s h↓e + 1 s h↓e</formula><p>) with s h↓e being the average #steps going down from node h to node e in the hierarchy (infinite if h is not an ancestor of e). This implements the intuition that an entity (e.g., "Iphone") is more relevant to its immediate categories (e.g., "Mobile phones") than to farther and more generic ancestors (e.g., "Technology"). The scaling factor γ e,e encodes the distance of the entities in the hierarchy and can be of various choices. We set γ e,e = min h {s h↓e + s h↓e } to mirror the least common ancestor.</p><p>In <ref type="figure">Figure 2</ref>, P e,e = {h 2 , h 3 , h 4 }, and the rel- ative weights of the categories are π ee ,h 2 ∝ 3/2 and π ee ,h 3 = π ee ,h 4 ∝ 1. Category h 2 is the least common ancestor and γ e,e = 3.</p><p>Based on the aggregated metric, the distance be- tween a target entity e T and a context entity e C can then be measured as</p><formula xml:id="formula_4">d (eT , eC ) = (ve T − ¯ ve C ) Me T ,e C (ve T − ¯ ve C ). (4)</formula><p>Note that nearby entities in the hierarchy tend to share a large proportion of local metrics in Eq 3, and hence can exhibit common semantic features when measuring distance with others.</p><p>Complexity of aggregation As computing dis- tance is a frequent operation in both training and application stages, a highly efficient aggregation algorithm is necessary in order to handle complex large entity hierarchies (with millions of nodes). Our formulation (Eq 3) avoids exhaustive enumer- ation over all paths by modeling the relevant nodes directly. We show that this allows linear complex- ity in the number of children of two entities' com- mon ancestors, which is efficient in practice. The most costly operation is to find P e,e , i.e., the set of all category nodes that can occur in any of e → e paths. We use a two-step procedure that (1) finds all common ancestors of entity e and e that are turning nodes of any e → e paths (e.g., h 2 in <ref type="figure">Figure 2</ref>), denoted as Q e,e ; (2) expands from Q e,e to construct the full P e,e . For the first step, the following theorem shows each common ances- tor can be efficiently assessed by testing only its children nodes. For the second step, it is straight- forward to see that P e,e can be constructed by ex- panding Q e,e with its descendants that are ances- tors of either e or e . Other parameters (π ee and γ e,e ) of aggregation can be computed during the above process.</p><p>We next provide the theorem for the first step. Let A e be the ancestor nodes of entity e (including e itself). For a node h ∈ A e ∪ A e , we define its critical node t h as the nearest (w.r.t the length of the shortest path) descendant of h (including h itself) that is in Q e,e ∪ {e, e }. E.g., in <ref type="figure">Figure 2</ref>, t h 1 = h 2 ; t h 2 = h 2 ; t h 3 = e. Let C h be the set of immediate child nodes of h.</p><formula xml:id="formula_5">Theorem 1. ∀h ∈ A e ∩ A e , h ∈ Q e,e iff it satis- fies the two conditions: (1) |C h ∩(A e ∪ A e ) | ≥ 2; (2) ∃a, b ∈ C h s.t. t a = t b .</formula><p>Proof. We outline the proof here, and provide the details in the appendix.</p><p>Sufficiency: Note that e, e / ∈ Q e,e . We prove the sufficiency by enumerating possible situation- s: (i) t a = e, t b = e ; (ii) t a = e, t b ∈ Q e,e ; (iii) t a , t b ∈ Q e,e . For (i): as t a = e, there exists a path e → · · · → a → h where any two con- secutive nodes is a (child, parent) pair. Similarly, there is a path h → b → · · · → e where any t- wo consecutive nodes is a (parent, child) pair. It is provable that the two paths intersect only at h, and thus can be combined to form an e → e path:</p><formula xml:id="formula_6">e → · · · → a → h → b → · · · → e , yielding h</formula><p>as a turning node. The cases (ii) and (iii) can be proved similarly.</p><p>Necessity: We prove by contradiction. Suppose that ∀a, b ∈ C h ∩ (A e ∪ A e ) we have t a = t b . W.l.o.g. we consider two cases: (i) t a = t b = e, and (ii) t a = t b ∈ Q e,e . It is provable that both cases will lead to contradiction.</p><p>Therefore, by checking common ancestors from the bottom up, we can construct Q e,e with time complexity linear to the number of all ancestors' children.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning</head><p>For efficiency, we use negative sampling to refor- mulate the training objective, which is then opti- mized through coordinate gradient ascent.</p><p>Specifically, given the training data {(e T , e C )} extracted from KB corpora, the representation learning is formulated as maximizing the objec- tive in Eq 1, subject to PSD constraints on distance metrics M h 0, and v e 2 = ¯ v e 2 = 1 to avoid scale ambiguity.</p><p>The likelihood of each data sample is defined as a softmax in Eq 2, which iterates over all en- tities in the denominator and is thus computation- ally prohibitive. We apply the negative sampling technique as in conventional skip-gram model, by replacing each log probability log p(e C |e T ) with</p><formula xml:id="formula_7">log σ(−d (eT , eC )) + k i=1 E e i ∼P (e) [log σ(−d (eT , ei))] ,</formula><p>where σ(x) = 1/(1 + exp(−x)) is the sigmoid function; and for each data sample we draw k neg- ative samples from the noise distribution P (e) ∝ U (e) 3/4 with U (e) being the unigram distribution ( <ref type="bibr" target="#b15">Mikolov et al., 2013b</ref>). The negative sampling objective is optimized using coordinate gradient ascent, as shown in Al- gorithm 1. To avoid overfitting and improve effi- ciency, in practice we restrict the distance metrics M h to be diagonal ( <ref type="bibr" target="#b25">Xing et al., 2002</ref>). Thus the PSD project of M h (line 17) is simply taking the positive part for each diagonal elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Entity Hierarchy Embedding</head><p>Input: The training data D = {(eT , eC )}, Entity hierarchy, Parameters: n -dimension of the embedding k -number of negative samples η -gradient learning rate B -minibatch size 1: Initialize v, ¯ v, M randomly such that v2 = ¯ v2 = 1 and M 0.</p><formula xml:id="formula_8">2: repeat 3: Sample a batch B = {(eT , eC )i} B i=1 from D 4:</formula><p>for all (eT , eC ) ∈ B do 5:</p><p>Compute {P, π, γ}e T ,e C for metric aggregation 6:</p><p>Sample negative pairs {(eT , ei)} k i=1</p><p>7:</p><formula xml:id="formula_9">Compute {{P, π, γ}e T ,e i } k i=1</formula><note type="other">for metric aggrega- tion 8: end for 9: repeat 10: for all e ∈ E included in B do 11: ve = ve + η ∂L ∂ve 12: ¯ ve = ¯ ve + η ∂L ∂ ¯ ve 13: ve, ¯ ve = Project to unit sphere(ve, ¯ ve) 14: end for 15: until convergence 16: repeat 17:</note><p>for all h included in B do 18:</p><formula xml:id="formula_10">M h = M h + η ∂L ∂M h 19: M h = Project to PSD(M h ) 20:</formula><p>end for 21:</p><p>until convergence 22: until convergence Output: Entity vectors v, ¯ v, and category dist. metrics M</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Applications</head><p>One primary goal of learning semantic embedding is to improve NLP tasks. The compact represen- tations are easy to work with because they en- able efficient computation of semantic relatedness. Compared to word embedding, entity embedding is particularly suitable for various language under- standing applications that extract underlying se- mantics of surface text. Incorporating entity hier- archies further enriches the embedding with struc- tured knowledge.</p><p>In this section, we demonstrate how the learned entity hierarchy embedding can be utilized in t- wo important tasks, i.e., entity linking and enti- ty search. In both tasks, we measure the seman- tic relatedness between entities as the reciprocal distance defined in Eq 4. This greatly simpli- fies previous methods which have used various hand-crafted features, and leads to improved per- formance as shown in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Entity Linking</head><p>The entity linking task is to link surface form- s (mentions) of entities in a document to entities in a reference KB. It is an essential first step for downstream tasks such as semantic search and K- B construction. The quality of entity relatedness measure is critical for entity linking performance, because of the key observation that entities in a document tend to be semantically coherent. For example, in sentence "Apple released an operating system Lion", The mentions "Apple" and "Lion" refer to Apple Inc. and Mac OS X Lion, respec- tively, as is more coherent than other configura- tions like (fruit apple, animal lion).</p><p>Our algorithm finds the optimal configuration for the mentions of a document by maximizing the overall relatedness among assigned entities, to- gether with the local mention-to-entity compatibil- ity. Specifically, we first construct a mention-to- entity dictionary based on Wikipedia annotations. For each mention m, the dictionary contains a set of candidate entities and for each candidate entity e a compatibility score P (e|m) which is propor- tional to the frequency that m refers to e. For effi- ciency we only consider the top-5 candidate enti- ties according to P (e|m). Given a set of mentions</p><formula xml:id="formula_11">M = {m i } M i=1 in a document, let A = {e m i } M i=1</formula><p>be a configuration of its entity assignments. The score of A is formulated as probability</p><formula xml:id="formula_12">P (A|M) ∝ M i=1 P (em i |mi) M j=1 j =i 1 d em i , em j + ,</formula><p>where for each entity assignment we define its global relatedness to other entity assignments as the sum of the reciprocal distances ( = 0.01 is a constant used to avoid divide-by-zero). Direct enumeration of all potential configurations is com- putationally prohibitive, we therefore use simulat- ed annealing to search for an optimal solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Entity Search</head><p>Entity search has attracted a growing interest (Chen et al., 2014b; Balog et al., 2011). Unlike conventional web search that finds unorganized web pages, entity search retrieves knowledge di- rectly by generating a list of relevant entities in re- sponse to a search request. The input of the entity search task is a natural language question Q along with one or more desired entity categories C. For example, a query can be Q ="films directed by Akira Kurosawa" and C ={Japanese films}.</p><p>Previous methods typically score candidate en- tities by measuring both the similarity between en- tity content and the query question Q (text match- ing), and the similarity between categories of en- tities and the query categories C (category match- ing).</p><p>We apply a similar category matching strate- gy as in previous work <ref type="bibr" target="#b4">(Chen et al., 2014b</ref>) that assesses lexical (e.g., head words) similarity be- tween category names, while replacing the text matching with entity relatedness measure. Specif- ically, we first extract the underlying entities men- tioned in Q through entity linking, then score each candidate entity by its average relatedness to the entities in Q. For instance, the entity Rashomon will obtain a high score in the above example as it is highly related with the entity Akira Kurosawa in the query. This scheme not only avoids complex document processing (e.g., topic modeling) in tex- t matching, but also implicitly augments the short query text with background knowledge, and thus improves the accuracy and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We validate the quality of our entity representa- tion by evaluating its applications of entity linking and entity search on public benchmarks. In the entity linking task, our approach improves the F1 score by 10% over state-of-the-art results. We al- so validate the advantage of incorporating hierar- chical structure. In the entity search task, our sim- ple algorithm shows competitive performance. We further qualitatively analyze the entity vectors and category metrics, both of which capture meaning- ful semantics, and can potentially open up a wide rage of other applications.</p><p>Knowledge base We use the Wikipedia snap- shot from Jan 12nd, 2015 as our training data and KB. After pruning administrative information we obtain an entity hierarchy including about 4.1M entities and 0.8M categories organized into 12 layers. Loops in the original hierarchy are re- moved by deleting bottom-up edges, yielding a DAG structure. We extract a set of 87.6M entity pairs from the wiki links on Wikipedia articles.</p><p>We train 100-dimensional vector represen- tations for the entities and distance metrics (100×100 diagonal matrixes) for the categories (we would study the impact of dimensionality in the future). We set the batch size B = 500, the initial learning rate η = 0.1 and decrease it by a factor of 5 whenever the objective value does not increase, and the negative sample size k = 5. The model is trained on a Linux machine with 128G RAM and 16 cores. It takes 5 days to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Entity Linking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Setup</head><p>Dataset As our entities based on English Wikipedia include not only named entities (e.g., persons, organizations) but also general concepts (e.g., "computer" and "human"), we use a stan- dard entity linking dataset IITB 1 where mentions of Wikipedia entities are manually annotated ex- haustively. The dataset contains about 100 docu- ments and 17K mentions in total. As in the base- line work, we use only the mentions whose refer- ent entities are contained in Wikipedia.</p><p>Criteria We adopt the common criteria, preci- sion, recall, and F1. Let A * be the golden stan- dard entity annotations, and A be the annotations by entity linking model, then</p><formula xml:id="formula_13">precision = |A * ∩ A| |A| recall = |A * ∩ A| |A * | .</formula><p>The F1 score is then computed based on the aver- age precision and recall across all documents.</p><p>Baselines We compare our algorithm with the following approaches. All the competitors are de- signed to be able to link general concept mentions to Wikipedia. CSAW ( <ref type="bibr" target="#b10">Kulkarni et al., 2009</ref>) has a similar framework as our algorithm. It measures entity relatedness using a variation of Jaccard similarity on Wikipedia page incoming links.</p><p>Entity-TM (Han and Sun, 2012) models an en- tity as a distribution over mentions and words, and sets up a probabilistic generative process for the observed text.</p><p>Ours-NoH. To validate the advantage of incor- porating hierarchical structure, we design a base- line that relies on entity embedding without enti- ty hierarchy. That is, we obtain entity vectors by fixing the distance metric in Eq 4 as an identity matrix.  <ref type="table" target="#tab_1">Table 1</ref>: Entity linking performance over 6% and 14% improvements in Precision and Recall, respectively. The CSAW model devises a set of entity features based on text content and link structures of Wikipedia pages, and combines them to measure relatedness. Compared to these hand-crafted features which are essentially heuris- tic and hard to verify, our embedding model in- duces semantic representations by optimizing a s- ingle well-defined objective. Note that the em- bedding actually also encodes the Wikipedia inter- page network, as we train on the entity-context pairs which are extracted from wiki links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Results</head><p>The Entity-TM model learns a representation for each entity as a word distribution. However, as noted in ( <ref type="bibr" target="#b1">Baroni et al., 2014</ref>), the counting-based distributional model usually shows inferior perfor- mance than context-predicting methods as ours. Moreover, in addition to the text context, our mod- el integrates the entity hierarchical structure which provides rich knowledge of semantic relatedness. The comparison between Ours and Ours-NoH fur- ther reveals the effect of integrating the hierarchy in learning entity vectors. With entity hierarchy, we obtain more semantically meaningful represen- tations that achieve 9% F1 improvement over en- tity vectors without hierarchical knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity Search</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Setup</head><p>Dataset We use the dataset from INEX 2009 en- tity ranking track 2 , which contains 55 queries. The golden standard results of each query contains a set of relevant entities each of which corresponds to a Wikipedia page.</p><p>Criteria We use the common criteria of precision@k, i.e., the percentage of relevant entities in the top-k results (we set k = 10), as well as precision@R where R is the number of golden standard entities for a query.</p><p>Baselines We compare our algorithm with the following recent competitors.</p><p>Balog (Balog et al., 2011) develops a proba- bilistic generative model which represents entities, as well as the query, as distributions over both words and categories. Entities are then ranked based on the KL-divergence between the distribu- tions.</p><p>K&amp;K <ref type="formula">(</ref> <ref type="bibr" target="#b9">Kaptein and Kamps, 2013</ref>) exploits Wikipedia entity hierarchy to derive the content of each category, which is in turn used to measure relatedness with the query categories. It further incorporates inter-entity links for relevance propa- gation.</p><p>Chen (Chen et al., 2014b) creates for each en- tity a context profile leveraging both the whole document (long-range) and sentences around en- tity (short-range) context, and models query text by a generative model. Categories are weighted based on the head words and other features. Our algorithm exploits a similar method for category matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods Precision@10</head><p>Precision@R   <ref type="table" target="#tab_3">Table 2</ref> lists the entity search results of the com- petitors. Our algorithm shows superiority over the previous best performing methods. Balog con- structs representations for each entity merely by counting (and smoothing) its co-occurrence be- tween words and categories, which is inadequate to capture relatedness accurately. K&amp;K leverages the rich resources in Wikipedia such as text, hi- erarchy, and link structures. However, the hand- crafted features are still suboptimal compared with our learned representations. Chen performs well by combining both long- and short-context of entities, as well as catego- ry lexical similarity. Our algorithm replaces it- s text matching component with a semantic en- richment step, i.e., grounding entity mentions in the query text onto KB entities. This augments the short query with rich background knowledge, facilitating accurate relatedness measure based on our high-quality entity embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Analysis</head><p>We qualitatively inspect the learned representa- tions of the entity hierarchy. The results show that both the entity vectors and the category distance metrics capture meaningful semantics, and can po- tentially boost a wide range of applications such as recommendation and knowledge base completion. <ref type="table" target="#tab_5">Table 3</ref> shows a list of target en- tities, and their top-4 nearest entities in the whole entity set or subsets belonging to given categories. Measuring under the whole set (column 2) results in nearest neighbors that are strongly related with the target entity. For instance, the nearest enti- ties for "black hole" are "faster-than-light", "event horizon", "white hole", and "time dilation", all of which are concepts from physical cosmology and the theory of relativity. Similar results can be ob- served from other 3 examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity vectors</head><p>Even more interesting is to specify a category and search for the most related entities under the category. The third column of <ref type="table" target="#tab_5">Table 3</ref> shows sev- eral examples. E.g., our model found that the most related Chinese websites to Youtube are "Tudou", "56.com", "Youku" (three top video hosting ser- vices in China), and "YinYueTai" (a major MV sharing site in China). The high-quality results show that our embedding model is able to discov- er meaningful relationships between entities from the complex entity hierarchy and plain text. This can be a useful feature in a wide range of appli- cations such as semantic search (e.g., looking for movies about black hole), recommendation (e.g., suggesting TV series of specific genre for kids), and knowledge base completion (e.g., extracting relations between persons), to name a few.   Category distance metrics In addition to learn- ing vector representations of entities, we also as- sociate with each category a local distance metric to capture the features of individual category. As we restrict the distance metrics to be diagonal ma- trixes, the magnitude of each diagonal value can be viewed as how much a category is character- ized by the corresponding dimension. Categories with close semantic meanings are expected to have similar metrics. <ref type="figure" target="#fig_1">Figure 3</ref> visualizes the metrics of all subcate- gories under the category "Microsoft", where we amplify some parts of the figure to showcase the clustering of semantically relevant categories. For instance, the categories of Microsoft Windows op- erating systems, and those of the Xbox games, are embedded close to each other, respectively. The results validate that our hierarchy embedding model can not only encode relatedness between leaf entities, but also capture semantic similarity of the internal categories. This can be helpful in taxonomy refinement and relation discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Distributed representation There has been a growing interest in distributed representation of words. Skip-gram model ( <ref type="bibr" target="#b14">Mikolov et al., 2013a</ref>) is one of the most popular methods to learn word representations. The model aims to find a rep- resentation for each word that is useful for pre- dicting its context words. Word-context simi- larity is measured by simple inner product. A set of recent works generalizing the basic skip- gram to incorporate dependency context ( <ref type="bibr" target="#b12">Levy and Goldberg, 2014</ref>), word senses ( <ref type="bibr" target="#b3">Chen et al., 2014a)</ref>, and multi-modal data <ref type="bibr" target="#b8">(Hill and Korhonen, 2014</ref>). However, these work leverages lim- ited structured knowledge. Our proposed method goes beyond skip-gram significantly such that we measures entity-context similarity under aggregat- ed distance metrics of hierarchical category n- odes. This effectively captures the structured knowledge. Another research line learn knowl- edge graph embedding ( <ref type="bibr" target="#b13">Lin et al., 2015;</ref><ref type="bibr" target="#b22">Wang et al., 2014;</ref><ref type="bibr" target="#b2">Bordes et al., 2013</ref>), which models enti- ties as vectors and relations as some operations on the vector space (e.g., translation). These work- s aim at relation prediction for knowledge graph completion, and can be viewed as a supplement to the above that extracts semantics from plain text.</p><p>Utilizing hierarchical knowledge Semantic hi- erarchies are key sources of knowledge. Previ- ous works <ref type="bibr" target="#b17">(Ponzetto and Strube, 2007;</ref><ref type="bibr" target="#b11">Leacock and Chodorow, 1998)</ref> use KB hierarchies to de- fine relatedness between concepts, typically based on path-length measure. Recent works ( <ref type="bibr" target="#b26">Yogatama et al., 2015;</ref><ref type="bibr" target="#b28">Zhao et al., 2011</ref>) learn representa- tions through hierarchical sparse coding that en- forces similar sparse patterns between nearby n- odes. Category hierarchies have also been wide- ly used in classification <ref type="bibr" target="#b24">(Xiao et al., 2011;</ref><ref type="bibr" target="#b23">Weinberger and Chapelle, 2009</ref>). E.g., in <ref type="bibr" target="#b21">(Verma et al., 2012</ref>) category nodes are endowed with discrim- inative power by learning distance metrics. Our approach differs in terms of entity vector learning and metric aggregation on DAG hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed to learn entity hierarchy embedding to boost semantic NLP tasks. A princi- pled framework was developed to incorporate both text context and entity hierarchical structure from large-scale knowledge bases. We learn a distance metric for each category node, and measure enti- ty vector similarity under aggregated metrics. A flexible and efficient metric aggregation scheme was also developed to model large-scale hierar- chies. Experiments in both entity linking and enti- ty search tasks show superiority of our approach.</p><p>The qualitative analysis indicates that our model can be potentially useful in a wide range of other applications such as knowledge base completion and ontology refinement. Another interesting as- pect of future work is to incorporate other sources of knowledge to further enrich the semantics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>í µí± → í µí± ′ path 1 í µí± → í µí± ′ path 2 turning node</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distance metric visualization for the subcategories of the category "Microsoft". The tSNE (Van der Maaten and Hinton, 2008) algorithm is used to map the high-dimensional (diagonal) matrixes into the 2D space.</figDesc><graphic url="image-1.png" coords="8,105.80,110.66,137.59,55.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>e1 ...e2….e3… ……….. Text Context e1 e2 e3 Entity Hierarchy h2 h1 í µí± ℎ 1</head><label></label><figDesc></figDesc><table>í µí± ℎ 2 

í µí±£ í µí± 1 

Entity pairs 

í µí± í µí± 1 .í µí± 3 = í µí¼ ℎ 1 í µí± ℎ 1 + í µí¼ ℎ 2 í µí± ℎ 2 
í µí±£ í µí± 2 

í µí±£ í µí± 3 

(í µí± 1 , í µí± 3 ) 

í µí± 1 , í µí± 2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 shows the performance of the competitors.</head><label>1</label><figDesc></figDesc><table>Our algorithm using the entity hierarchy embed-
ding gets 21% to 10% improvement in F1, and 

1 http://www.cse.iitb.ac.in/soumen/doc/CSAW/Annot 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Entity search performance.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Most related entities under specific cate-
gories. "Overall" represents the most general cat-
egory that includes all the entities. </table></figure>

			<note place="foot" n="2"> http://www.inex.otago.ac.nz/tracks/entityranking/entity-ranking.asp</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is supported by NSF IIS-1218282, IIS-12511827, and IIS-1450545.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Query modeling for entity search based on terms, categories, and examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Bron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dont count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving context and category matching for entity search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lexi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Aggregating continuous word embeddings for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">100</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Overview of the inex 2009 entity ranking track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluca</forename><surname>Demartini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tereza</forename><surname>Iofciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjen P De</forename><surname>Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Focused Retrieval and Evaluation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="254" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An entity-topic model for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="105" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning abstract concept embeddings from multi-modal data: Since you probably can&apos;t see what i mean</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="255" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploiting the category structure of wikipedia for entity ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Kaptein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="111" to="129" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Collective annotation of wikipedia entities in web text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayali</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD</title>
		<meeting>of KDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Lexicon Infused Phrase Embeddings for Named Entity Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="78" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge derived from wikipedia for computing semantic relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="181" to="212" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using information content to evaluate semantic similarity in a taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="448" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning sentiment-specific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning hierarchical similarity metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nakul</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sundararajan</forename><surname>Sellamanickam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2280" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge graph and text jointly embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large margin taxonomy embedding for document categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1737" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical classification via orthogonal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingrui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning word representations with hierarchical sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bilingually-constrained phrase embeddings for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Large-scale category structure aware image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1251" to="1259" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
