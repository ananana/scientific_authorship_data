<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ontology-Aware Token Embeddings for Prepositional Phrase Attachment</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ontology-Aware Token Embeddings for Prepositional Phrase Attachment</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2089" to="2098"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1191</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive em-beddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Type-level word embeddings map a word type (i.e., a surface form) to a dense vector of real num- bers such that similar word types have similar em- beddings. When pre-trained on a large corpus of unlabeled text, they provide an effective mecha- nism for generalizing statistical models to words which do not appear in the labeled training data for a downstream task.</p><p>In accordance with standard terminology, we make the following distinction between types and tokens in this paper: By word types, we mean the surface form of the word, whereas by tokens we mean the instantiation of the surface form in a con- text. For example, the same word type 'pool' oc- curs as two different tokens in the sentences "He sat by the pool," and "He played a game of pool."</p><p>Most word embedding models define a single vector for each word type. However, a fundamen- tal flaw in this design is their inability to distin- guish between different meanings and abstractions of the same word. In the two sentences shown above, the word 'pool' has different meanings, but the same representation is typically used for both of them. Similarly, the fact that 'pool' and 'lake' are both kinds of water bodies is not explicitly in- corporated in most type-level embeddings. Fur- thermore, it has become a standard practice to tune pre-trained word embeddings as model parameters during training for an NLP task (e.g., <ref type="bibr" target="#b6">Chen and Manning, 2014;</ref><ref type="bibr" target="#b14">Lample et al., 2016)</ref>, potentially allowing the parameters of a frequent word in the labeled training data to drift away from related but rare words in the embedding space.</p><p>Previous work partially addresses these prob- lems by estimating concept embeddings in Word- Net (e.g., <ref type="bibr" target="#b23">Rothe and Schütze, 2015)</ref>, or improv- ing word representations using information from knowledge graphs (e.g., <ref type="bibr" target="#b9">Faruqui et al., 2015)</ref>. However, it is still not clear how to use a lexical ontology to derive context-sensitive token embed- dings.</p><p>In this work, we represent a word token in a given context by estimating a context-sensitive probability distribution over relevant concepts in WordNet <ref type="bibr" target="#b17">(Miller, 1995)</ref> and use the expected value (i.e., weighted sum) of the concept embed- dings as the token representation (see §2). We take a task-centric approach towards doing this, and learn the token representations jointly with the task-specific parameters. In addition to providing context-sensitive token embeddings, the proposed method implicitly regularizes the embeddings of related words by forcing related words to share similar concept embeddings. As a result, the rep- resentation of a rare word which does not appear in the training data for a downstream task benefits from all the updates to related words which share one or more concept embeddings. <ref type="figure">Figure 1</ref>: An example grounding for the word 'pool'. Solid arrows represent possible senses and dashed arrows represent hypernym relations. Note that the same set of concepts are used to ground the word 'pool' regardless of its context. Other WordNet senses for 'pool' were removed from the figure for simplicity.</p><p>Our approach to context-sensitive embeddings assumes the availability of a lexical ontology. While this work relies on WordNet, and we ex- ploit the order of senses given by WordNet, our model is, in principle applicable to any ontology, with appropriate modifications. In this work, we do not assume the inputs are sense tagged. We use the proposed embeddings to predict prepositional phrase (PP) attachments (see §3), a challenging problem which emphasizes the selectional prefer- ences between words in the PP and each of the candidate head words. Our empirical results and detailed analysis (see §4) show that the proposed embeddings effectively use WordNet to improve the accuracy of PP attachment predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">WordNet-Grounded Context-Sensitive Token Embeddings</head><p>In this section, we focus on defining our context- sensitive token embeddings. We first describe our grounding of word types using WordNet con- cepts. Then, we describe our model of context- sensitive token-level embeddings as a weighted sum of WordNet concept embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">WordNet Grounding</head><p>We use WordNet to map each word type to a set of synsets, including possible generalizations or ab- stractions. Among the labeled relations defined in WordNet between different synsets, we focus on the hypernymy relation to help model generaliza- tion and selectional preferences between words, which is especially important for predicting PP at- tachments <ref type="bibr" target="#b22">(Resnik, 1993)</ref>. To ground a word type, we identify the set of (direct and indirect) hyper- nyms of the WordNet senses of that word. A sim- plified grounding of the word 'pool' is illustrated in <ref type="figure">Figure 1</ref>. This grounding is key to our model of token embeddings, to be described in the follow- ing subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Context-Sensitive Token Embeddings</head><p>Our goal is to define a context-sensitive model of token embeddings which can be used as a drop- in replacement for traditional type-level word em- beddings.</p><p>Notation. Let Senses(w) be the list of synsets defined as possible word senses of a given word type w in WordNet, and Hypernyms(s) be the list of hypernyms for a synset s. 1 For example, ac- cording to <ref type="figure">Figure 1</ref>:</p><formula xml:id="formula_0">Senses(pool) = [pond.n.01, pool.n.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>09], and</head><p>Hypernyms(pond.n.01) = [pond.n.01, lake.n.01, body of water.n.01, entity.n.01] Each WordNet synset s is associated with a set of parameters v s ∈ R n which represent its em- bedding. This parameterization is similar to that of <ref type="bibr" target="#b23">Rothe and Schütze (2015)</ref>.</p><p>Embedding model. Given a sequence of tokens t and their corresponding word types w, let u i ∈ R n be the embedding of the word token t i at po- sition i. Unlike most embedding models, the to- ken embeddings u i are not parameters. Rather, u i is computed as the expected value of concept em- beddings used to ground the word type w i corre- sponding to the token t i : The distribution which governs the expectation over synset embeddings factorizes into two com- ponents:</p><formula xml:id="formula_1">u i = s∈Senses(w i ) s ∈Hypernyms(s) p(s, s | t, w, i) v s (1) such that s∈Senses(w i ) s ∈Hypernyms(s) p(s, s | t, w, i) = 1</formula><formula xml:id="formula_2">p(s, s | t, w, i) ∝λ w i exp −λw i rank(s,w i ) × MLP([v s ; context(i, t)]) (2)</formula><p>The first component, λ w i exp −λw i rank(s,w i ) , is a sense prior which reflects the prominence of each word sense for a given word type. Here, we exploit 2 the fact that WordNet senses are or- dered in descending order of their frequencies, ob- tained from sense tagged corpora, and parameter- ize the sense prior like an exponential distribution. rank(s, w i ) denotes the rank of sense s for the word type w i , thus rank(s, w i ) = 0 corresponds to s being the first sense of w i . The scalar pa- rameter (λ w i ) controls the decay of the probability mass, which is learned along with the other pa- rameters in the model. Note that sense priors are defined for each word type (w i ), and are shared across all tokens which have the same word type. MLP([v s ; context(i, t)]), the second compo- nent, is what makes the token representations context-sensitive. It scores each concept in the WordNet grounding of w i by feeding the concate- nation of the concept embedding and a dense vec- 2 Note that for ontologies where such information is not available, our method is still applicable but without this com- ponent. We show the effect of using a uniform sense prior in <ref type="bibr">§4.2.</ref> tor that summarizes the textual context into a mul- tilayer perceptron (MLP) with two tanh layers fol- lowed by a sof tmax layer. This component is in- spired by the soft attention often used in neural machine translation ( <ref type="bibr" target="#b2">Bahdanau et al., 2014</ref>). <ref type="bibr">3</ref> The definition of the context function is dependent on the encoder used to encode the context. We de- scribe a specific instantiation of this function in §3. To summarize, <ref type="figure" target="#fig_0">Figure 2</ref> illustrates how to com- pute the embedding of a word token t i = 'pool' in a given context:</p><p>1. compute a summary of the context context(i, t),</p><p>2. enumerate related concepts for t i ,</p><p>3. compute p(s, s | t, w, i) for each pair (s, s ), and</p><formula xml:id="formula_3">4. compute u i = E[v s ].</formula><p>In the following section, we describe our model for predicting PP attachments, including our defi- nition for context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PP Attachment</head><p>Disambiguating PP attachments is an important and challenging NLP problem. Since modeling hypernymy and selectional preferences is criti- cal for successful prediction of PP attachments <ref type="bibr" target="#b22">(Resnik, 1993)</ref>, it is a good fit for evaluating our WordNet-grounded context-sensitive embeddings. <ref type="figure" target="#fig_1">Figure 3</ref>, reproduced from <ref type="bibr" target="#b4">Belinkov et al. (2014)</ref>, illustrates an example of the PP attach- ment prediction problem. The accuracy of a competitive English dependency parser at predict- ing the head word of an ambiguous prepositional phrase is 88.5%, significantly lower than the over- all unlabeled attachment accuracy of the same parser (94.2%). <ref type="bibr">4</ref> This section formally defines the problem of PP attachment disambiguation, describes our baseline model, then shows how to integrate the token-level embeddings in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>We follow <ref type="bibr" target="#b4">Belinkov et al. (2014)</ref>'s definition of the PP attachment problem. Given a preposition p and its direct dependent d in the prepositional phrase (PP), our goal is to predict the correct head word for the PP among an ordered list of candidate head words h. Each example in the train, validation, and test sets consists of an input tuple h, p, d and an output index k to identify the correct head among the candidates in h. Note that the order of words that form each h, p, d is the same as that in the corresponding original sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Definition</head><p>Both our proposed and baseline models for PP attachment use bidirectional RNN with LSTM cells (bi-LSTM) to encode the sequence t = h 1 , . . . , h K , p, d.</p><p>We score each candidate head by feeding the concatenation of the output bi-LSTM vectors for the head h k , the preposition p and the direct de- pendent d through an MLP, with a fully connected tanh layer to obtain a non-linear projection of the concatenation, followed by a fully-connected softmax layer:</p><formula xml:id="formula_4">p(h k is head) = MLP attach ([lstm out(h k );</formula><p>lstm out(p);</p><formula xml:id="formula_5">lstm out(d)]) (3)</formula><p>To train the model, we use cross-entropy loss at the output layer for each candidate head in the training set. At test time, we predict the candidate head with the highest probability according to the model in Eq. 3, i.e.,</p><formula xml:id="formula_6">ˆ k = arg max k p(h k is head = 1).<label>(4)</label></formula><p>This model is inspired by the Head-Prep-Child- Ternary model of <ref type="bibr" target="#b4">Belinkov et al. (2014)</ref>. The main difference is that we replace the input features for each token with the output bi-RNN vectors.</p><p>We now describe the difference between the proposed and the baseline models. Generally, let lstm in(t i ) and lstm out(t i ) represent the input and output vectors of the bi-LSTM for each token t i ∈ t in the sequence. The outputs at each timestep are obtained by concatenating those of the forward and backward LSTMs.</p><p>Baseline model. In the baseline model, we use type-level word embeddings to represent the input vector lstm in(t i ) for a token t i in the sequence. The word embedding parameters are initialized with pre-trained vectors, then tuned along with the parameters of the bi-LSTM and MLP attach . We call this model LSTM-PP.</p><p>Proposed model. In the proposed model, we use token level word embedding as described in §2 as the input to the bi-LSTM, i.e., lstm in(t i ) = u i . The context used for the attention compo- nent is simply the hidden state from the previous timestep. However, since we use a bi-LSTM, the model essentially has two RNNs, and accordingly we have two context vectors, and associated at- tentions. That is, context f (i, t) = lstm in(t i−1 ) for the forward RNN and context b (i, t) = lstm in(t i+1 ) for the backward RNN. Conse- quently, each token gets two representations, one from each RNN. The synset embedding param- eters are initialized with pre-trained vectors and tuned along with the sense decay (λ w ) and MLP parameters from Eq. 2, the parameters of the bi- LSTM and those of MLP attach . We call this model OntoLSTM-PP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Dataset and evaluation. We used the English PP attachment dataset created and made available by <ref type="bibr" target="#b4">Belinkov et al. (2014)</ref>. The training and test splits contain 33,359 and 1951 labeled examples respectively. As explained in §3.1, the input for each example is 1) an ordered list of candidate head words, 2) the preposition, and 3) the direct dependent of the preposition. The head words are either nouns or verbs and the dependent is always a noun. All examples in this dataset have at least two candidate head words. As discussed in <ref type="bibr" target="#b4">Belinkov et al. (2014)</ref>, this dataset is a more realistic PP at- tachment task than the RRR dataset ( <ref type="bibr" target="#b20">Ratnaparkhi et al., 1994)</ref>. The RRR dataset is a binary classifi- cation task with exactly two head word candidates in all examples. The context for each example in the RRR dataset is also limited which defeats the purpose of our context-sensitive embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model specifications and hyperparameters.</head><p>For efficient implementation, we use mini-batch updates with the same number of senses and hy- pernyms for all examples, padding zeros and trun- cating senses and hypernyms as needed. For each word type, we use a maximum of S senses and H indirect hypernyms from WordNet. In our ini- tial experiments on a held-out development set (10% of the training data), we found that values greater than S = 3 and H = 5 did not im- prove performance. We also used the develop- ment set to tune the number of layers in MLP attach separately for the OntoLSTM-PP and LSTM-PP, and the number of layers in the attention MLP in OntoLSTM-PP. When a synset has multiple hy- pernym paths, we use the shortest one. Finally, words types which do not appear in WordNet are assumed to have one unique sense per word type with no hypernyms. Since the POS tag for each word is included in the dataset, we exclude Word- Net synsets which are incompatible with the POS tag. The synset embedding parameters are initial- ized using the synset vectors obtained by running AutoExtend (Rothe and Schütze, 2015) on 100- dimensional GloVe ( <ref type="bibr" target="#b19">Pennington et al., 2014</ref>) vec- tors for WordNet 3.1. We refer to this embedding as GloVe-extended. Representation for the OOV word types in LSTM-PP and OOV synset types in OntoLSTM-PP were randomly drawn from a uni- form 100-d distribution. Initial sense prior param- eters (λ w ) were also drawn from a uniform 1-d dis- tribution.</p><p>Baselines. In our experiments, we compare our proposed model, OntoLSTM-PP with three base- lines -LSTM-PP initialized with GloVe em- bedding, LSTM-PP initialized with GloVe vec- tors retrofitted to WordNet using the approach of <ref type="bibr" target="#b9">Faruqui et al. (2015)</ref> (henceforth referred to as GloVe-retro), and finally the best performing stan- dalone PP attachment system from <ref type="bibr" target="#b4">Belinkov et al. (2014)</ref>, referred to as HPCD (full) in the paper. HPCD (full) is a neural network model that learns to compose the vector representations of each of the candidate heads with those of the preposition and the dependent, and predict attachments. The input representations are enriched using syntactic context information, POS, WordNet and VerbNet ( <ref type="bibr" target="#b13">Kipper et al., 2008</ref>) information and the distance of the head word from the PP is explicitly encoded in composition architecture. In contrast, we do not use syntactic context, VerbNet and distance infor- mation, and do not explicitly encode POS infor- mation. <ref type="table">Table 1</ref> shows that our proposed token level em- bedding scheme OntoLSTM-PP outperforms the better variant of our baseline LSTM-PP (with GloVe-retro intialization) by an absolute accuracy difference of 4.9%, or a relative error reduction of 32%. OntoLSTM-PP also outperforms HPCD (full), the previous best result on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">PP Attachment Results</head><p>Initializing the word embeddings with GloVe- retro (which uses WordNet as described in <ref type="bibr" target="#b9">Faruqui et al. (2015)</ref>) instead of GloVe amounts to a small improvement, compared to the improvements ob- tained using OntoLSTM-PP. This result illustrates that our approach of dynamically choosing a con- text sensitive distribution over synsets is a more effective way of making use of WordNet.</p><p>Effect on dependency parsing. Following <ref type="bibr" target="#b4">Belinkov et al. (2014)</ref>, we used RBG parser ( ), and modified it by adding a binary feature indicating the PP attachment predictions from our model. We compare four ways to compute the addi- tional binary features: 1) the predictions of the best standalone system HPCD (full) in <ref type="bibr" target="#b4">Belinkov et al. (2014)</ref>, 2) the predictions of our baseline model LSTM-PP, 3) the predictions of our im- proved model OntoLSTM-PP, and 4) the gold la- bels Oracle PP. <ref type="table" target="#tab_1">Table 2</ref> shows the effect of using the PP attach- ment predictions as features within a dependency parser. We note there is a relatively small differ- ence in unlabeled attachment accuracy for all de- pendencies (not only PP attachments), even when gold PP attachments are used as additional fea- tures to the parser. However, when gold PP attach- ment are used, we note a large potential improve-   <ref type="bibr" target="#b4">Belinkov et al. (2014)</ref>. This is due to the use of different versions of the RBG parser. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis</head><p>In this subsection, we analyze different aspects of our model in order to develop a better understand- <ref type="bibr">5</ref> The authors kindly provided their predictions for 1942 test examples (out of 1951 examples in the full test set). In <ref type="table" target="#tab_1">Table 2</ref>, we use the same subset of 1942 test examples and will include a link to the subset in the final draft. <ref type="bibr">6</ref> We use the latest commit (SHA: e07f74) on the GitHub repository of the RGB parser.</p><p>ing of its behavior.</p><p>Effect of context sensitivity and sense priors. We now show some results that indicate the rela- tive strengths of two components of our context- sensitive token embedding model. The second row in <ref type="table">Table 3</ref> shows the test accuracy of a sys- tem trained without sense priors (that is, making p(s|w i ) from Eq. 1 a uniform distribution), and the third row shows the effect of making the to- ken representations context-insensitive by giving a similar attention score to all related concepts, es- sentially making them type level representations, but still grounded in WordNet. As it can be seen, removing context sensitivity has an adverse effect on the results. This illustrates the importance of the sense priors and the attention mechanism.</p><p>It is interesting that, even without sense priors and attention, the results with WordNet grounding is still higher than that of the two LSTM-PP sys- tems in <ref type="table">Table 1</ref>. This result illustrates the regu- larization behavior of sharing concept embeddings across multiple words, which is especially impor- tant for rare words.</p><p>Effect of training data size. Since OntoLSTM- PP uses external information, the gap between the model and LSTM-PP is expected to be more pro- nounced when the training data sizes are smaller. To test this hypothesis, we trained the two mod- els with different amounts of training data and measured their accuracies on the test set. The plot is shown in <ref type="figure" target="#fig_2">Figure 4</ref>. As expected, the gap tends to be larger at smaller data sizes. Surpris- ingly, even with 2000 sentences in the training data set, OntoLSTM-PP outperforms LSTM-PP trained with the full data set. When both the models are trained with the full dataset, LSTM-PP reaches a training accuracy of 95.3%, whereas OntoLSTM- PP reaches 93.5%. The fact that LSTM-PP is over- fitting the training data more, indicates the regular- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>PPA Acc. full 89.7 -sense priors 88.4 -attention 87.5 <ref type="table">Table 3</ref>: Effect of removing sense priors and con- text sensitivity (attention) from the model. ization capability of OntoLSTM-PP.</p><p>Qualitative analysis. To better understand the effect of WordNet grounding, we took a sample of 100 sentences from the test set whose PP attach- ments were correctly predicted by OntoLSTM- PP but not by LSTM-PP. A common pattern ob- served was that those sentences contained words not seen frequently in the training data. <ref type="figure" target="#fig_3">Figure 5</ref> shows two such cases. In both cases, the weights assigned by OntoLSTM-PP to infrequent words are also shown. The word types soapsuds and buoyancy do not occur in the training data, but OntoLSTM-PP was able to leverage the parame- ters learned for the synsets that contributed to their token representations. Another important observa- tion is that the word type buoyancy has four senses in WordNet (we consider the first three), none of which is the metaphorical sense that is applicable to markets as shown in the example here. Select- ing a combination of relevant hypernyms from var- ious senses may have helped OntoLSTM-PP make the right prediction. This shows the value of us- ing hypernymy information from WordNet. More- over, this indicates the strength of the hybrid na- ture of the model, that lets it augment ontological information with distributional information.</p><p>Parameter space. We note that the vocabulary sizes in OntoLSTM-PP and LSTM-PP are compa- rable as the synset types are shared across word types. In our experiments with the full PP at- tachment dataset, we learned embeddings for 18k synset types with OntoLSTM-PP and 11k word types with LSTM-PP. Since the biggest contribu- tion to the parameter space comes from the em- bedding layer, the complexities of both the models are comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>This work is related to various lines of research within the NLP community: dealing with syn- onymy and homonymy in word representations both in the context of distributed embeddings and more traditional vector spaces; hybrid models of distributional and knowledge based semantics; and selectional preferences and their relation with syntactic and semantic relations. The need for going beyond a single vector per word-type has been well established for a while, and many efforts were focused on building multi-prototype vector space models of meaning <ref type="bibr" target="#b21">(Reisinger and Mooney, 2010;</ref><ref type="bibr" target="#b10">Huang et al., 2012;</ref><ref type="bibr" target="#b18">Neelakantan et al., 2015;</ref><ref type="bibr">Arora et al., 2016, etc.</ref>). However, the target of all these approaches is obtaining multi- sense word vector spaces, either by incorporating sense tagged information or other kinds of exter- nal context. The number of vectors learned is still fixed, based on the preset number of senses. In contrast, our focus is on learning a context depen- dent distribution over those concept representa- tions. Other work not necessarily related to multi- sense vectors, but still related to our work includes <ref type="bibr" target="#b3">Belanger and Kakade (2015)</ref>'s work which pro- posed a Gaussian linear dynamical system for es- timating token-level word embeddings, and Vil- nis and McCallum (2015)'s work which proposed mapping each word type to a density instead of a point in a space to account for uncertainty in meaning. These approaches do not make use of lexical ontologies and are not amenable for joint training with a downstream NLP task.</p><p>Related to the idea of concept embeddings is <ref type="bibr" target="#b23">Rothe and Schütze (2015)</ref> who estimated Word- Net synset representations, given pre-trained type- level word embeddings. In contrast, our work fo- cuses on estimating token-level word embeddings as context sensitive distributions of concept em- There is a large body of work that tried to im- prove word embeddings using external resources. <ref type="bibr" target="#b25">Yu and Dredze (2014)</ref> extended the CBOW model ( <ref type="bibr" target="#b16">Mikolov et al., 2013)</ref> by adding an extra term in the training objective for generating words con- ditioned on similar words according to a lexi- con.  extended the skipgram model ( <ref type="bibr" target="#b16">Mikolov et al., 2013</ref>) by representing word senses as latent variables in the generation pro- cess, and used a structured prior based on the on- tology. <ref type="bibr" target="#b9">Faruqui et al. (2015)</ref> used belief propa- gation to update pre-trained word embeddings on a graph that encodes lexical relationships in the ontology. Similarly, <ref type="bibr" target="#b12">Johansson and Pina (2015)</ref> improved word embeddings by representing each sense of the word in a way that reflects the topol- ogy of the semantic network they belong to, and then representing the words as convex combina- tions of their senses. In contrast to previous work that was aimed at improving type level word repre- sentations, we propose an approach for obtaining context-sensitive embeddings at the token level, while jointly optimizing the model parameters for the NLP task of interest. <ref type="bibr" target="#b22">Resnik (1993)</ref> showed the applicability of se- mantic classes and selectional preferences to re- solving syntactic ambiguity. <ref type="bibr" target="#b26">Zapirain et al. (2013)</ref> applied models of selectional preferences auto- matically learned from WordNet and distributional information, to the problem of semantic role la- beling. <ref type="bibr" target="#b22">Resnik (1993)</ref>; <ref type="bibr" target="#b5">Brill and Resnik (1994)</ref>; <ref type="bibr" target="#b0">Agirre (2008)</ref> and others have used WordNet in- formation towards improving prepositional phrase attachment predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed a grounding of lexical items which acknowledges the semantic ambigu- ity of word types using WordNet and a method to learn a context-sensitive distribution over their representations. We also showed how to integrate the proposed representation with recurrent neural networks for disambiguating prepositional phrase attachments, showing that the proposed WordNet- grounded context-sensitive token embeddings out- performs standard type-level embeddings for pre- dicting PP attachments. We provided a detailed qualitative and quantitative analysis of the pro- posed model. Implementation and code availability. The models are implemented using <ref type="bibr">Keras (Chollet, 2015)</ref>, and the functionality is avail- able at https://github.com/pdasigi/ onto-lstm in the form of Keras layers to make it easier to use the proposed embedding model in other NLP problems.</p><p>Future work. This approach may be extended to other NLP tasks that can benefit from using encoders that can access WordNet information. WordNet also has some drawbacks, and may not always have sufficient coverage given the task at hand. As we have shown in §4.2, our model can deal with missing WordNet information by aug- menting it with distributional information. More- over, the methods described in this paper can be extended to other kinds of structured knowledge sources like Freebase which may be more suitable for tasks like question answering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Steps for computing the contextsensitive token embedding for the word 'pool', as described in §2.2.</figDesc><graphic url="image-2.png" coords="3,73.14,62.81,216.00,213.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Two sentences illustrating the importance of lexicalization in PP attachment decisions. In the top sentence, the PP 'with butter' attaches to the noun 'spaghetti'. In the bottom sentence, the PP 'with chopsticks' attaches to the verb 'ate'. Note: This figure and caption have been reproduced from Belinkov et al. (2014).</figDesc><graphic url="image-3.png" coords="4,72.00,63.80,230.40,149.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effect of training data size on test accuracies of OntoLSTM-PP and LSTM-PP.</figDesc><graphic url="image-4.png" coords="7,72.00,63.80,230.40,151.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Two examples from the test set where OntoLSTM-PP predicts the head correctly and LSTM-PP does not, along with weights by OntoLSTM-PP to synsets that contribute to token representations of infrequent word types. The prepositions are shown in bold, LSTM-PP's predictions in red and OntoLSTMPP's predictions in green. Words that are not candidate heads or dependents are shown in brackets.</figDesc><graphic url="image-5.png" coords="8,75.57,62.81,446.41,206.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results from RBG dependency parser 
with features coming from various PP attachment 
predictors and oracle attachments. 

ment of 10.46 points in PP attachment accuracies 
(between the PPA accuracy for RBG and RBG + 
Oracle PP), which confirms that adding PP pre-
dictions as features is an effective approach. Our 
proposed model RBG + OntoLSTM-PP recovers 
15% of this potential improvement, while RBG + 
HPCD (full) recovers 10%, which illustrates that 
PP attachment remains a difficult problem with 
plenty of room for improvements even when us-
ing a dedicated model to predict PP attachments 
and using its predictions in a dependency parser. 
We also note that, although we use the same 
predictions of the HPCD (full) model in Belinkov 
et al. (2014) 5 , we report different results than Be-
linkov et al. (2014). For example, the unlabeled 
attachment score (UAS) of the baselines RBG and 
RBG + HPCD (full) are 94.17 and 94.19, respec-
tively, in Table 2, compared to 93.96 and 94.05, 
respectively, in </table></figure>

			<note place="foot" n="1"> For notational convenience, we assume that s ∈ Hypernyms(s).</note>

			<note place="foot" n="3"> Although soft attention mechanism is typically used to explicitly represent the importance of each item in a sequence, it can also be applied to non-sequential items. 4 See Table 2 in §4 for detailed results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The first author is supported by a fellowship from the Allen Institute for Artificial Intelligence. We would like to thank Matt Gardner, Jayant Krishna-murthy, Julia Hockenmaier, Oren Etzioni, Hector Liu, Filip Ilievski, and anonymous reviewers for their comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Improving parsing and pp attachment performance with sense information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<editor>ACL. Citeseer</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Linear algebraic structure of word senses, with applications to polysemy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.03764</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A linear dynamical system model for text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring compositional architectures and word vector representations for prepositional phrase attachment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="561" to="572" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A rule-based approach to prepositional phrase attachment disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th conference on Computational linguistics</title>
		<meeting>the 15th conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1198" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ontologically grounded multi-sense representation learning for semantic vector space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Sujay Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Embedding a semantic network in a word space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis Nieto</forename><surname>Pina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A large-scale classification of english verbs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Kipper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neville</forename><surname>Ryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="40" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Low-rank tensors for scoring dependency structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1310.4546</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient non-parametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06654</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A maximum entropy model for prepositional phrase attachment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Reynar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Human Language Technology</title>
		<meeting>the workshop on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-prototype vector-space models of word meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic classes and syntactic ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Human Language Technology. Association for Computational Linguistics</title>
		<meeting>the workshop on Human Language Technology. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Autoextend: Extending word embeddings to embeddings for synsets and lexemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Word representations via gaussian embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving lexical embeddings with semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Selectional preferences for semantic role classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Benat Zapirain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Marquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Surdeanu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
