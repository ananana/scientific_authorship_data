<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dependency-based Gated Recursive Neural Network for Chinese Word Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
							<email>{xujingjing, xusun}@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dependency-based Gated Recursive Neural Network for Chinese Word Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="567" to="572"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently, many neural network models have been applied to Chinese word seg-mentation. However, such models focus more on collecting local information while long distance dependencies are not well learned. To integrate local features with long distance dependencies, we propose a dependency-based gated recursive neural network. Local features are first collected by bi-directional long short term memory network, then combined and refined to long distance dependencies via gated re-cursive neural network. Experimental results show that our model is a competitive model for Chinese word segmentation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word segmentation is an important pre-process step in Chinese language processing. Most wide- ly used approaches treat Chinese word segmenta- tion (CWS) task as a sequence labeling problem in which each character in the input sequence is as- signed with a tag. Many previous approaches have been effectively applied to CWS problem <ref type="bibr" target="#b5">(Lafferty et al., 2001;</ref><ref type="bibr" target="#b14">Xue and Shen, 2003;</ref><ref type="bibr" target="#b11">Sun et al., 2012;</ref><ref type="bibr" target="#b13">Sun, 2014;</ref><ref type="bibr" target="#b1">Cheng et al., 2015)</ref>. However, these approaches incorpo- rated many handcrafted features, thus restricting the generalization ability of these models. Neural network models have the advantage of minimiz- ing the effort in feature engineering. Collobert et al. (2011) developed a general neural network ar- chitecture for sequence labeling tasks. Following this work, neural network approaches have been well studied and widely applied to CWS task with good results ( <ref type="bibr" target="#b19">Zheng et al., 2013;</ref><ref type="bibr" target="#b7">Pei et al., 2014;</ref><ref type="bibr" target="#b6">Ma and Hinrichs, 2015;</ref><ref type="bibr" target="#b0">Chen et al., 2015)</ref>.</p><p>"The ground is covered with thick snow " "This area is really not small." <ref type="figure">Figure 1</ref>: An illustration for the segmentation am- biguity. The character "面" is labeled as "E" (end of word) in the top sentence while labeled as "B" (begin of word) in the bottom one even though "面" has the same adjacent characters, "地" and "积".</p><p>However, these models focus more on collect- ing local features while long distance dependen- cies are not well learned. In fact, relying on the information of adjacent words is not enough for CWS task. An example is shown in <ref type="figure">Figure 1</ref>. The character "面" has different tags in two sentences, even with the same adjacent characters, " 地" and " 积". Only long distance dependencies can help the model recognize tag correctly in this example. Thus, long distance information is an importan- t factor for CWS task.</p><p>The main limitation of chain structure for se- quence labeling is that long distance dependencies decay inevitably. Though forget gate mechanis- m is added, it is difficult for bi-directional long short term memory network (Bi-LSTM), a kind of chain structure, to avoid this problem. In general, tree structure works better than chain structure to model long term information. Therefore, we use gated recursive neural network (GRNN) <ref type="bibr" target="#b0">(Chen et al., 2015)</ref> which is a kind of tree structure to cap- ture long distance dependencies.</p><p>Motivated by the fact, we propose the dependency-based gated recursive neural network (DGRNN) to integrate local features with long dis-tance dependencies. <ref type="figure" target="#fig_1">Figure 2</ref> shows the structure of DGRNN. First of all, local features are col- lected by Bi-LSTM. Secondly, GRNN recursive- ly combines and refines local features to capture long distance dependencies. Finally, with the help of local features and long distance dependencies, our model generates the probability of the tag of word.</p><p>The main contributions of the paper are as fol- lows:</p><p>• We present the dependency-based gated re- cursive neural network to combine local fea- tures with long distance dependencies.</p><p>• To verify the effectiveness of the proposed approach, we conduct experiments on three widely used datasets. Our proposed model achieves the best performance compared with other state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dependency-based Gated Recursive Neural Network</head><p>In order to capture local features and long distance dependencies, we propose dependency-based gat- ed recursive neural network. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the structure of the model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Collect Local Features</head><p>We use bi-directional long short term memory (Bi-LSTM) with single layer to collect local fea- tures. Bi-LSTM is composed of two directional</p><formula xml:id="formula_0">tanh sig sig tanh f (t) h (t) s (t) i (t) s (t-1) sig o (t) x (t)</formula><p>, h (t-1)</p><p>Figure 3: Structure of LSTM unit. The behavior of the LSTM cell is controlled by three "gates", namely input gate i (t) , forget gate f (t) and output gate o (t) . long short term memory networks with single lay- er, which can model word representation with con- text information. <ref type="figure">Figure 3</ref> shows the calculation process of LSTM. The behavior of LSTM cell is controlled by three "gates", namely input gate i (t) , forget gate f (t) and output gate o (t) . The input of LSTM cell are x (t) , s (t−1) and h (t−1) . x (t) is the character embeddings of input sentence. s (t−1) and h (t−1) stand for the state and output of the for- mer LSTM cell, respectively. The core of the L- STM model is s (t) , which is computed using the former state of cell and two gates, i (t) and f (t) . In the end, the output of LSTM cell h (t) is calculated making use of s (t) and o (t) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Refine Long Distance Dependencies</head><p>GRNN recursively combines and refines local fea- tures to capture long distance dependencies. The structure of GRNN is like a binary tree, where ev- ery two continuous vectors in a sentence is com- bined to form a new vector. For a sequence s with length n, there are n layers in total. <ref type="figure" target="#fig_2">Figure 4</ref> shows the calculation process of GRNN cell. The core of GRNN cell are two kinds of gates, reset gates, r L , r R , and update gates z. Reset gates control how to adjust the proportion of the input h i−1 and h i , which results to the current new activation h ′ . By the update gates, the activation of an output neu- ron can be regarded as a choice among the current new activation h ′ , the left child h i−1 and the right child h i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Loss Function</head><p>Following the work of <ref type="bibr" target="#b7">Pei et al. (2014)</ref>, we adop- t the max-margin criterion as loss function. For an input sentence c <ref type="bibr">[1:n]</ref> with a tag sequence t <ref type="bibr">[1:n]</ref> , a sentence-level score is given by the sum of net- work scores:</p><formula xml:id="formula_1">s(c [1:n] , t [1:n] , θ) = n ∑ i=1 f θ (t i |c [i−2:i+2] )<label>(1)</label></formula><p>where s(c <ref type="bibr">[1:n]</ref> , t <ref type="bibr">[1:n]</ref> , θ) is the sentence-level score. n is the length of c <ref type="bibr">[1:n]</ref> .</p><formula xml:id="formula_2">f θ (t i |c [i−2:i+2] )</formula><p>is the s- core output for tag t i at the i th character by the network with parameters θ. We define a structured margin loss ∆(y i , y) for predicting a tag sequence y and a given correct tag sequence y i :</p><formula xml:id="formula_3">∆(y i , y) = n ∑ j=1 κ1{y i,j ̸ = y i }<label>(2)</label></formula><p>where κ is a discount parameter. This leads to the regularized objective function for m training ex- amples:</p><formula xml:id="formula_4">J(θ) = 1 m m ∑ i=1 l i (θ) + λ 2 ∥θ∥ 2<label>(3)</label></formula><formula xml:id="formula_5">l i (θ) = max y⊆Y (x i ) ((s(x i , y, θ) + ∆(y i , y)) − s(x i , y i , θ)) (4)</formula><p>where J(θ) is a loss function with parameters θ. λ is regularization factor. By minimizing this ob- ject, the score of the correct tag sequence y i is in- creased and score of the highest scoring incorrect tag sequence y is decreased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Amplification Gate and Training</head><p>A direct adaptive method for faster backpropaga- tion learning method (RPROP) ( <ref type="bibr" target="#b8">Riedmiller and Braun, 1993</ref>) was a practical adaptive learning method to train large neural networks. We use mini-batch version RPROP (RMSPROP) <ref type="bibr" target="#b4">(Hinton, 2012)</ref> to minimize the loss function. Intuitively, extra hidden layers are able to im- prove accuracy performance. However, it is com- mon that extra hidden layers decrease classifica- tion accuracy. This is mainly because extra hidden layers lead to the inadequate training of later lay- ers due to the vanishing gradient problem. This problem will decline the utilization of local and long distance information in our model. To over- come this problem, we propose a simple ampli- fication gate mechanism which appropriately ex- pands the value of gradient while not changing the direction.</p><p>Higher amplification may not always perfor- m better while lower value may bring about the unsatisfied result. Therefore, the amplification gate must be carefully selected. Large magnifi- cation will cause expanding gradient problem. On the contrary, small amplification gate will hardly reach the desired effect. Thus, we introduce the threshold mechanism to guarantee the robustness of the algorithm, where gradient which is greater than threshold will not be expanded. Amplifica- tion gate of difference layer is distinct. For every sample, the training procedure is as follows.</p><p>First, recursively calculate m t and v t which de- pend on the gradient of time t − 1 or the square of gradient respectively. β 1 and β 2 aim to control the impact of last state.</p><formula xml:id="formula_6">m t = β 1 · m t−1 + (1 − β 1 ) · g t (5) v t = β 2 · v t−1 + (1 − β 2 ) · g 2 t<label>(6)</label></formula><p>Second, calculate ∆W (t) based on v t and square of m t . ϵ and µ are smooth parameters.</p><formula xml:id="formula_7">M (w, t) = v t − m 2 t (7) ∆W (t) = ϵg t,i √ M (w, t) + µ<label>(8)</label></formula><p>Third, update weight based on the amplification gate and ∆W (t). The parameter update for the i th parameter for the Θ t,i at time step t with amplifi- cation gate γ is as follows: (c) CTB6 <ref type="figure">Figure 5</ref>: Results for DGRNN with amplification gate (AG) on three development datasets.</p><formula xml:id="formula_8">Θ t,i = Θ t,i − γ∆W (t)<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data and Settings</head><p>We evaluate our proposed approach on three datasets, PKU, MSRA and CTB6. The PKU and MSRA data both are provided by the second In- ternational Chinese Word Segmentation Bakeof- f ( <ref type="bibr" target="#b3">Emerson, 2005</ref>) and CTB6 is from Chinese TreeBank 6.0 1 ( <ref type="bibr" target="#b15">Xue et al., 2005</ref>). We randomly divide the whole training data into the 90% sen- tences as training set and the rest 10% sentences as development set. All datasets are preprocessed by replacing the Chinese idioms and the continu- ous English characters. The character embeddings are pre-trained on unlabeled data, Chinese Giga- word corpus 2 . We use MSRA dataset to prepro- cess model weights before training on CTB6 and PKU datasets.</p><p>Following previous work and our experimen- tal results, hyper parameters configurations are set as follows: minibatch size n = 16, window size w = 5, character embedding size d 1 = 100, am- plification gate range γ = <ref type="bibr">[0,</ref><ref type="bibr">4]</ref> and margin loss discount κ = 0.2. All weight matrixes are diag- onal matrixes and randomly initialized by normal distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results and Discussions</head><p>We first compare our model with baseline meth- ods, Bi-LSTM and GRNN on three datasets. The results evaluated by F-score (F 1 score) are report- ed in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>• Bi-LSTM. First, the output of Bi-LSTM is concatenated to a vector. Second, softmax layer takes the vector as input and generates each tag probability.  <ref type="table">Table 2</ref>: Comparisons for DGRNN and state-of- the-art non-neural network approaches on F-score.</p><p>• GRNN. The structure of GRNN is recursive. GRNN combines adjacent word vectors to the more abstract representation in bottom-up way.</p><p>Furthermore, we conduct experiments with am- plification gate on three development datasets. <ref type="figure">Figure 5</ref> shows that amplification gate significant- ly increases F-score on three datasets. Amplifi- cation even achieves 0.9% improvement on CTB6 dataset. It is demonstrated that amplification gate is an effective mechanism.</p><p>We compare our proposed model with previ- ous neural approaches on PKU, MSRA and CT- B6 test datasets. Experimental results are report- ed in  We also compare DGRNN with other state-of- the-art non-neural networks, as shown in <ref type="table">Table 2</ref>. <ref type="bibr" target="#b0">Chen et al. (2015)</ref> implements the work of Sun and Xu (2011) on CTB6 dataset and achieves 95.7% F-score. We achieve the best result on P- KU dataset only with unigram embeddings. The experimental results show that our model is a com- petitive model for Chinese word segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Statistical Significance Tests</head><p>We use the t-test to intuitively show the improve- ment of DGRNN over baselines. According to the results shown in <ref type="table" target="#tab_3">Table 3</ref>, we can draw a conclu- sion that, by conventional criteria, this improve- ment is considered to be statistically significant between DGRNN with baselines, except for GRN- N approach on MSRA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this work, we propose dependency-based recur- sive neural network to combine local features with long distance dependencies, which achieves sub- stantial improvement over the state-of-the-art ap- proaches. Our work indicates that long distance dependencies can improve the performance of lo- cal segmenter. In the future, we will study alterna- tive ways of modeling long distance dependencies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This area is really not small."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of DGRNN for Chinese Word Segmentation. Cell is the basic unit of GRNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The structure of GRNN cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 .</head><label>1</label><figDesc>It can be clearly seen that our approach achieves the best results compared with</figDesc><table>Dataset 

Model 
Result 

MSRA 
Bi-LSTM 
t = 5.94, p &lt; 1 × 10 −4 
GRNN 
t = 1.22, p = 0.22 

PKU 
Bi-LSTM t = 15.54, p &lt; 1 × 10 −4 
GRNN 
t = 4.43, p &lt; 1 × 10 −4 

CTB6 
Bi-LSTM 
t = 5.01, p &lt; 1 × 10 −4 
GRNN 
t = 2.55, p = 2.48 × 10 −2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The t-test results for DGRNN and base-
lines. 

other neural networks on traditional unigram em-
beddings. It is possible that bigram embeddings 
may achieve better results. With the help of bi-
gram embeddings, Pei et al. (2014) can achieve 
95.2% and 97.2% F-scores on PKU and MSRA 
datasets and Chen et al. (2015) can achieve 96.4%, 
97.6% and 95.8% F-scores on PKU, MSRA and 
CTB6 datasets. However, performance varies a-
mong these bigram models since they have dif-
ferent ways of involving bigram embeddings. Be-
sides, the training speed would be very slow after 
adding bigram embeddings. Therefore, we only 
compare our model on traditional unigram embed-
dings. 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgments</head><p>We thank Xiaoyan Cai for her valuable sug-gestions. This work was supported in part by National Natural Science Foundation of China (No. 61300063), National High Technology Re-search and Development Program of <ref type="bibr">China (863 Program, No. 2015AA015404)</ref>, and Doctoral Fund of Ministry of Education of <ref type="bibr">China (No. 20130001120004)</ref>. Xu Sun is the corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gated recursive neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1744" to="1753" />
		</imprint>
	</monogr>
	<note>ACL (1)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Synthetic word parsing improves chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="262" to="267" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The second international chinese word segmentation bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Emerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fourth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="123" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Lecture 6.5: rmsprop: divide the gradient by a running average of its recent magnitude. coursera: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, number 8 in ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, number 8 in ICML &apos;01<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accurate linear-time chinese word segmentation via embedding matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Hinrichs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1733" to="1743" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Maxmargin tensor neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A direct adaptive method for faster backpropagation learning: The rprop algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Braun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enhancing chinese word segmentation using unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Edinburgh, Uk</addrLine></address></meeting>
		<imprint>
			<publisher>John Mcintyre Conference Centre</publisher>
			<date type="published" when="2011-07" />
			<biblScope unit="volume">2011</biblScope>
			<biblScope unit="page" from="970" to="979" />
		</imprint>
	</monogr>
	<note>A Meeting of Sigdat, A Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A discriminative latent variable chinese segmenter with hybrid word/character information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaozhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="56" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic chinese word segmentation with non-local information and stochastic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Yao Zhong Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="626" to="636" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structure regularization for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2402" to="2410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Chinese Word Segmentation as LMR Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the 2nd SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Penn Chinese TreeBank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Chinese segmentation with a word-based perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="840" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Subword-based tagging by conditional random fields for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genichiro</forename><surname>Kikui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, NAACL-Short &apos;06</title>
		<meeting>the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, NAACL-Short &apos;06<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="193" to="196" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploring representations from unlabeled data with co-training for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mairgup</forename><surname>Mansur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="311" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning for chinese word segmentation and pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
