<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Reinforcement Learning with a Natural Language Action Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
							<email>{jvking, ostendor}@uw.edu †</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Reinforcement Learning with a Natural Language Action Space</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1621" to="1630"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network (DRRN), the architecture represents action and state spaces with separate embedding vectors, which are combined with an interaction function to approximate the Q-function in reinforcement learning. We evaluate the DRRN on two popular text games, showing superior performance over other deep Q-learning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This work is concerned with learning strategies for sequential decision-making tasks, where a sys- tem takes actions at a particular state with the goal of maximizing a long-term reward. More specifi- cally, we consider tasks where both the states and the actions are characterized by natural language, such as in human-computer dialog systems, tutor- ing systems, or text-based games. In a text-based game, for example, the player (or system, in this case) is given a text string that describes the cur- rent state of the game and several text strings that describe possible actions one could take. After se- lecting one of the actions, the environment state is updated and revealed in a new textual description. A reward is given either at each transition or in the end. The objective is to understand, at each step, the state text and all the action texts to pick the most relevant action, navigating through the se- quence of texts so as to obtain the highest long- term reward. Here the notion of relevance is based on the joint state/action impact on the reward: an action text string is said to be "more relevant" (to a state text string) than the other action texts if taking that action would lead to a higher long- term reward. Because a player's action changes the environment, reinforcement learning <ref type="bibr" target="#b21">(Sutton and Barto, 1998</ref>) is appropriate for modeling long- term dependency in text games.</p><p>There is a large body of work on reinforcement learning. Of most interest here are approaches leveraging neural networks because of their suc- cess in handling a large state space. Early work - TD-gammon -used a neural network to approxi- mate the state value function <ref type="bibr" target="#b22">(Tesauro, 1995)</ref>. Re- cently, inspired by advances in deep learning ( <ref type="bibr" target="#b10">LeCun et al., 2015;</ref><ref type="bibr" target="#b8">Krizhevsky et al., 2012;</ref>), significant progress has been made by combining deep learning with reinforcement learning. Building on the approach of Q-learning ( <ref type="bibr" target="#b23">Watkins and Dayan, 1992)</ref>, the "Deep Q-Network" (DQN) was developed and ap- plied to Atari games ( <ref type="bibr" target="#b15">Mnih et al., 2013;</ref><ref type="bibr" target="#b16">Mnih et al., 2015</ref>) and shown to achieve human level per- formance by applying convolutional neural net- works to the raw image pixels. <ref type="bibr" target="#b17">Narasimhan et al. (2015)</ref> applied a Long Short-Term Memory network to characterize the state space in a DQN framework for learning control policies for parser- based text games. More recently, <ref type="bibr" target="#b18">Nogueira and Cho (2016)</ref> have also proposed a goal-driven web navigation task for language based sequential de- cision making study. Another stream of work fo- cuses on continuous control with deep reinforce- ment learning ( <ref type="bibr" target="#b12">Lillicrap et al., 2016)</ref>, where an actor-critic algorithm operates over a known con- tinuous action space.</p><p>Inspired by these successes and recent work us- ing neural networks to learn phrase-or sentence-level embeddings <ref type="bibr" target="#b3">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b6">Huang et al., 2013;</ref><ref type="bibr" target="#b9">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b20">Sutskever et al., 2014;</ref><ref type="bibr" target="#b7">Kiros et al., 2015)</ref>, we propose a novel deep architecture for text under- standing, which we call a deep reinforcement rele- vance network (DRRN). The DRRN uses separate deep neural networks to map state and action text strings into embedding vectors, from which "rel- evance" is measured numerically by a general in- teraction function, such as their inner product. The output of this interaction function defines the value of the Q-function for the current state-action pair, which characterizes the optimal long-term reward for pairing these two text strings. The Q-function approximation is learned in an end-to-end manner by Q-learning.</p><p>The DRRN differs from prior work in that ear- lier studies mostly considered action spaces that are bounded and known. For actions described by natural language text strings, the action space is inherently discrete and potentially unbounded due to the exponential complexity of language with re- spect to sentence length. A distinguishing aspect of the DRRN architecture -compared to sim- ple DQN extensions -is that two different types of meaning representations are learned, reflecting the tendency for state texts to describe scenes and action texts to describe potential actions from the user. We show that the DRRN learns a continuous space representation of actions that successfully generalize to paraphrased descriptions of actions unseen in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deep Reinforcement Relevance Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text Games and Q-learning</head><p>We consider the sequential decision making prob- lem for text understanding. At each time step t, the agent will receive a string of text that de- scribes the state s t (i.e., "state-text") and several strings of text that describe all the potential ac- tions a t (i.e., "action-text"). The agent attempts to understand the texts from both the state side and the action side, measuring their relevance to the current context s t for the purpose of maximizing the long-term reward, and then picking the best action. Then, the environment state is updated s t+1 = s according to the probability p(s |s, a), and the agent receives a reward r t for that partic- ular transition. The policy of the agent is defined to be the probability π(a t |s t ) of taking action a t at state s t . Define the Q-function Q π (s, a) as the expected return starting from s, taking the action a, and thereafter following policy π(a|s) to be:</p><formula xml:id="formula_0">Q π (s, a) = E +∞ k=0 γ k r t+k s t = s, a t = a</formula><p>where γ denotes a discount factor. The optimal policy and Q-function can be found by using the Q-learning algorithm ( <ref type="bibr" target="#b23">Watkins and Dayan, 1992)</ref>:</p><formula xml:id="formula_1">Q(s t , a t ) ← Q(s t , a t )+ (1) η t · r t + γ · max a Q(s t+1 , a) − Q(s t , a t )</formula><p>where η t is the learning rate of the algorithm. In this paper, we use a softmax selection strategy as the exploration policy during the learning stage, which chooses the action a t at state s t according to the following probability:</p><formula xml:id="formula_2">π(a t = a i t |s t ) = exp(α · Q(s t , a i t )) |At| j=1 exp(α · Q(s t , a j t )) ,<label>(2)</label></formula><p>where A t is the set of feasible actions at state s t , a i t is the i-th feasible action in A t , | · | denotes the cardinality of the set, and α is the scaling factor in the softmax operation. α is kept constant through- out the learning period. All methods are initialized with small random weights, so initial Q-value dif- ferences will be small, thus making the Q-learning algorithm more explorative initially. As Q-values better approximate the true values, a reasonable α will make action selection put high probability on the optimal action (exploitation), but still maintain a small exploration probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Natural language action space</head><p>Let S denote the state space, and let A denote the entire action space that includes all the unique ac- tions over time. A vanilla Q-learning recursion (1) needs to maintain a table of size |S| × |A|, which is problematic for a large state/action space. Prior work using a DNN in Q-function approximation has shown high capacity and scalability for han- dling a large state space, but most studies have used a network that generates |A| outputs, each of which represents the value of Q(s, a) for a par- ticular action a. It is not practical to have a DQN architecture of a size that is explicitly dependence on the large number of natural language actions. Further, in many text games, the feasible action set A t at each time t is an unknown subset of the unbounded action space A that varies over time.</p><p>For the case where the maximum number of possible actions at any point in time (max t |A t |) is known, the DQN can be modified to simply use that number of outputs ("Max-action DQN"), as illustrated in <ref type="figure">Figure 1(a)</ref>, where the state and ac- tion vectors are concatenated (i.e., as an extended state vector) as its input. The network computes the Q-function values for the actions in the current feasible set as its outputs. For a complex game, max t |A t | may be difficult to obtain, because A t is usually unknown beforehand. Nevertheless, we will use this modified DQN as a baseline.</p><p>An alternative approach is to use a function ap- proximation using a neural network that takes a state-action pair as input, and outputs a single Q- value for each possible action ("Per-action DQN" in <ref type="figure">Figure 1(b)</ref>). This architecture easily handles a varying number of actions and represents a second baseline.</p><p>We propose an alternative architecture for han- dling a natural language action space in sequential text understanding: the deep reinforcement rele- vance network (DRRN). As shown in <ref type="figure">Figure 1</ref>(c), the DRRN consists of a pair of DNNs, one for the state text embedding and the other for action text embeddings, which are combined using a pair- wise interaction function. The texts used to de- scribe states and actions could be very different in nature, e.g., a state text could be long, contain- ing sentences with complex linguistic structure, whereas an action text could be very concise or just a verb phrase. Therefore, it is desirable to use two networks with different structures to handle state/action texts, respectively. As we will see in the experimental sections, by using two separate deep neural networks for state and action sides, we obtain much better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">DRRN architecture: Forward activation</head><p>Given any state/action text pair (s t , a i t ), the DRRN estimates the Q-function Q(s t , a i t ) in two steps. First, map both s t and a i t to their embedding vec- tors using the corresponding DNNs, respectively. Second, approximate Q(s t , a i t ) using an interac- tion function such as the inner product of the em- bedding vectors. Then, given a particular state s t , we can select the optimal action a t among the set of actions via a t = arg max a i t Q(s t , a i t ). More formally, let h l,s and h l,a denote the l-th hidden layer for state and action side neural net- works, respectively. For the state side, W l,s and b l,s denote the linear transformation weight ma- trix and bias vector between the (l − 1)-th and l-th hidden layers. W l,a and b l,a denote the equivalent parameters for the action side. In this study, the DRRN has L hidden layers on each side.</p><formula xml:id="formula_3">h 1,s = f (W 1,s s t + b 1,s ) (3) h i 1,a = f (W 1,a a i t + b 1,a ) (4) h l,s = f (W l−1,s h l−1,s + b l−1,s ) (5) h i l,a = f (W l−1,a h i l−1,a + b l−1,a ) (6)</formula><p>where f (·) is the nonlinear activation function at the hidden layers, which, for example, could be chosen as tanh (x), and i = 1, 2, 3, ..., |A t | is the action index. A general interaction function g(·) is used to approximate the Q-function values, Q(s, a), in the following parametric form:</p><formula xml:id="formula_4">Q(s, a i ; Θ) = g h L,s , h i L,a<label>(7)</label></formula><p>where Θ denotes all the model parameters. The in- teraction function could be an inner product, a bi- linear operation, or a nonlinear function such as a deep neural network. In our experiments, the inner product and bilinear operation gave similar results. For simplicity, we present our experiments mostly using the inner product interaction function. The success of the DRRN in handling a natu- ral language action space A lies in the fact that the state-text and the action-texts are mapped into separate finite-dimensional embedding spaces. The end-to-end learning process (discussed next) makes the embedding vectors in the two spaces more aligned for "good" (or relevant) action texts compared to "bad" (or irrelevant) choices, result- ing in a higher interaction function output (Q- function value).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Learning the DRRN: Back propagation</head><p>To learn the DRRN, we use the "experience- replay" strategy <ref type="bibr" target="#b13">(Lin, 1993)</ref>, which uses a fixed exploration policy to interact with the environment to obtain a sample trajectory. Then, we randomly sample a transition tuple (s k , a k , r k , s k+1 ), com- pute the temporal difference error for sample k:</p><formula xml:id="formula_5">d k = r k +γ max a Q(s k+1 , a; Θ k−1 )−Q(s k , a k ; Θ k−1 ),</formula><p>and update the model according to the recursions:  for v ∈ {s, a}. Expressions for ∂Q ∂Wv , ∂Q ∂bv and other algorithm details are given in supplementary materials. Random sampling essentially scram- bles the trajectory from experience-replay into a "bag-of-transitions", which has been shown to avoid oscillations or divergence and achieve faster convergence in Q-learning ( <ref type="bibr" target="#b16">Mnih et al., 2015)</ref>. Since the models on the action side share the same parameters, models associated with all actions are effectively updated even though the back propaga- tion is only over one action. We apply back prop- agation to learn how to pair the text strings from the reward signals in an end-to-end manner. The representation vectors for the state-text and the action-text are automatically learned to be aligned with each other in the text embedding space from the reward signals. A summary of the full learning algorithm is given in Algorithm 1. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates learning with an inner product interaction function. We used Principal Component Analysis (PCA) to project the 100- dimension last hidden layer representation (before the inner product) to a 2-D plane. The vector em- beddings start with small values, and after 600 episodes of experience-replay training, the embed- dings are very close to the converged embedding (4000 episodes). The embedding vector of the op- timal action (Action 1) converges to a positive in- ner product with the state embedding vector, while Action 2 converges to a negative inner product.</p><formula xml:id="formula_6">W v,k = W v,k−1 + η k d k · ∂Q(s k , a k ; Θ k−1 ) ∂W v (8) b v,k = b v,k−1 + η k d k · ∂Q(s k , a k ; Θ k−1 ) ∂b v (9) í µí± í µí± ℎ ℎ í µí± (í µí± , í µí± ) í µí± í µí± (í µí± , í µí± ) (a) Max-action DQN í µí± í µí± ℎ ℎ í µí± í µí± ℎ ℎ í µí± (í µí± , í µí± ) í µí± í µí± (í µí± , í µí± ) í µí± (í µí± , í µí± ) (b) Per-action DQN í µí± pairwise interaction function (e.g. inner product) í µí± ℎ , ℎ , í µí± (í µí± , í µí± ) ℎ , ℎ ,<label>(c)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text games</head><p>Text games, although simple compared to video games, still enjoy high popularity in online com- munities, with annual competitions held online Algorithm 1 Learning algorithm for DRRN 1: Initialize replay memory D to capacity N . 2: Initialize DRRN with small random weights. 3: Initialize game simulator and load dictionary. 4: for episode = 1, . . . , M do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Restart game simulator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Read raw state text and a list of action text from the simulator, and convert them to representation s 1 and a 1 1 , a 2 1 , . . . , a |A 1 | 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>for t = 1, . . . , T do 8:</p><p>Compute Q(s t , a i t ; Θ) for the list of actions using DRRN forward activation (Section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Select an action a t based on probability distribution π(a t = a i t |s t ) ( <ref type="formula" target="#formula_2">Equation 2)</ref> 10:</p><p>Execute action a t in simulator 11:</p><p>Observe reward r t . Read the next state text and the next list of action texts, and convert them to representation s t+1 and a 1 t+1 , a 2 t+1 , . . . , a</p><formula xml:id="formula_7">|A t+1 | t+1 .</formula><p>12:</p><p>Store transition (s t , a t , r t , s t+1 , A t+1 ) in D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>Sample random mini batch of transitions (s k , a k , r k , s k+1 , A k+1 ) from D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>Set</p><formula xml:id="formula_8">y k = r k if s k+1 is terminal r k + γ max a ∈A k+1 Q(s k+1 , a ; Θ)) otherwise 15:</formula><p>Perform a gradient descent step on (y k − Q(s k , a k ; Θ)) 2 with respect to the network parameters Θ (Section 2.4). Back-propagation is performed only for a k even though there are |A k | actions at time k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>end for 17: end for since 1995. Text games communicate to players in the form of a text display, which players have to understand and respond to by typing or click- ing text <ref type="bibr" target="#b0">(Adams, 2014)</ref>. There are three types of text games: parser-based <ref type="figure" target="#fig_2">(Figure 3(a)</ref>), choice- based <ref type="figure" target="#fig_2">(Figure 3(b)</ref>), and hypertext-based ( <ref type="figure" target="#fig_2">Figure  3(c)</ref>). Parser-based games accept typed-in com- mands from the player, usually in the form of verb phrases, such as "eat apple", "get key", or "go east". They involve the least complex ac- tion language. Choice-based and hypertext-based games present actions after or embedded within the state text. The player chooses an action, and the story continues based on the action taken at this particular state. With the development of web browsing and richer HTML display, choice-based and hypertext-based text games have become more popular, increasing in percentage from 8% in 2010 to 62% in 2014. <ref type="bibr">1</ref> For parser-based text games, <ref type="bibr" target="#b17">Narasimhan et al. (2015)</ref> have defined a fixed set of 222 actions, which is the total number of possible phrases the parser accepts. Thus the parser-based text game is reduced to a problem that is well suited to a fixed-  <ref type="table">Table 1</ref>: Statistics for the games "Saving John" and and "Machine of Death".</p><p>action-set DQN. However, for choice-based and hypertext-based text games, the size of the action space could be exponential with the length of the action sentences, which is handled here by using a continuous representation of the action space. In this study, we evaluate the DRRN with two games: a deterministic text game task called "Sav- ing John" and a larger-scale stochastic text game called "Machine of Death" from a public archive. <ref type="bibr">2</ref> The basic text statistics of these tasks are shown in <ref type="table">Table 1</ref>. The maximum value of feasible actions (i.e., max t |A t |) is four in "Saving John", and nine in "Machine of Death". We manually annotate fi- nal rewards for all distinct endings in both games (as shown in supplementary materials). The mag- nitude of reward scores are given to describe sen- timent polarity of good/bad endings. On the other hand, each non-terminating step we assign with a small negative reward, to encourage the learner to finish the game as soon as possible. For the text game "Machine of Death", we restrict an episode to be no longer than 500 steps.</p><p>In "Saving John" all actions are choice-based, for which the mapping from text strings to a t are clear. In "Machine of Death", when actions are hypertext, the actions are substrings of the state. In this case s t is associated with the full state de- scription, and a t are given by the substrings with- out any surrounding context. For text input, we use raw bag-of-words as features, with different vocabularies for the state side and action side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiment setup</head><p>We apply DRRNs with both 1 and 2 hidden layer structures. In most experiments, we use dot- product as the interaction function and set the hidden dimension to be the same for each hid- den layer. We use DRRNs with 20, 50 and 100-dimension hidden layer(s) and build learn- ing curves during experience-replay training. The learning rate is constant: η t = 0.001. In testing, as in training, we apply softmax selection. We record average final rewards as performance of the model.</p><p>The DRRN is compared to multiple baselines: a linear model, two max-action DQNs (MA DQN) (L = 1 or 2 hidden layers), and two per-action DQNs (PA DQN) (again, L = 1, 2). All base- lines use the same Q-learning framework with dif- ferent function approximators to predict Q(s t , a t ) given the current state and actions. For the lin- ear and MA DQN baselines, the input is the text- based state and action descriptions, each as a bag of words, with the number of outputs equal to the maximum number of actions. When there are fewer actions than the maximum, the highest scor- ing available action is used. The PA DQN baseline  takes each pair of state-action texts as input, and generates a corresponding Q-value. We use softmax selection, which is widely applied in practice, to trade-off exploration vs. exploitation. Specifically, for each experience- replay, we first generate 200 episodes of data (about 3K tuples in "Saving John" and 16K tuples in "Machine of Death") using the softmax selec- tion rule in (2), where we set α = 0.2 for the first game and α = 1.0 for the second game. The α is picked according to an estimation of range of the optimal Q-values. We then shuffle the generated data tuples (s t , a t , r t , s t+1 ) update the model as described in Section 2.4. The model is trained with multiple epochs for all configurations, and is eval- uated after each experience-replay. The discount factor γ is set to 0.9. For DRRN and all baselines, network weights are initialized with small random values. To prevent algorithms from "remember- ing" state-action ordering and make choices based on action wording, each time the algorithm/player reads text from the simulator, we randomly shuffle the list of actions. 3 This will encourage the algo- rithms to make decisions based on the understand- ing of the texts that describe the states and actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Performance</head><p>In <ref type="figure">Figure 4</ref>, we show the learning curves of dif- ferent models, where the dimension of the hid- </p><formula xml:id="formula_9">1.0) PA DQN (L = 1) 0.9 (2.4) 2.3 (0.9) 3.1 (1.3) PA DQN (L = 2) 1.3 (1.2) 2.3 (1.6) 3.4 (1.7) MA DQN (L = 1) 2.0 (1.2) 3.7 (1.6) 4.8 (2.9) MA DQN (L = 2) 2.8 (0.9) 4.3 (0.9) 5.2 (1.2) DRRN (L = 1) 7.2 (1.5) 8.4 (1.3) 8.7 (0.9) DRRN (L = 2)</formula><p>9.2 (2.1) 10.7 (2.7) 11.2 (0.6) <ref type="table">Table 3</ref>: The final average rewards and standard deviations on "Machine of Death". den layers in the DQNs and DRRN are all set to 100. The error bars are obtained by running 5 independent experiments. The proposed meth- ods and baselines all start at about the same per- formance (roughly -7 average rewards for Game 1, and roughly -8 average rewards for Game 2), which is the random guess policy. After around 4000 episodes of experience-replay training, all methods converge. The DRRN converges much faster than the other three baselines and achieves a higher average reward. We hypothesize this is be- cause the DRRN architecture is better at capturing relevance between state text and action text. The faster convergence for "Saving John" may be due to the smaller observation space and/or the deter- ministic nature of its state transitions (in contrast to the stochastic transitions in the other game). The final performance (at convergence) for both baselines and proposed methods are shown in Ta- bles 2 and 3. We test for different model sizes with 20, 50, and 100 dimensions in the hidden layers. The DRRN performs consistently better than all baselines, and often with a lower variance. For Game 2, due to the complexity of the underly- ing state transition function, we cannot compute the exact optimal policy score. To provide more insight into the performance, we averaged scores of 8 human players for initial trials (novice) and after gaining experience, yielding scores of −5.5 and 16.0, respectively. The experienced players do outperform our algorithm. The converged per- formance is higher with two hidden layers for all models. However, deep models also converge more slowly than their 1 hidden layer versions, as shown for the DRRN in <ref type="figure">Figure 4</ref>.</p><p>Besides an inner-product, we also experimented with more complex interaction functions: a) a bi- linear operation with different action side dimen- sions; and b) a non-linear deep neural network us- ing the concatenated state and action space embed- dings as input and trained in an end-to-end fash- ion to predict Q values. For different configura- tions, we fix the state side embedding to be 100 dimensions and vary the action side embedding dimensions. The bilinear operation gave similar results, but the concatenation input to a DNN de- graded performance. Similar behaviors have been observed on a different task ( <ref type="bibr" target="#b14">Luong et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Actions with paraphrased descriptions</head><p>To investigate how our models handle actions with "unseen" natural language descriptions, we had two people paraphrase all actions in the game "Machine of Death" (used in testing phase), except a few single-word actions whose syn- onyms are out-of-vocabulary (OOV). The word- level OOV rate of paraphrased actions is 18.6%, We apply a well-trained 2-layer DRRN model (with hidden dimension 100), and predict Q- values for each state-action pair with fixed model parameters. <ref type="figure" target="#fig_5">Figure 5</ref> shows the correlation be- tween Q-values associated with paraphrased ac- tions versus original actions. The predictive R- squared is 0.95, showing a strong positive corre- lation. We also run Q-value correlation for the NN interaction and pR 2 = 0.90. For baseline MA-DQN and PA-DQN, their corresponding pR 2 is 0.84 and 0.97, indicating they also have some generalization ability. This is confirmed in the paraphrasing-based experiments too, where the test reward on the paraphrased setup is close to the original setup. This supports the claim that deep learning is useful in general for this language understanding task, and our findings show that a decoupled architecture most effectively leverages that approach.</p><p>In <ref type="table">Table 4</ref> we provide examples with predicted Q-values of original descriptions and paraphrased descriptions. We also include alternative action descriptions with in-vocabulary words that will lead to positive / negative / irrelevant game devel- opment at that particular state. <ref type="table">Table 4</ref> shows ac- tions that are more likely to result in good endings are predicted with high Q-values. This indicates that the DRRN has some generalization ability and gains a useful level of language understanding in the game scenario.</p><p>We use the baseline models and proposed DRRN model trained with the original action de- scriptions for "Machine of Death", and test on paraphrased action descriptions. For this game, the underlying state transition mechanism has not changed. The only change to the game interface is that during testing, every time the player reads the actions from the game simulator, it reads the para- phrased descriptions and performs selection based on these paraphrases. Since the texts in test time are "unseen" to the player, a good model needs to have some level of language understanding, while a naive model that memorizes all unique action texts in the original game will do poorly. The re- sults for these models are shown in <ref type="table" target="#tab_3">Table 5</ref>. All methods have a slightly lower average reward in this setting (10.5 vs. 11.2 for the original actions), but the DRRN still gives a high reward and sig- nificantly outperforms other methods. This shows that the DRRN can generalize well to "unseen" natural language descriptions of actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>There has been increasing interest in applying deep reinforcement learning to a variety problems, but only a few studies address problems with nat- ural language state or action spaces. In language processing, reinforcement learning has been ap- plied to a dialogue management system that con- verses with a human user by taking actions that generate natural language ( <ref type="bibr" target="#b19">Scheffler and Young, 2002;</ref><ref type="bibr" target="#b24">Young et al., 2013)</ref>. There has also been in- terest in extracting textual knowledge to improve game control performance <ref type="bibr" target="#b2">(Branavan et al., 2011)</ref>, and mapping text instructions to sequences of ex- ecutable actions <ref type="bibr" target="#b1">(Branavan et al., 2009</ref>). In some applications, it is possible to manually design fea- tures for state-action pairs, which are then used in reinforcement learning to learn a near-optimal policy ( <ref type="bibr" target="#b11">Li et al., 2009)</ref>. Designing such features, however, require substantial domain knowledge.</p><p>The work most closely related to our study in- olves application of deep reinforcement to learn- ing decision policies for parser-based text games. <ref type="bibr" target="#b17">Narasimhan et al. (2015)</ref> applied a Long Short- Term Memory DQN framework, which achieves higher average reward than the random and Bag- of-Words DQN baselines. In this work, actions are constrained to a set of known fixed command structures (one action and one argument object),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text (with predicted Q-values) State</head><p>As you move forward, the people surrounding you suddenly look up with terror in their faces, and flee the street. Actions in the original game Ignore the alarm of others and continue moving forward. (-21.5) Look up. (16.6) Paraphrased actions (not original) Disregard the caution of others and keep pushing ahead. (-11.9) Turn up and look. (17.5) Positive actions (not original) Stay there. (2.8) Stay calmly. (2.0) Negative actions (not original) Screw it. I'm going carefully. (-17.4) Yell at everyone. (-13.5) Irrelevant actions (not original) Insert a coin. (-1.4) Throw a coin to the ground. (-3.6)   based on a limited action-side vocabulary size. The overall action space is defined by the action- argument product space. This pre-specified prod- uct space is not feasible for the more complex text strings in other forms of text-based games. Our proposed DRRN, on the other hand, can handle the more complex text strings, as well as parser- based games. In preliminary experiments with the parser-based game from (Narasimhan et al., 2015), we find that the DRRN using a bag-of-words (BOW) input achieves results on par with their BOW DQN. The main advantage of the DRRN is that it can also handle actions described with more complex language.</p><p>The DRRN experiments described here lever- age only a simple bag-of-words representa- tion of phrases and sentences. As observed in <ref type="bibr" target="#b17">(Narasimhan et al., 2015</ref>), more complex sentence-based models can give further improve- ments. In preliminary experiments with "Machine of Death", we did not find LSTMs to give im- proved performance, but we conjecture that they would be useful in larger-scale tasks, or when the word embeddings are initialized by training on large data sets.</p><p>As mentioned earlier, other work has applied deep reinforcement learning to a problem with a continuous action space ( <ref type="bibr" target="#b12">Lillicrap et al., 2016</ref>). In the DRRN, the action space is inherently discrete, but we learn a continuous representation of it. As indicated by the paraphrasing experiment, the con- tinuous space representation seems to generalize reasonably well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we develop a deep reinforcement relevance network, a novel DNN architecture for handling actions described by natural language in decision-making tasks such as text games. We show that the DRRN converges faster and to a better solution for Q-learning than alternative ar- chitectures that do not use separate embeddings for the state and action spaces. Future work in- cludes: (i) adding an attention model to robustly analyze which part of state/actions text correspond to strategic planning, and (ii) applying the pro- posed methods to more complex text games or other tasks with actions defined through natural language.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>DRRN Figure 1 :</head><label>DRRN1</label><figDesc>Figure 1: Different deep Q-learning architectures: Max-action DQN and Per-action DQN both treat input text as concantenated vectors and compute output Q-values with a single NN. DRRN models text embeddings from state/action sides separately, and use an interaction function to compute Q-values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: PCA projections of text embedding vectors for state and associated action vectors after 200, 400 and 600 training episodes. The state is "As you move forward, the people surrounding you suddenly look up with terror in their faces, and flee the street." Action 1 (good choice) is "Look up", and action 2 (poor choice) is "Ignore the alarm of others and continue moving forward."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Different types of text games</figDesc><graphic url="image-1.png" coords="6,72.89,62.81,145.14,54.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 4: Learning curves of the two text games.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Scatterplot and strong correlation between Q-values of paraphrased actions versus original actions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The final average rewards and standard 
deviations on "Saving John". 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>The final average rewards and stan-
dard deviations on paraphrased game "Machine of 
Death". 

</table></figure>

			<note place="foot" n="2"> Simulators are available at https://github.com/ jvking/text-games</note>

			<note place="foot" n="3"> When in a specific state, the simulator presents the possible set of actions in random order, i.e. they may appear in a different order the next time a player is in this same state.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Karthik Narasimhan and Tejas Kulka-rni for providing instructions on setting up their parser-based games.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fundamentals of game design. Pearson Education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reinforcement learning for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th IJCNLP</title>
		<meeting>of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to win by reading manuals in a monte-carlo framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="268" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 25th International Conference on Machine learning</title>
		<meeting>of the 25th International Conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>of the ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Skipthought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3276" to="3284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reinforcement learning for spoken dialog management using least-squares policy iteration and fast feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Annual Conference of the International Speech Communication Association</title>
		<meeting>the Tenth Annual Conference of the International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">24752478</biblScope>
		</imprint>
	</monogr>
	<note>INTERSPEECH-09</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reinforcement learning for robots using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DTIC Document</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of the 2015 Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Playing Atari with Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop, December</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Humanlevel control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language understanding for text-based games using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of the 2015 Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Webnav: A new largescale task for natural language based sequential decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scheffler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the second International Conference on Human Language Technology Research</title>
		<meeting>of the second International Conference on Human Language Technology Research</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="12" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal difference learning and td-gammon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="58" to="68" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Q-learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="279" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
