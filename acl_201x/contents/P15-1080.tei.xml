<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Non-linear Learning for Statistical Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology Nanjing University Nanjing 210023</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology Nanjing University Nanjing 210023</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology Nanjing University Nanjing 210023</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology Nanjing University Nanjing 210023</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Non-linear Learning for Statistical Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="825" to="835"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Modern statistical machine translation (SMT) systems usually use a linear combination of features to model the quality of each translation hypothesis. The linear combination assumes that all the features are in a linear relationship and constrains that each feature interacts with the rest features in an linear manner, which might limit the expressive power of the model and lead to a under-fit model on the current data. In this paper, we propose a non-linear modeling for the quality of translation hypotheses based on neural networks, which allows more complex interaction between features. A learning framework is presented for training the non-linear models. We also discuss possible heuristics in designing the network structure which may improve the non-linear learning performance. Experimental results show that with the basic features of a hierarchical phrase-based machine translation system, our method produce translations that are better than a linear model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the core problems in the research of statis- tical machine translation is the modeling of trans- lation hypotheses. Each modeling method defines a score of a target sentence e = e 1 e 2 ...e i ...e I , given a source sentence f = f 1 f 2 ...f j ...f J , where each e i is the ith target word and f j is the jth source word. The well-known modeling method starts from the Source-Channel model <ref type="bibr" target="#b3">(Brown et al., 1993</ref>)(Equation 1). The scoring of e decom- poses to the calculation of a translation model and a language model. P r(e|f ) = P r(e)P r(f |e)/P r(f )</p><p>The modeling method is extended to log-linear models by <ref type="bibr" target="#b23">Och and Ney (2002)</ref>, as shown in Equa- tion 2, where h m (e|f ) is the mth feature function and λ m is the corresponding weight.</p><formula xml:id="formula_1">P r(e|f ) = p λ M 1 (e|f ) = exp[ ∑ M m=1 λ m h m (e|f )] ∑ e ′ exp[ ∑ M m=1 λ m h m (e ′ |f )]<label>(2)</label></formula><p>Because the normalization term in Equation 2 is the same for all translation hypotheses of the same source sentence, the score of each hypothesis, de- noted by s L , is actually a linear combination of all features, as shown in Equation 3.</p><formula xml:id="formula_2">s L (e) = M ∑ m=1 λ m h m (e|f )<label>(3)</label></formula><p>The log-linear models are flexible to incorpo- rate new features and show significant advantage over the traditional source-channel models, thus become the state-of-the-art modeling method and are applied in various translation settings <ref type="bibr" target="#b29">(Yamada and Knight, 2001;</ref><ref type="bibr" target="#b16">Koehn et al., 2003;</ref><ref type="bibr" target="#b5">Chiang, 2005;</ref><ref type="bibr" target="#b19">Liu et al., 2006</ref>).</p><p>It is worth noticing that log-linear models try to separate good and bad translation hypotheses us- ing a linear hyper-plane. However, complex inter- actions between features make it difficult to lin- early separate good translation hypotheses from bad ones <ref type="bibr" target="#b8">(Clark et al., 2014)</ref>.</p><p>Taking common features in a typical phrase- based ( <ref type="bibr" target="#b16">Koehn et al., 2003)</ref> or hierarchical phrase- based ( <ref type="bibr" target="#b5">Chiang, 2005</ref>) machine translation system as an example, the language model feature favors shorter hypotheses; the word penalty feature en- courages longer hypotheses. The phrase trans- lation probability feature selects phrases that oc- curs more frequently in the training corpus, which sometimes is long with a lower translation proba- bility, as in translating named entities or idioms; sometimes is short but with a high translation probability, as in translating verbs or pronouns. These three features jointly decide the choice of translations. Simply use the weighted sum of their values may not be the best choice for modeling translations. As a result, log-linear models may under-fit the data. This under-fitting may prevents the further improvement of translation quality.</p><p>In this paper, we propose a non-linear model- ing of translation hypotheses based on neural net- works. The traditional features of a machine trans- lation system are used as the input to the net- work. By feeding input features to nodes in a hid- den layer, complex interactions among features are modeled, resulting in much stronger expressive power than traditional log-linear models. (Sec- tion 3)</p><p>Employing a neural network for SMT model- ing has two issues to be tackled. The first is- sue is the parameter learning. Log-linear models rely on minimum error rate training (MERT) <ref type="bibr" target="#b24">(Och, 2003)</ref> to achieve best performance. When the scoring function become non-linear, the intersec- tion points of these non-linear functions could not be effectively calculated and enumerated. Thus MERT is no longer suitable for learning the pa- rameters. To solve the problem, we present a framework for effective training including several criteria to transform the training problem into a bi- nary classification task, a unified objective func- tion and an iterative training algorithm. (Sec- tion 4)</p><p>The second issue is the structure of neural net- work. Single layer neural networks are equivalent to linear models; two-layer networks with suffi- cient nodes are capable of learning any continuous function <ref type="bibr" target="#b2">(Bishop, 1995)</ref>. Adding more layers into the network could model complex functions with less nodes, but also brings the problem of van- ishing gradient <ref type="bibr" target="#b11">(Erhan et al., 2009</ref>). We adapt a two-layer feed-forward neural network to keep the training process efficient. We notice that one ma- jor problem that prevents a neural network training reaching a good solution is that there are too many local minimums in the parameter space. Thus we discuss how to constrain the learning of neural net- works with our intuitions and observations of the features. (Section 5) Experiments are conducted to compare vari- ous settings and verify the effectiveness of our proposed learning framework. Experimental re- sults show that our framework could achieve better translation quality even with the same traditional features as previous linear models. <ref type="table">(Section 6)</ref> 2 Related work Many research has been attempting to bring non- linearity into the training of SMT. These efforts could be roughly divided into the following three categories.</p><p>The first line of research attempted to re- interpret original features via feature transforma- tion or additional learning. For example, Maskey and Zhou (2012) use a deep belief network to learn representations of the phrase translation and lexical translation probability features. <ref type="bibr" target="#b8">Clark et al. (2014)</ref> used discretization to transform real- valued dense features into a set of binary indica- tor features. <ref type="bibr" target="#b21">Lu et al. (2014)</ref> learned new fea- tures using a semi-supervised deep auto encoder. These work focus on the explicit representation of the features and usually employ extra learning procedure. Our proposed method only takes the original features, with no transformation, as the input. Feature transformation or combination are performed implicitly during the training of the net- work and integrated with the optimization of trans- lation quality.</p><p>The second line of research attempted to use non-linear models instead of log-linear models, which is most similar in spirit with our work. <ref type="bibr" target="#b10">Duh and Kirchhoff (2008)</ref> used the boosting method to combine several results of MERT and achieved improvement in a re-ranking setting. <ref type="bibr" target="#b20">Liu et al. (2013)</ref> proposed an additive neural network which employed a two-layer neural network for embedding-based features. To avoid local min- imum, they still rely on a pre-training and post- training from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models.</p><p>The third line of research attempted to add non-linear features/components into the log-linear learning framework. Neural network based mod- els are trained as language models <ref type="bibr" target="#b27">(Vaswani et al., 2013;</ref><ref type="bibr" target="#b0">Auli and Gao, 2014</ref>), translation mod- els ( ) or joint language and transla- tion models ( <ref type="bibr" target="#b1">Auli et al., 2013;</ref><ref type="bibr" target="#b9">Devlin et al., 2014</ref>). <ref type="bibr" target="#b20">Liu et al. (2013)</ref> also introduced word embed- ding for source and target sides of the translation rules as local features. In this paper, we focus on enhancing the expressive power of the modeling, which is independent of the research of enhanc- ing translation systems with new designed fea- tures. We believe additional improvement could be achieved by incorporating more features into our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Non-linear Translation</head><p>The non-linear modeling of translation hypothe- ses could be used in both phrase-based system and syntax-based systems. In this paper, we take the hierarchical phrase based machine translation sys- tem ( <ref type="bibr" target="#b5">Chiang, 2005)</ref> as an example and introduce how we fit the non-linearity into the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Two-layer Neural Networks</head><p>We employ a two-layer neural network as the non- linear model for scoring translation hypotheses. The structure of a typical two-layer feed-forward neural network includes an input layer, a hidden layer, and a output layer (as shown in <ref type="figure" target="#fig_0">Figure 1</ref>). We use the input layer to accept input features, the hidden layer to combine different input fea- tures, the output layer with only one node to out- put the model score for each translation hypothesis based on the value of hidden nodes. More specifi- cally, the score of hypothesis e, denoted as s N , is defined as:</p><formula xml:id="formula_3">s N (e) = σ o (M o ·σ h (M h ·h m 1 (e|f )+b h )+b o ) (4)</formula><p>where M , b is the weight matrix, bias vector of the neural nodes, respectively; σ is the activation function, which is often set to non-linear functions such as the tanh function or sigmoid function; sub- script h and o indicates the parameters of hidden layer and output layer, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Features</head><p>We use the standard features of a typical hier- archical phrase based translation system <ref type="bibr" target="#b5">(Chiang, 2005)</ref>. Adding new features into the framework is left as a future direction. The features as listed as following:</p><p>• p(α|γ) and p(γ|α): conditional probability of translating α as γ and translating α as γ, where α and γ is the left and right hand side of a initial phrase or hierarchical translation rule, respectively;</p><p>• p w (α|γ) and p w (γ|α): lexical probability of translating words in α as words in γ and translating words in γ as words in α;</p><p>• p lm : language model probability;</p><p>• wc: accumulated count of individual words generated during translation;</p><p>• pc: accumulated count of initial phrases used;</p><p>• rc: accumulated count of hierarchical rule phrases used;</p><p>• gc: accumulated count of glue rule used in this hypothesis;</p><p>• uc: accumulated count of unknown source word. which has no entry in the translation table;</p><p>• nc: accumulated count of source phrases that translate into null;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoding</head><p>The basic decoding algorithm could be kept al- most the same as traditional phrase-based or syntax-based translation systems <ref type="bibr" target="#b29">(Yamada and Knight, 2001;</ref><ref type="bibr" target="#b16">Koehn et al., 2003;</ref><ref type="bibr" target="#b5">Chiang, 2005;</ref><ref type="bibr" target="#b19">Liu et al., 2006</ref>). For example, in the experiments of this paper, we use a CKY style decoding algo- rithm following <ref type="bibr" target="#b5">Chiang (2005)</ref>. Our non-linear translation system is different from traditional systems in the way to calculate the score for each hypothesis. Instead of calculat- ing the score as a linear combination, we use neu- ral networks (Section 3.1) to perform a non-linear combination of feature values.</p><p>We also use the cube-pruning algorithm <ref type="bibr" target="#b5">(Chiang, 2005</ref>) to keep the decoding efficient. Al- though the non-linearity in model scores may cause more search errors <ref type="bibr" target="#b15">(Huang and Chiang, 2007</ref>) finding the highest scoring hypothesis, in practice it still achieves reasonable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Non-linear Learning Framework</head><p>Traditional machine translation systems rely on MERT to tune the weights of different features. MERT performs efficient search by enumerating the score function of all the hypotheses and us- ing intersections of these linear functions to form the "upper-envelope" of the model score func- tion <ref type="bibr" target="#b24">(Och, 2003)</ref>. When the scoring function is non-linear, it is not feasible to find the intersec- tions of these functions. In this section, we discuss alternatives to train the parameters for non-linear models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Criteria</head><p>The task of machine translation is a complex prob- lem with structural output space. Decoding algo- rithms search for the translation hypothesis with the highest score, according to a given scoring function, from an exponentially large set of candi- date hypotheses. The purpose of training is to se- lect the scoring function, so that the function score the hypotheses "correctly". The correctness is of- ten introduced by some extrinsic metrics, such as BLEU ( <ref type="bibr" target="#b25">Papineni et al., 2002</ref>).</p><p>We denote the scoring function as s(f , e; ⃗ θ), or simply s, which is parameterized by ⃗ θ; denote the set of all translation hypotheses as C; denote the extrinsic metric as eval(·) 1 . Note that, in linear cases, s is a linear function as in Equation 3, while in the non-linear case described in this paper, s is the scoring function in Equation 4.</p><p>Ideally, the training objective is to select a scor- ing functionˆsfunctionˆ functionˆs, from all functions S, that scores the correct translation (or references) ˆ e, higher than any other hypotheses (Equation 5).</p><formula xml:id="formula_4">ˆ s = {s ∈ S|s(ˆ e) &gt; s(e) ∀e ∈ C}<label>(5)</label></formula><p>In practice, the candidate set C is exponentially large and hard to enumerate; the correct translationê translationˆtranslationê may not even exist in the current search space for various reasons, e.g. unknown source word. As a result, we use the n-best set C nbest to approximate C, use the extrinsic metric eval(·) to evaluate the quality of hypotheses in C nbest and use the fol- lowing three alternatives as approximations to the ideal objective. Best v.s. Rest (BR) To score the best hypothesis in the n-best set˜eset˜ set˜e higher than the rest hy- potheses. This objective is very similar to MERT in that it tries to optimize the score of˜eof˜ of˜e and doesn't concern about the ranking of rest hypotheses. In this case, ˜ e is an approxi- mation ofêofˆofê. Best v.s. Worst (BW) To score the best hypoth- esis higher than the worst hypothesis in the n-best set. This objective is motivated by the practice of separating the "hope" and "fear" translation hypotheses <ref type="bibr" target="#b6">(Chiang, 2012)</ref>. We take a simpler strategy which uses the best and worst hypothesis in C nbest as the "hope" and "fear" hypothesis, respectively, in order to avoid multi-pass decoding.</p><p>Pairwise (PW) To score the better hypothesis in sampled hypothesis pairs higher than the worse one in the same pair. This objective is adapted from the Pairwise Ranking Opti- mization (PRO) ( <ref type="bibr" target="#b14">Hopkins and May, 2011)</ref>, which tries to ranking all the hypotheses in- stead of selecting the best one. We use the same sampling strategy as their original pa- per.</p><p>Note that each of the above criteria transforms the original problem of selecting best hypothe- ses from an exponential space to a certain pair- wise comparison problem, which could be easily trained using binary classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Objective</head><p>For the binary classification task, we use a hinge loss following <ref type="bibr" target="#b28">Watanabe (2012)</ref>. Because the net- work has a lot of parameters compared with the linear model, we use a L 1 norm instead of L 2 norm as the regularization term, to favor sparse so- lutions. We define our training objective function in Equation 6.</p><formula xml:id="formula_5">arg min θ 1 N ∑ f ∈D ∑ (e 1 ,e 2 )∈T (f ) δ(f , e 1 , e 2 ; θ) + λ · ||θ|| 1 with δ(·) = max{s(f , e 1 ; θ) − s(f , e 2 ; θ) + 1, 0}<label>(6)</label></formula><p>where D is the given training data; (e 1 , e 2 ) is a training hypothesis-pair, with e 1 to be the one with higher eval(·) score; N is the total number of hypothesis-pairs in D; T (f ), or simply T , is the set of hypothesis-pairs for each source sentence f . The set T is decided by the criterion used for training. For the BR setting, the best hypothesis is paired with every other hypothesis in the n-best list (</p><note type="other">Equation 7); while for the BW setting, it is only paired with the worst hypothesis (Equation 8). The generation of T in PW setting is the same with PRO sampling, we refer the readers to the original paper of Hopkins and May (2011).</note><formula xml:id="formula_6">T BR = {(e 1 , e 2 )|e 1 = arg max e∈C nbest eval(e),</formula><p>e 2 ∈ C nbest and e 1 ̸ = e 2 }</p><p>T BW = {(e 1 , e 2 )|e 1 = arg max e∈C nbest eval(e),</p><formula xml:id="formula_8">e 2 = arg min e∈C nbest eval(e)}<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Procedure</head><p>In standard training algorithm for classification, the training instances stays the same in each itera- tion. In machine translation, decoding algorithms usually return a very different n-best set with dif- ferent parameters. This is due to the exponentially large size of search space. MERT and PRO extend the current n-best set by merging the n-best set of all previous iterations into a pool <ref type="bibr" target="#b24">(Och, 2003;</ref><ref type="bibr" target="#b14">Hopkins and May, 2011)</ref>. In this way, the enlarged n-best set may give a better approximation of the true hypothesis set C and may lead to better and more stable training results.</p><p>We argue that the training should still focus on hypotheses obtained in current round, because in each iteration the searching for the n-best set is in- dependent of previous iterations. To compromise the above two goals, in our practice, training hy- pothesis pairs are first generated from the current n-best set, then merged with the pairs generated from all previous iterations. In order to make the model focus more on pairs from current iteration, we assign pairs in previous iterations a small con- stant weight and assign pairs in current iteration a relatively large constant weight 2 . This is inspired by the AdaBoost algorithm <ref type="bibr" target="#b26">(Schapire, 1999</ref>) in weighting instances.</p><p>Following the spirit of MERT, we propose a iterative training procedure (Algorithm 1). The</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Iterative Training Algorithm</head><p>Input: the set of training sentences D, max num- ber of iteration I 1: θ 0 ← RandomInit(), 2: for i = 0 to I do 3:</p><formula xml:id="formula_9">T i ← ∅; 4:</formula><p>for each f ∈ D do 5:</p><formula xml:id="formula_10">C nbest ← NbestDecode(f ; θ i ) 6:</formula><p>T ← GeneratePair(C nbest ) 7:</p><formula xml:id="formula_11">T i ← T i ∪ T 8:</formula><p>end for 9:</p><formula xml:id="formula_12">T all ← WeightedCombine(∪ i−1 k=0 T k , T i ) 10:</formula><p>θ i+1 ← Optimize(T all , θ i ) 11: end for training starts by randomly initialized model pa- rameters θ 0 (line 1). In ith iteration, the decod- ing algorithm decodes each sentence f to get the n-best set C nbest (line 5). Training hypothesis pairs T are extracted from C nbest according to the training criterion described in Section 4.2 (line 6). Newly collected pairs T i are combined with pairs from previous iterations before used for training (line 9). θ i+1 is obtained by solving Equation 6 using the Conjugate Sub-Gradient method (Le et al., 2011) (line 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Structure of the Network</head><p>Although neural networks bring strong expressive power to the modeling of translation hypothesis, training a neural network is prone to resulting in local minimum which may affect the training re- sults. We speculate that one reason for these local minimums is that the structure of a well-connected network has too many parameters. Take a neu- ral network with k nodes in the input layer and m nodes in the hidden layer as an example. Every node in the hidden layer is connected to each of the k input nodes. This simple structure resulting in at least k × m parameters.</p><p>In Section 4.2, we use L 1 norm in the objec- tive function in order to get sparser solutions. In this section, we propose some constrained network structures according to our prior knowledge of the features. These structures have much less param- eters or simpler structures comparing to original neural networks, thus reduce the possibility of get- ting stuck in local minimums.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Network with two-degree Hidden Layer</head><p>We find the first pitfall of the standard two-layer neural network is that each node in the hidden layer receives input from every input layer node. Features used in SMT are usually manually de- signed, which has their concrete meanings. For a network of several hidden nodes, combining every features into every hidden node may be redundant and not necessary to represent the quality of a hy- pothesis.</p><p>As a result, we take a harsh step and constrain the nodes in hidden layer to have a in-degree of two, which means each hidden node only accepts inputs from two input nodes. We do not use any other prior knowledge about features in this set- ting. So for a network with k nodes in the in- put layer, the hidden layer should contain C 2 k = k(k − 1)/2 nodes to accept all combinations from the input layer. We name this network structure as Two-Degree Hidden Layer Network (TDN).</p><p>It is easy to see that a TDN has C 2 k × 2 = k(k − 1) parameters for the hidden layer because of the constrained degree. This is one order of magnitude less than a standard two-layer network with the same number of hidden nodes, which has</p><formula xml:id="formula_13">C 2 k × k = k 2 (k − 1)/2 parameters.</formula><p>Note that we perform a 2-degree combination that looks similar in spirit with those combina- tion of atomic features in large scale discrimina- tive learning for other NLP tasks, such as POS tag- ging and parsing. However, unlike the practice in these tasks that directly combines values of differ- ent features to generate a new feature type, we first linearly combine the value of these features and perform non-linear transformation on these values via an activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Network with Grouped Features</head><p>It might be a too strong constraint to require the hidden node have in-degree of 2. In order to re- lax this constraint, we need more prior knowledge from the features.</p><p>Our first observation is that there are different types of features. These types are different from each other in terms of value ranges, sources, im- portance, etc. For example, language model fea- tures usually take a very small value of probability, and word count feature takes a integer value and usually has a much higher weight in linear case than other count features.</p><p>The second observation is that features of the same type may not have complex interaction with each other. For example, it is reasonable to com- bine language model features with word count fea- tures in a hidden node. But it may not be neces- sary to combine the count of initial phrases and the count of unknown words into a hidden node. Based on the above two intuitions, we design a new structure of network that has the following constraints: given a disjoint partition of features: G 1 , G 2 ,..., G k , every hidden node takes input from a set of input nodes, where any two nodes in this set come from two different feature groups. Un- der this constraint, the in-degree of a hidden node is at most k. We name this network structure as Grouped Network <ref type="figure">(GN)</ref>.</p><p>In practice, we divide the basic features in Sec- tion 3.2 into five groups: language model features, translation probability features, lexical probability features, the word count feature, and the rest of count features. This division considers not only the value ranges, but also types of features and the possibility of them interact with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">General Settings</head><p>We conduct experiments on a large scale machine translation tasks. The parallel data comes from LDC, including LDC2002E18, LDC2003E14, LDC2004E12, LDC2004T08, LDC2005T10, LDC2007T09, which consists of 8.2 million of sentence pairs. Monolingual data includes Xinhua portion of Gigaword corpus. We use multi-references data MT03 as training data, MT02 as development data, and MT04, MT05 as test data. These data are mainly in the same genre, avoiding the extra consideration of domain adaptation.  <ref type="table">Table 2</ref>: BLEU4 in percentage on different training criteria ("BR", "BW" and "PW" refer to experiments with "Best v.s. Rest", "Best v.s. Worst" and "Pairwise" training criteria, respectively. "BR c " indicates generate hypothesis pairs from n-best set of current iteration only presented in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>tem is an in-house implementation of the hier- archical phrase-based translation system <ref type="bibr" target="#b5">(Chiang, 2005</ref>). We set the beam size to 20. We train a 5-gram language model on the monolingual data with MKN smoothing <ref type="bibr" target="#b4">(Chen and Goodman, 1998</ref>).</p><p>For each parameter tuning experiments, we ran the same training procedure 3 times and present the average results. The translation quality is evalu- ated use 4-gram case-insensitive BLEU ( <ref type="bibr" target="#b25">Papineni et al., 2002</ref>). Significant test is performed using bootstrap re-sampling implemented by <ref type="bibr" target="#b7">Clark et al. (2011)</ref>. We employ a two-layer neural network with 11 input layer nodes, corresponding to fea- tures listed in Section 3.2 and 1 output layer node. The number of nodes in the hidden layer varies in different settings. The sigmoid function is used as the activation function for each node in the hidden layer. For the output layer we use a linear activa- tion function. We try different λ for the L 1 norm from 0.01 to 0.00001 and use the one with best performance on the development set. We solve the optimization problem with ALGLIB package 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experiments of Training Criteria</head><p>This set experiments evaluates different training criteria discussed in Section 4.1. We generate hypothesis-pair according to BW, BR and PW cri- teria, respectively, and perform training with these pairs. In the PW criterion, we use the sampling method of PRO (Hopkins and May, 2011) and get the 50 hypothesis pairs for each sentence. We use 20 hidden nodes for all three settings to make a fair comparison.</p><p>The results are presented in <ref type="table">Table 2</ref>. The first two rows compare training with and with- out the weighted combination of hypothesis pairs we discussed in Section 4.3. As the result sug- gested, with the weighted combination of hypothe- sis pairs from previous iterations, the performance improves significantly on both test sets.</p><p>Although the system performance on the dev set varies, the performance on test sets are al- most comparable. This suggest that although the three training criteria are based on different as- sumptions, their are basically equivalent for train- ing translation systems.  We also compares the three training criteria in their number of new instances per iteration and final training accuracy <ref type="table" target="#tab_2">(Table 3)</ref>. Compared to BR which tries to separate the best hypothesis from the rest hypotheses in the n-best set, and PW which tries to obtain a correct ranking of all hy- potheses, BW only aims at separating the best and worst hypothesis of each iteration, which is a eas- ier task for learning a classifiers. It requires the least training instances and achieves the best per- formance in training. Note that, the accuracy for each system in <ref type="table" target="#tab_2">Table 3</ref> are the accuracy each sys- tem achieves after training stops. They are not cal- culated on the same set of instances, thus not di- rectly comparable. We use the differences in accu- racy as an indicator for the difficulties of the cor- responding learning task.</p><p>For the rest of this paper, we use the BW crite- rion because it is much simpler compared to sam- pling method of PRO (Hopkins and May, 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experiments of Network Structures</head><p>We make several comparisons of the network structures and compare them with a baseline hi- erarchical phrase-based translation system (HPB).   different systems 5 . All 5 two-layer feed forward neural networks models could achieve compara- ble or better performance comparing to the base- line system. We can see that training a larger net- work may lead to better translation quality (from TLayer 20 and TLayer 30 to TLayer 50 ). However, increasing the number of hidden node to 100 and 200 does not bring further improvement. One pos- sible reason is that training a larger network with arbitrary connections brings in too many param- eters which may be difficult to train with limited training data. TDN and GN are the two network structures proposed in Section 5. With the constraint that all input to the hidden node should be of degree 2, TDN performs comparable to the baseline sys- tem. With the grouped feature, we could design networks such as GN, which shows significant im- provement over the baseline systems (+0.57) and achieves the best performance among all neural systems. 5 TLayer20 is the same system as BW in <ref type="table" target="#tab_3">Table 2   Table 4</ref> shows statistics related to the efficiency issue of different systems. The baseline system (HPB) uses MERT for training. HPB has a very small number of parameters and searches for the best parameters exhaustively in each iteration. The non-linear systems with few nodes (TLayer 20 and TLayer 30 ) train faster than HPB in each iteration because they perform back-propagation instead of exhaustive search. We iterate 15 iterations for each non-linear system, while MERT takes about 10 rounds to reach its best performance.</p><p>When the number of nodes in the hidden layer increases (from 20 to 200), the number of param- eters in the system also increases, which requires longer time to compute the score for each hypoth- esis and to update the parameters through back- propagation. The network with 200 hidden nodes takes about twice the time to train for each itera- tion, compared to the linear system 6 .</p><p>TDN and GN have larger numbers of hidden nodes. However, because of our intuitions in de- signing the structure of the networks, the degree of the hidden node is constrained. So these two networks are sparser in parameters and take sig- nificant less training time than standard neural net- works. For example, GN has a comparable num- ber of hidden nodes with TLayer 200 , but only has half of its parameters and takes about 70% time to train in each iteration. In other words, our pro- posed network structure provides more efficient training in these cases and achieve better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we discuss a non-linear framework for modeling translation hypothesis for statisti- cal machine translation system. We also present a learning framework including training criterion and algorithms to integrate our modeling into a state of the art hierarchical phrase based machine translation system. Compared to previous effort in bringing in non-linearity into machine transla- tion, our method uses a single two-layer neural networks and performs training independent with any previous linear training methods (e.g. MERT).</p><p>Our method also trains its parameters without any pre-training or post-training procedure. Experi- ment shows that our method could improve the baseline system even with the same feature as input, in a large scale Chinese-English machine translation task. In training neural networks with hidden nodes, we use heuristics to reduce the complexity of net- work structures and obtain extra advantages over standard networks. It shows that heuristics and in- tuitions of the data and features are still important to a machine translation system.</p><p>Neural networks are able to perform feature learning by using hidden nodes to model the in- teraction among a large vector of raw features, as in image and speech processing ( <ref type="bibr" target="#b17">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b13">Hinton et al., 2012</ref>). We are trying to model the interaction between hand-crafted fea- tures, which is indeed similar in spirit with learn- ing features from raw features. Although our fea- tures already have concrete meaning, e.g. the probability of translation, the fluency of target sen- tence, etc. Combining these features may have ex- tra advantage in modeling the translation process.</p><p>As future work, it is necessary to integrate more features into our learning framework. It is also in- teresting to see how the non-linear modeling fits in to more complex learning tasks which involves domain specific learning techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A two-layer feed-forward neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison of different training criteria 
in number of new instances per iteration and train-
ing accuracy. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 shows the translation performance of</head><label>4</label><figDesc></figDesc><table>Systems 

MT03(train) MT02(dev) 
MT04 
MT05 
Test Average 
HPB 
39.25 
39.07 
38.81 
38.01 
38.41 
TLayer 20 
39.55  *  
39.36  *  
38.72 
37.81 
38.27(-0.14) 
TLayer 30 
39.70 + 
39.71  *  
38.89 
37.90 
38.40(-0.01) 
TLayer 50 
39.26 
38.97 
38.72 
38.79 + 
38.76(+0.35) 
TLayer 100 
39.42 
38.77 
38.65 
38.65 + 
38.69(+0.28) 
TLayer 200 
39.69 
38.68 
38.72 
38.80 + 
38.74(+0.32) 
TDN 
39.60 + 
38.94 
38.99  *  
38.13 
38.56(+0.15) 
GN 
39.73 + 
39.41 + 
39.45 + 
38.51 + 
38.98(+0.57) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>BLEU4 in percentage for comparing of systems using different network structures (HPB refers 
to the baseline hierarchical phrase-based system. TLayer, TDN, GN refer to the standard 2-layer network, 
Two-Degree Hidden Layer Network, Grouped Network, respectively. Subscript of TLayer indicates the 
number of nodes in the hidden layer.) + ,  *  marks results that are significant better than the baseline 
system with p &lt; 0.01 and p &lt; 0.05. 

Systems 
# Hidden Nodes # Parameters Training Time per iter.(s) 
HPB 
-
11 
1041 
TLayer 20 
20 
261 
671 
TLayer 30 
30 
391 
729 
TLayer 50 
50 
651 
952 
TLayer 100 
100 
1,301 
1,256 
TLayer 200 
200 
2,601 
2,065 
TDN 
55 
221 
808 
GN 
214 
1,111 
1,440 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison of network scales and training time of different systems, including the number of 
nodes in the hidden layer, the number of parameters, the average training time per iteration (15 iterations). 
The notations of systems are the same as in Table4. 

</table></figure>

			<note place="foot" n="1"> In our experiments, we use sentence level BLEU with +1 smoothing as the evaluation metric.</note>

			<note place="foot" n="2"> In our experiments, we empirically set the constants to be 0.1 and 0.9, respectively.</note>

			<note place="foot" n="3"> http://ictclas.nlpir.org/</note>

			<note place="foot" n="4"> http://www.alglib.net/</note>

			<note place="foot" n="6"> Matrix operation is CPU intensive. The cost will increase when multiple tasks are running.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Yue Zhang and the anonymous reviewers for their valu-able comments. This work is supported by the National Natural Science Foundation of China <ref type="figure">(No. 61300158, 61223003)</ref>, the Jiangsu Provin-cial Research Foundation for Basic Research (No. BK20130580).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Decoder integration and expected BLEU training for recurrent neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014-06-22" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="136" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint language and translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Grand Hyatt Seattle, Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="1044" to="1054" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural Networks for Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Oxford University Press, Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The mathematic of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Goodman</surname></persName>
		</author>
		<idno>TR-10-98</idno>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Group, Harvard University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A hierarchical phrase-based model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">annual meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hope and fear for discriminative training of statistical translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1159" to="1187" />
			<date type="published" when="2012-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Locally non-linear learning for statistical machine translation via discretization and structured regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="393" to="404" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014-06-22" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Beyond loglinear models: Boosted minimum error rate training for n-best re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, HLT-Short &apos;08</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, HLT-Short &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="37" to="40" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The difficulty of training deep architectures and the effect of unsupervised pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Pierre Antoine Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS-09)</title>
		<editor>David V. Dyk and Max Welling</editor>
		<meeting>the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS-09)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
	<note>Proceedings Track</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations for translation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014-06-22" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="699" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tuning as ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-05" />
			<biblScope unit="page" from="1352" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Forest rescoring: Faster decoding with integrated language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLTNAACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>Hinton</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On optimization methods for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahbik</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bobby</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Prochnow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning<address><addrLine>Bellevue, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06-28" />
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Treeto-string alignment template for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual Meeting of the Association of Computational Linguistics. The Association for Computer Linguistics</title>
		<meeting>the 44th Annual Meeting of the Association of Computational Linguistics. The Association for Computer Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Additive neural networks for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics, ACL<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013-08-09" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="791" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning new semi-supervised deep auto-encoder features for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="122" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised deep belief features for speech translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Maskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2012, 13th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Discriminative training and maximum entropy models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="295" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL &apos;03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL &apos;02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A brief introduction to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 16th International Joint Conference on Artificial Intelligence<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1401" to="1406" />
		</imprint>
	</monogr>
	<note>IJCAI&apos;99</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Decoding with large-scale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Grand Hyatt Seattle, Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="1387" to="1392" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimized online rank learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A syntaxbased statistical translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 39th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
