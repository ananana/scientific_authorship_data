<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stock Movement Prediction from Tweets and Historical Prices</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 1970</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumo</forename><surname>Xu</surname></persName>
							<email>yumo.xu@ed.ac.uk, scohen@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Stock Movement Prediction from Tweets and Historical Prices</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1970" to="1979"/>
							<date type="published">July 15-20, 2018. 2018. 1970</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Stock movement prediction is a challenging problem: the market is highly stochas-tic, and we make temporally-dependent predictions from chaotic data. We treat these three complexities and present a novel deep generative model jointly exploiting text and price signals for this task. Unlike the case with discriminative or topic modeling, our model introduces recurrent, continuous latent variables for a better treatment of stochasticity, and uses neural variational inference to address the intractable posterior inference. We also provide a hybrid objective with temporal auxiliary to flexibly capture predictive dependencies. We demonstrate the state-of-the-art performance of our proposed model on a new stock movement prediction dataset which we collected. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Stock movement prediction has long attracted both investors and researchers <ref type="bibr" target="#b9">(Frankel, 1995;</ref><ref type="bibr" target="#b8">Edwards et al., 2007;</ref><ref type="bibr" target="#b2">Bollen et al., 2011;</ref><ref type="bibr">Hu et al., 2018)</ref>. We present a model to predict stock price move- ment from tweets and historical stock prices.</p><p>In natural language processing (NLP), public news and social media are two primary content re- sources for stock market prediction, and the mod- els that use these sources are often discriminative. Among them, classic research relies heavily on feature engineering ( <ref type="bibr" target="#b18">Schumaker and Chen, 2009;</ref><ref type="bibr" target="#b14">Oliveira et al., 2013)</ref>. With the prevalence of deep neural networks ( <ref type="bibr">Le and Mikolov, 2014)</ref>, event- driven approaches were studied with structured event representations ( <ref type="bibr" target="#b6">Ding et al., 2014</ref><ref type="bibr" target="#b7">Ding et al., , 2015</ref>).</p><p>More recently, <ref type="bibr">Hu et al. (2018)</ref> propose to mine news sequence directly from text with hierarchical attention mechanisms for stock trend prediction.</p><p>However, stock movement prediction is widely considered difficult due to the high stochasticity of the market: stock prices are largely driven by new information, resulting in a random-walk pat- tern <ref type="bibr" target="#b12">(Malkiel, 1999</ref>). Instead of using only de- terministic features, generative topic models were extended to jointly learn topics and sentiments for the task ( <ref type="bibr" target="#b20">Si et al., 2013;</ref><ref type="bibr">Nguyen and Shirai, 2015</ref>). Compared to discriminative models, gener- ative models have the natural advantage in depict- ing the generative process from market informa- tion to stock signals and introducing randomness. However, these models underrepresent chaotic so- cial texts with bag-of-words and employ simple discrete latent variables.</p><p>In essence, stock movement prediction is a time series problem. The significance of the temporal dependency between movement predictions is not addressed in existing NLP research. For instance, when a company suffers from a major scandal on a trading day d 1 , generally, its stock price will have a downtrend in the coming trading days until day d 2 , i.e. [d <ref type="bibr">1 , d 2 ]</ref>. <ref type="bibr">2</ref> If a stock predictor can recognize this decline pattern, it is likely to benefit all the predic- tions of the movements during [d <ref type="bibr">1 , d 2 ]</ref>. Otherwise, the accuracy in this interval might be harmed. This predictive dependency is a result of the fact that public information, e.g. a company scandal, needs time to be absorbed into movements over time ( <ref type="bibr" target="#b11">Luss and d'Aspremont, 2015)</ref>, and thus is largely shared across temporally-close predictions.</p><p>Aiming to tackle the above-mentioned out- standing research gaps in terms of modeling high market stochasticity, chaotic market information and temporally-dependent prediction, we propose StockNet, a deep generative model for stock movement prediction.</p><p>To better incorporate stochastic factors, we gen- erate stock movements from latent driven factors modeled with recurrent, continuous latent vari- ables. Motivated by Variational Auto-Encoders (VAEs; <ref type="bibr">Kingma and Welling, 2013;</ref><ref type="bibr" target="#b17">Rezende et al., 2014</ref>), we propose a novel decoder with a vari- ational architecture and derive a recurrent varia- tional lower bound for end-to-end training (Sec- tion 5.2). To the best of our knowledge, StockNet is the first deep generative model for stock move- ment prediction.</p><p>To fully exploit market information, StockNet directly learns from data without pre-extracting structured events. We build market sources by referring to both fundamental information, e.g. tweets, and technical features, e.g. historical stock prices (Section 5.1). <ref type="bibr">3</ref> To accurately depict predic- tive dependencies, we assume that the movement prediction for a stock can benefit from learning to predict its historical movements in a lag window. We propose trading-day alignment as the frame- work basis (Section 4), and further provide a novel multi-task learning objective (Section 5.3).</p><p>We evaluate StockNet on a stock movement pre- diction task with a new dataset that we collected. Compared with strong baselines, our experiments show that StockNet achieves state-of-the-art per- formance by incorporating both data from Twitter and historical stock price listings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><p>We aim at predicting the movement of a target stock s in a pre-selected stock collection S on a target trading day d. Formally, we use the market information comprising of relevant social media corpora M, i.e. tweets, and historical prices, in the lag <ref type="bibr">[d − ∆d, d − 1]</ref> where ∆d is a fixed lag size. We estimate the binary movement where 1 denotes rise and 0 denotes fall,</p><formula xml:id="formula_0">y = 1 p c d &gt; p c d−1<label>(1)</label></formula><p>where p c d denotes the adjusted closing price ad- justed for corporate actions affecting stock prices, e.g. dividends and splits. <ref type="bibr">4</ref> The adjusted closing price is widely used for predicting stock price movement ( <ref type="bibr" target="#b21">Xie et al., 2013</ref>) or financial volatility ( <ref type="bibr" target="#b16">Rekabsaz et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Collection</head><p>In finance, stocks are categorized into 9 industries: Basic Materials, Consumer Goods, Healthcare, Services, Utilities, Conglomerates, Financial, In- dustrial Goods and Technology. 5 Since high-trade- volume-stocks tend to be discussed more on Twit- ter, we select the two-year price movements from 01/01/2014 to 01/01/2016 of 88 stocks to target, coming from all the 8 stocks in Conglomerates and the top 10 stocks in capital size in each of the other 8 industries (see supplementary material).</p><p>We observe that there are a number of tar- gets with exceptionally minor movement ratios. In a three-way stock trend prediction task, a com- mon practice is to categorize these movements to another "preserve" class by setting upper and lower thresholds on the stock price change ( <ref type="bibr">Hu et al., 2018</ref>). Since we aim at the binary clas- sification of stock changes identifiable from so- cial media, we set two particular thresholds, - 0.5% and 0.55% and simply remove 38.72% of the selected targets with the movement percents be- tween the two thresholds. Samples with the move- ment percents ≤-0.5% and &gt;0.55% are labeled with 0 and 1, respectively. The two thresholds are selected to balance the two classes, resulting in 26,614 prediction targets in the whole dataset with 49.78% and 50.22% of them in the two classes. We split them temporally and 20,339 movements be- tween 01/01/2014 and 01/08/2015 are for training, 2,555 movements from 01/08/2015 to 01/10/2015 are for development, and 3,720 movements from 01/10/2015 to 01/01/2016 are for test.</p><p>There are two main components in our dataset: 6 a Twitter dataset and a historical price dataset. We access Twitter data under the official license of Twitter, then retrieve stock-specific tweets by querying regexes made up of NASDAQ ticker symbols, e.g. "\$GOOG\b" for Google Inc.. We preprocess tweet texts using the NLTK package ( <ref type="bibr" target="#b1">Bird et al., 2009</ref>) with the particular Twitter paper, the problem is solved by keeping the notational con- sistency with our recurrent model and using its time step t to index trading days. Details will be provided in Section 4. We use d here to make the formulation easier to follow.</p><p>5 https://finance.yahoo.com/industries 6 Our dataset is available at https://github.com/ yumoxu/stocknet-dataset. mode, including for tokenization and treatment of hyperlinks, hashtags and the "@" identifier. To al- leviate sparsity, we further filter samples by ensur- ing there is at least one tweet for each corpus in the lag. We extract historical prices for the 88 se- lected stocks to build the historical price dataset from Yahoo Finance. <ref type="bibr">7</ref>  <ref type="figure">Figure 1</ref>: Illustration of the generative process from observed market information to stock move- ments. We use solid lines to denote the generation process and dashed lines to denote the variational approximation to the intractable posterior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model Overview</head><formula xml:id="formula_1">X |D| Z ✓ y</formula><p>We provide an overview of data alignment, model factorization and model components.</p><p>As explained in Section 1, we assume that pre- dicting the movement on trading day d can ben- efit from predicting the movements on its former trading days. However, due to the general princi- ple of sample independence, building connections directly across samples with temporally-close tar- get dates is problematic for model training.</p><p>As an alternative, we notice that within a sam- ple with a target trading day d there are likely to be other trading days than d in its lag that can simulate the prediction targets close to d. Moti- vated by this observation and multi-task learning (Caruana, 1998), we make movement predictions not only for d, but also other trading days exist- ing in the lag. For instance, as shown in <ref type="figure">Figure 2</ref>, for a sample targeting 07/08/2012 and a 5-day lag, 03/08/2012 and 06/08/2012 are eligible trad- ing days in the lag and we also make predictions for them using the market information in this sam- ple. The relations between these predictions can thus be captured within the scope of a sample.</p><p>As shown in the instance above, not every sin- gle date in a lag is an eligible trading day, e.g. weekends and holidays. To better organize and use the input, we regard the trading day, instead of the 7 http://finance.yahoo.com calendar day used in existing research, as the ba- sic unit for building samples. To this end, we first find all the T eligible trading days referred in a sample, in other words, existing in the time in- terval <ref type="bibr">[d − ∆d + 1, d]</ref>. For clarity, in the scope of one sample, we index these trading days with t ∈ <ref type="bibr">[1, T ]</ref>, 8 and each of them maps to an ac- tual (absolute) trading day d t . We then propose trading-day alignment: we reorganize our inputs, including the tweet corpora and historical prices, by aligning them to these T trading days. Specif- ically, on the tth trading day, we recognize mar- ket signals from the corpus M t in [d t−1 , d t ) and the historical prices p t on d t−1 , for predicting the movement y t on d t . We provide an aligned sam- ple for illustration in <ref type="figure">Figure 2</ref>. As a result, ev- ery single unit in a sample is a trading day, and we can predict a sequence of movements y = [y 1 , . . . , y T ]. The main target is y T while the re- mainder y * = [y 1 , . . . , y T −1 ] serves as the tempo- ral auxiliary target. We use these in addition to the main target to improve prediction accuracy (Sec- tion 5.3).</p><p>We model the generative process shown in Fig- ure 1. We encode observed market information as a random variable X = [x 1 ; . . . ; x T ], from which we generate the latent driven factor Z = [z 1 ; . . . ; z T ] for our prediction task. For the afore- mentioned multi-task learning purpose, we aim at modeling the conditional probability distribution p θ (y|X) = Z p θ (y, Z|X) instead of p θ (y T |X). We write the following factorization for genera- tion,</p><formula xml:id="formula_2">p θ (y, Z|X) = p θ (y T |X, Z) p θ (z T |z &lt;T , X) (2) T −1 t=1 p θ (y t |x ≤t , z t ) p θ (z t |z &lt;t , x ≤t , y t )</formula><p>where for a given indexed matrix of T vectors</p><formula xml:id="formula_3">[v 1 ; . . . ; v T ], we denote by v &lt;t and v ≤t the subma- trix [v 1 ; . . . ; v t−1 ] and the submatrix [v 1 ; . . . ; v t ],</formula><p>respectively. Since y * is known in generation, we use the posterior p θ (z t |z &lt;t , x ≤t , y t ) , t &lt; T to incorporate market signals more accurately and only use the prior p θ (z T |z &lt;T , X) when generat- ing z T . Besides, when t &lt; T , y t is independent of z &lt;t while our main prediction target, y T is made dependent on z &lt;T through a temporal attention mechanism (Section 5.3).</p><p>We show StockNet modeling the above gener- ative process in <ref type="figure">Figure 2</ref>. In a nutshell, StockNet 2. Variational Movement Decoder (VMD) that infers Z with X, y and decodes stock move- ments y from X, Z;</p><formula xml:id="formula_4">z 1 z 2 z 3 h 2 h 3 02/08 Input Output h dec h enc µ log 2 z N (0, I) DKL ⇥ N (µ, 2 ) k N (0, I) ⇤ " Variational</formula><p>3. Attentive Temporal Auxiliary (ATA) that in- tegrates temporal loss through an attention mechanism for model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Components</head><p>We detail next the components of our model (MIE, VMD, ATA) and the way we estimate our model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Market Information Encoder</head><p>MIE encodes information from social media and stock prices to enhance market information qual- ity, and outputs the market information input X for VMD. Each temporal input is defined as</p><formula xml:id="formula_5">x t = [c t , p t ]<label>(3)</label></formula><p>where c t and p t are the corpus embedding and the historical price vector, respectively.</p><p>The basic strategy of acquiring c t is to first feed messages into the Message Embedding Layer for their low-dimensional representations, then selec- tively gather them according to their quality. To handle the circumstance that multiple stocks are discussed in one single message, in addition to text information, we incorporate the position informa- tion of stock symbols mentioned in messages as well. Specifically, the layer consists of a forward GRU and a backward GRU for the preceding and following contexts of a stock symbol, s, respec- tively. Formally, in the message corpus of the tth trading day, we denote the word sequence of the kth message, k ∈ <ref type="bibr">[1, K]</ref>, as W where W = s, ∈ <ref type="bibr">[1, L]</ref>, and its word embedding matrix as</p><formula xml:id="formula_6">E = [e 1 ; e 2 ; . . . ; e L ].</formula><p>We run the two GRUs as follows,</p><formula xml:id="formula_7">− → h f = − −− → GRU(e f , − → h f −1 ) (4) ← − h b = ← −− − GRU(e b , ← − h b+1 )<label>(5)</label></formula><formula xml:id="formula_8">m = ( − → h + ← − h )/2<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">f ∈ [1, . . . , ], b ∈ [ , . . . , L].</formula><p>The stock symbol is regarded as the last unit in both the preceding and the following contexts where the hidden values, − → h l , ← − h l , are averaged to acquire the message embedding m. Gathering all message embeddings for the tth trading day, we have a mes-sage embedding matrix M t ∈ R dm×K . In prac- tice, the layer takes as inputs a five-rank tensor for a mini-batch, and yields all M t in the batch with shared parameters.</p><p>Tweet quality varies drastically. Inspired by the news-level attention ( <ref type="bibr">Hu et al., 2018)</ref>, we weight messages with their respective salience in col- lective intelligence measurement. Specifically, we first project M t non-linearly to u t , the normalized attention weight over the corpus,</p><formula xml:id="formula_10">u t = ζ(w u tanh(W m,u M t ))<label>(7)</label></formula><p>where ζ(·) is the softmax function and W m,u ∈ R dm×dm , w u ∈ R dm×1 are model parameters.</p><p>Then we compose messages accordingly to ac- quire the corpus embedding,</p><formula xml:id="formula_11">c t = M t u t .<label>(8)</label></formula><p>Since it is the price change that determines the stock movement rather than the absolute price value, instead of directly feeding the raw price vector˜pvector˜ vector˜p t = ˜ p c t , ˜ p h t , ˜ p l t comprising of the adjusted closing, highest and lowest price on a trading day t, into the networks, we normalize it with its last adjusted closing price, p t = ˜ p t /˜ p c t−1 − 1. We then concatenate c t with p t to form the final market in- formation input x t for the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Variational Movement Decoder</head><p>The purpose of VMD is to recurrently infer and decode the latent driven factor Z and the move- ment y from the encoded market information X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference</head><p>While latent driven factors help to depict the mar- ket status leading to stock movements, the pos- terior inference in the generative model shown in Eq. <ref type="formula">(2)</ref> is intractable. Following the spirit of the VAE, we use deep neural networks to fit la- tent distributions, i.e. the prior p θ (z t |z &lt;t , x ≤t ) and the posterior p θ (z t |z &lt;t , x ≤t , y t ), and sidestep the intractability through neural approximation and reparameterization ( <ref type="bibr">Kingma and Welling, 2013;</ref><ref type="bibr" target="#b17">Rezende et al., 2014</ref>). We first employ a varia- tional approximator q φ (z t |z &lt;t , x ≤t , y t ) for the in- tractable posterior. We observe the following fac- torization,</p><formula xml:id="formula_12">q φ (Z|X, y) = T t=1 q φ (z t |z &lt;t , x ≤t , y t ) .<label>(9)</label></formula><p>Neural approximation aims at minimizing the Kullback-Leibler divergence between the q φ (Z|X, y) and p θ (Z|X, y). Instead of optimiz- ing it directly, we observe that the following equa- tion naturally holds,</p><formula xml:id="formula_13">log p θ (y|X) (10) =D KL [q φ (Z|X, y) p θ (Z|X, y)] +E q φ (Z|X,y) [log p θ (y|X, Z)] −D KL [q φ (Z|X, y) p θ (Z|X)]</formula><p>where D KL [q p] is the Kullback-Leibler diver- gence between the distributions q and p. There- fore, we equivalently maximize the following vari- ational recurrent lower bound by plugging Eq. (2, 9) into Eq. (10),</p><formula xml:id="formula_14">L (θ, φ; X, y)<label>(11)</label></formula><formula xml:id="formula_15">= T t=1 E q φ( zt|z&lt;t,x ≤t ,yt) log p θ (y t |x ≤t , z ≤t ) − D KL [q φ (z t |z &lt;t , x ≤t , y t ) p θ (z t |z &lt;t , x ≤t )] ≤ log p θ (y|X)</formula><p>where the likelihood term <ref type="bibr" target="#b10">Li et al. (2017)</ref> also provide a lower bound for inferring directly-connected recurrent latent vari- ables in text summarization. In their work, priors are modeled with p θ (z t ) ∼ N (0, I), which, in fact, turns the KL term into a static regularization term encouraging sparsity. In Eq. (11), we provide a more theoretically rigorous lower bound where the KL term with p θ (z t |z &lt;t , x ≤t ) plays a dynamic role in inferring dependent latent variables for ev- ery different model input and latent history.</p><formula xml:id="formula_16">p θ (y t |x ≤t , z ≤t ) = p θ (y t |x ≤t , z t ) , if t &lt; T p θ (y T |X, Z) , if t = T.<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoding</head><p>As per time series, VMD adopts an RNN with a GRU cell to extract features and decode stock sig- nals recurrently,</p><formula xml:id="formula_17">h s t = GRU(x t , h s t−1 ).<label>(13)</label></formula><p>We let the approximator q φ (z t |z &lt;t , x ≤t , y t ) subject to a standard multivariate Gaussian distri- bution N (µ, δ 2 I). We calculate µ and δ as</p><formula xml:id="formula_18">µ t = W φ z,µ h z t + b φ µ (14) log δ 2 t = W φ z,δ h z t + b φ δ<label>(15)</label></formula><p>and the shared hidden representation h z t as</p><formula xml:id="formula_19">h z t = tanh(W φ z [z t−1 , x t , h s t , y t ] + b φ z )<label>(16)</label></formula><p>where W φ z,µ , W φ z,δ , W φ z are weight matrices and b φ µ , b φ δ , b φ z are biases. Since Gaussian distribution belongs to the "location-scale" distribution family, we can fur- ther reparameterize z t as</p><formula xml:id="formula_20">z t = µ t + δ t<label>(17)</label></formula><p>where denotes an element-wise product. The noise term ∼ N (0, I) naturally involves stochas- tic signals in our model. Similarly, We let the prior p θ (z t |z &lt;t , x ≤t ) ∼ N (µ , δ 2 I). Its calculation is the same as that of the posterior except the absence of y t and indepen- dent model parameters,</p><formula xml:id="formula_21">µ t = W θ o,µ h z t + b θ µ (18) log δ 2 t = W θ o,δ h z t + b θ δ<label>(19)</label></formula><p>where</p><formula xml:id="formula_22">h z t = tanh(W θ z [z t−1 , x t , h s t ] + b θ z ).<label>(20)</label></formula><p>Following <ref type="bibr" target="#b22">Zhang et al. (2016)</ref>, differently from the posterior, we set the prior z t = µ t during de- coding. Finally, we integrate deterministic features and the final prediction hypothesis is given as</p><formula xml:id="formula_23">g t = tanh(W g [x t , h s t , z t ] + b g ) (21) ˜ y t = ζ(W y g t + b y ), t &lt; T<label>(22)</label></formula><p>where W g , W y are weight matrices and b g , b y are biases. The softmax function ζ(·) outputs the con- fidence distribution over up and down. As intro- duced in Section 4, the decoding of the main target y T depends on z &lt;T and thus lies at the interface between VMD and ATA. We will elaborate on it in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Attentive Temporal Auxiliary</head><p>With the acquisition of a sequence of auxiliary predictions˜Ypredictions˜ predictions˜Y * = [˜ y 1 ; . . . ; ˜ y T −1 ], we incorporate two-folded auxiliary effects into the main predic- tion and the training objective flexibly by first in- troducing a shared temporal attention mechanism.</p><p>Since each hypothesis of a temporal auxiliary contributes unequally to the main prediction and model training, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>, temporal at- tention calculates their weights in these two contri- butions by employing two scoring components: an  information score and a dependency score. Specif- ically,</p><formula xml:id="formula_24">v i = w i tanh(W g,i G * ) (23) v d = g T tanh(W g,d G * ) (24) v * = ζ(v i v d )<label>(25)</label></formula><p>where</p><formula xml:id="formula_25">W g,i , W g,d ∈ R dg×dg , w i ∈ R dg×1 are model parameters. The integrated representations G * = [g 1 ; . . . ; g T −1</formula><p>] and g T are reused as the fi- nal representations of temporal market informa- tion. The information score v i evaluates historical trading days as per their own information qual- ity, while the dependency score v d captures their dependencies with our main target. We integrate the two and acquire the final normalized attention weight v * ∈ R 1×(T −1) by feeding their element- wise product into the softmax function.</p><p>As a result, the main prediction can benefit from temporally-close hypotheses have been made and we decode our main hypothesis˜yhypothesis˜ hypothesis˜y T as˜y</p><formula xml:id="formula_26">as˜ as˜y T = ζ(W T [ ˜ Y * v * , g T ] + b T )<label>(26)</label></formula><p>where W T is a weight matrix and b T is a bias. As to the model objective, we use the Monte Carlo method to approximate the expectation term in Eq. (11) and typically only one sample is used for gradient computation. To incorporate varied temporal importance at the objective level, we first break down the approximated L into a series of temporal objectives f ∈ R T ×1 where f t comprises a likelihood term and a KL term for a trading day t,</p><formula xml:id="formula_27">f t = log p θ (y t |x ≤t , z ≤t ) (27) − λD KL [q φ (z t |z &lt;t , x ≤t , y t ) p θ (z t |z &lt;t , x ≤t )]</formula><p>where we adopt the KL term annealing trick <ref type="bibr" target="#b3">(Bowman et al., 2016;</ref><ref type="bibr" target="#b19">Semeniuta et al., 2017)</ref> and add a linearly-increasing KL term weight λ ∈ (0, 1] to gradually release the KL regularization effect in the training procedure. Then we reuse v * to build the final temporal weight vector v ∈ R 1×T ,</p><formula xml:id="formula_28">v = [αv * , 1]<label>(28)</label></formula><p>where 1 is for the main prediction and we adopt the auxiliary weight α ∈ [0, 1] to control the overall auxiliary effects on the model training. α is tuned on the development set and its effects will be dis- cussed at length in Section 6.5. Finally, we write the training objective F by recomposition,</p><formula xml:id="formula_29">F (θ, φ; X, y) = 1 N N n v (n) f (n) (29)</formula><p>where our model can learn to generalize with the selective attendance of temporal auxiliary. We take the derivative of F with respect to all the model parameters {θ, φ} through backpropagation for the update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we detail our experimental setup and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Training Setup</head><p>We use a 5-day lag window for sample construc- tion and 32 shuffled samples in a batch. <ref type="bibr">9</ref> The max- imal token number contained in a message and the maximal message number on a trading day are empirically set to 30 and 40, respectively, with the excess clipped. Since all tweets in the batched samples are simultaneously fed into the model, we set the word embedding size to 50 instead of larger sizes to control memory costs and make model training feasible on one single GPU (11GB memory). We set the hidden size of Message Em- bedding Layer to 100 and that of VMD to 150. All weight matrices in the model are initialized with the fan-in trick and biases are initialized with zero. We train the model with an Adam optimizer ( <ref type="bibr">Kingma and Ba, 2014</ref>) with the initial learning rate of 0.001. Following <ref type="bibr" target="#b3">Bowman et al. (2016)</ref>, we use the input dropout rate of 0.3 to regularize latent variables. <ref type="bibr">Tensorflow (Abadi et al., 2016</ref>) is used to construct the computational graph of StockNet and hyper-parameters are tweaked on the develop- ment set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation Metrics</head><p>Following previous work for stock prediction <ref type="bibr" target="#b21">(Xie et al., 2013;</ref><ref type="bibr" target="#b7">Ding et al., 2015)</ref>, we adopt the stan- dard measure of accuracy and Matthews Corre- lation Coefficient (MCC) as evaluation metrics. MCC avoids bias due to data skew. Given the con- fusion matrix tp fn fp tn containing the number of samples classified as true positive, false positive, true negative and false negative, MCC is calcu- lated as</p><formula xml:id="formula_30">MCC = tp × tn − fp × fn (tp + fp)(tp + fn)(tn + fp)(tn + fn)</formula><p>.</p><p>(30)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Baselines and Proposed Models</head><p>We construct the following five baselines in differ- ent genres, <ref type="bibr">10</ref> • RAND: a naive predictor making random guess in up or down.</p><p>• ARIMA: Autoregressive Integrated Moving Average, an advanced technical analysis method using only price signals <ref type="bibr" target="#b4">(Brown, 2004</ref>) .</p><p>• RANDFOREST: a discriminative Random For- est classifier using Word2vec text represen- tations ( <ref type="bibr" target="#b15">Pagolu et al., 2016</ref>).</p><p>• TSLDA: a generative topic model jointly learning topics and sentiments <ref type="bibr">(Nguyen and Shirai, 2015</ref>).</p><p>• HAN: a state-of-the-art discriminative deep neural network with hierarchical attention (Hu et al., 2018). To make a detailed analysis of all the primary components in StockNet, in addition to HEDGE- FUNDANALYST, the fully-equipped StockNet, we also construct the following four variations,</p><p>• TECHNICALANALYST: the generative StockNet using only historical prices.</p><p>• FUNDAMENTALANALYST: the generative Stock- Net using only tweet information.</p><p>• INDEPENDENTANALYST: the generative Stock- Net without temporal auxiliary targets.  <ref type="bibr" target="#b4">(Brown, 2004)</ref> 51.39 -0.020588 FUNDAMENTALANALYST 58.23 0.071704 RANDFOREST ( <ref type="bibr" target="#b15">Pagolu et al., 2016)</ref> 53.08 0.012929 INDEPENDENTANALYST 57.54 0.036610 TSLDA <ref type="bibr">(Nguyen and Shirai, 2015)</ref> 54.07 0.065382 DISCRIMINATIVEANALYST 56.15 0.056493 HAN ( <ref type="bibr">Hu et al., 2018)</ref> 57.64 0.051800 HEDGEFUNDANALYST 58.23 0.080796  • DISCRIMINATIVEANALYST: the discriminative StockNet directly optimizing the likelihood objective. Following <ref type="bibr" target="#b22">Zhang et al. (2016)</ref>, we set z t = µ t to take out the effects of the KL term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Results</head><p>Since stock prediction is a challenging task and a minor improvement usually leads to large po- tential profits, the accuracy of 56% is generally reported as a satisfying result for binary stock movement prediction <ref type="bibr">(Nguyen and Shirai, 2015)</ref>. We show the performance of the baselines and our proposed models in Compared with DISCRIMINATIVEANALYST, the performance improvements of HEDGEFUNDANA- LYST are not from enlarging the networks, demon- strating that modeling underlying market status explicitly with latent driven factors indeed benefits stock movement prediction. The comparison with INDEPENDENTANALYST also shows the effectiveness of capturing temporal dependencies between pre- dictions with the temporal auxiliary. However, the effects of the temporal auxiliary are more complex and will be analyzed further in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Effects of Temporal Auxiliary</head><p>We provide a detailed discuss of how the tempo- ral auxiliary affects model performance. As intro- duced in Eq. (28), the temporal auxiliary weight α controls the overall effects of the objective-level temporal auxiliary to our model. <ref type="figure" target="#fig_2">Figure 4</ref> presents how the performance of HEDGEFUNDANALYST and DISCRIMINATIVEANALYST fluctuates with α.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, enhanced by the temporal auxiliary, HEDGEFUNDANALYST approaches the best performance at 0.5, and DISCRIMINATIVEANALYST achieves its maximum at 0.7. In fact, objective- level auxiliary can be regarded as a denoising reg- ularizer: for a sample with a specific movement as the main target, the market source in the lag can be heterogeneous, e.g. affected by bad news, tweets on earlier days are negative but turn to pos- itive due to timely crises management. Without temporal auxiliary tasks, the model tries to iden- tify positive signals on earlier days only for the main target of rise movement, which is likely to result in pure noise. In such cases, temporal aux- iliary tasks help to filter market sources in the lag as per their respective aligned auxiliary move- ments. Besides, from the perspective of training variational models, the temporal auxiliary helps HEDGEFUNDANALYST to encode more useful infor- mation into the latent driven factor Z, which is consistent with recent research in VAEs <ref type="bibr" target="#b19">(Semeniuta et al., 2017</ref>). Compared with HEDGEFUND- ANALYST that contains a KL term performing dy- namic regularization, DISCRIMINATIVEANALYST re- quires stronger regularization effects coming with a bigger α to achieve its best performance.</p><p>Since y * also involves in generating y T through the temporal attention, tweaking α acts as a trade- off between focusing on the main target and gener- alizing by denoising. Therefore, as shown in <ref type="figure" target="#fig_2">Fig- ure 4</ref>, our models do not linearly benefit from incorporating temporal auxiliary. In fact, the two models follow a similar pattern in terms of per- formance change: the curves first drop down with the increase of α, except the MCC curve for DIS- CRIMINATIVEANALYST rising up temporarily at 0.3. After that, the curves ascend abruptly to their max- imums, then keep descending till α = 1. Though the start phase of increasing α even leads to worse performance, when auxiliary effects are properly introduced, the two models finally gain better re- sults than those with no involvement of auxiliary effects, e.g. INDEPENDENTANALYST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We demonstrated the effectiveness of deep gen- erative approaches for stock movement predic- tion from social media data by introducing StockNet, a neural network architecture for this task. We tested our model on a new compre- hensive dataset and showed it performs better than strong baselines, including implementation of previous work. Our comprehensive dataset is publicly available at https://github.com/ yumoxu/stocknet-dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ziniu Hu, Weiqing Liu, Jiang Bian, Xuanzhe Liu, and</head><p>Tie-Yan Liu. 2018. Listening to chaotic whispers: A deep learning framework for news-oriented stock trend prediction. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The temporal attention in our model. Squares are the non-linear projections of g t and points are scores or normalized weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Performance of HEDGEFUNDANALYST with varied α, see Eq. (28). (b) Performance of DISCRIMINATIVEANALYST with varied α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Performance of baselines and StockNet variations in accuracy and MCC. 

0.0 
0.1 
0.3 
0.5 
0.7 
0.9 
1.0 

46 

48 

50 

52 

54 

56 

58 

60 

Acc. 

57.54 
57.24 
55.56 

58.23 
57.54 
57.44 

54.27 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>TLSDA is the 
best baseline in MCC while HAN is the best 
baseline in accuracy. Our model, HEDGEFUNDAN-
ALYST achieves the best performance of 58.23 in 
accuracy and 0.080796 in MCC, outperforming 
TLSDA and HAN with 4.16, 0.59 in accuracy, and 
0.015414, 0.028996 in MCC, respectively. 
Though slightly better than random guess, clas-
sic technical analysis, e.g. ARIMA, does not yield 
satisfying results. Similar in using only histori-
cal prices, TECHNICALANALYST shows an obvious 
advantage in this task compared ARIMA. We be-
lieve there are two major reasons: (1) TECHNICAL-
ANALYST learns from training data and incorpo-
rates more flexible non-linearity; (2) our test set 
contains a large number of stocks while ARIMA 
is more sensitive to peculiar sequence station-
arity. It is worth noting that FUNDAMENTALANA-

LYST gains exceptionally competitive results with 
only 0.009092 less in MCC than HEDGEFUNDAN-
ALYST. The performance of FUNDAMENTALANALYST 
and TECHNICALANALYST confirm the positive ef-
fects from tweets and historical prices in stock 
movement prediction, respectively. As an effective 
ensemble of the two market information, HEDGE-
FUNDANALYST gains even better performance. 
</table></figure>

			<note place="foot" n="1"> https://github.com/yumoxu/ stocknet-dataset</note>

			<note place="foot" n="2"> We use the notation [a, b] to denote the interval of integer numbers between a and b.</note>

			<note place="foot" n="3"> To a fundamentalist, stocks have their intrinsic values that can be derived from the behavior and performance of their company. On the contrary, technical analysis considers only the trends and patterns of the stock price. 4 Technically, d − 1 may not be an eligible trading day and thus has no available price information. In the rest of this</note>

			<note place="foot" n="8"> It holds that T ≥ 1 since d is undoubtedly a trading day.</note>

			<note place="foot" n="9"> Typically the lag size is set between 3 and 10. As introduced in Section 4, trading days are treated as basic units in StockNet and 3 calendar days are thus too short to guarantee the existence of more than one trading day in a lag, e.g. the prediction for the movement of Monday. We also experiment with 7 and 10 but they do not yield better results than 5.</note>

			<note place="foot" n="10"> We do not treat event-driven models as comparable methods since our model uses no event pre-extraction tool.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the three anony-mous reviewers and Miles Osborne for their help-ful comments. This research was supported by a grant from Bloomberg and by the H2020 project SUMMA, under grant agreement 688139.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Natural language processing with Python: analyzing text with the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Twitter mood predicts the stock market</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huina</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Smoothing, forecasting and prediction of discrete time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert Goodell Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Courier Corporation</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="95" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using structured events to predict stock price movement: An empirical investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwen</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1415" to="1425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning for event-driven stock prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwen</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence</title>
		<meeting>the 24th International Conference on Artificial Intelligence<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2327" to="2333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Technical analysis of stock trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Whc</forename><surname>Robert D Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bassetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Financial markets and monetary policy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep recurrent generative decoder for abstractive text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2081" to="2090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting abnormal returns from news using text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronny</forename><surname>Luss And Alexandre D&amp;apos;aspremont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Finance</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="999" to="1012" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A random walk down Wall Street: including a life-cycle guide to personal investing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burton Gordon Malkiel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>WW Norton &amp; Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Topic modeling based sentiment analysis on social media for stock market prediction</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Beijing</title>
		<editor>Thien Hai Nguyen and Kiyoaki Shirai</editor>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Beijing<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1354" to="1364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Some experiments on modeling stock market behavior using investor sentiment analysis and posting volume from twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Areal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Web Intelligence, Mining and Semantics</title>
		<meeting>the 3rd International Conference on Web Intelligence, Mining and Semantics<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sentiment analysis of twitter data for predicting stock market movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkata Sasank Pagolu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganapati</forename><surname>Kamal Nayan Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babita</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Majhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2016 International Conference on Signal Processing, Communication, Power and Embedded System. IEEE, Rajaseetapuram</title>
		<meeting>2016 International Conference on Signal Processing, Communication, Power and Embedded System. IEEE, Rajaseetapuram<address><addrLine>India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1345" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Volatility prediction using financial disclosures sentiments with word embedding-based ir models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navid</forename><surname>Rekabsaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Baklanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Dür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1712" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Textual analysis of stock market prediction using breaking financial news: The azfin text system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsinchun</forename><surname>Schumaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A hybrid convolutional variational autoencoder for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislau</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhardt</forename><surname>Barth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="627" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploiting topic based twitter sentiment for stock prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotie</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic frames to predict stock price movement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><forename type="middle">J</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán G</forename><surname>Creamer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Variational neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
