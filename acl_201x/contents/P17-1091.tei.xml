<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Argument Mining with Structured SVMs and RNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonsuk</forename><surname>Park</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
						</author>
						<title level="a" type="main">Argument Mining with Structured SVMs and RNNs</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="985" to="995"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1091</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel factor graph model for argument mining, designed for settings in which the argumentative relations in a document do not necessarily form a tree structure. (This is the case in over 20% of the web comments dataset we release.) Our model jointly learns elementary unit type classification and argumentative relation prediction. Moreover, our model supports SVM and RNN parametrizations, can enforce structure constraints (e.g., transitivity), and can express dependencies between adjacent relations and propositions. Our approaches outperform unstruc-tured baselines in both web comments and argumentative essay datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Argument mining consists of the automatic identi- fication of argumentative structures in documents, a valuable task with applications in policy mak- ing, summarization, and education, among others. The argument mining task includes the tightly-knit subproblems of classifying propositions into ele- mentary unit types and detecting argumentative re- lations between the elementary units. The desired output is a document argumentation graph struc- ture, such as the one in <ref type="figure" target="#fig_0">Figure 1</ref>, where proposi- tions are denoted by letter subscripts, and the asso- ciated argumentation graph shows their types and support relations between them.</p><p>Most annotation and prediction efforts in ar- gument mining have focused on tree or forest structures ( <ref type="bibr" target="#b23">Peldszus and Stede, 2015;</ref><ref type="bibr" target="#b26">Stab and Gurevych, 2016)</ref>, constraining argument struc- tures to form one or more trees. This makes the problem computationally easier by enabling the use of maximum spanning tree-style parsing ap- proaches. However, argumentation in the wild can be less well-formed. The argument put forth in <ref type="figure" target="#fig_0">Figure 1</ref>, for instance, consists of two compo- nents: a simple tree structure and a more com- plex graph structure (c jointly supports b and d).</p><p>In this work, we design a flexible and highly expressive structured prediction model for argu- ment mining, jointly learning to classify elemen- tary units (henceforth propositions) and to identify the argumentative relations between them (hence- forth links). By formulating argument mining as inference in a factor graph ( <ref type="bibr" target="#b10">Kschischang et al., 2001</ref>), our model (described in Section 4) can ac- count for correlations between the two tasks, can consider second order link structures (e.g., in <ref type="figure">Fig- ure</ref> 1, c → b → a), and can impose arbitrary con- straints (e.g., transitivity).</p><p>To parametrize our models, we evaluate two alternative directions: linear structured SVMs ( <ref type="bibr" target="#b28">Tsochantaridis et al., 2005)</ref>, and recurrent neural networks with structured loss, extending <ref type="bibr" target="#b8">(Kiperwasser and Goldberg, 2016)</ref>. Interestingly, RNNs perform poorly when trained with classification losses, but become competitive with the feature- engineered structured SVMs when trained within our proposed structured learning model. We evaluate our approach on two argument mining datasets. Firstly, on our new Cornell eRulemaking Corpus -CDCP, 2 consisting of ar- gument annotations on comments from an eRule- making discussion forum, where links don't al- ways form trees <ref type="figure" target="#fig_0">(Figure 1</ref> shows an abridged example comment, and Section 3 describes the dataset in more detail). Secondly, on the UKP ar- gumentative essays v2 (henceforth UKP), where argument graphs are annotated strictly as multiple trees <ref type="bibr" target="#b26">(Stab and Gurevych, 2016)</ref>. In both cases, the results presented in Section 5 confirm that our models outperform unstructured baselines. On UKP, we improve link prediction over the best re- ported result in <ref type="bibr" target="#b26">(Stab and Gurevych, 2016)</ref>, which is based on integer linear programming postpro- cessing. For insight into the strengths and weak- nesses of the proposed models, as well as into the differences between SVM and RNN parameteriza- tions, we perform an error analysis in Section 5.1. To support argument mining research, we also re- lease our Python implementation, Marseille. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Our factor graph formulation draws from ideas previously used independently in parsing and ar- gument mining. In particular, maximum spanning tree (MST) methods for arc-factored dependency parsing have been successfully used by <ref type="bibr" target="#b17">McDonald et al. (2005)</ref> and applied to argument min- ing with mixed results by <ref type="bibr" target="#b23">Peldszus and Stede (2015)</ref>. As they are not designed for the task, MST parsers cannot directly handle proposition classifi- cation or model the correlation between proposi- tion and link prediction-a limitation our model addresses. Using RNN features in an MST parser with a structured loss was proposed by <ref type="bibr" target="#b8">Kiperwasser and Goldberg (2016)</ref>; their model can be seen as a particular case of our factor graph ap- proach, limited to link prediction with a tree struc- ture constraint. Our models support multi-task learning for proposition classification, parameter-izing adjacent links with higher-order structures (e.g., c → b → a) and enforcing arbitrary con- straints on the link structure, not limited to trees. Such higher-order structures and logic constraints have been successfully used for dependency and semantic parsing by <ref type="bibr" target="#b15">Martins et al. (2013) and</ref><ref type="bibr" target="#b14">Almeida (2014)</ref>; to our knowledge we are the first to apply them to argument mining, as well as the first to parametrize them with neural net- works. <ref type="bibr" target="#b26">Stab and Gurevych (2016)</ref> used an inte- ger linear program to combine the output of in- dependent proposition and link classifiers using a hand-crafted scoring formula, an approach simi- lar to our baseline. Our factor graph method can combine the two tasks in a more principled way, as it fully learns the correlation between the two tasks without relying on hand-crafted scoring, and therefore can readily be applied to other argumen- tation datasets. Furthermore, our model can en- force the tree structure constraint, required on the UKP dataset, using MST cycle constraints used by <ref type="bibr" target="#b26">Stab and Gurevych (2016)</ref>, thanks to the AD 3 in- ference algorithm <ref type="bibr" target="#b16">(Martins et al., 2015)</ref>.</p><p>Sequence tagging has been applied to the re- lated structured tasks of proposition identifica- tion and classification <ref type="bibr" target="#b26">(Stab and Gurevych, 2016;</ref><ref type="bibr" target="#b5">Habernal and Gurevych, 2016;</ref><ref type="bibr" target="#b22">Park et al., 2015b)</ref>; integrating such models is an important next step. Meanwhile, a new direction in argument mining explores pointer networks ( <ref type="bibr" target="#b25">Potash et al., 2016)</ref>; a promising method, currently lacking support for tree structures and domain-specific constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>We release a new argument mining dataset consist- ing of user comments about rule proposals regard- ing Consumer Debt Collection Practices (CDCP) by the Consumer Financial Protection Bureau col- lected from an eRulemaking website, http:// regulationroom.org.</p><p>Argumentation structures found in web discus- sion forums, such as the eRulemaking one we use, can be more free-form than the ones encountered in controlled, elicited writing such as ( <ref type="bibr" target="#b23">Peldszus and Stede, 2015)</ref>. For this reason, we adopt the model proposed by <ref type="bibr" target="#b21">Park et al. (2015a)</ref>, which does not constrain links to form tree structures, but un- restricted directed graphs. Indeed, over 20% of the comments in our dataset exhibit local struc- tures that would not be allowable in a tree. Possi- ble link types are reason and evidence, and propo-sition types are split into five fine-grained cate- gories: POLICY and VALUE contain subjective judge- ments/interpretations, where only the former spec- ifies a specific course of action to be taken. On the other hand, TESTIMONY and FACT do not con- tain subjective expressions, the former being about personal experience, or "anecdotal." Lastly, REFER- ENCE covers URLs and citations, which are used to point to objective evidence in an online setting.</p><p>In comparison, the UKP dataset (Stab and Gurevych, 2016) only makes the syntactic dis- tinction between CLAIM, MAJOR CLAIM, and PREMISE types, but it also includes attack links. The per- missible link structure is stricter in UKP, with links constrained in annotation to form one or more disjoint directed trees within each paragraph. Also, since web arguments are not necessarily fully developed, our dataset has many argumenta- tive propositions that are not in any argumentation relations. In fact, it isn't unusual for comments to have no argumentative links at all: 28% of CDCP comments have no links, unlike UKP, where all essays have complete argument structures. Such comments with no links make the problem harder, emphasizing the importance of capturing the lack of argumentative support, not only its presence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Annotation results</head><p>Each user comment was annotated by two anno- tators, who independently annotated the bound- aries and types of propositions, as well as the links among them. <ref type="bibr">4</ref> To produce the final corpus, a third annotator manually resolved the conflicts, <ref type="bibr">5</ref> and two automatic preprocessing steps were applied: we take the link transitive closure, and we remove a small number of nested propositions. <ref type="bibr">6</ref> The re- sulting dataset contains 731 comments, consist- ing of about 3800 sentences (≈4700 propositions) and 88k words. Out of the 43k possible pairs of propositions, links are present between only 1300 (roughly 3%). In comparison, UKP has fewer doc- uments (402), but they are longer, with a total of 7100 sentences (6100 propositions) and 147k words. Since UKP links only occur within the same paragraph and propositions not connected to the argument are removed in a preprocessing step, link prediction is less imbalanced in UKP, with 3800 pairs of propositions being linked out of a to- tal of 22k (17%). We reserve a test set of 150 doc- uments (973 propositions, 272 links) from CDCP, and use the provided 80-document test split from UKP (1266 propositions, 809 links).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Structured learning</head><p>for argument mining</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preliminaries</head><p>Binary and multi-class classification have been ap- plied with some success to proposition and link prediction separately, but we seek a way to jointly learn the argument mining problem at the docu- ment level, to better model contextual dependen- cies and constraints. We therefore turn to struc- tured learning, a framework that provides the de- sired level of expressivity.</p><p>In general, learning from a dataset of documents x i ∈ X and their associated labels y i ∈ Y involves seeking model parameters w that can "pick out" the best label under a scoring function f :</p><formula xml:id="formula_0">ˆ y := arg max y∈Y f (x, y; w).<label>(1)</label></formula><p>Unlike classification or regression, where X is usu- ally a feature space R d and Y ⊆ R (e.g., we predict an integer class index or a probability), in struc- tured learning, more complex inputs and outputs are allowed. This makes the arg max in Equa- tion 1 impossible to evaluate by enumeration, so it is desirable to find models that decompose over smaller units and dependencies between them; for instance, as factor graphs. In this section, we give a factor graph description of our proposed struc- tured model for argument mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model description</head><p>An input document is a string of words with proposition offsets delimited. We denote the propositions in a document by {a, b, c, ...} and the possible directed link between a and b as a → b. The argument structure we seek to predict consists of the type of each proposition y a ∈ P and a bi- nary label for each link <ref type="figure">Figure 2</ref>: Factor graphs for a document with three propositions (a, b, c) and the six possible edges be- tween them, and some of the factors used, illustrating differences and similarities between our models for the two datasets. Unary factors are light gray; compatibility factors are black. Factors not part of the basic model have curved edges: higher-order factors are orange and on the right; link structure factors are hollow, as that they don't have any parameters. Strict constraint factors are omitted for simplicity.</p><formula xml:id="formula_1">y a→b ∈ R = {on, off}. 7 a b c a → b b → c a → c a ← b b ← c a ← c (a) CDCP a b c a → b b → c a → c a ← b b ← c a ← c (b) UKP</formula><p>The possible proposition types P differ for the two datasets; such differences are documented in Ta- ble 1. As we describe the variables and factors constituting a document's factor graph, we shall refer to <ref type="figure">Figure 2</ref> for illustration.</p><p>Unary potentials. Each proposition a and each link a → b has a corresponding random variable in the factor graph (the circles in <ref type="figure">Figure 2)</ref>. To encode the model's belief in each possible value for these variables, we parametrize the unary fac- tors (gray boxes in <ref type="figure">Figure 2</ref>) with unary poten- tials: φ(a) ∈ R |P| is a score of y a for each pos- sible proposition type. Similarly, link unary po- tentials φ(a → b) ∈ R |R| are scores for y a→b be- ing on/off. Without any other factors, this would amount to independent classifiers for each task.</p><p>Compatibility factors. For every possible link a → b, the variables (a, b, a → b) are bound by a dense factor scoring their joint assignment (the black boxes in <ref type="figure">Figure 2</ref>). Such a factor could automatically learn to encourage links from compatible types (e.g., from TESTIMONY to POLICY) or discourage links between less compatible ones (e.g., from FACT to TESTIMONY). In the simplest form, this factor would be parametrized as a ten- sor T ∈ R |P|×|P|×|R| , with t ijk retaining the score of a source proposition of type i to be (k = on) or not to be (k = off) in a link with a proposi- tion of type j. For more flexibility, we parametrize this factor with compatibility features depending only on simple structure: t ijk becomes a vector, and the score of configuration (i, j, k) is given by v ⊤ ab t ijk where v ab consists of three binary features:</p><p>• bias: a constant value of 1, allowing T to learn a base score for a label configuration (i, j, k), as in the simple form above,</p><p>• adjacency: when there are no other proposi- tions between the source and the target,</p><p>• order: when the source precedes the target.</p><p>Second order factors. Local argumentation graph structures such as a → b → c might be modeled better together rather than through sep- arate link factors for a → b and b → c. As in higher-order structured models for semantic and dependency parsing <ref type="bibr" target="#b15">(Martins et al., 2013;</ref><ref type="bibr" target="#b14">Martins and Almeida, 2014</ref>), we implement three types of second order factors:</p><formula xml:id="formula_2">grandparent (a → b → c), sibling (a ← b → c), and co-parent (a → b ← c)</formula><p>. Not all of these types of factors make sense on all datasets: as sibling structures cannot exist in directed trees, we don't use sibling factors on UKP. On CDCP, by transitivity, every grandpar- ent structure implies a corresponding sibling, so it is sufficient to parametrize siblings. This differ- ence between datasets is emphasized in <ref type="figure">Figure 2</ref>, where one example of each type of factor is pic- tured on the right side of the graphs (orange boxes with curved edges): on CDCP we illustrate a co- parent factor (top right) and a sibling factor (bot-tom right), while on UKP we show a co-parent factor (top right) and a grandparent factor (bottom right). We call these factors second order because they involve two link variables, scoring the joint assignment of both links being on.</p><p>Valid link structure. The global structure of ar- gument links can be further constrained using do- main knowledge. We implement this using con- straint factors; these have no parameters and are denoted by empty boxes in <ref type="figure">Figure 2</ref>. In general, well-formed arguments should be cycle-free. In the UKP dataset, links form a directed forest and can never cross paragraphs. This particular con- straint can be expressed as a series of tree factors, 8 one for each paragraph (the factor connected to all link variables in <ref type="figure">Figure 2</ref>). In CDCP, links do not form a tree, but we use logic constraints to enforce transitivity (top left factor in <ref type="figure">Figure 2</ref>) and to pre- vent symmetry (bottom left); the logic formulas implemented by these factors are described in <ref type="table">Ta- ble 1</ref>. Together, the two constraints have the desir- able side effect of preventing cycles.</p><p>Strict constraints. We may include further domain-specific constraints into the model, to ex- press certain disallowed configurations. For in- stance, proposition types that appear in CDCP data can be ordered by the level of objectivity ( <ref type="bibr" target="#b21">Park et al., 2015a</ref>), as shown in <ref type="table">Table 1</ref>. In a well- formed argument, we would want to see links from more objective to equally or less objective propo- sitions: it's fine to provide FACT as reason for VALUE, but not the other way around. While the training data sometimes violates this constraint, enforcing it might provide a useful inductive bias.</p><p>Inference. The arg max in Equation 1 is a MAP over a factor graph with cycles and many overlap- ping factors, including logic factors. While ex- act inference methods are generally unavailable, our setting is perfectly suited for the Alternat- ing Directions Dual Decomposition (AD 3 ) algo- rithm: approximate inference on expressive factor graphs with overlapping factors, logic constraints, and generic factors (e.g., directed tree factors) de- fined through maximization oracles (Martins et al., 2015). When AD 3 returns an integral solution, it is globally optimal, but when solutions are frac-tional, several options are available. At test time, for analysis, we retrieve exact solutions using the branch-and-bound method. At training time, how- ever, fractional solutions can be used as-is; this makes better use of each iteration and actually in- creases the ratio of integral solutions in future iter- ations, as well as at test time, as proven by <ref type="bibr" target="#b18">Meshi et al. (2016)</ref>. We also find that after around 15 training iterations with fractional solutions, over 99% of inference calls are integral.</p><p>Learning. We train the models by minimizing the structured hinge loss ( <ref type="bibr" target="#b27">Taskar et al., 2004)</ref>:</p><formula xml:id="formula_3">(x,y)∈D max y ′ ∈Y (f (x, y ′ ; w) + ρ(y, y ′ )) − f (x, y; w)<label>(2)</label></formula><p>where ρ is a configurable misclassification cost. The max in Equation 2 is not the same as the one used for prediction, in Equation 1. However, when the cost function ρ decomposes over the variables, cost-augmented inference amounts to regular in- ference after augmenting the potentials accord- ingly. We use a weighted Hamming cost:</p><formula xml:id="formula_4">ρ(y, ˆ y) := v ρ(y v )I[y v = ˆ y v ]</formula><p>where v is summed over all variables in a docu- ment {a} ∪ {a → b}, and ρ(y v ) is a misclassifi- cation cost. We assign uniform costs ρ to 1 for all mistakes except false-negative links, where we use higher cost proportional to the class imbalance in the training split, effectively giving more weight to positive links during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Argument structure SVM</head><p>One option for parameterizing the potentials of the unary and higher-order factors is with linear models, using proposition, link, and higher-order features. This gives birth to a linear structured SVM ( <ref type="bibr" target="#b28">Tsochantaridis et al., 2005</ref>), which, when using l 2 regularization, can be trained efficiently in the dual using the online block-coordinate Frank- </p><formula xml:id="formula_5">• a → b &amp; b → c =⇒ a → c • ATMOSTONE(a → b, b → a)</formula><p>directed forest:</p><p>• TREEFACTOR over each paragraph • zero-potential "root" links a → * strict constraints link source must be as least as objective as the target: a → b =⇒ a b link source must be premise: a → b =⇒ a = PREMISE <ref type="table">Table 1</ref>: Instantiation of model design choices for each dataset.</p><p>lexical (unigrams and dependency tuples), struc- tural (token statistics and proposition location), indicators (from hand-crafted lexicons), contex- tual, syntactic (subclauses, depth, tense, modal, and POS), probability, discourse ( <ref type="bibr" target="#b13">Lin et al., 2014</ref>), and average GloVe embeddings ( <ref type="bibr" target="#b24">Pennington et al., 2014</ref>). Link features are lexical (unigrams), syn- tactic (POS and productions), structural (token statistics, proposition statistics and location fea- tures), hand-crafted indicators, discourse triples, PMI, and shared noun counts. Our proposed higher-order factors for grandpar- ent, co-parent, and sibling structures require fea- tures extracted from a proposition triplet a, b, c. In dependency and semantic parsing, higher-order factors capture relationships between words, so sparse indicator features can be efficiently used. In our case, since propositions consist of many words, BOW features may be too noisy and too dense; so for simplicity we again take a cue from the link-specific features used by <ref type="bibr" target="#b26">Stab and Gurevych (2016)</ref>. Our higher-order factor fea- tures are: same sentence indicators (for all 3 and for each pair), proposition order (one for each of the 6 possible orderings), Jaccard similarity (be- tween all 3 and between each pair), presence of any shared nouns (between all 3 and between each pair), and shared noun ratios: nouns shared by all 3 divided by total nouns in each proposition and each pair, and shared nouns between each pair with respect to each proposition. Up to vocabulary size difference, our total feature dimensionality is approximately 7000 for propositions and 2100 for links. The number of second order features is 35.</p><p>Hyperparameters. We pick the SVM regular- ization parameter C ∈ {0.001, 0.003, 0.01, 0.03, 0.1, 0.3} by k-fold cross validation at document level, optimizing for the average between link and proposition F 1 scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Argument structure RNN</head><p>Neural network methods have proven effective for natural language problems even with minimal- to-no feature engineering. Inspired by the use of LSTMs <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997</ref>) for MST dependency parsing by <ref type="bibr" target="#b8">Kiperwasser and Goldberg (2016)</ref>, we parametrize the potentials in our factor graph with an LSTM-based neural net- work, <ref type="bibr">9</ref> replacing MST inference with the more general AD 3 algorithm, and using relaxed solu- tions for training when inference is inexact.</p><p>We extract embeddings of all words with a corpus frequency &gt; 1, initialized with GloVe word vectors. We use a deep bidirectional LSTM to encode contextual information, representing a proposition a as the average of the LSTM outputs of its words, henceforth denoted ↔ a.</p><p>Proposition potentials. We apply a multi-layer perceptron (MLP) with rectified linear activations to each proposition, with all layer dimensions equal except the final output layer, which has size |P| and is not passed through any nonlinearities. </p><formula xml:id="formula_6">φ on (a → b) := a ⊤ W b + w ⊤ src a + w ⊤ trg b + w (on) 0 .</formula><p>Since the bilinear expression returns a scalar, but the link potentials must have a value for both the on and off states, we set the full potential to</p><formula xml:id="formula_7">φ(a → b) := [φ on (a → b), w (off) 0 ] where w (off) 0</formula><p>is a learned scalar bias. We initialize W to the diagonal identity matrix.</p><p>Second order potentials. Grandparent poten- tials φ(a → b → c) score two adjacent directed edges, in other words three propositions. We again first pass each proposition representation through a slot-specific dense layer. We implement a multi- linear scorer analogously to the link potentials:</p><formula xml:id="formula_8">φ(a → b → c) := i,j,k a i b j c k w ijk</formula><p>where W = (w) ijk is a third-order cube ten- sor. To reduce the large numbers of parameters, we implicitly represent W as a rank r tensor:</p><formula xml:id="formula_9">w ijk = r s=1 u (1) is u (2) js u (3)</formula><p>ks . Notably, this model captures only third-order interactions between the representation of the three propositions. To cap- ture first-order "bias" terms, we could include slot- specific linear terms, e.g., w ⊤ a a; but to further capture quadratic backoff effects (for instance, if two propositions carry a strong signal of being siblings regardless of their parent), we would re- quire quadratically many parameters. Instead of explicit lower-order terms, we propose augment- ing a, b, and c with a constant feature of 1, which has approximately the same effect, while benefit- ing from the parameter sharing in the low-rank factorization; an effect described by . Siblings and co-parents factors are simi- larly parametrized with their own tensors.</p><p>Hyperparameters. We perform grid search us- ing k-fold document-level cross-validation, tun- ing the dropout probability in the dense MLP lay- ers over {0.05, 0.1, 0.15, 0.2, 0.25} and the opti- mal number of passes over the training data over {10, 25, 50, 75, 100}. We use 2 layers for the LSTM and the proposition classifier, 128 hidden units in all layers, and a multilinear decomposition with rank r = 16, after preliminary CV runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Baseline models</head><p>We compare our proposed models to equivalent in- dependent unary classifiers. The unary-only ver- sion of a structured SVM is an l 2 -regularized lin- ear SVM. <ref type="bibr">10</ref> For the RNN, we compute unary po- tentials in the same way as in the structured model, but apply independent hinge losses at each vari- able, instead of the global structured hinge loss. Since the RNN weights are shared, this is a form of multi-task learning. The baseline predictions can be interpreted as unary potentials, therefore we can simply round their output to the highest scor- ing labels, or we can, alternatively, perform test- time inference, imposing the desired structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We evaluate our proposed models on both datasets. For model selection and development we used k- fold cross-validation at document level: on CDCP we set k = 3 to avoid small validation folds, while on UKP we follow <ref type="bibr" target="#b26">Stab and Gurevych (2016)</ref> set- ting k = 5. We compare our proposed structured learning systems (the linear structured SVM and the structured RNN) to the corresponding baseline versions. We organize our experiments in three in- cremental variants of our factor graph: basic, full, and strict, each with the following components: 11 component basic full strict (baseline) unaries compat. factors compat. features higher-order link structure strict constraints</p><p>Following <ref type="bibr" target="#b26">Stab and Gurevych (2016)</ref>, we compute F 1 scores at proposition and link level, and also report their average as a summary of overall per- formance. <ref type="bibr">12</ref> The results of a single prediction run on the test set are displayed in <ref type="table">Table 2</ref>. The over- all trend is that training using a structured objec- tive is better than the baseline models, even when structured inference is applied on the baseline pre- dictions. On UKP, for link prediction, the linear baseline can reach good performance when us- ing inference, similar to the approach of <ref type="bibr" target="#b26">Stab and Gurevych (2016)</ref>, but the improvement in propo- sition prediction leads to higher overall F 1 for the structured models. Meanwhile, on the more dif- ficult CDCP setting, performing inference on the baseline output is not competitive. While feature engineering still outperforms our RNN model, we find that RNNs shine on proposition classification, especially on UKP, and that structured training can make them competitive, reducing their observed lag on link prediction <ref type="bibr" target="#b7">(Katiyar and Cardie, 2016)</ref>, possibly through mitigating class imbalance.  <ref type="table">Table 2</ref>: Test set F 1 scores for link and proposition classification, as well as their average, on the two datasets. The number of test instances is shown in parentheses; best scores on overall tasks are in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Discussion and analysis</head><p>Contribution of compatibility features. The compatibility factor in our model can be visual- ized as conditional odds ratios given the source and target proposition types. Since there are only four possible configurations of the compatibility features, we can plot all cases in <ref type="figure">Figure 3</ref>, along- side the basic model. Not using compatibility features, the basic model can only learn whether certain configurations are more likely than others (e.g. a REFERENCE supporting another REFERENCE is unlikely, while a REFERENCE supporting a FACT is more likely; essentially a soft version of our domain-specific strict constraints. The full model with compatibility features is finer grained, captur- ing, for example, that links from REFERENCE to FACT are more likely when the reference comes after, or that links from VALUE to POLICY are extremely likely only when the two are adjacent.</p><p>Proposition errors. The confusion matrices in <ref type="figure" target="#fig_2">Figure 4</ref> reveal that the most common confusion is misclassifying FACT as VALUE. The strongest dif- ference between the various models tested is that the RNN-based models make this error less often. For instance, in the proposition:</p><p>And the single most frequently used excuse of any debtor is "I didn't receive the let- ter/invoice/statement" the pronouns in the nested quote may be mistaken for subjectivity, leading to the structured SVMs predictions of VALUE or TESTIMONY, while the ba- sic structured RNN correctly classifies it as FACT.</p><p>Link errors. While structured inference cer- tainly helps baselines by preventing invalid struc- tures such as cycles, it still depends on local deci- sions, losing to fully structured training in cases where joint proposition and link decisions are needed. For instance, in the following conclusion of an UKP essay, the annotators found no links: Indeed, no reasons are provided, but baseline are misled by the connectives: the SVM baseline out- puts that b and c are PREMISEs supporting the CLAIM a. The full structured SVM combines the two tasks and correctly recognizes the link structure. Linear SVMs are still a very good baseline, but they tend to overgenerate links due to class imbal- ance, even if we use class weights during training. Surprisingly, RNNs are at the opposite end, be- ing extremely conservative, and getting the high- est precision among the models. On CDCP, where the number of true links is 272, the linear base- line with strict inference predicts 796 links with a precision of only 16%, while the strict structured RNN only predicts 52 links, with 33% precision; the example in <ref type="figure" target="#fig_3">Figure 5</ref> illustrates this. In terms of higher-order structures, we find that using higher- order factors increases precision, at a cost in recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P V F T R Target</head><formula xml:id="formula_10">Policy Value Fact Testimony Reference Source -0.3 -0.1 -0.1 -0.2 -0.1 +0.1 -0.0 -0.1 -0.1 -0.2 -0.0 +0.0 -0.1 -0.2 -0.1 -0.2 -0.1 -0.1 -0.3 +0.1 -0.3 +0.0 +0.6 +0.1 -0.4 Non-adjacent, trg precedes src P V F T R Target -0.2 -0.3 -0.0 -0.2 -0.2 -0.4 -0.3 -0.2 -0.1 -0.3 -0.3 -0.1 +0.0 -0.0 -0.3 -0.3 -0.0 -0.1 +0.1 -0.2 -0.2 -0.0 +0.4 +0.1 -0.4</formula><p>Non-adjacent, src precedes trg P V F T R Target +0.6 +0.9 +0.3 +0.1 -0.1 +2.2 +1.7 +1.0 +0.9 -0.1 +2.0 +1.7 +1.0 +0.6 -0.1 +1.5 +1.5 +0.9 +0.9 +0.1 -0.2 +0.1 +0.5 +0.1 -0.8 Adjacent, trg precedes src P V F T R Target +0.7 +0.7 +0.3 +0.1 -0.2 +1.7 +1.4 +0.9 +0.9 -0.2 +1.7 +1.5 +1.1 +0.7 -0.3 +1.4 +1.5 +0.9 +1.4 -0.1 -0.1 +0.1 +0.3 +0.1 -0.9</p><p>Adjacent, src precedes trg P V F T R Target -0.8 -0.7 -1.1 -1.0 -0.4</p><p>+0.9 +0.3 -0.5 -0.5 -0.5 +0.6 +0.6 -0.2 -0.5 -0.3</p><formula xml:id="formula_11">+0.1 +0.3 -0.3 -0.2 -0.1 -0.7 -0.0 +1.3 -0.0 -1.0</formula><p>Basic (no compatibility features) <ref type="figure">Figure 3</ref>: Learned conditional log-odds log p(on|·) p(off|·) , given the source and target proposition types and compatibility feature settings. First four figures correspond to the four possible settings of the compati- bility features in the full structured SVM model. For comparison, the rightmost figure shows the same parameters in the basic structured SVM model, which does not use compatibility features. This is most beneficial for the 856 co-parent struc- tures in the UKP test set: the full structured SVM has 53% F 1 , while the basic structured SVM and the basic baseline get 47% and 45% respectively. On CDCP, while higher-order factors help, perfor- mance on siblings and co-parents is below 10% F 1 score. This is likely due to link sparsity and sug- gests plenty of room for further development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and future work</head><p>We introduce an argumentation parsing model based on AD 3 relaxed inference in expressive fac- tor graphs, experimenting with both linear struc- tured SVMs and structured RNNs, parametrized with higher-order factors and link structure con- straints. We demonstrate our model on a new argumentation mining dataset with more permis- sive argument structure annotation. Our model also achieves state-of-the-art link prediction per- formance on the UKP essays dataset.</p><p>Future work. <ref type="bibr" target="#b26">Stab and Gurevych (2016)</ref> found polynomial kernels useful for modeling feature interactions, but kernel structured SVMs scale poorly, we intend to investigate alternate ways to capture feature interactions. While we focus on monological argumentation, our model could be extended to dialogs, for which argumentation theory thoroughly motivates non-tree structures <ref type="bibr" target="#b0">(Afantenos and Asher, 2014</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>[Figure 1 :</head><label>1</label><figDesc>Figure 1: Example annotated CDCP comment. 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Link potentials.</head><label></label><figDesc>To score a dependency a → b, Kiperwasser and Goldberg (2016) pass the con- catenation [ ↔ a; ↔ b ] through an MLP. After trying this, we found slightly better performance by first passing each proposition through a slot-specific dense layer a := σ src ( ↔ a), b := σ trg ( ↔ b ) fol- lowed by a bilinear transformation:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Normalized confusion matrices for proposition type classification.</figDesc><graphic url="image-8.png" coords="9,94.51,405.81,92.60,92.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>[Figure 5 :</head><label>5</label><figDesc>Figure 5: Predictions on a CDCP comment where the structured RNN outperforms the other models.</figDesc><graphic url="image-9.png" coords="9,195.03,405.81,92.60,92.60" type="bitmap" /></figure>

			<note place="foot" n="1"> We describe proposition types (FACT, etc.) in Section 3.</note>

			<note place="foot" n="2"> Dataset available at http://joonsuk.org. 3 Available at https://github.com/vene/marseille.</note>

			<note place="foot" n="4"> The annotators used the GATE annotation tool (Cunningham et al., 2011). 5 Inter-annotator agreement is measured with Krippendorf&apos;s α (Krippendorff, 1980) with respect to elementary unit type (α=64.8%) and links (α=44.1%). A separate paper describing the dataset is under preparation. 6 When two propositions overlap, we keep the one that results in losing the fewest links. For generality, we release the dataset without this preprocessing, and include code to reproduce it; we believe that handling nested argumentative units is an important direction for further research.</note>

			<note place="foot" n="7"> For simplicity and comparability, we follow Stab and Gurevych (2016) in using binary link labels even if links could be of different types. This can be addressed in our model by incorporating &quot;labeled link&quot; factors.</note>

			<note place="foot" n="8"> A tree factor regards each bound variable as an edge in a graph and assigns −∞ scores to configurations that are not valid trees. For inference, we can use maximum spanning arborescence algorithms such as Chu-Liu/Edmonds.</note>

			<note place="foot" n="9"> We use the dynet library (Neubig et al., 2017).</note>

			<note place="foot" n="10"> We train our SVM using SAGA (Defazio et al., 2014) in lightning (Blondel and Pedregosa, 2016).</note>

			<note place="foot" n="11"> Components are described in Section 4. The baselines with inference support only unaries and factors with no parameters, as indicated in the last column. 12 For link F1 scores, however, we find it more intuitive to only consider retrieval of positive links rather than macroaveraged two-class scores.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to André Martins, Andreas Müller, Arzoo Katyiar, Chenhao Tan, Felix Wu, Jack Hes-sel, Justine Zhang, Mathieu Blondel, Tianze Shi, Tobias Schnabel, and the rest of the Cornell NLP seminar for extremely helpful discussions. We thank the anonymous reviewers for their thorough and well-argued feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Counterargumentation and discourse: A case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stergos</forename><surname>Afantenos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ArgNLP</title>
		<meeting>ArgNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Polynomial networks and factorization machines: New insights and efficient training algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Ishihata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Lightning: large-scale linear classification, regression and ranking in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<idno type="doi">10.5281/zenodo.200504</idno>
		<ptr target="https://doi.org/10.5281/zenodo.200504" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamish</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Tablan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niraj</forename><surname>Aswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><surname>Gorrell</surname></persName>
		</author>
		<editor>Horacio Saggion, Johann Petrak, Yaoyong Li, and Wim Peters</editor>
		<imprint>
			<date type="published" when="2011" />
			<pubPlace>Adam Funk, Angus Roberts, Danica Damljanovic, Thomas Heitz, Mark A. Greenwood</pubPlace>
		</imprint>
	</monogr>
	<note>Text Processing with GATE (Version 6</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Defazio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacostejulien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Argumentation mining in user-generated web discourse. Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Investigating LSTMs for joint extraction of opinion entities and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Simple and accurate dependency parsing using bidirectional LSTM feature representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04351</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Content Analysis: An Introduction to Its Methodology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Krippendorff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commtext. Sage</title>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Factor graphs and the sum-product algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">J</forename><surname>Frank R Kschischang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H-A</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loeliger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="498" to="519" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pletscher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Block-coordinate</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Frank-Wolfe optimization for structural SVMs</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A PDTB-styled end-to-end discourse parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="151" to="184" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Priberam: A Turbo Semantic Parser with second order features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana Sc</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Turning on the Turbo: Fast thirdorder non-projective Turbo Parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">B</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">AD3: Alternating directions dual decomposition for MAP inference in graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Mário</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Q</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="495" to="545" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing using spanning tree algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Ribarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Train and test tightness of LP relaxations in structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Meshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PyStruct: learning structured prediction in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2055" to="2060" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
		<title level="m">Swabha Swayamdipta, and Pengcheng Yin. 2017. DyNet: The dynamic neural network toolkit</title>
		<meeting><address><addrLine>Yusuke Oda, Matthew Richardson, Naomi Saphra</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toward machine-assisted participation in eRulemaking: An argumentation model of evaluability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonsuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheryl</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICAIL</title>
		<meeting>ICAIL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conditional random fields for identifying appropriate types of support for propositions in online user comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonsuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Argumentation Mining. Association for Computational Linguistics</title>
		<meeting>the 2nd Workshop on Argumentation Mining. Association for Computational Linguistics<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="39" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint prediction in MST-style discourse parsing for argumentation mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Peldszus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Stede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Here&apos;s my point: Argumentation mining with pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Potash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08994</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07370</idno>
		<title level="m">Parsing argumentation structures in persuasive essays</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Max-margin Markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1453" to="1484" />
			<date type="published" when="2005-09" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
