<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sentiment Domain Adaptation with Multiple Sources</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
							<email>wufangzhao@gmail.com, yfhuang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua National Laboratory for Information Science and Technology Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua National Laboratory for Information Science and Technology Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sentiment Domain Adaptation with Multiple Sources</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="301" to="310"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Domain adaptation is an important research topic in sentiment analysis area. Existing domain adaptation methods usually transfer sentiment knowledge from only one source domain to target domain. In this paper, we propose a new domain adaptation approach which can exploit sentiment knowledge from multiple source domains. We first extract both global and domain-specific sentiment knowledge from the data of multiple source domains using multi-task learning. Then we transfer them to target domain with the help of words&apos; sentiment polarity relations extracted from the un-labeled target domain data. The similarities between target domain and different source domains are also incorporated into the adaptation process. Experimental results on benchmark dataset show the effectiveness of our approach in improving cross-domain sentiment classification performance .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment classification is a hot research topic in natural language processing field, and has many applications in both academic and industrial ar- eas ( <ref type="bibr" target="#b17">Pang and Lee, 2008;</ref><ref type="bibr" target="#b13">Liu, 2012;</ref><ref type="bibr" target="#b21">Wu and Huang, 2016)</ref>. Sentiment classi- fication is widely known as a domain-dependent task <ref type="bibr" target="#b0">(Blitzer et al., 2007;</ref><ref type="bibr" target="#b8">Glorot et al., 2011</ref>). The sentiment classifier trained in one domain may not perform well in another domain. This is because sentiment expressions used in different domain- s are usually different. For example, "boring" * Corresponding author. and "lengthy" are frequently used to express neg- ative sentiments in Book domain. However, they rarely appear in Electronics domain <ref type="bibr" target="#b1">(Bollegala et al., 2011</ref>). Thus a sentiment classifier trained in Electronics domain cannot accurately predict their sentiments in Book domain. In addition, the same word may convey different sentiments in differen- t domains. For example, in Electronics domain "easy" is usually used in positive reviews, e.g., "this digital camera is easy to use." However, it is frequently used as a negative word in Movie domain. For instance, "the ending of this movie is easy to guess." Thus, the sentiment classifier trained in one domain usually cannot be applied to another domain directly <ref type="bibr" target="#b17">(Pang and Lee, 2008)</ref>.</p><p>In order to tackle this problem, sentiment do- main adaptation has been widely studied <ref type="bibr" target="#b13">(Liu, 2012)</ref>. For example, <ref type="bibr" target="#b0">Blitzer et al. (2007)</ref> pro- posed to compute the correspondence among fea- tures from different domains using their associa- tions with pivot features based on structural corre- spondence learning (SCL).  pro- posed a spectral feature alignment (SFA) algorith- m to align the domain-specific words from differ- ent domains in order to reduce the gap between source and target domains. However, all of these methods transfer sentiment information from only one source domain. When the source and target domains have significant difference in feature dis- tributions, the adaptation performance will heav- ily decline. In some cases, the performance of sentiment domain adaptation is even worse than that without adaptation, which is usually known as negative transfer .</p><p>In this paper we propose a new domain adapta- tion approach for cross-domain sentiment classi- fication. Our approach can exploit the sentimen- t information in multiple source domains to re- duce the risk of negative transfer effectively. Our approach consists of two steps, i.e., training and adaptation. At the training stage, we extract two kinds of sentiment models, i.e., the global mod- el and the domain-specific models, from the da- ta of multiple source domains using multi-task learning. The global sentiment model can capture the common sentiment knowledge shared by var- ious domains, and has better generalization per- formance than the sentiment model trained in a single source domain. The domain-specific sen- timent model can capture the specific sentiment knowledge in each source domain. At the adap- tation stage, we transfer both kinds of sentiment knowledge to target domain with the help of the words' sentiment graph of target domain. The sen- timent graph contains words' domain-specific sen- timent polarity relations extracted from the syntac- tic parsing results of the unlabeled data in target domain. Since sentiment transfer between similar domains is more effective than dissimilar domains, we incorporate the similarities between target do- main and different source domains into the adap- tation process. In order to estimate the similarity between two domains, we propose a novel domain similarity measure based on their sentiment graph- s. Extensive experiments were conducted on the benchmark Amazon product review dataset. The experimental results show that our approach can improve the performance of cross-domain senti- ment classification effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Sentiment classification is widely known as a domain-dependent task, since different expres- sions are used to express sentiments in different domains <ref type="bibr" target="#b0">(Blitzer et al., 2007)</ref>. The sentiment clas- sifier trained in one domain may not perform well in another domain. Since there are massive do- mains, it is impractical to annotate enough data for each new domain. Thus, domain adaptation, or so called cross-domain sentiment classification, which transfers the sentiment knowledge from do- mains with sufficient labeled data (i.e., source do- main) to a new domain with no or scarce labeled data (i.e., target domain), has been widely stud- ied. Existing domain adaptation methods main- ly transfer sentiment information from only one source domain. For example, <ref type="bibr" target="#b0">Blitzer et al. (2007)</ref> proposed a domain adaptation method based on structural correspondence learning (SCL). In their method, a set of pivot features are first selected ac- cording to their associations with source domain labels. Then the correspondence among features from source and target domains is computed using their associations with pivot features. In order to reduce the gap between source and target domain- s,  proposed a spectral feature alignment (SFA) algorithm to align the domain- specific sentiment words from different domains into clusters. <ref type="bibr" target="#b9">He et al. (2011)</ref> proposed to extract polarity-bearing topics using joint sentiment-topic (JST) model to expand the feature representation- s of texts from both source and target domain- s. <ref type="bibr" target="#b10">Li et al. (2009)</ref> proposed to transfer sentiment knowledge from source domain to target domain using nonnegative matrix factorization. A com- mon shortcoming of above methods is that if the source and target domains have significantly dif- ferent distributions of sentiment expressions, then the domain adaptation performance will heavily decline ( <ref type="bibr" target="#b11">Li et al., 2013</ref>).</p><p>Using multiple source domains in cross-domain sentiment classification has also been explored. <ref type="bibr" target="#b8">Glorot et al. (2011)</ref> proposed a sentiment domain adaptation method based on a deep learning tech- nique, i.e., Stacked Denoising Auto-encoders. The core idea of their method is to learn a high-level representation that can capture generic concepts using the unlabeled data from multiple domains. <ref type="bibr" target="#b23">Yoshida et al. (2011)</ref> proposed a probabilistic gen- erative model for cross-domain sentiment classi- fication with multiple source and target domains. In their method, each word is assigned three at- tributes, i.e., the domain label, the domain depen- dence/independence label, and sentiment polari- ty. <ref type="bibr" target="#b1">Bollegala et al. (2011)</ref> proposed to construct a sentiment sensitive thesaurus for cross-domain sentiment classification using data from multiple source domains. This thesaurus is used to expand the feature vectors for both training and classifi- cation. However, the similarities between target domain and different source domains are not con- sidered in these methods. In addition, although un- labeled data is utilized in these methods, the useful word-level sentiment knowledge in the unlabeled target domain data is not exploited.</p><p>General-purpose multiple source domain adap- tation methods have also been studied. For ex- ample, <ref type="bibr" target="#b14">Mansour et al. (2009)</ref> proposed a distribu- tion weighted hypothesis combination approach, and gave theoretical guarantees for it. However, this method is based on the assumption that tar- get distribution is some mixture of source distri-butions, which may not hold in sentiment domain adaptation scenario. <ref type="bibr" target="#b6">Duan et al. (2009)</ref> proposed a Domain Adaptation Machine (DAM) method to learn a Least-Squares SVM classifier for target do- main by leveraging the classifiers independently trained in multiple source domains. <ref type="bibr" target="#b3">Chattopadhyay et al. (2011)</ref> explored to assign psuedo label- s to unlabeled samples in the target domain using the classifiers from multiple source domains. Then target domain classifier is trained on these psuedo labeled samples. Compared with these general- purpose domain adaptation methods with multi- ple source domains, our approach is more suit- able for sentiment domain adaptation because our approach exploits more sentiment-related charac- teristics and knowledge, such as the general senti- ment knowledge shared by different domains and the word-level sentiment polarity relations, which is validated by experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sentiment Graph Extraction and Domain Similarity Measure</head><p>In this section we introduce two important com- ponents used in our sentiment domain adaptation approach, i.e., the words' sentiment graph and do- main similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentiment Graph Extraction</head><p>Compared with labeled data, unlabeled data is usually much easier and cheaper to collect on a large scale. Although unlabeled samples are not associated with sentiment labels, they can stil- l provide a lot of useful sentiment information for domain adaptation. For example, if "great" and "quick" are frequently used to describe the same target in the same review of Kitchen domain, then they probably convey the same sentiment polarity in this domain. Since "great" is a general positive word in both Book and Kitchen domains, we can infer that "quick" is also a positive word in Kitchen domain when transferring from Book domain to K- itchen domain. Motivated by above observations, in this paper we propose to extract sentiment polarity relations among words from massive unlabeled data for sen- timent domain adaptation. Two kinds of polarity relations are explored, i.e., sentiment coherent re- lation and sentiment opposite relation. The former means that two words convey the same sentiment polarity while the latter indicates opposite senti- ment polarities. These polarity relations are ex- tracted from the syntactic parsing results accord- ing to manually selected rules. Two rules are used to extract sentiment coherent relations. The first one is that two words are connected by coordi- nating conjunctions such as "and" and "as well as". For example, a review in Kitchen domain may be "it is so high-quality and professional." Since "high-quality" and "professional" are connected by the coordinating conjunction "and", we infer that they probably convey the same sentiment po- larity. The second rule is that two words are not di- rectly connected but are used to describe the same target in the same sentence. For example, a re- view in Electronics domain may be "It is a beau- tiful, durable, easy-to-use camera." Since "beauti- ful", "durable", and "easy-to-use" are all used to describe the same camera in the same review, they tend to convey the same sentiment polarity. We al- so propose two rules for extracting sentiment op- posite relations. The first rule is that two words are connected by adversative conjunctions such as "but" and "however". The second rule is that two words are connected by coordinating conjunctions but there is a negation symbol before one of them.</p><p>For example, a review may be "The battery of this camera is small and not durable." We can infer that "small" and "durable" may convey opposite sentiments when they are used to describe cam- era battery. An illustrative example of extracting sentiment polarity relations from syntactic parsing results is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Based on the sentiment polarity relations among words extracted from the unlabeled data, we can build a words' sentiment graph for each domain. The nodes of the sentiment graph represent words and the edges stand for sentiment polarity relation- s. We denote R ∈ R D×D as the words' sentimen- t graph of a specific domain. R i,j represents the sentiment polarity relation score between words i and j. In this paper we define R i,j as</p><formula xml:id="formula_0">n C i,j −n O i,j n C i,j +n O i,j ,</formula><p>where n C i,j and n O i,j represent the frequencies of words i and j sharing coherent and opposite senti- ment polarity relations respectively in all the unla- beled samples. Thus, R i,j ∈ [−1, 1]. If R i,j &gt; 0, then words i and j tend to convey the same sen- timent polarity. Similarly, if R i,j &lt; 0, then these two words are more likely to convey opposite sen- timents. The absolute value of R i,j represents the confidence of this sentiment polarity relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Domain Similarity</head><p>Different pairs of domains have different senti- ment relatedness <ref type="bibr" target="#b18">(Remus, 2012;</ref>. Researchers have found that sentiment do- main adaptation between similar domains, such as Kitchen and Electronics, is much more effective than that between dissimilar domains, such as K- itchen and Book ( <ref type="bibr" target="#b0">Blitzer et al., 2007;</ref>. Thus, it is beneficial if we take the similar- ity between source and target domains into consid- eration when transferring sentiment knowledge.</p><p>In this paper we explore two methods to mea- sure domain similarity. The first one is based on term distribution. The assumption behind this method is that similar domains usually share more common terms than dissimilar domains. For ex- ample, Smart Phone and Digital Camera domains share many common terms such as "screen", "bat- tery", "light", and "durable", while the term dis- tributions of Digital Camera and Book domains may have significant difference. Term distribu- tion based domain similarity measures, such as A-distance, have been explored in previous work- s ( <ref type="bibr" target="#b0">Blitzer et al., 2007)</ref>. Inspired by <ref type="bibr" target="#b18">(Remus, 2012)</ref>, here we apply Jensen-Shannon divergence to mea- sure domain similarity based on term distribution- s, which is more easy to compute than A-distance. Denote t m ∈ R D×1 as the term distribution of do- main m, where t m w is the probability of term w ap- pearing in domain m. Then the similarity between domains m and n is formulated as:</p><formula xml:id="formula_1">T ermSim(m, n) = 1 − D JS (t m , t n ) = 1 − 1 2 (D KL (t m , t) + D KL (t n , t)),<label>(1)</label></formula><p>where t = 1 2 (t m + t n ) is the average distribution, and D KL (·, ·) is the Kullback-Leibler divergence:</p><formula xml:id="formula_2">D KL (p, q) = D i=1 p i log 2 p i q i .<label>(2)</label></formula><p>We can verify that D JS (t m , t n ) ∈ <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. Thus, the range of T ermSim(m, n) is also <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. The term distribution based domain similarity can measure whether similar words are used in t- wo domains. However, sharing similar terms does not necessarily mean that sentiment expressions are used similarly in these domains. For exam- ple, CPU and Battery are both related to electron- ics. The word "fast" is positive when used to de- scribe CPU. However, it is frequently used as a negative word in Battery domain. For example, "this battery runs out fast." Thus, it is more useful to measure domain similarity based on sentiment word distributions. However, although we can in- fer the sentiment word distributions of source do- mains according to labeled samples, it is difficult to compute the sentiment word distribution of tar- get domain, since the labeled data does not exist or is very scarce in target domain.</p><p>In order to tackle this problem, in this paper we propose to estimate the similarity between two do- mains based on their sentiment graphs. Similar domains usually share more common sentimen- t words and sentiment word pairs than dissimilar domains. In addition, the polarity relation scores of a pair of words in the sentiment graphs of simi- lar domains are also more similar. In other words, they tend to be both positive or negative in these two domains. Motivated by above observations, the domain similarity based on sentiment graph is formulated as follows:</p><formula xml:id="formula_3">SentiSim(m, n) = D w=1 v =w |R m w,v + R n w,v | · N m∩n w,v D w=1 v =w (|R m w,v | · N m w,v + |R n w,v | · N n w,v ) ,<label>(3)</label></formula><p>where R m w,v is the sentiment polarity relation s- core between words w and v in domain m, and N m w,v is its frequency in this domain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Sentiment Domain Adaptation with Multiple Sources</head><p>In this section we introduce our sentiment domain adaptation approach in detail. First we introduce several notations that will be used in following dis- cussions. Assume there are M source domains.</p><p>Denote {X m ∈ R Nm×D , y m ∈ R Nm×1 } as the labeled data in source domain m, where N m is the number of labeled samples and D is the size of feature vector. x m i ∈ R D×1 is the feature vec- tor of the i th sample in domain m, and its senti- ment label is y m i . In this paper we focus on senti-</p><note type="other">ment polarity classification, and y m i ∈ {+1, −1}. Denote w ∈ R D×1 as the global sentiment mod- el extracted from multiple source domains and w m ∈ R D×1 as the domain-specific sentimen- t model of source domain m. Denote w t ∈ R D×1 as the domain-specific sentiment model of target domain. Denote f (x, y, w) as the loss of clas- sifying sample x into label y under linear clas- sification model w. Our approach is flexible to the selection of loss function f , which can be square loss, logistic loss, and hinge loss. Denote R m ∈ R D×D as the sentiment graph knowledge of domain m, and S m,t ∈ [0, 1] as the similarity between source domain m and target domain.</note><p>Our sentiment domain adaptation with multiple sources approach (SDAMS) consists of two step- s, i.e., training and adaptation. At the training stage, the global and domain-specific sentimen- t knowledge are extracted from the data of mul- tiple source domains. And at the adaptation stage, these two kinds of sentiment knowledge are trans- ferred to target domain by incorporating the sen- timent graph knowledge of target domain and the similarities between target and source domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training</head><p>Given the labeled data and the sentiment graph knowledge of multiple source domains, at the training stage, our goal is to train a robust glob- al sentiment model to capture the general senti- ment knowledge shared by various domains and a domain-specific sentiment model for each source domain. The model of the training process is mo- tivated by multi-task learning (Evgeniou and Pon- til </p><p>where α, λ 1 , and λ 2 are nonnegative regularization coefficients. The sentiment classification model of each source domain is decomposed into two com- ponents, i.e., a global one and a domain-specific one. The global sentiment model is shared by al- l source domains and is trained in these domains simultaneously. It is used to capture the general sentiment knowledge, such as the general senti- ment words "great", "worst", "perfect" and so on. The domain-specific sentiment model is trained on the labeled data within one source domain and is used to capture the specific sentiment knowledge of this domain. For example, the domain-specific sentiment word "easy" is a positive word in Elec- tronics domain but is used as a negative word in Movie domain. In Eq. (4), the first term means minimizing the empirical classification loss on the labeled data of multiple source domains. In this way we incorpo- rate the sentiment information in labeled samples into sentiment classifier learning. In the second term we incorporate the sentiment graph knowl- edge of each source domain. It is motivated by graph-guided fused <ref type="bibr">Lasso (Chen et al., 2012)</ref>. If two words have strong coherent (or opposite) sen- timent polarity relations, then we constrain that their sentiment scores are more similar (or dissim- ilar) with each other in the final classification mod- el. The L 1 -norm regularization terms are motivat- ed by <ref type="bibr">Lasso (Tibshirani, 1996)</ref>. It can set many minor sentiment scores in the models to exact ze- ros. Since not all the words convey sentiments, these terms can help conduct sentiment word se- lection. We also incorporate the L 2 -norm regular- ization terms in order to improve model stability in high-dimensional problems, which is inspired by elastic net regularization (Zou and Hastie, 2003).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Adaptation</head><p>At the adaptation stage, we incorporate the glob- al sentiment knowledge, the domain-specific sen- timent knowledge of multiple source domains, the sentiment graph knowledge of target domain, and the domain similarities between target and source domains into a unified framework to learn an ac- curate sentiment classifier for target domain. The model of our adaptation framework is formulated as follows:</p><formula xml:id="formula_5">arg min w t L(wt) = M m=1 Sm,twm − wt 2 2 + λ1wt 2 2 + λ2wt1 + β D i=1 j =i R t i,j |(wi + wt,i) − (wj + wt,j)|,<label>(5)</label></formula><p>where β, λ 1 , and λ 2 are nonnegative regularization coefficients. The final sentiment classifier of the target domain is a linear combination of w and w t , i.e., w+w t , where w is the global sentiment mod- el extracted from multiple source domains at the training stage, and w t is the domain-specific sen- timent model of target domain learned at the adap- tation stage. In the first term of Eq. <ref type="formula" target="#formula_5">(5)</ref>, we transfer the knowledge in domain-specific sentiment mod- els from multiple source domains to w t . Since the sentiment knowledge transfer between similar do- mains is more effective, the transfer of domain- specific sentiment knowledge is weighted by the similarities between target domain and differen- t source domains. If target domain is more similar with source domain m than source domain n (i.e., S m,t &gt; S n,t ), then more domain-specific senti- ment knowledge will be transferred to w t from w m than w n . Through the last term we incorpo- rate the sentiment graph knowledge extracted from massive unlabeled data of target domain into the adaptation process. If two words share strong co- herent (or opposite) sentiment polarity relations in the target domain, then we constrain that their sen- timent scores in the sentiment classification model of target domain are more similar (or dissimilar). This term can help transfer the sentiment knowl- edge from source domains to target domain more effectively. For example, if we know that "great" is a positive word in the global sentiment model and there is a strong coherent polarity relation be- tween "easy" and "great" in Electronics domain, then we can infer that "easy" is also a positive word in this domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset and Experimental Settings</head><p>The dataset used in our experiments is the fa- mous Amazon product review dataset collected by <ref type="bibr" target="#b0">Blitzer et al. (2007)</ref>. It is widely used as a bench- mark dataset for cross-domain sentiment classifi- cation. Four domains, i.e., Book, DVD, Electron- ics and Kitchen, are included in this dataset. Each domain contains 1,000 positive and 1,000 negative reviews. Besides, a large number of unlabeled re- views are provided. The detailed statistics of this dataset are shown in <ref type="table">Table 1</ref>.</p><p>In our experiments, each domain was select- ed in turn as target domain, and remaining do- mains as source domains. In each experiment, we randomly selected N labeled samples from the <ref type="table" target="#tab_0">Table 1: The statistics of the dataset.   Domain  Book  DVD  Electronics Kitchen  positive  1,000  1,000  1,000  1,000  negative  1,000  1,000  1,000  1,000  unlabeled 973,194 122,438  21,009  17,856</ref> source domains to train sentiment models at the training stage. These samples were balanced a- mong different source domains. In order to per- form fair comparisons with baseline methods, fol- lowing (Bollegala et al., 2011), we limited the to- tal number of training samples, i.e., N , to 1,600. The target domain sentiment classifier was test- ed on all the labeled samples of target domain. Following <ref type="bibr" target="#b0">(Blitzer et al., 2007</ref>), unigrams and bi- grams were used as features. The sentiment po- larity relations of bigrams were extracted by ex- panding the polarity relations between unigram- s using modifying relations. For example, from the review "this phone is very beautiful and not expensive," we extract not only sentiment polari- ty relation between "beautiful" and "expensive", but also polarity relation between "beautiful" and "not expensive" (coherent sentiment), and that be- tween "very beautiful" and "expensive" (opposite sentiment), since "very" and "not" are used to modify "beautiful" and "expensive" respectively. Classification accuracy was selected as the eval- uation metric. We manually set β in Eq. (5) to 0.01. The values of α, λ 1 , and λ 2 in Eq. (4) were selected using cross validation. The optimization problems in Eq. (4) and Eq. (5) were solved using alternating direction method of multipliers (ADM- M) <ref type="bibr" target="#b2">(Boyd et al., 2011</ref>). Each experiment was re- peated 10 times independently and average results were reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison of Domain Similarity Measures</head><p>In this section, we conducted experiments to com- pare the effectiveness of the two kinds of domain similarity measures introduced in Section 3.2 in sentiment domain adaptation task. The experi- mental results are summarized in <ref type="figure" target="#fig_3">Fig. 2</ref>. The classification loss function used in our approach is hinge loss. The results of other loss functions show similar patterns.</p><p>From <ref type="figure" target="#fig_3">Fig. 2</ref> we can see that the domain simi- larity measure based on sentiment graph performs consistently better than that based on term distri- bution in our approach. This result validates our assumption in Section 3.2 that the sentiment graph based domain similarity can better model the sen- timent relatedness between different domains than that based on term distribution in sentiment do- main adaptation task. In all the following exper- iments, the sentiment graph based domain similar- ities were used in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance Evaluation</head><p>In this section we conducted experiments to evalu- ate the performance of our approach by comparing it with a series of baseline methods. The meth- ods to be compared are: 1) SCL, domain adap- tation based on structural correspondence learn- ing ( <ref type="bibr" target="#b0">Blitzer et al., 2007)</ref>; 2) SFA, domain adap- tation based on spectral feature alignment (Pan et al., 2010); 3) SCL-com and SFA-com, adapt- ing SCL and SFA to multiple source domain sce- nario by first training a cross-domain sentimen- t classifier in each source domain and then com- bining their classification results using majority voting; 4) SST, cross-domain sentiment classifi- cation by using multiple source domains to con- struct a sentiment sensitive thesaurus for feature expansion (Bollegala et al., 2011); 5) IDDIWP, multiple-domain sentiment analysis by identify- ing domain dependent/independent word polari- ty (Yoshida et al., 2011); 6) DWHC, DAM and CP- MDA, three general-purpose multiple source do- main adaptation methods proposed in <ref type="bibr" target="#b14">(Mansour et al., 2009)</ref>, <ref type="bibr" target="#b6">(Duan et al., 2009)</ref> and (Chattopadhyay et al., 2011) respectively; 7) SDAMS-LS, SDAMS- SVM, and SDAMS-Log, our proposed sentimen- t domain adaptation approaches with square loss, hinge loss, and logistic loss respectively; 8) All- Training, all the domains were involved in the training phase of our approach and there is no adaptation phase. This method is introduced to provide an upper bound for the performance of our approach. The experimental results of these meth- ods are summarized in <ref type="table" target="#tab_0">Table 2</ref>. From <ref type="table" target="#tab_0">Table 2</ref> we can see that our approach achieves the best performance among all the meth- ods compared here. SCL and SFA are famous cross-domain sentiment classification methods. In these methods, the sentiment knowledge is trans- ferred from one source domain to target domain. According to <ref type="table" target="#tab_0">Table 2</ref>, our approach performs sig- nificantly better than them. This result indicates that the sentiment knowledge extracted from one source domain may contain heavy domain-specific bias and may be inappropriate for the target do- main. Our approach can tackle this problem by ex- tracting the global sentiment model from multiple source domains. This global model can capture the general sentiment knowledge shared by vari- ous domains and has better generalization perfor- mance. It can reduce the risk of negative transfer effectively. Our approach also outperforms SCL- com and SFA-com. In SCL-com and SFA-com, the sentiment information in different source domain- s is combined at the classification stage, while in our approach it is combined at the learning stage. The superior performance of our approach com- pared with SCL-com and SFA-com shows that our approach is a more appropriate way to exploit the sentiment knowledge in different source domain- s. SST and IDDIWP also utilize data from mul- tiple source domains as our approach. But our approach can still outperform them. This is be- cause in these methods, the similarities between target domain and different source domains are not considered. Since different domains usually have different sentiment relatedness, our approach can exploit the sentiment information in multi- ple source domains more accurately by incorpo- rating the similarities between target domain and each source domain into the adaptation process. Our approach also outperforms the state-of-the- art general-purpose multiple source domain adap- tation methods, such as DWHC, DAM, and CP- MDA. This is because our approach can exploit more sentiment-related characteristics and knowl- edge for sentiment domain adaptation, such as the general sentiment knowledge shared by various domains, the sentiment graph based domain sim- ilarities, and the word-level sentiment polarity re- lations. Thus, our approach is more suitable for sentiment domain adaptation than these general- purpose multiple source domain adaptation meth- ods. Another observation from <ref type="table" target="#tab_0">Table 2</ref> is that the performance of our approach is quite close to the upper bound, i.e., All-Training, especially in Elec- tronics and Kitchen domains. This result validates the effectiveness of our approach in sentiment do- main adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Case Study</head><p>In this section we conducted several case studies to further explore how our sentiment domain adapta- tion approach works. As an illustrative example, we selected Electronics domain as the target do- main and remaining domains as source domains. The top sentiment words in the global and domain- specific sentiment models learned from the data of multiple source domains are shown in <ref type="table">Table 3</ref>. A subgraph of the sentiment graph extracted from the unlabeled data of target domain (Electronic- s) is shown in <ref type="figure" target="#fig_4">Fig. 3</ref>. The top words in the final domain-specific sentiment model of target domain returned by our approach are shown in <ref type="table">Table 3</ref>.</p><p>From <ref type="table">Table 3</ref> we have following observations. First, the global sentiment model extracted from multiple source domains can capture the gener- al sentiment knowledge quite well. It contain- s many general sentiment words, such as "excel- lent", "great", "waste" and so on. These general sentiment words convey strong sentiment orienta- tions. In addition, their sentiment polarities are consistent in different domains. Thus, the glob- al sentiment model extracted from multiple source domains has good generalization ability and is more suitable for domain adaptation than the sen- timent model trained in a single source domain, which may contain heavy domain-specific senti- ment bias. Second, the domain-specific sentiment models can capture rich specific sentiment expres- sions in each source domain. For example, "easy" is a positive word in Kitchen domain while "re- turn" is a negative word in this domain. Third, different domains have different domain-specific sentiment expressions. For example, "read" is frequently used as a positive word in Book do- main, while it is a negative word in DVD domain. Thus, it is important to separate the global and the domain-specific sentiment knowledge. In addi- tion, although different sentiment expressions are used in different domains, similar domains may share many common domain-specific sentimen- t expressions. For example, "easy" and "works" are positive words in both Electronics and Kitchen domains, and "return" and "broken" are both nega- tive words in them. Thus, transferring the domain- specific sentiment models from similar source do- mains to target domain is helpful. From <ref type="figure" target="#fig_4">Fig. 3</ref> we can see that the sentiment polarity relation- s in the sentiment graph extracted from massive unlabeled data are reasonable. Words with pos- itive relation scores tend to convey similar sen- timents, and words with negative relation scores usually convey opposite sentiments. In addition, this sentiment graph contains rich domain-specific sentiment information in target domain, which is useful to transfer the sentiment knowledge from multiple source domains to target domain. For ex- ample, "excellent", "easy", "simple", and "quick" share the same sentiment polarity in Electronic- s domain according to <ref type="figure" target="#fig_4">Fig. 3</ref>. We can infer that "easy" is positive in this domain using the senti- ment of "excellent" in the global sentiment mod- el and the sentiment relation between "easy" and "excellent". Then we can further infer the sen- timents of the domain-specific sentiment words <ref type="table">Table 3</ref>: The top words in the global and domain-specific sentiment models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global</head><p>Positive excellent, great, best, perfect, love, wonderful, the best, loved, well, fantastic, enjoy, favorite Negative bad, waste, boring, disappointed, worst, poor, disappointing, disappointment, terrible, poorly Book Positive excellent, wonderful, easy, loved, enjoyable, life, fun, favorite, a must, read, important, novel Negative no, boring, disappointing, bad, instead, waste, little, writing, poorly, pages, unfortunately DVD Positive enjoy, hope, loved, season, better than, best, a must, first, superman, classic, times, back Negative worst, boring, bad, the worst, terrible, waste, awful, book, horrible, dull, lame, read, hard Kitchen Positive easy, great, perfect, love, works, easy to, best, little, well, good, nice, long, durable, clean Negative disappointed, back, poor, broken, too, return, off, returned, broke, waste, tried, times, doesn't Electronics Positive excellent, great, perfect, best, love, easy to, easy, little, the best, works, good, nice, wonderful Negative disappointed, poor, waste, too, bad, worst, back, broken, return, horrible, off, tried, poorly "simple" and "quick" in target domain using the polarity of "easy" and their sentiment relations with it, even if they may be covered by no source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presents a sentiment domain adaptation approach which transfers the sentiment knowledge from multiple source domains to target domain. Our approach consists of two steps. First, we ex- tract both global and domain-specific sentiment knowledge from the data of multiple source do- mains. Second, we transfer these two kinds of sen- timent knowledge to target domain with the help of the words' sentiment graph. We proposed to build words' sentiment graph for target domain by extracting their sentiment polarity relations from massive unlabeled data. Besides, we proposed a novel domain similarity measure based on senti- ment graphs, and incorporated the domain similar- ities between target and different source domains into the domain adaptation process. The experi- mental results on a benchmark dataset show that our approach can effectively improve the perfor- mance of cross-domain sentiment classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustrative example of extracting sentiment polarity relations from syntactic parsing results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>We can verify that SentiSim(m, n) ∈ [0, 1]. If two domains have more common sentiment word pairs and the po- larity relation scores of these word pairs are more similar, then these two domains share higher do- main similarity according to Eq. (3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The performance of our approach with different kinds of domain similarity measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An illustrative example of the sentiment graph of Electronics domain. The value on the line represents the sentiment polarity relation score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 2 : The performance of different methods.</head><label>2</label><figDesc></figDesc><table>Book 
DVD Electronics Kitchen 
SCL 
0.7457 0.7630 
0.7893 
0.8207 
SFA 
0.7598 0.7848 
0.7808 
0.8210 
SCL-com 
0.7523 0.7675 
0.7918 
0.8247 
SFA-com 
0.7629 0.7869 
0.7864 
0.8258 
SST 
0.7632 0.7877 
0.8363 
0.8518 
IDDIWP 
0.7524 0.7732 
0.8167 
0.8383 
DWHC 
0.7611 0.7821 
0.8312 
0.8478 
DAM 
0.7563 0.7756 
0.8284 
0.8419 
CP-MDA 
0.7597 0.7792 
0.8331 
0.8465 
SDAMS-LS 
0.7795 0.7880 
0.8398 
0.8596 
SDAMS-SVM 0.7786 0.7902 
0.8418 
0.8578 
SDAMS-Log 0.7829 0.7913 
0.8406 
0.8629 
All-Training 
0.7983 0.8104 
0.8463 
0.8683 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL:HLT</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borja</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Eckstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Davidson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation and its application to early detection of fatigue</title>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="717" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Smoothing proximal gradient method for general structured sparse regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="719" to="752" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain adaptation from multiple sources via auxiliary classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ivor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Regularized multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatically extracting polarity-bearing topics for cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harith</forename><surname>Alani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL:HLT</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="123" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge transformation for cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="716" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Active learning for crossdomain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoushan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxia</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2127" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multitask feature learning via efficient l2,1-norm minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Shuiwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="339" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain adaptation with multiple sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1041" to="1048" />
		</imprint>
	</monogr>
	<note>Mehryar Mohri, and Afshin Rostamizadeh</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cross-domain sentiment classification via spectral feature alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="751" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and trends in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain adaptation using domain similarity-and domain complexity-based instance selection for cross-domain sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Remus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE 12th International Conference on Data Mining Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="717" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Collaborative multi-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Personalized microblog sentiment classification via multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3059" to="3065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microblog sentiment classification with contextual knowledge regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2332" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transfer learning for multiple-domain sentiment analysisłidentifying domain dependent/independent word polarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhisa</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoharu</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1286" to="1291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
