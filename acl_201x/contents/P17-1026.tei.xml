<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A* CCG Parsing with a Supertag and Dependency Factored Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Yoshikawa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
						</author>
						<title level="a" type="main">A* CCG Parsing with a Supertag and Dependency Factored Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="277" to="287"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1026</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a new A* CCG parsing model in which the probability of a tree is decomposed into factors of CCG categories and its syntactic dependencies both defined on bi-directional LSTMs. Our factored model allows the precomputation of all probabilities and runs very efficiently, while mod-eling sentence structures explicitly via dependencies. Our model achieves the state-of-the-art results on English and Japanese CCG parsing. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Supertagging in lexicalized grammar parsing is known as almost parsing <ref type="bibr">(Bangalore and Joshi, 1999)</ref>, in that each supertag is syntactically infor- mative and most ambiguities are resolved once a correct supertag is assigned to every word. Re- cently this property is effectively exploited in A* Combinatory Categorial Grammar (CCG; <ref type="bibr" target="#b23">Steedman (2000)</ref>) parsing ( <ref type="bibr" target="#b15">Lewis and Steedman, 2014;</ref>, in which the probability of a CCG tree y on a sentence x of length N is the product of the probabilities of supertags (cate- gories) c i (locally factored model):</p><formula xml:id="formula_0">P (y|x) = ∏ i∈[1,N ] P tag (c i |x).<label>(1)</label></formula><p>By not modeling every combinatory rule in a derivation, this formulation enables us to employ efficient A* search (see Section 2), which finds the most probable supertag sequence that can build a well-formed CCG tree. Although much ambiguity is resolved with this supertagging, some ambiguity still remains. <ref type="bibr">Fig</ref>  <ref type="figure">Figure 1</ref>: CCG trees that are equally likely under Eq. 1. Our model resolves this ambiguity by mod- eling the head of every word (dependencies).</p><p>parses are derived from the same supertags. <ref type="bibr">Lewis et al.'</ref>s approach to this problem is resorting to some deterministic rule. For example,  employ the attach low heuristics, which is motivated by the right-branching tendency of English, and always prioritizes (b) for this type of ambiguity. Though for English it empirically works well, an obvious limitation is that it does not always derive the correct parse; consider a phrase "a house in Paris with a garden", for which the correct parse has the structure corresponding to (a) instead.</p><p>In this paper, we provide a way to resolve these remaining ambiguities under the locally factored model, by explicitly modeling bilexical dependen- cies as shown in <ref type="figure">Figure 1</ref>. Our joint model is still locally factored so that an efficient A* search can be applied. The key idea is to predict the head of every word independently as in Eq. 1 with a strong unigram model, for which we utilize the scoring model in the recent successful graph-based depen- dency parsing on LSTMs <ref type="bibr" target="#b8">(Kiperwasser and Goldberg, 2016;</ref><ref type="bibr" target="#b2">Dozat and Manning, 2016)</ref>. Specif-ically, we extend the bi-directional LSTM (bi- LSTM) architecture of  predict- ing the supertag of a word to predict the head of it at the same time with a bilinear transformation.</p><p>The importance of modeling structures beyond supertags is demonstrated in the performance gain in , which adds a recursive com- ponent to the model of Eq. 1. Unfortunately, this formulation loses the efficiency of the original one since it needs to compute a recursive neural net- work every time it searches for a new node. Our model does not resort to the recursive networks while modeling tree structures via dependencies.</p><p>We also extend the tri-training method of  to learn our model with dependen- cies from unlabeled data. On English CCGbank test data, our model with this technique achieves 88.8% and 94.0% in terms of labeled and unla- beled F1, which mark the best scores so far.</p><p>Besides English, we provide experiments on Japanese CCG parsing. Japanese employs freer word order dominated by the case markers and a deterministic rule such as the attach low method may not work well. We show that this is actually the case; our method outperforms the simple ap- plication of  in a large margin, 10.0 points in terms of clause dependency accu- racy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Our work is built on A* CCG parsing (Section 2.1), which we extend in Section 3 with a head prediction model on bi-LSTMs (Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Supertag-factored A* CCG Parsing</head><p>CCG has a nice property that since every category is highly informative about attachment decisions, assigning it to every word (supertagging) resolves most of its syntactic structure. <ref type="bibr" target="#b15">Lewis and Steedman (2014)</ref> utilize this characteristics of the gram- mar. Let a CCG tree y be a list of categories ⟨c 1 , . . . , c N ⟩ and a derivation on it. Their model looks for the most probable y given a sentence x of length N from the set Y (x) of possible CCG trees under the model of Eq. 1:</p><formula xml:id="formula_1">ˆ y = arg max y∈Y (x) ∑ i∈[1,N ] log P tag (c i |x).</formula><p>Since this score is factored into each supertag, they call the model a supertag-factored model. Exact inference of this problem is possible by A* parsing , which uses the following two scores on a chart:</p><formula xml:id="formula_2">b(C i,j ) = ∑ c k ∈c i,j log P tag (c k |x), a(C i,j ) = ∑ k∈[1,N ]\[i,j] max c k log P tag (c k |x),</formula><p>where C i,j is a chart item called an edge, which abstracts parses spanning interval <ref type="bibr">[i, j]</ref> rooted by category C. The chart maps each edge to the derivation with the highest score, i.e., the Viterbi parse for C i,j . c i,j is the sequence of categories on such Viterbi parse, and thus b is called the Viterbi inside score, while a is the approximation (upper bound) of the Viterbi outside score. A* parsing is a kind of CKY chart parsing aug- mented with an agenda, a priority queue that keeps the edges to be explored. At every step it pops the edge e with the highest priority b(e) + a(e) and inserts that into the chart, and enqueue any edges that can be built by combining e with other edges in the chart. The algorithm terminates when an edge C 1,N is popped from the agenda.</p><p>A* search for this model is quite efficient be- cause both b and a can be obtained from the uni- gram category distribution on every word, which can be precomputed before search. The heuris- tics a gives an upper bound on the true Viterbi outside score (i.e., admissible). Along with this the condition that the inside score never increases by expansion (monotonicity) guarantees that the first found derivation on C 1,N is always optimal. a(C i,j ) matches the true outside score if the one- best category assignments on the outside words (arg max c k log P tag (c k |x)) can comprise a well- formed tree with C i,j , which is generally not true.</p><p>Scoring model For modeling P tag , <ref type="bibr" target="#b15">Lewis and Steedman (2014)</ref> use a log-linear model with fea- tures from a fixed window context.  extend this with bi-LSTMs, which encode the complete sentence and capture the long range syntactic information. We base our model on this bi-LSTM architecture, and extend it to modeling a head word at the same time.</p><p>Attachment ambiguity In A* search, an edge with the highest priority b + a is searched first, but as shown in <ref type="figure">Figure 1</ref> the same categories (with the same priority) may sometimes derive more than one tree. In <ref type="bibr" target="#b15">Lewis and Steedman (2014)</ref>, they pri- oritize the parse with longer dependencies, which they judge with a conversion rule from a CCG tree to a dependency tree (Section 4).  employ another heuristics prioritizing low attachments of constituencies, but inevitably these heuristics cannot be flawless in any situations. We provide a simple solution to this problem by ex- plicitly modeling bilexical dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bi-LSTM Dependency Parsing</head><p>For modeling dependencies, we borrow the idea from the recent graph-based neural dependency parsing <ref type="bibr" target="#b8">(Kiperwasser and Goldberg, 2016;</ref><ref type="bibr" target="#b2">Dozat and Manning, 2016</ref>) in which each dependency arc is scored directly on the outputs of bi-LSTMs. Though the model is first-order, bi-LSTMs enable conditioning on the entire sentence and lead to the state-of-the-art performance. Note that this mech- anism is similar to modeling of the supertag distri- bution discussed above, in that for each word the distribution of the head choice is unigram and can be precomputed. As we will see this keeps our joint model still locally factored and A* search tractable. For score calculation, we use an ex- tended bilinear transformation by <ref type="bibr" target="#b2">Dozat and Manning (2016)</ref> that models the prior headness of each token as well, which they call biaffine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A* parsing with Supertag and Dependency Factored Model</head><p>We define a CCG tree y for a sentence x = ⟨x i , . . . , x N ⟩ as a triplet of a list of CCG cat- egories c = ⟨c 1 , . . . , c N ⟩, dependencies h = ⟨h 1 , . . . , h N ⟩, and the derivation, where h i is the head index of x i . Our model is defined as follows:</p><formula xml:id="formula_3">P (y|x) = ∏ i∈[1,N ] P tag (c i |x) ∏ i∈[1,N ] P dep (h i |x).<label>(2)</label></formula><p>The added term P dep is a unigram distribution of the head choice. A* search is still tractable under this model. The search problem is changed as: <ref type="figure">Figure 2</ref>: Viterbi inside score for edge e3 under our model is the sum of those of e1 and e2 and the score of dependency arc going from the head of e2 to that of e1 (the head direction changes according to the child categories).</p><formula xml:id="formula_4">ˆ y = arg max y∈Y (x) ( ∑ i∈[1,N ] log P tag (c i |x) + ∑ i∈[1,N ] log P dep (h i |x) ) , John met NP S\NP/NP NP Mary b(e 2 ) b(e 1 ) b(e 3 ) = b(e 1 ) + b(e 2 ) + logP dep (met → John) NP S\NP/NP NP John saw Mary NP S\NP S</formula><p>and the inside score is given by:</p><formula xml:id="formula_5">b(Ci,j) = ∑ ck∈ci,j log Ptag(ck|x) (3) + ∑ k∈[i,j]\{root(h C i,j )} log Pdep(hk|x),</formula><p>where h C i,j is a dependency subtree for the Viterbi parse on Ci,j and root(h) returns the root index. We exclude the head score for the subtree root to- ken since it cannot be resolved inside <ref type="bibr">[i, j]</ref>. This causes the mismatch between the goal inside score b(C1,N ) and the true model score (log of Eq. 2), which we adjust by adding a special unary rule that is always applied to the popped goal edge C1,N .</p><p>We can calculate the dependency terms in Eq. 3 on the fly when expanding the chart. Let the cur- rently popped edge be Ai,k, which will be com- bined with Bk,j into Ci,j. The key observation is that only one dependency arc (between root(h A i,k ) and root(h B k,j )) is resolved at every combination (see <ref type="figure">Figure 2</ref>). For every rule C → A B we can define the head direction (see Section 4) and Pdep is obtained accordingly. For example, when the right child B becomes the head,</p><formula xml:id="formula_6">b(Ci,j) = b(Ai,k) + b(Bk,j) + log Pdep(hl = m|x), where l = root(h A i,k ) and m = root(h B k,j ) (l &lt; m).</formula><p>The Viterbi outside score is changed as:</p><formula xml:id="formula_7">a(Ci,j) = ∑ k∈[1,N ]\[i,j] max ck log Ptag(ck|x) + ∑ k∈L max hk log Pdep(hk|x),</formula><p>where</p><formula xml:id="formula_8">L = [1, N ] \ [k ′ |k ′ ∈ [i, j], root(h C i,j ) ̸ = k ′ ]. We regard root(h C i,j )</formula><p>as an outside word since its head is undefined yet. For every outside word we independently assign the weight of its argmax head, which may not comprise a well-formed de- pendency tree. We initialize the agenda by adding an item for every supertag C and word x i with the score a(C i,i ) = ∑ k∈I\{i} max log P tag (c k |x) + ∑ k∈I max log P dep (h k |x). Note that the depen- dency component of it is the same for every word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Architecture</head><p>Following  and <ref type="bibr" target="#b2">Dozat and Manning (2016)</ref>, we model P tag and P dep using bi- LSTMs for exploiting the entire sentence to cap- ture the long range phenomena. See <ref type="figure">Figure 3</ref> for the overall network architecture, where P tag and P dep share the common bi-LSTM hidden vectors.</p><p>First we map every word x i to their hidden vec- tor r i with bi-LSTMs. The input to the LSTMs is word embeddings, which we describe in Sec- tion 6. We add special start and end tokens to each sentence with the trainable parameters following . For P dep , we use the biaffine transformation in <ref type="bibr" target="#b2">Dozat and Manning (2016)</ref>:</p><formula xml:id="formula_9">g dep i = M LP dep child (r i ), g dep h i = M LP dep head (r h i ), P dep (h i |x)<label>(4)</label></formula><formula xml:id="formula_10">∝ exp((g dep i ) T W dep g dep h i + w dep g dep h i ),</formula><p>where M LP is a multilayered perceptron. </p><formula xml:id="formula_11">g tag i = M LP tag child (r i ), g tag h i = M LP tag head (r h i ),<label>(5)</label></formula><formula xml:id="formula_12">ℓ = (g tag i ) T U tag g tag h i + W tag [ g tag i g tag h i ] + b tag , P tag (c i |x) ∝ exp(ℓ c ),</formula><p>where U tag is a third order tensor. As in Lewis et al. these values can be precomputed before search, which makes our A* parsing quite efficient. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P dep P tag</head><p>Figure 3: Neural networks of our supertag and dependency factored model. First we map every word x i to a hidden vector r i by bi-LSTMs, and then apply biaffine (Eq. 4) and bilinear (Eq. 5) transformations to obtain the distributions of de- pendency heads (P dep ) and supertags (P tag ).</p><p>poses: 1) creation of the training data for the de- pendency component of our model; and 2) extrac- tion of a dependency arc at each combinatory rule during A* search (Section 3.1). Lewis and Steed- man (2014) describe one way to extract dependen- cies from a CCG tree (LEWISRULE). Below in addition to this we describe two simpler alterna- tives (HEADFIRST and HEADFINAL), and see the effects on parsing performance in our experiments (Section 6). See <ref type="figure">Figure 4</ref> for the overview.</p><p>LEWISRULE This is the same as the conversion rule in <ref type="bibr" target="#b15">Lewis and Steedman (2014)</ref>. As shown in <ref type="figure">Figure 4c</ref> the output looks a familiar English de- pendency tree.</p><p>For forward application and (generalized) for- ward composition, we define the head to be the left argument of the combinatory rule, unless it matches either X/X or X/(X\Y ), in which case the right argument is the head. For example, on "Black Monday" in <ref type="figure">Figure 4a</ref> we choose Mon- day as the head of Black. For the backward rules, the conversions are defined as the reverse of the corresponding forward rules. For other rules, Re- movePunctuation (rp) chooses the non punctua- tion argument as the head, while Conjunction (Φ) chooses the right argument. 3 (e) HEADFINAL <ref type="figure">Figure 4</ref>: Examples of applying conversion rules in Section 4 to English and Japanese sentences.</p><note type="other">No , it was n ′ t Black Monday .</note><formula xml:id="formula_13">S /S , NP (S \NP)/NP (S\N P )\(S\N P ) NP/NP NP . &lt;B × &gt; (S \NP)/NP NP &gt; S \NP &lt; S rp S &gt; S rp S (a) English sentence I SUB English ACC speak want . Boku wa eigo wo hanasi tai . NP NP\NP NP NP\NP (S \NP)\NP S \S S \S &lt; &lt; &lt;B 2 NP NP (S \NP)\NP &lt; S \NP &lt; S &lt; S (b)</formula><p>One issue when applying this method for ob- taining the training data is that due to the mis- match between the rule set of our CCG parser, for which we follow <ref type="bibr" target="#b15">Lewis and Steedman (2014)</ref>, and the grammar in English CCGbank (Hockenmaier and Steedman, 2007) we cannot extract dependen- cies from some of annotated CCG trees. <ref type="bibr">4</ref> For this reason, we instead obtain the training data for this method from the original dependency annotations on CCGbank. Fortunately the dependency annota- tions of CCGbank matches LEWISRULE above in most cases and thus they can be a good approxi- mation to it.</p><p>HEADFINAL Among SOV languages, Japanese is known as a strictly head final language, mean- ing that the head of every word always follows it. Japanese dependency parsing ( <ref type="bibr" target="#b27">Uchimoto et al., 1999;</ref><ref type="bibr" target="#b10">Kudo and Matsumoto, 2002</ref>) has exploited this property explicitly by only allowing left-to- right dependency arcs. Inspired by this tradition, we try a simple HEADFINAL rule in Japanese CCG parsing, in which we always select the right argument as the head. For example we obtain the head final dependency tree in <ref type="figure">Figure 4e</ref> from the Japanese CCG tree in <ref type="figure">Figure 4b</ref>.</p><p>HEADFIRST We apply the similar idea as HEADFINAL into English. Since English has the opposite, SVO word order, we define the simple "head first" rule, in which the left argument always becomes the head <ref type="figure">(Figure 4d</ref>).</p><p>Though this conversion may look odd at first sight it also has some advantages over LEWIS- RULE. First, since the model with LEWISRULE is trained on the CCGbank dependencies, at infer- ence, occasionally the two components P dep and P tag cause some conflicts on their predictions. For example, the true Viterbi parse may have a lower score in terms of dependencies, in which case the parser slows down and may degrade the ac- curacy. HEADFIRST, in contract, does not suffer from such conflicts. Second, by fixing the direc- tion of arcs, the prediction of heads becomes eas- ier, meaning that the dependency predictions be- come more reliable. Later we show that this is in fact the case for existing dependency parsers (see Section 5), and in practice, we find that this simple conversion rule leads to the higher parsing scores than LEWISRULE on English (Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Tri-training</head><p>We extend the existing tri-training method to our models and apply it to our English parsers.</p><p>Tri-training is one of the semi-supervised meth- ods, in which the outputs of two parsers on un- labeled data are intersected to create (silver) new training data. This method is successfully applied to dependency parsing ( <ref type="bibr" target="#b30">Weiss et al., 2015)</ref> and CCG supertagging ( .</p><p>We simply combine the two previous ap- proaches.  obtain their sil- ver data annotated with the high quality supertags. Since they make this data publicly available <ref type="bibr">5</ref> , we obtain our silver data by assigning dependency structures on top of them. <ref type="bibr">6</ref> We train two very different dependency parsers from the training data extracted from CCGbank Section 02-21. This training data differs depend- ing on our dependency conversion strategies (Sec- tion 4). For LEWISRULE, we extract the orig- inal dependency annotations of CCGbank. For HEADFIRST, we extract the head first dependen- cies from the CCG trees. Note that we cannot an- notate dependency labels so we assign a dummy "none" label to every arc. The first parser is graph-based RBGParser ( <ref type="bibr" target="#b12">Lei et al., 2014</ref>) with the default settings except that we train an unla- beled parser and use word embeddings of <ref type="bibr" target="#b26">Turian et al. (2010)</ref>. The second parser is transition-based lstm-parser ( <ref type="bibr" target="#b3">Dyer et al., 2015</ref>) with the de- fault parameters.</p><p>On the development set (Section 00), with LEWISRULE dependencies RBGParser shows 93.8% unlabeled attachment score while that of lstm-parser is 92.5% using gold POS tags. Interestingly, the parsers with HEADFIRST de- pendencies achieve higher scores: 94.9% by RBGParser and 94.6% by lstm-parser, sug- gesting that HEADFIRST dependencies are easier to parse. For both dependencies, we obtain more than 1.7 million sentences on which two parsers agree.</p><p>Following , we include 15 copies of CCGbank training set when using these silver data. Also to make effects of the tri-train samples smaller we multiply their loss by 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We perform experiments on English and Japanese CCGbanks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">English Experimental Settings</head><p>We follow the standard data splits and use Sections 02-21 for training, Section 00 for development, and Section 23 for final evaluation. We report la- beled and unlabeled F1 of the extracted CCG se- mantic dependencies obtained using generate program supplied with C&amp;C parser.</p><p>For our models, we adopt the pruning strate- gies in <ref type="bibr" target="#b15">Lewis and Steedman (2014)</ref> and allow at most 50 categories per word, use a variable-width beam with β = 0.00001, and utilize a tag dictio- nary, which maps frequent words to the possible supertags <ref type="bibr">7</ref> . Unless otherwise stated, we only al- low normal form parses <ref type="bibr" target="#b5">(Eisner, 1996;</ref><ref type="bibr" target="#b6">Hockenmaier and Bisk, 2010)</ref>, choosing the same subset of the constraints as <ref type="bibr" target="#b15">Lewis and Steedman (2014)</ref>.</p><p>We use as word representation the concatena- tion of word vectors initialized to GloVe 8 <ref type="bibr" target="#b20">(Pennington et al., 2014)</ref>, and randomly initialized pre- fix and suffix vectors of the length 1 to 4, which is inspired by . All affixes ap- pearing less than two times in the training data are mapped to "UNK".</p><p>Other model configurations are: 4-layer bi- LSTMs with left and right 300-dimensional LSTMs, 1-layer 100-dimensional MLPs with ELU non-linearity <ref type="bibr">(Clevert et al., 2015</ref>) for all M LP dep child , M LP dep head , M LP tag child and M LP tag head , and the Adam optimizer with β 1 = 0.9, β 2 = 0.9, L2 norm (1e −6 ), and learning rate decay with the ratio 0.75 for every 2,500 iteration starting from 2e −3 , which is shown to be effective for training the biaffine parser <ref type="bibr" target="#b2">(Dozat and Manning, 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Japanese Experimental Settings</head><p>We follow the default train/dev/test splits of Japanese CCGbank ( <ref type="bibr" target="#b28">Uematsu et al., 2013)</ref>. For the baselines, we use an existing shift-reduce CCG parser implemented in an NLP tool Jigg 9 <ref type="bibr" target="#b17">(Noji and Miyao, 2016)</ref>, and our implementation of the supertag-factored model using bi-LSTMs.</p><p>For Japanese, we use as word representation the concatenation of word vectors initialized to Japanese Wikipedia Entity Vector 10 , and 100- dimensional vectors computed from randomly initialized 50-dimensional character embeddings through convolution <ref type="bibr">(dos Santos and Zadrozny, 2014</ref>). We do not use affix vectors as affixes are less informative in Japanese. All characters ap- pearing less than two times are mapped to "UNK". We use the same parameter settings as English for bi-LSTMs, MLPs, and optimization.</p><p>One issue in Japanese experiments is evalua- tion. The Japanese CCGbank is encoded in a dif- ferent format than the English bank, and no stan- dalone script for extracting semantic dependen- cies is available yet. For this reason, we evaluate the parser outputs by converting them to bunsetsu    dependencies, the syntactic representation ordi- nary used in Japanese NLP ( <ref type="bibr" target="#b10">Kudo and Matsumoto, 2002</ref>). Given a CCG tree, we obtain this by first segment a sentence into bunsetsu (chunks) using CaboCha <ref type="bibr">11</ref> and extract dependencies that cross a bunsetsu boundary after obtaining the word-level, head final dependencies as in <ref type="figure">Figure 4b</ref>. For ex- ample, the sentence in <ref type="figure">Figure 4e</ref> is segmented as "Boku wa | eigo wo | hanashi tai", from which we extract two dependencies (Boku wa) ← (hanashi tai) and (eigo wo) ← (hanashi tai). We perform this conversion for both gold and output CCG trees and calculate the (unlabeled) attachment accuracy. Though this is imperfect, it can detect important parse errors such as attachment errors and thus can be a good proxy for the performance as a CCG parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">English Parsing Results</head><p>Effect of Dependency We first see how the de- pendency components added in our model affect the performance. <ref type="table" target="#tab_5">Table 1</ref> shows the results on the development set with the several configurations, in which "w/o dep" means discarding the depen- <ref type="bibr">CCGbank C&amp;C (Clark and Curran, 2007)</ref> 85.5 91.7 w/ LSTMs ( <ref type="bibr" target="#b29">Vaswani et al., 2016)</ref> 88.3 - EasySRL (  87.2 - EasySRL reimpl 86.8 92.3 HEADFIRST w/o NF (Ours) <ref type="bibr">87.7 93.4 Tri-training EasySRL (Lewis et al., 2016)</ref> 88.0 92.9 neuralccg (  88.7 93.7 HEADFIRST w/o NF (Ours)</p><formula xml:id="formula_14">11 http://taku910.github.io/cabocha/ Method Labeled Unlabeled</formula><p>88.8 94.0 dency terms of the model and applying the attach low heuristics (Section 1) instead (i.e., a supertag- factored model; Section 2.1). We can see that for both LEWISRULE and HEADFIRST, adding de- pendency terms improves the performance.</p><p>Choice of Dependency Conversion Rule To our surprise, our simple HEADFIRST strategy al- ways leads to better results than the linguistically motivated LEWISRULE. The absolute improve- ments by tri-training are equally large (about 1.0 points), suggesting that our model with dependen- cies can also benefit from the silver data.</p><p>Excluding Normal Form Constraints One ad- vantage of HEADFIRST is that the direction of arcs is always right, making the structures sim- pler and more parsable (Section 5). From another viewpoint, this fixed direction means that the con- stituent structure behind a (head first) dependency tree is unique. Since the constituent structures of CCGbank trees basically follow the normal form (NF), we hypothesize that the model learned with HEADFIRST has an ability to force the outputs in NF automatically. We summarize the results with- out the NF constraints in <ref type="table" target="#tab_6">Table 2</ref>, which shows that the above argument is correct; the number of violating NF rules on the outputs of HEAD- FIRST is much smaller than that of LEWISRULE (89 vs. 283). Interestingly the scores of HEAD- FIRST slightly increase from the models with NF (e.g., 86.8 vs. 86.6 for CCGbank), suggesting that the NF constraints hinder the search of HEAD- FIRST models occasionally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Test Set</head><p>Parsing results on the test set (Section 23) are shown in <ref type="table" target="#tab_7">Table 3</ref>, where we compare our best performing HEADFIRST depen- dency model without NF constraints with the sev- eral existing parsers. In the CCGbank experi-  <ref type="table">Table 4</ref>: Results of the efficiency experiment, where each number is the number of sentences processed per second. We compare our proposed parser against neuralccg and our reimplemen- tation of EasySRL.</p><p>ment, our parser shows the better result than all the baseline parsers except C&amp;C with an LSTM supertagger ( <ref type="bibr" target="#b29">Vaswani et al., 2016)</ref>. Our parser outperforms EasySRL by 0.5% and our reimple- mentation of that parser (EasySRL reimpl) by 0.9% in terms of labeled F1. In the tri-training experiment, our parser shows much increased per- formance of 88.8% labeled F1 and 94.0% unla- beled F1, outperforming the current state-of-the- art neuralccg ( ) that uses recur- sive neural networks by 0.1 point and 0.3 point in terms of labeled and unlabeled F1. This is the best reported F1 in English CCG parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficiency Comparison</head><p>We compare the ef- ficiency of our parser with neuralccg and EasySRL reimpl. <ref type="bibr">12</ref> The results are shown in <ref type="table">Table 4</ref>. For the overall speed (the third row), our parser is faster than neuralccg al- though lags behind EasySRL reimpl. Inspect- ing the details, our supertagger runs slower than those of neuralccg and EasySRL reimpl, while in A* search our parser processes over 7 times more sentences than neuralccg. The delay in supertagging can be attributed to sev- eral factors, in particular the differences in net- work architectures including the number of bi- LSTM layers (4 vs. 2) and the use of bilin- ear transformation instead of linear one. There are also many implementation differences in our parser (C++ A* parser with neural network model implemented with Chainer ( <ref type="bibr" target="#b24">Tokui et al., 2015)</ref>) and neuralccg (Java parser with C++ Tensor- Flow ( <ref type="bibr">Abadi et al., 2015</ref>) supertagger and recur- sive neural model in C++ DyNet ( <ref type="bibr">Neubig et al., 2017)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Japanese Parsing Result</head><p>We show the results of the Japanese parsing exper- iment in   <ref type="figure">Figure 5</ref>: Examples of ambiguous Japanese sen- tence given fixed supertags. The English transla- tion is "I ate the curry I bought yesterday".</p><formula xml:id="formula_15">ta S /S S NP S \NP &gt; S un NP/NP &gt; NP &lt; S Yesterday buy−PAST curry−ACC eat−PAST Kinoo kat − ta karee − wo tabe − ta S /S S NP S \NP un NP/NP &gt; NP &lt; S &gt; S</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>et al. (2016) (Supertag model) is not effective for</head><p>Japanese, showing the lowest attachment score of 81.5%. We observe a performance boost with our method, especially with HEADFINAL dependen- cies, which outperforms the baseline shift-reduce parser by 1.1 points on category assignments and 4.0 points on bunsetsu dependencies. The degraded results of the simple application of the supertag-factored model can be attributed to the fact that the structure of a Japanese sentence is still highly ambiguous given the supertags <ref type="figure">(Fig- ure 5)</ref>. This is particularly the case in construc- tions where phrasal adverbial/adnominal modi- fiers (with the supertag S/S) are involved. The result suggests the importance of modeling depen- dencies in some languages, at least Japanese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>There is some past work that utilizes dependencies in lexicalized grammar parsing, which we review briefly here.</p><p>For Head-driven Phrase Structure Gram- mar (HPSG; <ref type="bibr" target="#b21">Pollard and Sag (1994)</ref>), there are studies to use the predicted dependency structure to improve HPSG parsing accuracy. <ref type="bibr" target="#b22">Sagae et al. (2007)</ref> use dependencies to constrain the form of the output tree. As in our method, for every rule (schema) application they define which child becomes the head and impose a soft constraint that these dependencies agree with the output of the dependency parser. Our method is different in that we do not use the one-best dependency structure alone, but rather we search for a CCG tree that is optimal in terms of dependencies and CCG supertags. <ref type="bibr" target="#b32">Zhang et al. (2010)</ref> use the syntactic dependencies in a different way, and show that dependency-based features are useful for predicting HPSG supertags.</p><p>In the CCG parsing literature, some work op- timizes a dependency model, instead of supertags or a derivation <ref type="bibr">(Clark and Curran, 2007;</ref><ref type="bibr" target="#b31">Xu et al., 2014</ref>). This approach is reasonable given that the objective matches the evaluation metric. Instead of modeling dependencies alone, our method finds a CCG derivation that has a higher dependency score. <ref type="bibr" target="#b13">Lewis et al. (2015)</ref> present a joint model of CCG parsing and semantic role labeling (SRL), which is closely related to our approach. They map each CCG semantic dependency to an SRL relation, for which they give the A* upper bound by the score from a predicate to the most proba- ble argument. Our approach is similar; the largest difference is that we instead model syntactic de- pendencies from each token to its head, and this is the key to our success. Since dependency parsing can be formulated as independent head selections similar to tagging, we can build the entire model on LSTMs to exploit features from the whole sen- tence. This formulation is not straightforward in the case of multi-headed semantic dependencies in their model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have presented a new A* CCG parsing method, in which the probability of a CCG tree is decomposed into local factors of the CCG cat- egories and its dependency structure. By explic- itly modeling the dependency structure, we do not require any deterministic heuristics to resolve at- tachment ambiguities, and keep the model locally factored so that all the probabilities can be pre- computed before running the search. Our parser efficiently finds the optimal parse and achieves the state-of-the-art performance in both English and Japanese parsing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Method</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>1 r 2 r 3 r 4</head><label></label><figDesc></figDesc><table>LSTM 
LSTM 
LSTM 

LSTM 
LSTM 
LSTM 
LSTM 

concat 
concat 
concat 
concat 

x 1 
x 2 
x 3 
x 4 

Bilinear 
Biaffine 

S NP S/S .. 

.. 

x1 x2 x3 .. 

.. 

r </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Parsing results (F1) on English develop-
ment set. "w/o dep" means that the model discards 
dependency components at prediction. 

Method 
Labeled Unlabeled # violations 
CCGbank 
LEWISRULE w/o dep 
85.8 
91.7 
2732 
LEWISRULE 
85.4 
92.2 
283 
HEADFIRST w/o dep 
85.6 
91.6 
2773 
HEADFIRST 
86.8 
93.0 
89 
Tri-training 
LEWISRULE 
86.7 
92.8 
253 
HEADFIRST 
87.7 
93.5 
66 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Parsing results (F1) on English develop-
ment set when excluding the normal form con-
straints. # violations is the number of combina-
tions violating the constraints on the outputs. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Parsing results (F1) on English test 
set (Section 23). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="true"><head>Table 5 . The simple application of Lewis</head><label>5</label><figDesc></figDesc><table>Method 
Category Bunsetsu Dep. 
Noji and Miyao (2016) 
93.0 
87.5 
Supertag model 
93.7 
81.5 
LEWISRULE (Ours) 
93.8 
90.8 
HEADFINAL (Ours) 
94.1 
91.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 5 : Results of Japanese CCGbank.</head><label>5</label><figDesc></figDesc><table>Yesterday buy−PAST 
curry−ACC 
eat−PAST 
Kinoo 
kat − ta 
karee − wo 
tabe − </table></figure>

			<note place="foot" n="1"> Our software and the pretrained models are available at: https://github.com/masashi-y/depccg.</note>

			<note place="foot" n="4"> CCG to Dependency Conversion Now we describe our conversion rules from a CCG tree to a dependency one, which we use in two pur2 This is inspired by the formulation of label prediction in Dozat and Manning (2016), which performs the best among other settings that remove or reverse the dependence between the head model and the supertag model.</note>

			<note place="foot" n="3"> When applying LEWISRULE to Japanese, we ignore the feature values in determining the head argument, which we find often leads to a more natural dependency structure. For example, in &quot;tabe ta&quot; (eat PAST), the category of auxiliary verb &quot;ta&quot; is S f 1 \S f 2 with f1 ̸ = f2, and thus S f 1 ̸ = S f 2. We choose &quot;tabe&quot; as the head in this case by removing the feature values, which makes the category X\X.</note>

			<note place="foot" n="4"> For example, the combinatory rules in Lewis and Steedman (2014) do not contain Nconj → N N in CCGbank. Another difficulty is that in English CCGbank the name of each combinatory rule is not annotated explicitly.</note>

			<note place="foot" n="5"> https://github.com/uwnlp/taggerflow</note>

			<note place="foot" n="6"> We annotate POS tags on this data using Stanford POS tagger (Toutanova et al., 2003).</note>

			<note place="foot" n="7"> We use the same tag dictionary provided with their biLSTM model. 8 http://nlp.stanford.edu/projects/ glove/ 9 https://github.com/mynlp/jigg 10 http://www.cl.ecei.tohoku.ac.jp/ ˜ m-suzuki/jawiki_vector/</note>

			<note place="foot" n="12"> This experiment is performed on a laptop with 4-thread 2.0 GHz CPU.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to Mike Lewis for answering our questions and your Github repository from which we learned many things. We also thank Yuichiro Sawai for the faster LSTM implementa-tion. This work was in part supported by JSPS KAKENHI Grant Number 16H06981, and also by JST CREST Grant Number JPMJCR1301.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning Character-level Representations for Part-of-Speech Tagging</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep Biaffine Attention for Neural Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>CoRR abs/1611.01734</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">TransitionBased Dependency Parsing with Stack Long ShortTerm Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Noah</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<idno type="doi">10.3115/v1/P15-</idno>
		<ptr target="https://doi.org/10.3115/v1/P15-" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient Normal-Form Parsing for Combinatory Categorial Grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P96-1011" />
	</analytic>
	<monogr>
		<title level="m">34th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Normalform parsing for Combinatory Categorial Grammars with generalized composition and type-raising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C10-1053" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="465" to="473" />
		</imprint>
	</monogr>
	<note>Coling 2010 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/J07-3004" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="396" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="313" to="327" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A* Parsing: Fast Exact Viterbi Parse Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N03-1016" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Japanese Dependency Analysis using Cascaded Chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W/W02/W02-2016.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Conference on Natural Language Learning</title>
		<meeting>the 6th Conference on Natural Language Learning<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Global Neural CCG Parsing with Optimality Guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2366" to="2376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Low-Rank Tensors for Scoring Dependency Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P14-1130</idno>
		<ptr target="https://doi.org/10.3115/v1/P14-1130" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1381" to="1391" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint A* CCG Parsing and Semantic Role Labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/D15-1169</idno>
		<ptr target="https://doi.org/10.18653/v1/D15-1169" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1444" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">LSTM CCG Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/N16-1026</idno>
		<ptr target="https://doi.org/10.18653/v1/N16-1026" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A* CCG Parsing with a Supertag-factored Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/D14-1107</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1107" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="990" to="1000" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
		<title level="m">Swabha Swayamdipta, and Pengcheng Yin. 2017. DyNet: The Dynamic Neural Network Toolkit</title>
		<meeting><address><addrLine>Yusuke Oda, Matthew Richardson, Naomi Saphra</addrLine></address></meeting>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Jigg: A Framework for an Easy Natural Language Processing Pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="103" to="108" />
		</imprint>
	</monogr>
	<note>System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<idno type="doi">10.18653/v1/P16-4018</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-4018" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Head-driven phrase structure grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">HPSG Parsing with Shallow Dependency Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P07-1079" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="624" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The Syntactic Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Chainer: a Next-Generation Open Source Framework for Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature-Rich Partof-Speech Tagging with a Cyclic Dependency Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N03-" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Word Representations: A Simple and General Method for Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P10-1040" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Japanese Dependency Structure Analysis Based on Maximum Entropy Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyotaka</forename><surname>Uchimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/E99-1026" />
	</analytic>
	<monogr>
		<title level="m">Ninth Conference of the European Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Integrating Multiple Dependency Corpora for Inducing Wide-coverage Japanese CCG Resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumire</forename><surname>Uematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Hanaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Mima</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P13-1103" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1042" to="1051" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Supertagging With LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Musa</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/N16-1027</idno>
		<ptr target="https://doi.org/10.18653/v1/N16-1027" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="232" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structured Training for Neural Network Transition-Based Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P15-1032</idno>
		<ptr target="https://doi.org/10.3115/v1/P15-1032" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="323" to="333" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Shift-Reduce CCG Parsing with a Dependency Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenduan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P14-1021</idno>
		<ptr target="https://doi.org/10.3115/v1/P14-1021" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="218" to="227" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Simple Approach for HPSG Supertagging Using Dependency Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Yao-Zhong Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsujii</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N10-1090" />
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="645" to="648" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
