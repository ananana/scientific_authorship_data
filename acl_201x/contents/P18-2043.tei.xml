<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conditional Generators of Words Definitions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artyom</forename><surname>Gadetsky</surname></persName>
							<email>artygadetsky@yandex.ru</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Research University Higher School of Economics Samsung-HSE Laboratory</orgName>
								<orgName type="institution">National Research University Higher School of Economics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><forename type="middle">Yakubovskiy</forename><surname>Joom</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Research University Higher School of Economics Samsung-HSE Laboratory</orgName>
								<orgName type="institution">National Research University Higher School of Economics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
							<email>vetrovd@yandex.ru</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Research University Higher School of Economics Samsung-HSE Laboratory</orgName>
								<orgName type="institution">National Research University Higher School of Economics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Conditional Generators of Words Definitions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="266" to="271"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>266</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We explore recently introduced definition modeling technique that provided the tool for evaluation of different distributed vector representations of words through modeling dictionary definitions of words. In this work, we study the problem of word ambiguities in definition modeling and propose a possible solution by employing latent variable modeling and soft attention mechanisms. Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words ambiguity and polysemy leads to performance improvement.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Continuous representations of words are used in many natural language processing (NLP) applica- tions. Using pre-trained high-quality word em- beddings are most effective if not millions of training examples are available, which is true for most tasks in NLP ( <ref type="bibr" target="#b9">Kumar et al., 2016;</ref><ref type="bibr" target="#b7">Karpathy and Fei-Fei, 2015)</ref>. Recently, several unsu- pervised methods were introduced to learn word vectors from large corpora of texts ( <ref type="bibr" target="#b11">Mikolov et al., 2013;</ref><ref type="bibr" target="#b15">Pennington et al., 2014;</ref><ref type="bibr">Joulin et al., 2016)</ref>. Learned vector representations have been shown to have useful and interesting properties. For ex- ample, <ref type="bibr" target="#b11">Mikolov et al. (2013)</ref> showed that vec- tor operations such as subtraction or addition re- flect semantic relations between words. Despite all these properties it is hard to precisely evalu- ate embeddings because analogy relation or word similarity tasks measure learned information indi- rectly.</p><p>Quite recently <ref type="bibr" target="#b13">Noraset et al. (2017)</ref> introduced a more direct way for word embeddings evalu- ation. Authors suggested considering definition modeling as the evaluation task. In definition modeling vector representations of words are used for conditional generation of corresponding word definitions. The primary motivation is that high- quality word embedding should contain all useful information to reconstruct the definition. The im- portant drawback of <ref type="bibr" target="#b13">Noraset et al. (2017)</ref> defini- tion models is that they cannot take into account words with several different meanings. These problems are related to word disambiguation task, which is a common problem in natural language processing. Such examples of polysemantic words as "bank" or "spring" whose meanings can only be disambiguated using their contexts. In such cases, proposed models tend to generate defini- tions based on the most frequent meaning of the corresponding word. Therefore, building models that incorporate word sense disambiguation is an important research direction in natural language processing.</p><p>In this work, we study the problem of word ambiguity in definition modeling task. We pro- pose several models which can be possible so- lutions to it. One of them is based on recently proposed Adaptive Skip Gram model ( <ref type="bibr" target="#b1">Bartunov et al., 2016)</ref>, the generalized version of the orig- inal SkipGram Word2Vec, which can differ word meanings using word context. The second one is the attention-based model that uses the context of a word being defined to determine components of embedding referring to relevant word meaning. Our contributions are as follows: (1) we intro- duce two models based on recurrent neural net- work (RNN) language models, (2) we collect new dataset of definitions, which is larger in number of unique words than proposed in <ref type="bibr" target="#b13">Noraset et al. (2017)</ref> and also supplement it with examples of the word usage (3) finally, in the experiment section we show that our models outperform previously proposed models and have the ability to generate definitions depending on the meaning of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Constructing Embeddings Using Dictionary Definitions</head><p>Several works utilize word definitions to learn em- beddings. For example, <ref type="bibr" target="#b2">Hill et al. (2016)</ref> use defi- nitions to construct sentence embeddings. Authors propose to train recurrent neural network produc- ing an embedding of the dictionary definition that is close to an embedding of the corresponding word. The model is evaluated with the reverse dictionary task. <ref type="bibr" target="#b0">Bahdanau et al. (2017)</ref> suggest using definitions to compute embeddings for out- of-vocabulary words. In comparison to <ref type="bibr" target="#b2">Hill et al. (2016)</ref> work, dictionary reader network is trained end-to-end for a specific task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Definition Modeling</head><p>Definition modeling was introduced in <ref type="bibr" target="#b13">Noraset et al. (2017)</ref> work. The goal of the definition model p(D|w * ) is to predict the probability of words in the definition D = {w 1 , . . . , w T } given the word being defined w * . The joint probability is decomposed into separate conditional probabil- ities, each of which is modeled using the recurrent neural network with soft-max activation, applied to its logits.</p><formula xml:id="formula_0">p(D|w * ) = T t=1 p(w t |w i&lt;t , w * )<label>(1)</label></formula><p>Authors of definition modeling consider follow- ing conditional models and their combinations: Seed (S) -providing word being defined at the first step of the RNN, Input (I) -concatenation of em- bedding for word being defined with embedding of word on corresponding time step of the RNN, Gated (G), which is the modification of GRU cell. Authors use a character-level convolutional neu- ral network (CNN) to provide character-level in- formation about the word being defined, this fea- ture vector is denoted as (CH). One more type of conditioning referred to as (HE), is hypernym re- lations between words, extracted using Hearst-like patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Word Embeddings</head><p>Many natural language processing applications treat words as atomic units and represent them as continuous vectors for further use in machine learning models. Therefore, learning high-quality vector representations is the important task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Skip-gram</head><p>One of the most popular and frequently used vec- tor representations is Skip-gram model. The orig- inal Skip-gram model consists of grouped word prediction tasks. Each task is formulated as a pre- diction of the word v given word w using their in- put and output representations:</p><formula xml:id="formula_1">p(v|w, θ) = exp(in T w out v ) V v =1 exp(in T w out v )<label>(2)</label></formula><p>where θ and V stand for the set of input and out- put word representations, and dictionary size re- spectively. These individual prediction tasks are grouped in a way to independently predict all ad- jacent (with some sliding window) words y = {y 1 , . . . y C } given the central word x:</p><formula xml:id="formula_2">p(y|x, θ) = j p(y j |x, θ)<label>(3)</label></formula><p>The joint probability of the model is written as fol- lows:</p><formula xml:id="formula_3">p(Y |X, θ) = N i=1 p(y i |x i , θ)<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">(X, Y ) = {x i , y i } N i=1</formula><p>are training pairs of words and corresponding contexts and θ stands for trainable parameters.</p><p>Also, optimization of the original Skip-gram objective can be changed to a negative sampling procedure as described in the original paper or hi- erarchical soft-max prediction model <ref type="bibr" target="#b12">(Mnih and Hinton, 2009)</ref> can be used instead of <ref type="formula" target="#formula_1">(2)</ref> to deal with computational costs of the denominator. Af- ter training, the input representations are treated as word vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adaptive Skip-gram</head><p>Skip-gram model maintains only one vector repre- sentation per word that leads to mixing of mean- ings for polysemantic words. <ref type="bibr" target="#b1">Bartunov et al. (2016)</ref> propose a solution to the described prob- lem using latent variable modeling. They extend Skip-gram to Adaptive Skip-gram (AdaGram) in a way to automatically learn the required num- ber of vector representations for each word using Bayesian nonparametric approach. In comparison with Skip-gram AdaGram assumes several mean- ings for each word and therefore keeps several vectors representations for each word. They in- troduce latent variable z that encodes the index of meaning and extend (2) to p(v|z, w, θ). They use hierarchical soft-max approach rather than nega- tive sampling to overcome computing denomina- tor.</p><formula xml:id="formula_5">p(v|z = k, w, θ) = n∈path(v) σ(ch(n)in T wk out n )<label>(5)</label></formula><p>Here in wk stands for input representation of word w with meaning index k and output representa- tions are associated with nodes in a binary tree, where leaves are all possible words in model vo- cabulary with unique paths from the root to the corresponding leaf. ch(n) is a function which re- turns 1 or -1 to each node in the path(·) depending on whether n is a left or a right child of the previ- ous node in the path. Huffman tree is often used for computational efficiency.</p><p>To automatically determine the number of meanings for each word authors use the con- structive definition of Dirichlet process via stick- breaking representation (p(z = k|w, β)), which is commonly used prior distribution on discrete la- tent variables when the number of possible values is unknown (e.g. infinite mixtures).</p><formula xml:id="formula_6">p(z = k|w, β) = β wk k−1 r=1 (1 − β wr ) p(β wk |α) = Beta(β wk |1, α)<label>(6)</label></formula><p>This model assumes that an infinite number of meanings for each word may exist. Providing that we have a finite amount of data, it can be shown that only several meanings for each word will have non-zero prior probabilities.</p><p>Finally, the joint probability of all variables in AdaGram model has the following form:</p><formula xml:id="formula_7">p(Y, Z, β|X, α, θ) = V w=1 ∞ k=1 p(β wk |α)· · N i=1 [p(z i |x i , β) C j=1 p(y ij |z i , x i , θ)]<label>(7)</label></formula><p>Model is trained by optimizing Evidence Lower Bound using stochastic variational inference <ref type="bibr" target="#b4">(Hoffman et al., 2013</ref>) with fully factorized vari- ational approximation of the posterior distribution p(Z, β|X, Y, α, θ) ≈ q(Z)q(β).</p><p>One important property of the model is an abil- ity to disambiguate words using context. More formally, after training on data</p><formula xml:id="formula_8">D = {x i , y i } N i=1</formula><p>we may compute the posterior probability of word meaning given context and take the word vector with the highest probability.:</p><formula xml:id="formula_9">p(z = k|x, y, θ) ∝ ∝ p(y|x, z = k, θ) p(z = k|β, x)q(β)dβ<label>(8)</label></formula><p>This knowledge about word meaning will be further utilized in one of our models as disambiguation(x|y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models</head><p>In this section, we describe our extension to orig- inal definition model. The goal of the extended definition model is to predict the probability of a definition D = {w 1 , . . . , w T } given a word being defined w * and its context C = {c 1 , . . . , c m } (e.g. example of use of this word). As it was motivated earlier, the context will provide proper information about word meaning. The joint probability is also decomposed in the conditional probabilities, each of which is provided with the information about context:</p><formula xml:id="formula_10">p(D|w * , C) = T t=1</formula><p>p(w t |w i&lt;t , w * , C) (9)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">AdaGram based</head><p>Our first model is based on original Input (I) con- ditioned on Adaptive Skip-gram vector represen- tations. To determine which word embedding to provide as Input (I) we disambiguate word being defined using its context words C. More formally our Input (I) conditioning is turning in:</p><formula xml:id="formula_11">h t = g([v * ; v t ], h t−1 ) v * = disambiguation(w * |C)<label>(10)</label></formula><p>where g is the recurrent cell, [a; b] denotes vec- tor concatenation, v * and v t are embedding of word being defined w and embedding of word w t respectively. We refer to this model as Input Adap- tive (I-Adaptive).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Attention based</head><p>Adaptive Skip-gram model is very sensitive to the choice of concentration parameter in Dirich- let process. The improper setting will cause many similar vectors representations with smoothed meanings due to theoretical guarantees on a num- ber of learned components. To overcome this problem and to get rid of careful tuning of this hyper-parameter we introduce following model:</p><formula xml:id="formula_12">h t = g([a * ; v t ], h t−1 ) a * = v * mask mask = σ(W m i=1 AN N (c i ) m + b)<label>(11)</label></formula><p>where is an element-wise product, σ is a logistic sigmoid function and AN N is attention neural network, which is a feed-forward neural network. We motivate these updates by the fact, that after learning Skip-gram model on a large cor- pus, vector representation for each word will ab- sorb information about every meaning of the word. Using soft binary mask dependent on word context we extract components of word embedding rele- vant to corresponding meaning. We refer to this model as Input Attention (I-Attention).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Attention SkipGram</head><p>For attention-based model, we use different em- beddings for context words. Because of that, we pre-train attention block containing embeddings, attention neural network and linear layer weights by optimizing a negative sampling loss function in the same manner as the original Skip-gram model:</p><formula xml:id="formula_13">log σ(v T w O v w I ) + k i=1 E w i ∼Pn(w) [log σ(−v T w i v w I )]<label>(12)</label></formula><p>where v w O , v w I and v w i are vector representa- tion of "positive" example, anchor word and nega- tive example respectively. Vector v w I is computed using embedding of w I and attention mechanism proposed in previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>We collected new dataset of definitions using Ox- fordDictionaries.com (2018) API. Each entry is a triplet, containing the word, its definition and ex- ample of the use of this word in the given meaning. It is important to note that in our data set words can have one or more meanings, depending on the cor- responding entry in the Oxford Dictionary. <ref type="table" target="#tab_0">Table  1 shows basic statistics of the new dataset.   Split  train  val  test  #Words  33,128  8,867  8,850  #Entries  97,855  12,232  12,232  #Tokens 1,078,828 134,486 133,987  Avg length</ref> 11.03 10.99 10.95 <ref type="table">Table 1</ref>: Statistics of new dataset <ref type="figure">Figure 1</ref>: Perplexities of S+I Attention model for the case of pre-training (solid lines) and for the case when the model is trained from scratch (dashed lines).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Pre-training</head><p>It is well-known that good language model can of- ten improve metrics such as BLEU for a particu- lar NLP task ( <ref type="bibr" target="#b6">Jozefowicz et al., 2016)</ref>. According to this, we decided to pre-train our models. For this purpose, WikiText-103 dataset (Merity et al., 2016) was chosen. During pre-training we set v * (eq. 10) to zero vector to make our models purely unconditional. Embeddings for these language models were initialized by Google Word2Vec vec- tors 1 and were fine-tuned. <ref type="figure">Figure 1</ref> shows that this procedure helps to decrease perplexity and prevents over-fitting. Attention Skip-gram vectors were also trained on the WikiText-103.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>Both our models are LSTM networks (Hochre- iter and Schmidhuber, 1997) with an embedding layer. The attention-based model has own em- bedding layer, mapping context words to vector representations. Firstly, we pre-train our mod- els using the procedure, described above. Then, we train them on the collected dataset maximiz- ing log-likelihood objective using Adam <ref type="bibr" target="#b8">(Kingma and Ba, 2014</ref>). Also, we anneal learning rate by <ref type="table">Word  Context  Definition  star  she got star treatment  a person who is very important   star  bright star in the sky  a small circle of a celestial object  or planet that is seen in a circle  sentence  sentence in prison  an act of restraining someone or something  sentence</ref> write up the sentence a piece of text written to be printed head the head of a man the upper part of a human body head he will be the head of the office the chief part of an organization, institution, etc reprint they never reprinted the famous treatise a written or printed version of a book or other publication rape the woman was raped on her way home at night the act of killing invisible he pushed the string through an inconspicuous hole not able to be seen shake my faith has been shaken cause to be unable to think clearly nickname the nickname for the u.s. constitution is 'old ironsides ' a name for a person or thing that is not genuine  a factor of 10 if validation loss doesn't decrease per epochs. We use original Adaptive Skip-gram vectors as inputs to S+I-Adaptive, which were ob- tained from the official repository 2 . We compare different models using perplexity and BLEU score on the test set. BLEU score is computed only for models with the lowest perplexity and only on the test words that have multiple meanings. The re- sults are presented in <ref type="table" target="#tab_1">Table 3</ref>. We see that both models that utilize knowledge about meaning of the word have better performance than the com- peting one. We generated definitions using S + I- Attention model with simple temperature sampling 2 https://github.com/sbos/AdaGram.jl algorithm (τ = 0.1). <ref type="table" target="#tab_0">Table 2</ref> shows the examples. The source code and dataset will be freely avail- able 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In the paper, we proposed two definition models which can work with polysemantic words. We evaluate them using perplexity and measure the definition generation accuracy with BLEU score. Obtained results show that incorporating informa- tion about word senses leads to improved met- rics. Moreover, generated definitions show that even implicit word context can help to differ word meanings. In future work, we plan to explore in- dividual components of word embedding and the mask produced by our attention-based model to get a deeper understanding of vectors representa- tions of words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Examples of definitions generated by S + I-Attention model for the words and contexts from the 
test set. 

Model 
PPL 
BLEU 
S+G+CH+HE (1) 45.62 11.62 ± 0.05 
S+G+CH+HE (2) 46.12 
-
S+G+CH+HE (3) 46.80 
-
S + I-Adaptive (2) 46.08 11.53 ± 0.03 
S + I-Adaptive (3) 46.93 
-
S + I-Attention (2) 43.54 12.08 ± 0.02 
S + I-Attention (3) 
44.9 
-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance comparison between best 
model proposed by Noraset et al. (2017) and our 
models on the test set. Number in brackets means 
number of LSTM layers. BLEU is averaged across 
3 trials. 

</table></figure>

			<note place="foot" n="1"> https://code.google.com/archive/p/word2vec/</note>

			<note place="foot" n="3"> https://github.com/agadetsky/pytorch-definitions</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partly supported by Samsung Re-search, Samsung Electronics, Sberbank AI Lab and the Russian Science Foundation grant <ref type="bibr">17-7120072.</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Bosc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00286</idno>
		<title level="m">Learning to compute word embeddings on the fly</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Breaking sticks and ambiguities with adaptive skip-gram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kondrashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="130" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to understand phrases by embedding the dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="17" to="30" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03651</idno>
		<title level="m">Matthijs Douze, Hérve Jégou, and Tomas Mikolov. 2016. Fasttext.zip: Compressing text classification models</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Definition modeling: Learning to define word embeddings in natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanapon</forename><surname>Noraset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Birnbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st AAAI Conference on Artificial Intelligence, AAAI 2017</title>
		<imprint>
			<publisher>AAAI press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3259" to="3266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oxforddictionaries</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
