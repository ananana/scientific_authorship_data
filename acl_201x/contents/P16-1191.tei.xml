<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supersense Embeddings: A Unified Model for Supersense Interpretation, Prediction, and Utilization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucie</forename><surname>Flekova</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Supersense Embeddings: A Unified Model for Supersense Interpretation, Prediction, and Utilization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2029" to="2041"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
					<note>‡ Ubiquitous Knowledge Processing Lab (UKP-DIPF) German Institute for Educational Research</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Coarse-grained semantic categories such as supersenses have proven useful for a range of downstream tasks such as question answering or machine translation. To date, no effort has been put into integrating the supersenses into distributional word representations. We present a novel joint embedding model of words and supersenses, providing insights into the relationship between words and supersenses in the same vector space. Using these embeddings in a deep neural network model, we demonstrate that the supersense enrichment leads to a significant improvement in a range of downstream classification tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The effort of understanding the meaning of words is central to the NLP community. The word sense dis- ambiguation (WSD) task has therefore received a substantial amount of attention (see <ref type="bibr" target="#b6">Navigli (2009)</ref> or <ref type="bibr" target="#b7">Pal and Saha (2015)</ref> for an overview). Words in training and evaluation data are usually annotated with senses taken from a particular lexical semantic resource, most commonly WordNet <ref type="bibr" target="#b4">(Miller, 1995)</ref>. However, WordNet has been criticized to provide too fine-grained distinctions for end level applica- tions. e.g. in machine translation or information retrieval ( <ref type="bibr">Izquierdo et al., 2009)</ref>. Although some researchers report an improvement in sentiment pre- diction using WSD ( <ref type="bibr" target="#b17">Rentoumi et al., 2009;</ref><ref type="bibr">Akkaya et al., 2011;</ref><ref type="bibr" target="#b30">Sumanth and Inkpen, 2015)</ref>, the pub- lication bias toward positive results ( <ref type="bibr" target="#b12">Plank et al., 2014</ref>) impedes the comparison to experiments with the opposite conclusion, and the contribution of WSD to downstream document classification tasks remains "mostly speculative"( <ref type="bibr">Ciaramita and Altun, 2006</ref>), which can be attributed to the too subtle sense distinctions <ref type="bibr" target="#b6">(Navigli, 2009)</ref>. This is why su- persenses, the coarse-grained word labels based on WordNet's <ref type="bibr">(Fellbaum, 1998)</ref>  Usage of supersense labels has been shown to improve dependency parsing <ref type="bibr">(Agirre et al., 2011</ref>), named entity recognition ( <ref type="bibr">Marrero et al., 2009;</ref><ref type="bibr" target="#b21">Rüd et al., 2011)</ref>, non-factoid question answering ( <ref type="bibr" target="#b31">Surdeanu et al., 2011</ref>), question gen- eration <ref type="bibr">(Heilman, 2011)</ref>, semantic role labeling ( <ref type="bibr">Laparra and Rigau, 2013)</ref>, personality profiling ( <ref type="bibr">Flekova and Gurevych, 2015)</ref>, semantic similar- ity ( <ref type="bibr" target="#b27">Severyn et al., 2013</ref>) and metaphor detection ( <ref type="bibr" target="#b32">Tsvetkov et al., 2013</ref>).</p><p>An alternative path to semantic interpretation follows the distributional hypothesis <ref type="bibr">(Harris, 1954)</ref>. Recently, word vector representations learned with neural-network based language models have con- tributed to state-of-the-art results on various lin- guistic tasks <ref type="bibr">(Bordes et al., 2011;</ref><ref type="bibr" target="#b0">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b10">Pennington et al., 2014;</ref><ref type="bibr">Levy et al., 2015)</ref>.</p><p>In this work, we present a novel approach for incorporating the supersense information into the word embedding space and propose a new method- ology for utilizing these to label the text with su- persenses and to exploit these joint word and su- persense embeddings in a range of applied text classification tasks. Our contributions in this work include the following:</p><p>• We are the first to provide a joint word- and supersense-embedding model, which we make publicly available 1 for the research com- munity. This provides an insight into the word and supersense positions in the vector space through similarity queries and visualizations, and can be readily used in any word embed- ding application.</p><p>• Using this information, we propose a super- sense tagging model which achieves competi- tive performance on recently published social media datasets.</p><p>• We demonstrate how these predicted super- senses and their embeddings can be used in a range of text classification tasks. Using a deep neural network architecture, we achieve an im- provement of 2-6% in accuracy for the tasks of sentiment polarity classification, subjectivity classification and metaphor prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantically Enhanced Word Embeddings</head><p>An idea of combining the distributional informa- tion with the expert knowledge is attractive and has been newly pursued in multiple directions. One of them is creating the word sense or synset em- beddings ( <ref type="bibr">Iacobacci et al., 2015;</ref><ref type="bibr">Chen et al., 2014;</ref><ref type="bibr" target="#b19">Rothe and Schütze, 2015;</ref><ref type="bibr">Bovi et al., 2015)</ref>. While the authors demonstrate the utility of these em- beddings in tasks such as WSD, knowledge base unification or semantic similarity, the contribution of such vectors to downstream document classi- fication problems can be challenging, given the fine granularity of the WordNet senses (cf. the dis- cussion in <ref type="bibr" target="#b6">Navigli (2009)</ref>). As discussed above, supersenses have been shown to be better suited for carrying the relevant amount of semantic informa- tion. An alternative approach focuses on altering the objective of the learning mechanism to capture relational and similarity information from knowl- edge bases <ref type="bibr">(Bordes et al., 2011;</ref><ref type="bibr">Bordes et al., 2012;</ref><ref type="bibr" target="#b39">Yu and Dredze, 2014;</ref><ref type="bibr">Bian et al., 2014;</ref><ref type="bibr" target="#b33">Faruqui and Dyer, 2014;</ref><ref type="bibr">Goikoetxea et al., 2015)</ref>. While, in principle, supersenses could be seen as a relation between a word and its hypernym, to our knowl- edge they have not been explicitly employed in these works. Moreover, an important advantage of our explicit supersense embeddings compared to the retrained vectors is their direct interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Supersense Tagging</head><p>Supersenses, also known as lexicographer files or semantic fields, were originally used to organize lexical-semantic resources <ref type="bibr">(Fellbaum, 1990</ref>). The supersense tagging task was introduced by Cia- ramita and Johnson (2003) for nouns and later expanded for verbs <ref type="bibr">(Ciaramita and Altun, 2006</ref>). Their state-of-the-art system is trained and eval- uated on the SemCor data <ref type="bibr" target="#b2">(Miller et al., 1994)</ref> with an F-score of 77.18%, using a hidden Markov model. Since then, the system, resp. its reimple- mentation by Heilman 2 , was widely used in applied tasks <ref type="bibr">(Agirre et al., 2011;</ref><ref type="bibr" target="#b31">Surdeanu et al., 2011;</ref><ref type="bibr">Laparra and Rigau, 2013)</ref>. Supersense taggers have then been built also for Italian ( <ref type="bibr" target="#b11">Picca et al., 2008</ref>), Chinese (Qiu et al., 2011) and Arabic ( <ref type="bibr" target="#b25">Schneider et al., 2013)</ref>. <ref type="bibr" target="#b34">Tsvetkov et al. (2015)</ref> proposes the us- age of SemCor supersense frequencies as a way to evaluate word embedding models, showing that a good alignment of embedding dimensions to super- senses correlates with performance of the vectors in word similarity and text classification tasks. Re- cently, <ref type="bibr" target="#b12">Johannsen et al. (2014)</ref> introduced a task of multiword supersense tagging on Twitter. On their newly constructed dataset, they show poor do- main adaptation performance of previous systems, achieving a maximum performance with a search- based structured prediction model <ref type="bibr">(Daumé III et al., 2009</ref>) trained on both Twitter and SemCor data. In parallel, <ref type="bibr" target="#b23">Schneider and Smith (2015)</ref> expanded a multiword expression (MWE) annotated corpus of online reviews with supersense information, fol- lowing an alternative annotation scheme focused on MWE. Similarly to <ref type="bibr" target="#b12">Johannsen et al. (2014)</ref>, they find that SemCor may not be a sufficient re- source for supersense tagging adaption to different domains. Therefore, in our work, we explore the potential of using an automatically annotated Ba- belfied Wikipedia corpus ( <ref type="bibr" target="#b26">Scozzafava et al., 2015</ref>) for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Building Supersense Embeddings</head><p>To learn our embeddings, we adapt the freely avail- able sample of 500k articles of Babelfied English Wikipedia ( <ref type="bibr" target="#b26">Scozzafava et al., 2015</ref>). To our knowl- edge, this is one of the largest published and evalu- ated sense-annotated corpora, containing over 500 million words, of which over 100 million are anno- tated with Babel synsets, with an estimated synset annotation accuracy of 77.8%. Few other automati- cally sense-annotated Wikipedia corpora are avail- able (Jordi Atserias and Attardi, 2008; Reese et 1 About 10.9% of families were below the poverty line, including 13.6% of those under age 18. 2 About 10.9% of N.GROUP were below the N.POSSESSION V.CHANGE 13.6% of those under N.ATTRIBUTE 18. 3 About 10.9% of FAMILIES N.GROUP were below the POVERTY LINE N.POSSESSION INCLUDING V.CHANGE 13.6% of those under AGE N.ATTRIBUTE 18.  <ref type="bibr" target="#b4">(Miller, 1995)</ref> using the BabelNet API ( <ref type="bibr" target="#b5">Navigli and Ponzetto, 2012)</ref>, and map these synsets to their corresponding WordNet's super- sense categories <ref type="bibr" target="#b3">(Miller, 1990;</ref><ref type="bibr">Fellbaum, 1990)</ref>. For the nested named entities, only the largest BabelNet span is considered, hence there are no nested supersense labels in our data. In this manner we obtain an alternative Wikipedia corpus, where each word is replaced by its corresponding super- sense (see <ref type="table" target="#tab_1">Table 1</ref>, second row) and another al- ternative corpus where each word has its super- sense appended <ref type="table" target="#tab_1">(Table 1</ref>, third row). Using the Gen- sim <ref type="bibr">( ˇ Rehůřek and Sojka, 2010</ref>) implementation of Word2vec ( <ref type="bibr">Mikolov et al., 2013a</ref>), we applied the skip-gram model with negative sampling on these three Wikipedia corpora jointly (i.e., on the rows 1, 2 and 3 in <ref type="table" target="#tab_1">Table 1</ref>) to produce continuous rep- resentations of words, supersense-disambiguated words and standalone supersenses in one vector space based on the distributional information ob- tained from the data. <ref type="bibr">3</ref> The benefits of learning this information jointly are threefold:</p><p>1. Vectorial representations of the original words are altered (compared to training on text only), taking into account the similarity to super- senses in the vector space 2. Standalone supersenses are positioned in the vector space, enabling insightful similarity queries between words and supersenses, esp. for unannotated words 3. Disambiguated word+supersense vectors of annotated words can be employed similarly to sense embeddings ( <ref type="bibr">Iacobacci et al., 2015;</ref><ref type="bibr">Chen et al., 2014</ref>) to improve downstream tasks and serve as input for supersense disam- biguation or contextual similarity systems</p><p>In the following, the designation WORDS de- notes the experiments with the word embeddings learned on plain Wikipedia text (as in row 1 of <ref type="table" target="#tab_1">Table 1</ref>) while the designation SUPER denotes the experiments with the word embeddings learned jointly on the supersense-enriched Wikipedia (i.e., rows 1, 2 and 3 in <ref type="table" target="#tab_1">Table 1</ref> together). <ref type="table">Table 2</ref> shows the most similar word vectors to each of the verb supersense vectors using cosine simi- larity. Note that while no explicit part-of-speech information is specified, the most similar words hold both the semantic and syntactic information - most of the assigned words are verbs. Furthermore, using a large corpus such as Wikipedia conveniently reduces the current need of lemmatization for supersense tagging, as the words are sufficiently represented in all their forms. The most frequent error originates from assigning the adverbs to their related verb categories, e.g. jokingly to COMMUNICATION and drastically to CHANGE. Such information, however, can be bene- ficial for context analysis in supersense tagging. <ref type="figure" target="#fig_0">Figure 1</ref> displays the verb supersenses using the t-distributed Stochastic Neighbor Embedding (Van der Maaten and Hinton, 2008), a technique de- signed to visualize structures in high-dimensional data. While many of the distances are probable to be dataset-agnostic, such as the proximity of BODY, CONSUMPTION and EMOTION, other appear em- phasized by the nature of Wikipedia corpus, e.g. the proximity of supersenses COMMUNICATION and CREATION or SOCIAL and MOTION, as can be explained by table 2 (see e.g. led and followed).  <ref type="table" target="#tab_3">Table 3</ref> displays the most similar word embeddings for noun supersenses. In accordance with previ- ous work on suppersense tagging <ref type="bibr">(Ciaramita and Altun, 2006;</ref><ref type="bibr" target="#b24">Schneider et al., 2012;</ref><ref type="bibr" target="#b12">Johannsen et al., 2014</ref>), the assignments of more specific super- senses such as FOOD, PLANT, TIME or PERSON are in general more plausible than those for ab- stract concepts such as ACT, ARTIFACT or COG- NITION. The same is visible in <ref type="figure" target="#fig_1">Figure 2</ref>, where these supersense embeddings are more central, with closer neighbors. In contrast to the observations by <ref type="bibr" target="#b24">Schneider et al. (2012)</ref> and <ref type="bibr" target="#b12">Johannsen et al. (2014)</ref>, the COMMUNICATION supersense appears well de- fined, likely due to the character of Wikipedia.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Verb Supersenses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Noun Supersenses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NOUNS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Word Analogy and Word Similarity Tasks</head><p>We also assess the changes between the individual word embeddings learned on plain Wikipedia text (WORDS) and jointly with the supersense-enriched Wikipedia (SUPER). With this aim we perform two standard embedding evaluation tasks: word similarity and word analogy. <ref type="bibr" target="#b0">Mikolov et al. (2013b)</ref> introduce a word analogy dataset containing 19544 analogy questions that can be answered with word vector operations (Paris is to France as Athens are to...?). The questions are grouped into 13 categories. <ref type="table">Table 4</ref>   <ref type="table">Table 4</ref>: Accuracy and standard error on analogy tasks. Tasks related to noun supersense distinctions show the tendency to improve, while syntax-related information is pushed to the background. In most cases, however, the difference is not significant.</p><note type="other">presents our results. Word vectors trained in the SUPER setup achieve better results on groups related to entities, e.g. Family Relations and Citizen to State questions, where the PERSON and LOCATION su- persenses can provide additional information to reduce noise. At the same time, performance on questions such as Opposites or Plurals drops, as this information is pushed to the background. Enriching our data with the recently proposed adjective super- senses (Tsvetkov et al., 2014) could be of interest for these</note><p>Without explicitly exploiting the sense infro- mation, we compare the performance of our text- trained (WORDS) to our jointly trained (SU- PER) word vectors on the following word similar- ity datasets: WordSim353-Similarity (353-S) and WordSim353-Relatedness (353-R) ( <ref type="bibr">Agirre et al., 2009</ref>), MEN dataset ( <ref type="bibr">Bruni et al., 2014</ref>), RG-65 dataset <ref type="bibr" target="#b20">(Rubenstein and Goodenough, 1965)</ref> and <ref type="bibr">MC-30 (Miller and Charles, 1991</ref>  The word embeddings for words trained jointly with supersenses achieve higher performance than those trained solely on the same text without super- senses on 4 out of 5 tasks <ref type="table" target="#tab_6">(Table 5</ref>). In addition, the explicit supersense information could be further exploited, similarly to previous sense embedding works ( <ref type="bibr">Iacobacci et al., 2015;</ref><ref type="bibr" target="#b19">Rothe and Schütze, 2015;</ref><ref type="bibr">Chen et al., 2014</ref>). Furthermore, note that while we report the performance of our embeddings on the word similarity tasks for completeness, there has been a substantial discussion on seeking alter- native ways to quantify embedding quality with the focus on their purpose in downstream applica- tions ( <ref type="bibr">Li and Jurafsky, 2015;</ref><ref type="bibr">Faruqui et al., 2016)</ref>. Therefore, in the remainder of this paper we ex- plore the usefulness of supersense embeddings in text classification tasks.</p><note type="other">). Data: MEN 353-S 353-R RG-65 MC-30 WORDS 73.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Building a Supersense Tagger</head><p>The task of predicting supersenses has recently regained its popularity ( <ref type="bibr" target="#b12">Johannsen et al., 2014;</ref><ref type="bibr" target="#b23">Schneider and Smith, 2015)</ref>, since supersenses pro- vide disambiguating information, useful for numer- ous downstream NLP tasks, without the need of tedious fine-grained WSD. Exploiting our joint em- beddings, we build a deep neural network model to predict supersenses on the Twitter supersense corpus created by <ref type="bibr" target="#b12">Johannsen et al. (2014)</ref>, based on the Twitter NER task ( <ref type="bibr" target="#b18">Ritter et al., 2011</ref>), us- ing the same training data as the authors. <ref type="bibr">45</ref> The datasets follow the token-level annotation which combines the B-I-O flags <ref type="bibr" target="#b14">(Ramshaw and Marcus, 1995</ref>) with the supersense class labels to represent the multiword expression segmentation and super- sense labeling in a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We implement a window-based approach with a multi-channel multi-layer perceptron model using the Theano framework ( <ref type="bibr">Bastien et al., 2012</ref>). With a sliding window of size 5 for the sequence learning setup we extract for each word the following seven feature vectors:</p><p>1. <ref type="bibr">300</ref> After a dropout regularization, the embedding sets are flattened, concatenated and fed into fully con- nected dense layers with a rectified linear unit (ReLU) activation function and a final softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Supersense Prediction</head><p>We evaluate our system on the same Twitter dataset with provided training and development (Tw-R-dev) set and two test sets: Tw-R-eval, reported by Johannsen et al. as RITTER, and Tw-J-eval, reported by Johannsen et al. as IN- HOUSE. Our results are shown in table 6 and com- pared to results reported in previous work by <ref type="bibr" target="#b12">Johannsen et al. (2014)</ref>, with two additional base- lines: The SemCor system of Ciaramita and Altun (2006) and the most frequent sense. Our system achieves comparable performance to the best previ- ously used supervised systems, without using any explicit gazetteers.</p><p>To get an intuition. 6 of how the individual feature vectors contribute to the prediction, we perform an ablation test by removing one feature group at a time. The biggest performance drop in the F-score (2.7-5.4) occurs when removing the the part of <ref type="bibr">6</ref> Intuition, since there are many additional aspects that may affect the performance. For example, we keep the network parameters fixed for the ablation, although the feature vectors are of different lengths. Furthermore, our model performs a concatenation of the feature vectors, hence only the ablation extended to all possible permutations would verify the feature order effect. speech information, followed by the supersense similarity features and supersense frequency priors (0.2-3.0). The casing information has only a minor contribution to Twitter supersense tagging (0-0.9  <ref type="table">Table 6</ref>: Weighted F-score performance on super- sense prediction for the development set and two test sets provided by <ref type="bibr">Johannsen et al. (2004)</ref>. Our system performs comparably to state-of-the-art sys- tems. † For the system of Ciaramita et al, the publicly avaliable reimplementation of Heilman was used</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Using Supersense Embeddings in Document Classification Tasks</head><p>Word sense disambiguation is to some extent an artificial stand-alone task. Despite its popularity, its contribution to downstream document classifica- tion tasks remains rather limited, which might be attributed to the complexity of document prepro- cessing and the errors cumulated along the pipeline. In this section, we demonstrate an alternative, deep learning approach, in which we process the origi- nal text in parallel to the supersense information. The model can then flexibly learn the usefulness of provided input. We demonstrate that the model extended with supersense embeddings outperforms the same model using only word-based features on a range of classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>Both Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) <ref type="bibr">(Hochreiter and Schmidhuber, 1997</ref>) are state-of-the-art seman- tic composition models for a variety of text classifi- cation tasks <ref type="bibr">(Kim, 2014;</ref><ref type="bibr">Li et al., 2015;</ref><ref type="bibr">Johnson and Zhang, 2014</ref>). Recently, their combinations have been proposed, achieving an unprecedented performance ( <ref type="bibr" target="#b22">Sainath et al., 2015)</ref>. We extend the CNN-LSTM approach from the publicly available</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cat sat on a mat</head><p>Noun.Animal Verb.Contact on a Noun.Ar4fact</p><p>LSTM LSTM LSTM LSTM <ref type="figure">Figure 3</ref>: Network architecture. Each of the four different embedding channels serves as input to its CNN layer, followed by an LSTM layer. After- wards, the outputs are concatenated and fed into a dense layer.</p><p>Keras demo 7 , into which we incorporate the su- persense information. <ref type="figure">Figure 3</ref> displays our net- work architecture. First, we use three channels of word embeddings on the plain textual input. The first channel are the 300-dimensional word em- beddings obtained from our enriched Wikipedia corpus. The second embedding channel consists of 41-dimensional vectors capturing the cosine simi- larity of the word to each supersense embedding. The third channel contains the vector of relative frequencies of the word occurring in the enriched Wikipedia together with its supersense, i.e. provid- ing the background supersense distribution for the word. Each of the document embeddings is then convoluted with the filter size of 3, followed by a pooling layer of length 2 and fed into a long- short-term-memory (LSTM) layer. In parallel, we feed as input a processed document text, where the words are replaced by their predicted super- senses. Given that we have the Wikipedia-based supersense embeddings in the same vector space as the word embeddings, we can now proceed to creating the 300-dimensional embedding channel also for the supersense text. As in the plain text channels, we feed also these embeddings into the convolutional and LSTM layers in a similar fashion. Afterwards, we concatenate all LSTM outputs and feed them into a standard fully connected neural network layer, followed by the sigmoid for the bi- nary output. The following subsections discuss our results on a range of classification tasks: subjectiv- ity prediction, sentiment polarity classification and metaphor detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Sentiment Polarity Classification</head><p>Sentiment classification has been a widely explored task which received a lot of attention. The Movie Review dataset, published by Pang and <ref type="bibr" target="#b9">Lee (2005)</ref>  <ref type="bibr">8</ref> , has become a standard machine learning bench- mark task for binary sentence classification. Socher et al. (2011) address this task with recursive au- toencoders and Wikipedia word embeddings, later improving their score using recursive neural net- work with parse trees <ref type="bibr" target="#b29">(Socher et al., 2012)</ref>. Com- petitive results were achieved also by a sentiment- analysis-specific parser ( <ref type="bibr">Dong et al., 2015)</ref>, with a fast dropout logistic regression ( <ref type="bibr" target="#b37">Wang and Manning, 2013)</ref>, and with convolutional neural net- works <ref type="bibr">(Kim, 2014</ref>). <ref type="table">Table 7</ref> compares these ap- proaches to our results for a 10-fold crossvalidation with 10% of the data withheld for parameter tuning. The line WORDS displays the performance using only the leftmost part of our architecture, i.e. only the text input with our word embeddings. The line SUPER shows the result of using the full super- sense architecture. As it can be seen from the  <ref type="table">Table 7</ref>: 10-fold cross-validation accuracy and stan- dard error of our system and as reported in previous work for the sentiment classification task on Pang and Lee (2005) movie review data A detailed analysis of the supersense-tagged data and the classification output revealed that super- senses help to generalize over rare terms. Noun</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positive reviews</head><p>Text Supersenses beating the austin powers film at their own game , verbstative the nounlocation nouncognition nounartifact at their own nouncommunication , this blaxploitation spoof downplays the raunch in favor this nounact nouncommunication verbstative the nouncognition in nouncommunication of gags that rely on the strength of their own cleverness of that verbcognition on the nouncognition of their own nouncognition as oppose to the extent of their outrageousness .</p><p>as verbcommunication to the nounevent of their nounattribute . there is problem with this film that there verbstative nouncognition with this nouncommunication that even 3 oscar winner ca n't overcome , even 3 nounevent nounperson ca n't verbemotion , but it 's a nice girl-buddy movie but it verbstative a nice girl-buddy nouncommunication once it get rock-n-rolling .</p><p>once it verbstative rock-n-rolling godard 's ode to tackle life 's wonderment is a nounperson nouncommunication to verbstative nouncognition 's nouncognition verbstative rambling and incoherent manifesto about the vagueness of topical a rambling and incoherent nouncommunication about the nounattribute of topical excess . in praise of love remain a ponderous and pretentious excess . in nouncognition of nouncognition verbstative a ponderous and pretentious endeavor that 's unfocused and tediously exasperating .</p><p>nounact that verbstative unfocused and tediously exasperating Negative reviews Text Supersenses the action scene has all the suspense of a 20-car pileup , the nounact nounlocation verbstative all the nouncognition of a 20-car nouncognition , while the plot hole is big enough for a train car to drive while the nounlocation verbstative big enough for a nounartifact nounartifact to verbmotion through -if kaos have n't blow them all up .</p><p>through -if nounperson have n't verbcommunication them all up . the scriptwriter is no less a menace to society the nounperson verbstative no less nounstate to noungroup than the film 's character .</p><p>than the nouncommunication nounperson . a very slow , uneventful ride a very slow , uneventful nounact around a pretty tattered old carousel .</p><p>around a pretty tattered old nounartifact . the milieu is wholly unconvincing . . . the nouncognition verbstative wholly unconvincing and the histrionics reach a truly annoying pitch . and the nouncommunication verbstative a truly annoying nounattribute .  <ref type="table" target="#tab_10">Table 8</ref> shows an example of positive and neg- ative reviews which were consistently (5x in re- peated experiments with different random seeds) classified incorrectly with word embeddings and classified correctly with supersense embeddings. Often the wit of unusual expressions is lost for the benefit of generalization. Some improvements ap- pear to be a result of replacing proper names by NOUN.PERSON.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Subjectivity Classification</head><p>Pang and <ref type="bibr" target="#b8">Lee (2004)</ref> demonstrate that the subjec- tivity detection can be a useful input for a sen- timent classifier. They compose a publicly avail- able dataset 9 of 5000 subjective and 5000 objec- tive sentences, classifying them with a reported accuracy of 90-92% and further show that predict- ing this information improves the end-level sen- timent classification on a movie review dataset. <ref type="bibr">Kim (2014)</ref> and <ref type="bibr" target="#b37">Wang and Manning (2013)</ref> fur- ther improve the performance through different machine learning methods. Supersenses are a nat- ural candidate for subjectivity prediction, as we hypothesize that the nouns and verbs in the sub- jective and objective sentences often come from different semantic classes (e.g. VERB.FEELING vs. VERB.COGNITION). We employ the same archi- tecture as in previous task, automatically annotat- ing the words in the documents with their super- senses. Our results are reported in <ref type="table">Table 9</ref>. The supersenses (SUPER) provide an additional infor- mation, improving the model performance by up to 2% over word embeddings (WORDS). The dif- ference between both systems is significant. Based on a manual error analysis, the supersense informa- tion contributes here in a similar manner as in the previous case. Subjective sentences contain more verbs of supersense PERCEPTION, while objective ones more frequently feature the supersenses POS- SESSION and SOCIAL. Nouns in the subjective cat- egory are characterized by supersenses COMMUNI- CATION and ATTRIBUTE, while in objective ones the PERSON and POSSESSION are more frequent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Accuracy SVM ( <ref type="bibr" target="#b8">Pang and Lee, 2004)</ref> 90.0 NB ( <ref type="bibr" target="#b8">Pang and Lee, 2004)</ref> 92.0 CNN <ref type="bibr">(Kim, 2014)</ref> 93.4 F-Dropout ( <ref type="bibr" target="#b37">Wang and Manning, 2013)</ref>   <ref type="table">Table 9</ref>: 10-fold cross-validation accuracy and stan- dard error of our system and as reported in previous work for binary classification on the subjectivity dataset of <ref type="bibr" target="#b8">Pang and Lee (2004)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Metaphor Identification</head><p>Supersenses have recently been shown to provide improvements in metaphor prediction tasks <ref type="bibr">(Gershman et al., 2014</ref>), as they hold the informa- tion of coarse semantic concepts. <ref type="bibr" target="#b35">Turney et al. (2011)</ref> explore the task of discriminating literal and metaphoric adjective-noun expressions. They report an accuracy of 79% on a small dataset rated by five annotators. <ref type="bibr" target="#b32">Tsvetkov et al. (2013)</ref> pursue this work further by constructing and publishing a dataset of 985 literal and 985 methaphorical adjective-noun pairs <ref type="bibr">10</ref> and classify them. Gersh- man et al. <ref type="formula">(2014)</ref> further expand on this work using 64-dimensional vector-space word representations constructed by <ref type="bibr" target="#b33">Faruqui and Dyer (2014)</ref> for clas- sification. They report a state-of-the-art F-score of 85% with random decision forests, including also abstractness and imageability features <ref type="bibr" target="#b38">(Wilson, 1988</ref>) and supersenses from WordNet, aver- aged across senses.</p><p>System F1-score on test set ( <ref type="bibr">Gershman et al., 2014</ref> Table 10: F1-score and a standard error on a pro- vided test set for the adjective-noun metaphor pre- diction task <ref type="bibr">Gershman et al. (2014)</ref>. WORDS: word embeddings only, SUPER: multi-channel word em- beddings with the supersense similarity and fre- quency vectors added</p><p>Since this setup is simpler than the sentence clas- sification tasks, we use only a subset of our archi- tecture, specifically the left half of <ref type="figure">Figure 3</ref>, i.e. our word embeddings, similarity vectors and super- sense frequency vectors. Since there are only two words in each document, we leave out the LSTM layer. We merge the similarity and frequency lay- ers by multiplication and concatenate the result to the word embedding convolution, feeding the out- put of the concatenation directly to the dense layer. <ref type="table" target="#tab_1">Table 10</ref> shows our results on a provided test set. Based on McNemar's test, there is a significant dif- ference (p &lt; 0.01) between our system based on words only and the one with supersenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Unlike previous research on supersenses, our work is not based on a manually produced gold stan-dard, but on an automatically annotated large cor- pus. While <ref type="bibr" target="#b26">Scozzafava et al. (2015)</ref> report a high accuracy estimate of 77.8% on sense level, the performance and possible bias on tagged super- senses are yet to be evaluated. We are also aware that some of the previously proposed approaches for building word sense embeddings <ref type="bibr" target="#b19">(Rothe and Schütze, 2015;</ref><ref type="bibr">Chen et al., 2014;</ref><ref type="bibr">Iacobacci et al., 2015)</ref> could be eventually extended to supersenses. We strongly encourage the authors to do so and perform a contrastive evaluation comparing these methods. Additionaly, a different level of granu- larity of the concepts, such as WordNet Domains <ref type="bibr">(Magnini and Cavaglia, 2000</ref>) could be explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions and Future Work</head><p>We have presented a novel joint embedding set of words and supersenses, which provides a new insight into the word and supersense positions in the vector space. We demonstrated the utility of these embeddings for predicting supersenses and manifested that the supersense enrichment can lead to a significant improvement in a range of down- stream classification tasks, using our embeddings in a neural network model. The outcomes of this work are available to the research community. 11 . In follow-up work, we aim to apply our embedding method on smaller, yet gold-standard corpora such as <ref type="bibr">SemCor (Miller et al., 1994)</ref> and STREUSLE ( <ref type="bibr" target="#b23">Schneider and Smith, 2015)</ref> to examine the impact of the corpus choice in detail and extend the train- ing data beyond WordNet vocabulary. Moreover, the coarse semantic categorization contained in su- persenses was shown to be preserved in translation ( <ref type="bibr" target="#b25">Schneider et al., 2013)</ref>, making them a perfect can- didate for a multilingual adaptation of the vector space, e.g. extending <ref type="bibr" target="#b33">Faruqui and Dyer (2014)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Verb supersense embeddings visualized in the vector space (t-SNE)</figDesc><graphic url="image-1.png" coords="4,72.00,62.81,218.27,158.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Noun supersense embeddings (t-SNE)</figDesc><graphic url="image-2.png" coords="4,72.00,563.10,174.61,176.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>lexicographer files, have recently gained attention for text classification tasks. Supersenses contain 26 labels for nouns, such as ANIMAL, PERSON or FEELING and 15 labels for verbs, such as COMMUNICATION, MOTION or COGNITION.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Example of plain (1), generalized (2) and 
disambiguated (3) Wikipedia 

al., 2010). However, their annotation quality was 
assessed only on the training domain and as At-
serias et al. state (p.2316): "Wikipedia text differs 
significantly ... from the corpora used to train the 
taggers ... Therefore the quality of these NLP pro-
cessors is considerably lower than the results of 
the evaluation in-domain." 
We map the Babel synsets to WordNet 3.0 
synsets </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Top 10 most similar word embeddings for 
noun supersense vectors </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>categories.</figDesc><table>Group/Vectors: 
WORDS SUPER 
Capitals -common 
91.1 
94.7±0.99 
Capitals -world 
87.6 
89.5±0.69 
City in state 
65.2 
65.7±1.03 
Nationality to state 
94.5 
95.2±0.58 
Family relations 
93.0 
94.4±1.28 
Opposites 
56.7 
54.6±3.21 
Plurals 
89.4 
86.4±1.08 
Comparatives 
90.6 
90.4±0.85 
Superlatives 
79.4 
79.6±1.83 
Adjective to adverb 
20.2 
22.2±1.53 
Present to participle 64.2 
64.6±1.57 
Present to past 
60.0 
59.2±1.30 
3rd person verbs 
84.3 
82.1±1.44 
Total 
75.0 
76.0±0.28 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Performance of our vectors (Spearman's ρ) 
on five similarity datasets. Results indicate a trend 
of better performance of vectors trained jointly with 
supersenses. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>System/Data: 
Tw-R-dev 
Tw-R-eval 
Tw-J-eval 
Baseline and upper bound 
Most frequent sense 
47.54 
44.98 
38.65 
Inter-annotator agreement 
69.15 
61.15 
SemCor-trained systems 
(Ciaramita and Altun, 2006)  † 
48.96 
45.03 
39.65 
Searn (Johannsen et al., 2014) 
56.59 
50.89 
40.50 
HMM (Johannsen et al., 2014) 
57.14 
50.98 
41.84 
Ours Semcor 
54.47 
50.30 
35.61 
Twitter-trained systems 
Searn (Johannsen et al., 2014) 
67.72 
57.14 
42.42 
HMM (Johannsen et al., 2014) 
60.66 
51.40 
41.60 
Ours Twitter (all features) 
61.12 
57.16 
41.97 
Ours Twitter no casing 
61.06 
56.20 
41.13 
Ours Twitter no similarities 
63.47 
56.78 
39.44 
Ours Twitter no frequencies 
61.10 
57.32 
39.02 
Ours Twitter no part-of-speech 
57.08 
54.45 
36.50 
Ours Twitter no word embed. 
57.57 
53.43 
34.91 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Example of documents classified incorrectly with word embeddings and correctly with word and 
supersense embeddings on Pang and Lee (2005) movie review data. 

concepts such as GROUP, LOCATION, TIME and 
PERSON appear somewhat more frequently in posi-
tive reviews while certain verb supersenses such as 
PERCEPTION, SOCIAL and COMMUNICATION are 
more frequent in the negative ones. On the other 
hand, the supersense tagging introduces additional 
errors too -for example the director's cut is persis-
tently classified into FOOD. 
</table></figure>

			<note place="foot" n="1"> https://github.com/UKPLab/ acl2016-supersense-embeddings</note>

			<note place="foot" n="2"> https://github.com/kutschkem/ SmithHeilmann_fork/tree/master/ MIRATagger</note>

			<note place="foot" n="3"> The embeddings are learned using skip-gram as training algorithm with downsampling of 0.001 higher-frequency words, negative sampling of 5 noise words, minimal word frequency of 100, window of size 2 and alpha of 0.025, using 10 epochs to produce 300-dimensional vectors. Our experiments with less dimensions and with the CBOW model performed worse.</note>

			<note place="foot" n="4"> https://github.com/kutschkem/ SmithHeilmann_fork/tree/master/ MIRATagger/data 5 https://github.com/coastalcph/ supersense-data-twitter</note>

			<note place="foot" n="7"> https://github.com/fchollet/keras/ blob/master/examples/imdb_cnn_lstm.py</note>

			<note place="foot" n="8"> http://www.cs.uic.edu/liub/FBS/ sentiment-analysis.html</note>

			<note place="foot" n="9"> https://www.cs.cornell.edu/people/ pabo/movie-review-data/</note>

			<note place="foot" n="10"> http://www.cs.cmu.edu/ ˜ ytsvetko/ metaphor/datasets.zip</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the Volkswagen Foundation as part of the Lichtenberg Professor-ship Program under grant No. I/82806 and by the German Research Foundation under grant No. GU 798/14-1. Additional support was provided by the German Federal Ministry of Education and Re-search (BMBF) as a part of the Software Campus program under the reference 01-S12054 and by the German Institute for Educational Research (DIPF). We thank the anonymous reviewers for their input.</p><p>11 https://github.com/UKPLab/ acl2016-supersense-embeddings</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics-Human Language Technologies</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contextual correlates of semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><forename type="middle">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Cognitive Processes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using a semantic concordance for sense identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shari</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Landes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert G</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Human Language Technology</title>
		<meeting>the workshop on Human Language Technology</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="240" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nouns in WordNet: a lexical inheritance system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="245" to="264" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">WordNet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="217" to="250" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Word sense disambiguation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Word sense disambiguation: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Ranjan Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diganta</forename><surname>Saha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01346</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">271</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supersense tagger for italian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Picca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Massimiliano Gliozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Language Resources and Evaluation</title>
		<meeting>the Sixth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Importance weighting and unsupervised domain adaptation of pos taggers: a negative result</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="968" to="973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combining contextual and structural information for supersense tagging of Chinese unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Likun</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqiu</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Text chunking using transformation-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitch</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Workshop on Very Large Corpora</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="82" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Word-sense disambiguated multilingual Wikipedia corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Reese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda Torrent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuadros</forename><surname>Montserrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Oller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><forename type="middle">Rigau</forename><surname>Padró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Claramunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Language Resources and Evaluation</title>
		<meeting>the 7th International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Radimřehůřekradimˇradimřehůřek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks<address><addrLine>Valletta, Malta, May. ELRA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sentiment analysis of figurative language using a word sense disambiguation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassiliki</forename><surname>Rentoumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Giannakopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vangelis</forename><surname>Karkaletsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George A</forename><surname>Vouros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Recent Advances in Natural Language Processing</title>
		<meeting>the Conference on Recent Advances in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="370" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods on Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autoextend: Extending word embeddings to embeddings for synsets and lexemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1793" to="1803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Piggyback: Using search engines for robust cross-domain named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Rüd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="965" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional, long shortterm memory, fully connected deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Tara N Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasim</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4580" to="4584" />
		</imprint>
	</monogr>
	<note>2015 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A corpus and model integrating multiword expressions and supersenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coarse lexical semantic annotation with supersenses: an arabic case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrang</forename><surname>Mohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oflazer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="253" to="258" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Supersense tagging for arabic: the mt-in-the-middle attack. Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrang</forename><surname>Mohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Olflazer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic identification and disambiguation of concepts and named entities in the multilingual wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Scozzafava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AI* IA 2015 Advances in Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning semantic textual similarity with structural representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="714" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods on Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">How much does word sense disambiguation help in sentiment analysis of micropost data?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiraag</forename><surname>Sumanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6TH Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">115</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to rank answers to nonfactoid questions from web collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="351" to="383" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-lingual metaphor detection using common semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatole</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Mukomel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The First Workshop on Metaphor in NLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">45</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Augmenting english adjective senses with supersenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Archna</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluation of word vector representations by subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods on Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods on Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Literal and metaphorical sense identification through concrete and abstract context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Peter D Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohai</forename><surname>Assaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on the Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on the Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="680" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast dropout training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">MRC psycholinguistic database: Machine-usable dictionary, version 2.00. Behavior Research Methods, Instruments, &amp; Computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving lexical embeddings with semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">MGNC-CNN: A simple approach to exploiting multiple word embeddings for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00968</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
