<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Off-topic Response Detection for Spontaneous Spoken English Assessment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Malinin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Trumpington St</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogier</forename><forename type="middle">C</forename><surname>Van Dalen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Trumpington St</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Trumpington St</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><forename type="middle">M</forename><surname>Knill</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Trumpington St</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Trumpington St</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Off-topic Response Detection for Spontaneous Spoken English Assessment</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1075" to="1084"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Automatic spoken language assessment systems are becoming increasingly important to meet the demand for English second language learning. This is a challenging task due to the high error rates of, even state-of-the-art, non-native speech recognition. Consequently current systems primarily assess fluency and pronunciation. However, content assessment is essential for full automation. As a first stage it is important to judge whether the speaker responds on topic to test questions designed to elicit spontaneous speech. Standard approaches to off-topic response detection assess similarity between the response and question based on bag-of-words representations. An alternative framework based on Recurrent Neural Network Language Models (RNNLM) is proposed in this paper. The RNNLM is adapted to the topic of each test question. It learns to associate example responses to questions with points in a topic space constructed using these example responses. Classification is done by ranking the topic-conditional posterior probabilities of a response. The RNNLMs associate a broad range of responses with each topic, incorporate sequence information and scale better with additional training data, unlike standard methods. On experiments conducted on data from the Business Language Testing Service (BULATS) this approach outper-forms standard approaches.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As English has become the global lingua franca, there is growing demand worldwide for assess- ment of English as a second language <ref type="bibr" target="#b16">(Seidlhofer, 2005)</ref>. To assess spoken communication, sponta- neous speech is typically elicited through a series of questions such as 'describe the photo' or 'plan a meeting'. Grades are awarded based on a candi- date's responses.</p><p>Automatic assessment systems are becoming at- tractive as they allow second language assessment programmes to economically scale their opera- tions while decreasing throughput time and pro- vide testing on demand. Features for automatic graders are derived from the audio and from hy- potheses produced by automatic speech recogni- tion (ASR) systems. The latter is highly errorful due to the large variability in the input speech; disfluencies common to spontaneous speech, non- native accents and pronunciations. Current sys- tems, such as ETS' SpeechRater ( <ref type="bibr" target="#b24">Zechner et al., 2009</ref>) and Pearson's AZELLA ( <ref type="bibr" target="#b10">Metallinou and Cheng, 2014)</ref>, primarily assess pronunciation and fluency. Although these are clearly indicative of spoken language ability, full assessment of spo- ken communication requires judgement of high- level content and communication skills, such as re- sponse construction and relevance. The first stage of this is to assess whether the responses are off- topic, that is, has the candidate misunderstood the question and/or memorised a response.</p><p>While there has been little work done on de- tecting off-topic responses for spoken language assessment, detection of off-topic responses and content assessment has been studied for essay as- sessment. One approach for essay content as- sessment uses features based on semantic similar- ity metrics between vector space representations of responses. Common vector representations in- clude lexical Vector Space Models and Latent Se- mantic Analysis (LSA) <ref type="bibr" target="#b20">(Yannakoudakis, 2013)</ref>. This approach was first applied to spoken assess-ment in <ref type="bibr" target="#b19">(Xie et al., 2012</ref>) and then in <ref type="bibr" target="#b6">(Evanini et al., 2013)</ref>. Following this, <ref type="bibr" target="#b21">(Yoon and Xie, 2014)</ref> investigated the detection of responses for which an automatic assessment system will have diffi- culty in assigning a valid score, of which off- topic responses are a specific type. A decision tree classifier is used with features based on co- sine similarity between a test response and tf-idf vectors of both aggregate example responses and questions, as well as pronunciation and fluency. In ( <ref type="bibr" target="#b5">Evanini and Wang, 2014</ref>) text reuse and pla- giarism in spoken responses are detected using a decision tree classifier based on vector similarity and lexical matching features which compare a re- sponse to a set of example 'source texts' . This task is similar to off-topic response detection in that it is based on comparing a test response to example responses. Thus, a standard approach to off-topic response detection would be based on measuring the similarity between vector represen- tations of a spoken response and the test ques- tion. A major deficiency of this approach is that it is based on bag-of-words vector representations, which loses information about the sequential na- ture of speech, which is important to evaluating response construction and relevance. Addition- ally, adapting the approach to model a range of responses for each topic causes classification time to scale poorly with training data size and the num- ber of questions.</p><p>To address these issues a general off-topic content detection framework based on topic adapted Recurrent Neural Network language mod- els (RNNLM) has been developed and applied to off-topic response detection for spoken language assessment. This framework uses example re- sponses to test questions in training of the lan- guage model and construction of the topic-space. The RNNLM learns to associate the example re- sponses with points in the topic-space. Classi- fication is done by ranking the topic-conditional posterior probabilities of a response. The advan- tage of this approach is that sequence information can be taken into account and broad ranges of re- sponses can be associated with each topic with- out affecting classifcation speed. Two topic vec- tor representations are investigated: Latent Dirich- let Allocation (LDA) ( <ref type="bibr" target="#b0">Blei et al., 2003;</ref><ref type="bibr" target="#b7">Griffiths and Steyvers, 2004</ref>) and Latent Semantic Analysis (LSA) ( <ref type="bibr" target="#b8">Landauer et al., 1998</ref>). They are compared to standard approaches on data from the Cam- bridge Business English (BULATS) exam.</p><p>The rest of this paper is structured as follows: Section 2 discusses the RNNLM adaptation and topic spaces; Section 3 discusses approaches to topic detection; Section 4 presents data sets and experimental infrastructure; Section 5 analyzes experimental results; Section 6 concludes the pa- per.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Topic Adapted RNNLMs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RNNLM Architecture</head><p>A statistical language model is used to model the semantic and syntactic information in text in the form of a probability distribution over word se- quences. It assigns a probability to a word se- quence w = {w 0 , w 1 , · · · , w L } as follows:</p><formula xml:id="formula_0">P(w i |w i−1 , · · · , w 0 ) = P(w i |h i−1 0 ) P(w) = L i=1 P(w i |h i−1 0 ) (1)<label>(2)</label></formula><p>where w 0 is the start of sentence symbol &lt;s&gt;. In this work a language model is trained to model example responses to questions on a spoken lan- guage assessment test. P(w i |h i−1 0 ) can be es- timated by a number of approaches, most no- tably N-grams and Recurrent Neural Networks ( <ref type="bibr" target="#b12">Mikolov et al., 2010)</ref>.</p><p>Recurrent Neural Network language models (RNNLMs) <ref type="figure" target="#fig_1">(Figure 1</ref> RNNLMs can be adapted by adding a feature vector f which represents information absent from the RNN ( <ref type="bibr" target="#b11">Mikolov and Zweig, 2012)</ref>. In this work, the vector representation of a spoken lan- guage test question topic f q is used for the con- text vector f . Architecturally, a context adapted RNNLM is described by equations 3-5. e(x) and g(x) are element-wise sigmoid and softmax acti- </p><formula xml:id="formula_1">P(w i |h i−1 0 , f ) = P RNN (w i |w i−1 , s i−2 , f ) P RNN (w i |w i−1 , s i−2 , f ) = g(Vs i−1 + Hf ) s i−1 = e(Uw i−1 + Ws i−2 + Gf ) (3) (4) (5)</formula><p>Through the process of adaptation the RNNLM learns to associate particular types of responses with particular topics, thereby becoming more dis- criminative. Thus, a sentence's topic-conditional probability P RNN (w|f q ) will be higher if it corre- sponds to the topic q than if it does not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Example Response Based Topic Space</head><p>In order for the topic vectors f q to be informa- tive they must span the space of all question top- ics in the test. Thus a topic space needs to be de- fined. Example responses, which are necessary to train the RNNLM, are used to define a topic space because typical responses to a question will be definitive of the question's topic. Multiple exam- ple responses to a particular question are merged into one aggregate response to capture a broad range of response variations and increase the ro- bustness of the vector representation estimation.</p><p>By default a topic t is defined for each ques- tion q. However, multi-part questions are com- mon, where candidates are given a scenario such as providing tourist information in which indi- vidual questions ask about food, hotels or sights. Since the underlying topic is related this can con- fuse a classifier. The responses for all these related questions could be merged to form a single aggre- gate vector, but the statistics of the responses to each question can be sufficiently different that less distinct topics are formed. Instead the aggregrate example responses for each question are assigned the same topic label. Thus, a mapping between questions and topics and its inverse is introduced:</p><formula xml:id="formula_2">M : q → t M −1 t : {q ∈ Q|M(q) = t} (6)<label>(7)</label></formula><p>A vector representation of a question topic is computed using the aggregate example responses. As mentioned in Section 1, two common represen- tations are LDA and LSA; both are investigated in this work.</p><p>LDA is a generative model which allows docu- ments to be modelled as distributions over latent topics z ∈ Z. Each latent topic z is described by a multinomial distribution over words P(w i |z), and each word in a document is attributed to a particu- lar latent topic ( <ref type="bibr" target="#b0">Blei et al., 2003</ref>). Thus, the adap- tation vector f w represents a vector of posterior probabilities over latent topics for word sequence w:</p><formula xml:id="formula_3">f w = [P(z = 1|w), · · · , P(z = K|w)] T P(z = k|w) = N i=1 δ(z w i = k) N (8)<label>(9)</label></formula><p>LDA was found to perform better for RNNLM adaptation than other representations in <ref type="bibr" target="#b11">(Mikolov and Zweig, 2012;</ref><ref type="bibr" target="#b3">Chen et al., 2015</ref>). LSA ( <ref type="bibr" target="#b8">Landauer et al., 1998</ref>) is a popular repre- sentation for information retrieval tasks. A word- document matrix F is constructed using example responses and each word is weighted by its term frequency-inverse document frequency (TF-IDF). Then a low-rank approximation F k is computed using Singular Value Decomposition (SVD):</p><formula xml:id="formula_4">F k = U k Σ k V T k F k = [f 1 , · · · , f Q ] T f w = Σ −1 k U T k f tf idf (10) (11)<label>(12)</label></formula><p>Only the k largest singular values of the singular value matrix Σ are kept. F k is a representation of the data which retains only the k most significant factors of variation. Each row is a topic vector f q . New vectors f w for test responses can be projected into the LSA space via equation 12, where f tf idf is the TF-IDF weighted bag-of-word representation of a test response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Topic Detection and Features</head><p>This section discusses the standard, vector similar- ity feature-based, and the proposed topic adapted RNNLM approaches for topic detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Vector Similarity Features</head><p>The essence of the standard approach to topic de- tection is to assess the semantic distance D sem be- tween the test response w and the aggregate exam- ple response w q by approximating it using a vec- tor distance metric D vec between vector represen- tations of the response f w and the topic f q . Clas- sification is done by selecting the topic closest to the test response:</p><formula xml:id="formula_5">ˆ t w = M arg min q {D sem (w, w q )} D sem (w, w q ) ≈ D vec (f w , f q )<label>(13)</label></formula><p>The selection of an appropriate distance metric</p><formula xml:id="formula_7">D vec (f w , f q )</formula><p>can have a large effect on the classifi- cation outcome. A common metric used in topic classification and information retrieval is cosine similarity, which measures the cosine of the an- gle between two vectors. A distance metric based on this, cosine distance, can be defined as:</p><formula xml:id="formula_8">D cos (f w , f q ) = 1 − f w T f q |f w ||f q |<label>(15)</label></formula><p>While topics are robustly defined, this approach fails to capture the range of responses which can be given to a question. A different approach would be to maintain a separate point in this topic space for every example response. This retains the ro- bust topic definition while allowing each topic to be represented by a cloud of points in topic space, thereby capturing a range of responses which can be given. A K-nearest neighbour (KNN) classifier can be used to detect the response topic by com- puting distances of the test response to each of the training points in topic space. However, classifi- cation may become impractical for large data sets, as the number of response points scales with the size of the training data. Low-cost distance mea- sures, such as cosine distance, allow this approach to be used on large data sets before it becomes computationally infeasible. This approach is used as the baseline for comparison with the proposed RNNLM based method. For multi-part questions, topic vectors relating to the same overall topic are simply given the same topic label.</p><p>The classification rate can be improved by tak- ing the top N ˆ t N = { ˆ t 1 , · · · , ˆ t N } results into account. The KNN classifier can be modified to yield the N-best classification by removing all training points from the 1-best class from the KNN classifier and re-running the classification to get the 2-best results, and so on.</p><p>One of the main deficiencies of methods based on computing distances between vector represen- tations is that commonly used representations, such as LSA and LDA, ignore word-order in docu- ments, thereby throwing away information which is potentially useful for topic classification. Ad- ditionally, if any of the test or example response utterances are short, then their topic vector repre- sentations may not be robustly estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topic Adapted RNNLM Framework</head><p>The RNNLM based approach to topic detection is based on different principles. By combining equations 2 and 3 the log-probability L(q) of a response sentence given a particular topic vector P RNN (w|f q ) is computed. For each response w in the test set L(q) is computed (equation 16) for all topic vectors f q . L(q) is calculated using equa- tion 17 for multi-part questions with responses w p where p ∈ t. Classification is done by ranking log-probability L(q) for an utterance w and L(q) for all q ∈ M −1 t are averaged (equation 18).</p><formula xml:id="formula_9">L(q) =      log[P RNN (w|f q )] p 1 N p log[P RNN (w p |f q )]<label>(16)</label></formula><formula xml:id="formula_10">ˆ t w = arg max t { 1 |M −1 t | q∈M −1 t L(q)}<label>(17)</label></formula><p>It is trivial to extend this approach to yield the N- best solutions by simply taking the top N outputs of equation 18. The RNNLM approach has several benefits over standard approaches. Firstly, this approach explic- itly takes account of word-order in determining the topical similarity of the response. Secondly, there is no need to explicitly select a distance met- ric. Thirdly, the problems of robustly estimating a vector representation f w of the test response are sidestepped. Furthermore, the RNNLM accounts for a broad range of responses because it is trained on individual response utterances which it asso- ciates with a question topic vector. This makes it more scalable than the KNN approach because the number of comparisons which need to be made scales only with the number of questions, not the size of the training data. Thus, arbitrarily large data sets can be used to train the model without affecting classification time.</p><p>The RNNLM could be used in a KNN-style ap- proach, where it associates each example response with its individual topic vector, using L(q) as a distance metric. However, this is computationally infeasible since computing L(q) is significantly more expensive than cosine distance and the pre- viously mentioned scalability would be lost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data and Experimental Set-up</head><p>Data from the Business Language Testing Service (BULATS) English tests is used for training and testing. At test time, each response is recognised using an ASR system and the 1-best hypothesis is passed to the topic classifier. The topic detection system decides whether the candidate has spoken off topic by comparing the classifier output to the topic of the question being answered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BULATS Test Format and Data</head><p>The BULATS Online Speaking Test has five sec- tions ( <ref type="bibr" target="#b1">Chambers and Ingham, 2011</ref> Training, development and evaluation data sets composed of predominantly Gujarati L1 candi- dates are used in these experiments. The data sets are designed to have an (approximately) even dis- tribution over grades as well as over the different question scripts. During operation the system will detect off- topic responses based on ASR transcriptions, so for the system to be matched it needs to be trained on ASR transcriptions as well. Thus, two train- ing sets are made by using the ASR architec- ture described in section 4.2 to transcribe candi- date responses. Each training set covers the same set of 282 unique topics. The first training set consists of data from 490 candidates, containing 9.9K responses, with an average of 35.1 responses per topic. The second, much larger, training set consists of data from 10004 candidates, contain- ing 202K responses, with an average of 715.5 re- sponses per topic. As <ref type="table">Table 1</ref> shows, the average response length varies across sections due to the nature of the sec- tions. Shorter responses to questions are observed for sections A, B and E, with longer responses to C and D. Estimating topic representations for sec- tions A, B and E questions based on individual re- sponses would be problematic due to the short re- sponse lengths. However, by aggregating example responses across candidates, as described in sec- tion 2.2, the average length of responses in all sec- tions is significantly longer, allowing the example- response topic space to be robustly defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Characteristic</head><p>Section E topics correspond to topics of sub- questions relating to an overall question, thus there are only 15 unique questions in section E. How- ever, the sub-questions are sufficiently distinct to merit their own topic vectors. At classification time confusions between sub-questions of an over- all section E question are not considered mistakes.</p><p>Held-out test sets are used for development, DEV, and evaluation, EVAL, composed of 84 and 223 candidates, respectively. ASR transcriptions are used for these test sets, as per the operating scenario. A version of the DEV set with pro- fessionally produced transcriptions, DEV REF, is also used in training and development.</p><p>The publicly available Gibbs-LDA toolkit <ref type="bibr" target="#b15">(Phan and Nguyen, 2007</ref>) is used to estimate LDA posterior topic vectors and the scikit-learn 17.0 toolkit <ref type="bibr" target="#b14">(Pedregosa et al., 2011</ref>) to estimate LSA topic representations. The topic adapted RNNLM uses a 100-dimensional hidden layer. DEV REF is used as a validation set for early stopping to prevent over-fitting. The CUED RNNLM toolkit v0.1 <ref type="bibr" target="#b4">(Chen et al., 2016</ref>) is used for RNNLM training, details of which can be found in ( <ref type="bibr" target="#b12">Mikolov et al., 2010)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ASR System</head><p>A hybrid deep neural network DNN-HMM system is used for ASR ( <ref type="bibr" target="#b18">Wang et al., 2015)</ref>. The acoustic models are trained on 108.6 hours of BULATS test data (Gujarati L1 candidates) using an extended version of the HTK v3.4.1 toolkit ( <ref type="bibr">Young et al., 2009;</ref>. A Kneser-Ney trigram LM is trained on 186K words of BULATs test data and interpolated with a general English LM trained on a large broadcast news corpus, us- ing the SRILM toolkit <ref type="bibr" target="#b17">(Stolcke, 2002</ref>). Lattices are re-scored with an interpolated trigram+RNN LM ( <ref type="bibr" target="#b12">Mikolov et al., 2010)</ref> by applying the 4- gram history approximation described in ( , where the RNNLM is trained using the CUED RNNLM toolkit <ref type="bibr" target="#b4">(Chen et al., 2016)</ref>. Inter- polation weights are optimized on the DEV REF data set. <ref type="table" target="#tab_3">Table 2</ref> shows the word error rate (WER) on the DEV test set relative to the DEV REF ref- erences for each section and the combined sponta- neous speech sections (C-E).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>% WER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Two forms of experiment are conducted in or- der to assess the performance of the topic-adapted RNNLM. First, a topic classification experiment is run where the ability of the system to accurately recognize the topic of a response is evaluated. Sec- ond, a closed-set off-topic response detection ex- periment is done.</p><p>In the experimental configuration used here a response is classified into a topic and the accuracy is measured. The topic of the question being an- swered is known and all responses are actually on- topic. A label (on-topic/off-topic) is given for each response based on the output of the classifier rela- tive to the question topic. Thus, results presented are in terms of false rejection (FR) and false accep- tance (FA) rates rather than precision and recall.</p><p>Initial topic detection experiments were run us- ing the DEV test set with both the reference transcriptions (REF) and recognition hypotheses (ASR) to compare different KNN and RNN sys- tems. After this, performance is evaluated on the EVAL test set. The systems were trained using data sets of 490 and 10004 candidates, as de- scribed in section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Topic Classification</head><p>Performance of the topic-adapted RNNLM is compared to the KNN classifier in <ref type="table" target="#tab_5">Table 3</ref>. The RNN1 system outperforms the KNN system by 20-35 % using the LDA topic representation. Fur- thermore, the KNN system performs worse on sec- tion E than it does on section C, while RNN1 per- formance is better on section E by 7-10% than on section C. The LSA topic representation consis- tently yields much better performance than LDA by 25-50% for both systems. Thus, the LDA rep- resentation is not further investigated in any exper- iments.</p><p>When using the LSA representation the RNN1 system outperforms the KNN system only marginally, due to better performance on section E. Additionally, unlike the KNN-LDA system, the KNN-LSA system does not have a performance degradation on section E relative to section C. No- tably, the RNN1 system performs better on sec- tion E by 5-13% than on section C. Clearly, sec- tion C questions are hardest to assess. Combining both representations through concatenation does not effect performance of the KNN system and slightly degrades RNN1 performance on section C. KNN and RNN1 systems with either topic rep- resentation perform comparably on REF and ASR. This suggests that the systems are quite robust to WER rates of 31.5% and the differences are mostly noise.</p><p>Training the RNN2 system on 20 times as much data leads to greatly improved performance over KNN and RNN1 systems, almost halving the over-  Performance on section D is always best, as sec- tion D questions relate to discussion of charts and graphs for different conditions for which the vo- cabulary is very specific. Section C and E ques- tions are the less distinct because they have free- form answers to broad questions, leading to higher response variability. This makes the linking of topic from the training data to the test data more challenging, particularly for 1-best classification, leading to higher error rates. <ref type="figure" target="#fig_2">Figure 2</ref> shows the topic classification confu- sion matrix for the RNN1 LSA system. A similar matrix is observed for the KNN LSA system. Most confusions are with topics from the same section. This is because each section has a distinct style of questions and some questions within a section are similar. An example is shown below. Ques- tion SC-EX1 relates to personal local events in the workplace. SC-EX2, which relates to similar is- sues, is often confused with it. On the other hand, SC-EX3 is rarely confused with SC-EX1 as it is about non-personal events on a larger scale.</p><formula xml:id="formula_12">Topic System # Cands. C D E ALL (C-E) Repn</formula><p>• SC-EX1: Talk about some advice from a colleague.</p><p>You should say: what the advice was, how it helped you and whether you would give the same advice to another colleague.</p><p>• SC-EX2: Talk about a socially challenging day you had at work. You should say: what was the challenging situation, how you resolved it and why you found it challenging.</p><p>• SC-EX3: Talk about a company in your local town which you admire. You should say: what company it is, what they do, why you admire them, and how the company impacts life in your town. System performance can be increased by con- sidering N -best results, as described in Section 3. Results for systems trained on 490 and 10004 can- didates are presented in <ref type="table" target="#tab_7">Table 4</ref>. The error rate decreases as the value of N increases for all sys- tems. However, performance scales better with N for the RNN systems than for KNN. Notably, for values of N &gt; 1 performance of all systems on REF is better, which suggests that ASR errors do have a minor impact on system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Off-topic response detection</head><p>In the second experiment off-topic response de- tection is investigated. Performance is measured N System # Cands. REF ASR  in terms of the false acceptance (FA) probability of an off-topic response and false rejection (FR) probability of an on-topic response. The experi- ment is run on DEV and EVAL test sets. Since neither DEV nor EVAL contain real off-topic re- sponses, a pool W q of such responses is synthet- ically generated for each question by using valid responses to other questions in the data set. Off- topic responses are then selected from this pool. A selection strategy defines which responses are present in W q . Rather than using a single se- lection of off-topic responses, an expected per- formance over all possible off-topic response se- lections is estimated. The overall probability of falsely accepting an off-topic response can be ex- pressed using equation 19.</p><formula xml:id="formula_13">P(FA) = Q q=1</formula><p>w∈Wq P(FA|w, q)P(w|q)P(q) <ref type="bibr">(19)</ref> In equation 19, the question q is selected with uni- form probability from the set Q of possible ques- tions. The candidate randomly selects with uni- form probability P(w|q) a response w from the pool W q . The correct response to the question is not present in the pool. The conditional probabil- ity of false accept P(FA|w, q) = 1 if M(q) ∈ ˆ t N , and M(q) is not the real topic of the response w, otherwise P(FA|w, q) = 0. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, the main confusions will occur if the response is from the same section as the question. Two strategies for selecting off-topic responses are considered based on this: naive, where an incorrect response can be selected from any section; and directed, where an incorrect response can only be selected from the same sec- tion as the question. The naive strategy rep- resents candidates who have little knowledge of the system and memorise responses unrelated to the test, while the directed strategy represents those who are familiar with the test system and have access to real responses from previous tests.   A Receiver Operating Characteristic (ROC) curve <ref type="figure" target="#fig_3">(Figure 3)</ref> can be constructed by plotting the FA and FR rates for a range of N . The RNN1 sys- tem performs better at all operating points than the KNN system for both selection strategies and eval- uation test sets. Equal Error Rates (EER), where FA = FR, are given in <ref type="table" target="#tab_9">Table 5</ref>. Results on EVAL are more representative of the difference between the KNN and RNN performance, as they are eval- uated on nearly 3 times as many candidates. The RNN2 system achieves the lowest EER. It is in- teresting that for better systems the difference in performance against the naive and directed strategies decreases. This indicates that the sys- tems become increasingly better at discriminating between similar questions.</p><p>As expected, the equal error rate for the directed strategy is higher than for the naive strategy. In relation to the stated task of detect- ing when a test candidate is giving a memorized response, the naive strategy represents a lower- bound on realistic system performance, as students are not likely to respond with a valid response to a different question. Most likely they will fail to construct a valid response or will add com- pletely unrelated phrases memorised beforehand, which, unlike responses from other sections, may not come from the same domain as the test (eg: Business for BULATs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this work a novel off-topic content detection framework based on topic-adapted RNNLMs was developed. The system was evaluated on the task of detecting off-topic spoken responses on the BU- LATS test. The proposed approach achieves better topic classification and off-topic detection perfor- mance than the standard approaches.</p><p>A limitation of both the standard and proposed approach is that if a new question is created by the test-makers, then it will be necessary to col- lect example responses before it can be widely de- ployed. However, since the system can be trained on ASR transcriptions, the example responses do not need to be hand-transcribed. This is an at- tractive deployment scenario, as only a smaller hand-transcribed data set is needed to train an ASR system with which to cost-effectively transcribe a large number of candidate recordings.</p><p>Further exploration of different topic vector rep- resentations and their combinations is necessary in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) (Mikolov, 2012) are a variable context length language model, capable of representing the entire context efficiently, un- like N-grams. RNNLMs represent the full un- truncated history h i−1 0 = {w i−1 , · · · , w 0 } for word w i as the hidden layer s i−1 , a form of short- term memory, whose representation is learned from the data. Words and phrases are represented in a continuous space, which gives RNNLMs greater generalization abilities than other language models, such as N-grams.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Context adapted RNN language model</figDesc><graphic url="image-1.png" coords="3,67.83,69.17,226.76,198.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: RNN1 LSA confusion matrix on DEV ASR.</figDesc><graphic url="image-2.png" coords="7,309.37,395.49,206.64,132.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: ROC curves of LSA topic space systems on the EVAL test set.</figDesc><graphic url="image-3.png" coords="8,304.84,378.54,227.88,204.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : ASR performance on DEV.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>. REF ASR REF ASR REF ASR REF ASR</head><label></label><figDesc></figDesc><table>LDA 
KNN 
490 
75.0 81.0 37.0 42.0 91.8 91.1 68.0 71.4 
RNN1 
61.9 58.3 28.4 25.9 48.8 51.2 46.6 45.4 

LSA 
KNN 
490 
32.1 28.6 2.5 
3.7 31.3 33.3 22.0 21.9 
RNN1 
29.8 31.0 4.9 
6.2 23.8 23.8 19.7 20.5 
RNN2 
10004 19.0 19.0 3.7 
3.7 
9.5 10.7 10.8 11.2 

LDA 
KNN 
490 
30.9 29.8 2.5 
3.7 31.5 33.3 21.7 22.3 

+LSA 
RNN1 
32.1 35.7 4.9 
4.9 23.8 22.6 20.5 21.3 
RNN2 
10004 25.0 22.6 4.9 
4.9 10.7 10.7 13.7 12.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>% False rejection in topic detection using KNN classifier with 6 nearest neighbour and distance 
weights and RNNLM classifier on the DEV test set. 280 dim. topic spaces for LDA and LSA, and 560 
dim. for LDA+LSA. 

all error rate. The KNN system could not be eval-
uated effectively in reasonable time using 20 times 
as many example responses and results are not 
shown, while RNN2 evaluation times are unaf-
fected. Notably, RNN performance using the LSA 
representation scales with training data size better 
than with the LDA+LSA representation. Thus, we 
further investigate systems only with the LSA rep-
resentation. Interestingly, section D performance 
is improved only marginally. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>N -Best % false rejection performance of 
KNN and RNNLM classifiers with the LSA topic 
space on the DEV test set 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 :</head><label>5</label><figDesc>% Equal Error Rate for LSA topic space systems on the DEV and EVAL test sets.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was funded under the ALTA Insti-tute, University of Cambridge. Thanks to Cam-bridge English, University of Cambridge, for sup-porting this research and providing access to the BULATS data.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The BULATS online speaking test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Ingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research Notes</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="21" to="25" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient GPUbased Training of Recurrent Neural Network Language Models Using Spliced Sentence Bunch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recurrent Neural Network Language Model Adaptation for Multi-Genre Broadcast Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moquan</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">C</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CUED-RNNLM-an open-source toolkit for efficient training and evaluation of recurrent neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic detection of plagiarized spoken responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keelan</forename><surname>Evanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Ninth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Prompt-based Content Scoring for Automated Spoken Language Assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keelan</forename><surname>Evanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Zechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finding Scientific Topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Introduction to Latent Semantic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Thomas K Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename><surname>Foltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Discourse Processes</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="259" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient Lattice Rescoring using Recurrent Neural Network Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">C</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using Deep Neural Networks to Improve Proficiency Assessment for Children English Language Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Metallinou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Context Dependent Recurrent Neural Network Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Spoken Language Technology Workshop (SLT)</title>
		<meeting>IEEE Spoken Language Technology Workshop (SLT)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent Neural Network Based Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukás</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Statistical Language Models Based on Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">GibbsLDA++: A C/C++ implementation of latent Dirichlet allocation (LDA)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Hieu</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cam-Tu</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">English as a lingua franca</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Seidlhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ELT journal</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">339</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modelling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICSLP</title>
		<meeting>ICSLP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint Decoding of Tandem and Hybrid Systems for Improved Keyword Spotting on Low Resource Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Ragni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><forename type="middle">M</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">C</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Woodland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploring Content Features for Automated Speech Scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keelan</forename><surname>Evanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Zechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Automated assessment of English-learner writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<idno>UCAM-CL-TR-842</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge Computer Laboratory</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SimilarityBased Non-Scorable Response Detection for Automated Speech Scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Su-Youn Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Ninth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Evermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hain</surname></persName>
		</author>
		<imprint>
			<publisher>Andrew</publisher>
			<pubPlace>Dan Kershaw, Xunying</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The HTK book (for HTK Version 3.4.1)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gareth</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Odell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ollason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valtcho</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Valtchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic scoring of non-native spontaneous speech in tests of spoken English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Zechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derrick</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="883" to="895" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Spoken Language Technology for Education Spoken Language</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A General Artificial Neural Network Extension for HTK</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chau</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
