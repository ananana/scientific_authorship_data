<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Latent Concept Topic Model for Robust Topic Inference Using Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><forename type="middle">&amp;apos;</forename><surname>Ichi Tsujii</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Artificial Intelligence Research Center</orgName>
								<orgName type="institution">AIST</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Manchester</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Latent Concept Topic Model for Robust Topic Inference Using Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="380" to="386"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Uncovering thematic structures of SNS and blog posts is a crucial yet challenging task, because of the severe data spar-sity induced by the short length of texts and diverse use of vocabulary. This hinders effective topic inference of traditional LDA because it infers topics based on document-level co-occurrence of words. To robustly infer topics in such contexts, we propose a latent concept topic model (LCTM). Unlike LDA, LCTM reveals topics via co-occurrence of latent concepts, which we introduce as latent variables to capture conceptual similarity of words. More specifically, LCTM models each topic as a distribution over the latent concepts , where each latent concept is a localized Gaussian distribution over the word embedding space. Since the number of unique concepts in a corpus is often much smaller than the number of unique words, LCTM is less susceptible to the data spar-sity. Experiments on the 20Newsgroups show the effectiveness of LCTM in dealing with short texts as well as the capability of the model in handling held-out documents with a high degree of OOV words.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Probabilistic topic models such as Latent Dirich- let allocation (LDA) ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref>, are widely used to uncover hidden topics within a text corpus. LDA models each document as a mixture of top- ics where each topic is a distribution over words. In essence, LDA reveals latent topics in a corpus by implicitly capturing document-level word co- occurrence patterns ( <ref type="bibr">Wang and McCallum, 2006</ref>).</p><p>In recent years, Social Networking Services and blogs have become increasingly prevalent due to the explosive growth of the Internet. Uncover- ing the themantic structures of these posts is cru- cial for tasks like market review, trend estimation <ref type="bibr" target="#b0">(Asur and Huberman, 2010)</ref> and so on. How- ever, compared to more conventional documents, such as news articles and academic papers, ana- lyzing the thematic content of blog posts can be challenging, because of their typically short length and the use of diverse vocabulary by various au- thors. These factors can substantially decrease the chance of topically related words co-occurring in the same post, which in turn hinders effective topic inference in conventional topic models. Addition- ally, sometimes small corpus size can further exac- erbate topic inference, since word co-occurrence statistics becomes more sparse as the number of documents decreases.</p><p>Recently, word embedding models, such as word2vec ( <ref type="bibr">Mikolov et al., 2013</ref>) and GloVe <ref type="bibr">(Pennington et al., 2014</ref>) have gained much attention with their ability to form clusters of conceptually similar words in the embedding space. Inspired by this, we propose a latent concept topic model (LCTM) that infers topics based on document- level co-occurrence of references to the same con- cept. More specifically, we introduce a new la- tent variable, termed a latent concept to capture conceptual similarity of words, and redefine each topic as a distribution over the latent concepts. Each latent concept is then modeled as a localized Gaussian distribution over the embedding space. This is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, where we denote the centers of the Gaussian distributions as con- cept vectors. We see that each concept vector captures a representative concept of surrounding words, and the Gaussian distributions model the small variation between the latent concepts and the actual use of words. Since the number of unique concepts that are referenced in a corpus is often much smaller than the number of unique words, we expect topically-related latent concepts to co-occur many times, even in short texts with diverse usage of words. This in turn promotes topic inference in LCTM.</p><p>LCTM further has the advantage of using con- tinuous word embedding. Traditional LDA as- sumes a fixed vocabulary of word types. This modeling assumption prevents LDA from han- dling out of vocabulary (OOV) words in held-out documents. On the other hands, since our topic model operates on the continuous vector space, it can naturally handle OOV words once their vector representation is provided.</p><p>The main contributions of our paper are as fol- lows: We propose LCTM that infers topics via document-level co-occurrence patterns of latent concepts, and derive a collapsed Gibbs sampler for approximate inference. We show that LCTM can accurately represent short texts by outperform- ing conventional topic models in a clustering task. By means of a classification task, we furthermore demonstrate that LCTM achieves superior perfor- mance to other state-of-the-art topic models in handling documents with a high degree of OOV words.</p><p>The remainder of the paper is organized as fol- lows: related work is summarized in Section 2, while LCTM and its inference algorithm are pre- sented in Section 3. Experiments on the 20News- groups are presented in Section 4, and a conclu- sion is presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There have been a number of previous studies on topic models that incorporate word embeddings. The closest model to LCTM is Gaussian LDA ( <ref type="bibr" target="#b4">Das et al., 2015)</ref>, which models each topic as a Gaussian distribution over the word embedding space. However, the assumption that topics are unimodal in the embedding space is not appropri- ate, since topically related words such as 'neural' and 'networks' can occur distantly from each other in the embedding space. <ref type="bibr">Nguyen et al. (2015)</ref> pro- posed topic models that incorporate information of word vectors in modeling topic-word distribu- tions. Similarly, <ref type="bibr">Petterson et al. (Petterson et al., 2010</ref>) exploits external word features to improve the Dirichlet prior of the topic-word distributions. However, both of the models cannot handle OOV words, because they assume fixed word types.</p><p>Latent concepts in LCTM are closely related to 'constraints' in interactive topic models (ITM) ( <ref type="bibr">Hu et al., 2014</ref>). Both latent concepts and con- straints are designed to group conceptually simi- lar words using external knowledge in an attempt to aid topic inference. The difference lies in their modeling assumptions: latent concepts in LCTM are modeled as Gaussian distributions over the embedding space, while constraints in ITM are sets of conceptually similar words that are interac- tively identified by humans for each topic. Each constraint for each topic is then modeled as a multinomial distribution over the constrained set of words that were identified as mutually related by humans. In Section 4, we consider a variant of ITM, whose constraints are instead inferred using external word embeddings.</p><p>As regards short texts, a well-known topic model is Biterm Topic Model (BTM) <ref type="bibr">(Yan et al., 2013)</ref>. BTM directly models the genera- tion of biterms (pairs of words) in the whole cor- pus. However, the assumption that pairs of co- occurring words should be assigned to the same topic might be too strong <ref type="bibr" target="#b3">(Chen et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Latent Concept Topic Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generative Model</head><p>The primary difference between LCTM and the conventional topic models is that LCTM describes the generative process of word vectors in docu- ments, rather than words themselves.</p><p>Suppose α and β are parameters for the Dirich- let priors and let v d,i denote the word embedding for a word type w d,i . The generative model for LCTM is as follows.</p><formula xml:id="formula_0">1. For each topic k (a) Draw a topic concept distribution ϕ k ∼ Dirichlet(β). (a) LDA. (b) LCTM.</formula><p>Figure 2: Graphical representation.</p><formula xml:id="formula_1">2. For each latent concept c (a) Draw a concept vector µ c ∼ N (µ, σ 2 0 I). 3. For each document d (a) Draw a document topic distribution θ d ∼ Dirichlet(α). (b) For the i-th word w d,i in document d i. Draw its topic assignment z d,i ∼ Categorical(θ d ). ii. Draw its latent concept assignment c d,i ∼ Categorical(ϕ z d,i ). iii. Draw a word vector v d,i ∼ N (µ c d,i , σ 2 I).</formula><p>The graphical models for LDA and LCTM are shown in <ref type="figure">Figure 2</ref>. Compared to LDA, LCTM adds another layer of latent variables to indicate the conceptual similarity of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Posterior Inference</head><p>In our application, we observe documents consist- ing of word vectors and wish to infer posterior dis- tributions over all the hidden variables. Since there is no analytical solution to the posterior, we derive a collapsed Gibbs sampler to perform approximate inference. During the inference, we sample a la- tent concept assignment as well as a topic assign- ment for each word in each document as follows:</p><formula xml:id="formula_2">p(z d,i = k | c d,i = c, z −d,i , c −d,i , v) ∝ ( n −d,i d,k + α k ) · n −d,i k,c + βc n −d,i k,· + ∑ c ′ β c ′ ,<label>(1)</label></formula><formula xml:id="formula_3">P (c d,i = c | z d,i = k, v d,i , z −d,i , c −d,i , v −d,i ) ∝ ( n −d,i k,c + βc ) · N (v d,i |µ c , σ 2 c I),<label>(2)</label></formula><p>where n d,k is the number of words assigned to topic k in document d, and n k,c is the number of words assigned to both topic k and latent concept c. When an index is replaced by '·', the number is obtained by summing over the index. The super- script −d,i indicates that the current assignments of z d,i and c d,i are ignored. N (·|µ, Σ) is a mul- tivariate Gaussian density function with mean µ and covariance matrix Σ. µ c and σ 2 c in Eq. (2) are parameters associated with the latent concept c and are defined as follows:</p><formula xml:id="formula_4">µ c = 1 σ 2 + n −d,i ·,c σ 2 0   σ 2 µ + σ 2 0 · ∑ (d ′ ,i ′ )∈A −d,i c v d ′ ,i ′   ,<label>(3)</label></formula><formula xml:id="formula_5">σ 2 c = ( 1 + σ 2 0 n −d,i ·,c σ 2 0 + σ 2 ) σ 2 ,<label>(4)</label></formula><p>where <ref type="bibr">Murphy, 2012)</ref>. Eq. <ref type="formula" target="#formula_2">(1)</ref> is similar to the collapsed Gibbs sampler of LDA ( <ref type="bibr" target="#b5">Griffiths and Steyvers, 2004</ref>) except that the second term of Eq. <ref type="formula" target="#formula_2">(1)</ref> is concerned with topic-concept distribu- tions. Eq. (2) of sampling latent concepts has an intuitive interpretation: the first term encourages concept assignments that are consistent with the current topic assignment, while the second term encourages concept assignments that are consis- tent with the observed word. The Gaussian vari- ance parameter σ 2 acts as a trade-off parameter between the two terms via σ 2 c . In Section 4.2, we study the effect of σ 2 on document representation.</p><formula xml:id="formula_6">A −d,i c ≡ {(d ′ , i ′ ) | c d ′ ,i ′ = c ∧ (d ′ , i ′ ) ̸ = (d, i)} (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prediction of Topic Proportions</head><p>After the posterior inference, the posterior means of {θ d }, {ϕ k } are straightforward to calculate:</p><formula xml:id="formula_7">θ d,k = n d,k + α k n d,· + ∑ k ′ α k ′ , ϕ k,c = n k,c + βc n k,· + ∑ c ′ β c ′ .<label>(5)</label></formula><p>Also posterior means for {µ c } are given by Eq. (3). We can then use these values to predict a topic proportion θ dnew of an unseen document d new using collapsed Gibbs sampling as follows:</p><formula xml:id="formula_8">p(z dnew,i = k | v dnew,i , v −dnew,i , z −dnew,i , ϕ, µ) ∝ ( n −dnew,i dnew,k + α k ) · ∑ c ϕ k,c N (v dnew,i |µ c , σ 2 c ) ∑ c ′ N (v dnew,i |µ c ′ , σ 2 c ′ ) .<label>(6)</label></formula><p>The second term of Eq. <ref type="formula" target="#formula_8">(6)</ref> is a weighted average of ϕ k,c with respect to latent concepts. We see that more weight is given to the concepts whose corre- sponding vectors µ c are closer to the word vec- tor v dnew,i . This to be expected because statistics of nearby concepts should give more information about the word. We also see from Eq. (6) that the topic assignment of a word is determined by its embedding, instead of its word type. Therefore, LCTM can naturally handle OOV words once their embeddings are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Reducing the Computational Complexity</head><p>From Eqs. <ref type="formula" target="#formula_2">(1)</ref> and <ref type="formula" target="#formula_3">(2)</ref>, we see that the computa- tional complexity of sampling per word is O(K + SD), where K, S and D are numbers of topics, la- tent concepts and embedding dimensions, respec- tively. Since K ≪ S holds in usual settings, the dominant computation involves the sampling of latent concept, which costs O(SD) computation per word. However, since LCTM assumes that Gaussian variance σ 2 is relatively small, the chance of a word being assigned to distant concepts is negli- gible. Thus, we can reasonably assume that each word is assigned to one of M ≪ S nearest con- cepts. Hence, the computational complexity is reduced to O(M D). Since concept vectors can move slightly in the embedding space during the inference, we periodically update the nearest con- cepts for each word type.</p><p>To further reduce the computational complexity, we can apply dimensional reduction algorithms such as PCA and t-SNE (Van der Maaten and Hin- ton, 2008) to word embeddings to make D smaller. We leave this to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Models Description</head><p>In this section, we study the empirical perfor- mance of LCTM on short texts. We used the 20Newsgroups corpus, which consists of discus- sion posts about various news subjects authored by diverse readers. Each document in the corpus is tagged with one of twenty newsgroups. Only posts with less than 50 words are extracted for training datasets. For external word embeddings, we used 50-dimensional GloVe 1 that were pre-trained on Wikipedia. The datasets are summarized in Ta- ble 1. See appendix A for the detail of the dataset preprocessing.</p><p>We compare the performance of the LCTM to the following six baselines:</p><p>• LFLDA ( <ref type="bibr">Nguyen et al., 2015)</ref>, an extension of Latent Dirichlet Allocation that incorpo- rates word embeddings information. <ref type="table">Doc size Vocab size Avg len  400short  400  4729  31.87  800short  800  7329  31.78  1561short 1561  10644  31.83  held-out  7235  37944  140.15   Table 1</ref>: Summary of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>• LFDMM ( <ref type="bibr">Nguyen et al., 2015)</ref>, an extension of Dirichlet Multinomial Mixtures that incor- porates word embeddings information.</p><p>• nI-cLDA, non-interactive constrained Latent Dirichlet Allocatoin, a variant of ITM ( <ref type="bibr">Hu et al., 2014)</ref>, where constraints are inferred by applying k-means to external word embed- dings. Each resulting word cluster is then re- garded as a constraint. See appendix B for the detail of the model.</p><p>• GLDA ( <ref type="bibr" target="#b4">Das et al., 2015</ref>), Gaussian LDA.</p><p>• BTM ( <ref type="bibr">Yan et al., 2013</ref>), Biterm Topic Model.</p><p>• LDA ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref>.</p><p>In all the models, we set the number of topics to be 20. For LCTM (resp. nI-ITM), we set the number of latent concepts (resp. constraints) to be 1000. See appendix C for the detail of hyper- parameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Document Clustering</head><p>To demonstrate that LCTM results in a superior representation of short documents compared to the baselines, we evaluated the performance of each model on a document clustering task. We used a learned topic proportion as a feature for each document and applied k-means to cluster the doc- uments. We then compared the resulting clus- ters to the actual newsgroup labels. Clustering performance is measured by Adjusted Mutual In- formation (AMI) ( <ref type="bibr">Manning et al., 2008)</ref>. Higher AMI indicates better clustering performance. <ref type="figure" target="#fig_1">Fig- ure 3</ref> illustrates the quality of clustering in terms of Gaussian variance parameter σ 2 . We see that setting σ 2 = 0.5 consistently obtains good clus- tering performance for all the datasets with vary- ing sizes. We therefore set σ 2 = 0.5 in the later evaluation. <ref type="figure" target="#fig_2">Figure 4</ref> compares AMI on four topic models. We see that LCTM outperforms the topic models without word embeddings. Also, we see that LCTM performs comparable to LFLDA and nl-cLDA, both of which incorporate information of word embeddings to aid topic inference. How- ever, as we will see in the next section, LCTM can  better handle OOV words in held-out documents than LFLDA and nl-cLDA do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Representation of Held-out Documents with OOV words</head><p>To show that our model can better predict topic proportions of documents containing OOV words than other topic models, we conducted an exper- iment on a classification task. In particular, we infer topics from the training dataset and predicted topic proportions of held-out documents using col- lapsed Gibbs sampler. With the inferred topic proportions on both training dataset and held-out documents, we then trained a multi-class classi- fier (multi-class logistic regression implemented in sklearn 2 python module) on the training dataset and predicted newsgroup labels of the held-out documents.</p><p>We compared classification accuracy using LFLDA, nI-cLDA, LDA, GLDA, LCTM and a variant of LCTM (LCTM-UNK) that ignores OOV in the held-out documents. A higher classifica- tion accuracy indicates a better representation of unseen documents. <ref type="table">Table 2</ref> shows the propor- tion of OOV words and classification accuracy <ref type="bibr">2</ref> See http://scikit-learn.org/stable/.  <ref type="table">Table 2</ref>: Proportions of OOV words and classifi- cation accuracy in the held-out documents.</p><p>of the held-out documents. We see that LCTM- UNK outperforms other topic models in almost every setting, demonstrating the superiority of our method, even when OOV words are ignored. However, the fact that LCTM outperforms LCTM- UNK in all cases clearly illustrates that LCTM can effectively make use of information about OOV to further improve the representation of unseen docu- ments. The results show that the level of improve- ment of LCTM over LCTM-UNK increases as the proportion of OOV becomes greater.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have proposed LCTM that is well suited for application to short texts with di- verse vocabulary. LCTM infers topics according to document-level co-occurrence patterns of la- tent concepts, and thus is robust to diverse vocab- ulary usage and data sparsity in short texts. We showed experimentally that LCTM can produce a superior representation of short documents, com- pared to conventional topic models. We addition- ally demonstrated that LCTM can exploit OOV to improve the representation of unseen documents.</p><p>Although our paper has focused on improving per- formance of LDA by introducing the latent con- cept for each word, the same idea can be readily applied to other topic models that extend LDA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Preprocessing</head><p>We preprocessed the 20Newsgroups as follows:</p><p>We downloaded bag-of-words representation of the corpus available online 3 . Stop words 4 and words that were not covered in the GloVe were both removed. After the preprocessing, we ex- tracted short texts containing less than 50 words for training datasets. We created three training datasets with varying numbers of documents, and one held-out dataset. Each dataset was balanced in terms of the proportion of documents belonging to each newsgroup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Non-Interactive Contained LDA (nI-cLDA)</head><p>We describe nI-cLDA, a variant of interactive topic model ( <ref type="bibr">Hu et al., 2014</ref>). nl-cLDA is non- interactive in the sense that constraints are inferred from the word embeddings instead of being in- teractively identified by humans. In particular, we apply k-means to word embeddings to cluster words. Each resulting cluster is then regarded as a constraint. In general, constraints can be differ- ent from topic to topic. Let r k,w be a constraint of topic k which word w belongs to. The generative process of nl-cLDA is as follows. It is essentially the same as ( <ref type="bibr">Hu et al., 2014)</ref> 1. For each topic k ii. Draw its constraint l d,i</p><formula xml:id="formula_9">(a) Draw a topic constraint distribution ϕ k ∼ Dirichlet(β). (b) For each constraint s of topic k i. Draw a constraint word distribution π k,s ∼ Dirichlet(γ).</formula><formula xml:id="formula_10">∼ Categorical(ϕ z d,i ). iii. Draw a word w d,i ∼ Categorical(π z d,i ,l d,i ).</formula><p>Let V be the set of vocabulary. We note that π k,s is a multinomial distribution over W k,s , which is a subset of V , defined as W k,s ≡ {w ∈ V | r k,w = s}. W k,s represents a constrained set of words that are conceptually related to each other under topic k.</p><p>In our application, we observe documents and constraints for each topic, and wish to infer poste- rior distributions over all the hidden variables. We apply collapsed Gibbs sampling for the approxi- mate inference. For the detail of the inference, see ( <ref type="bibr">Hu et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyperparameter Settings</head><p>For all the topic models, we used symmetric Dirichlet priors. The hyperparameters were set as follows: for our model (LCTM and LCTM- UNK), nI-cLDA and LDA, we set α = 0.1 and β = 0.01. For nl-cLDA, we set the parameter of Dirichlet prior for constraint-word distribution (γ in appendix B) as 0.1. Also for our model, we set, σ 2 0 = 1.0 and µ to be the average of word vectors. We randomly initialized the topic assign- ments in all the models. Also, we initialized the la- tent concept assignments using k-means clustering on the word embeddings. The k-means clustering was implemented using sklearn 5 python module. We set M (number of nearest concepts to sample from) to be 300, and updated the nearest concepts every 5 iterations. For LFLDA, LFDMM, BTM and Gaussian LDA, we used the original imple- mentations available online 6 and retained the de- fault hyperparameters.</p><p>We ran all the topic models for 1500 iterations for training, and 500 iterations for predicting held- out documents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Projected latent concepts on the word embedding space. Concept vectors are annotated with their representative concepts in parentheses.</figDesc><graphic url="image-1.png" coords="2,72.00,76.38,240.95,137.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Relationship between σ 2 and AMI.</figDesc><graphic url="image-4.png" coords="5,72.00,53.28,226.77,131.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparisons on clustering performance of the topic models.</figDesc><graphic url="image-5.png" coords="5,72.00,212.54,240.95,140.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(b) For the i-th word w d,i in document d i. Draw its topic assignment z d,i ∼ Categorical(θ d ).</figDesc></figure>

			<note place="foot" n="1"> Downloaded at http://nlp.stanford.edu/projects/glove/</note>

			<note place="foot" n="3"> http://qwone.com/ ˜ jason/20Newsgroups/ 4 Available at http://www.nltk.org/</note>

			<note place="foot" n="5"> See http://scikit-learn.org/stable/. 6 LFTM: https://github.com/datquocnguyen/LFTM BTM: https://github.com/xiaohuiyan/BTM GLDA: https://github.com/rajarshd/Gaussian LDA</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank anonymous reviewers for their construc-tive feedback. We also thank Hideki Mima for helpful discussions and Paul Thompson for in-sightful reviews on the paper. This paper is based on results obtained from a project commissioned by the New Energy and Industrial Technology De-velopment Organization (NEDO).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting the future with social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitaram</forename><surname>Asur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernardo A Huberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Intelligence and Intelligent Agent Technology</title>
		<imprint>
			<publisher>WI-IAT</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">ACM International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="492" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent Dirichlet allocation. the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">User based aggregation for biterm topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="489" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gaussian LDA for topic models with word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="795" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
