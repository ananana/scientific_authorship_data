<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PhraseCTM: Correlated Topic Modeling on Phrases within Markov Random Fields</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Lab of High Confidence Software Technologies (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengjiao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Lab of High Confidence Software Technologies (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Lab of High Confidence Software Technologies (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Notre Dame</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Lab of High Confidence Software Technologies (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PhraseCTM: Correlated Topic Modeling on Phrases within Markov Random Fields</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="521" to="526"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>521</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent emerged phrase-level topic models are able to provide topics of phrases, which are easy to read for humans. But these models are lack of the ability to capture the correlation structure among the discovered numerous topics. We propose a novel topic model PhraseCTM and a two-stage method to find out the correlated topics at phrase level. In the first stage, we train PhraseCTM, which models the generation of words and phrases simultaneously by linking the phrases and component words within Markov Random Fields when they are semantically coherent. In the second stage, we generate the correlation of topics from PhraseCTM. We evaluate our method by a quantitative experiment and a human study, showing the correlated topic modeling on phrases is a good and practical way to interpret the underlying themes of a corpus.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, topic modeling on phrases has been developed for providing more interpretable topics <ref type="bibr" target="#b6">(El-Kishky et al., 2014;</ref><ref type="bibr" target="#b10">Kawamae, 2014;</ref><ref type="bibr" target="#b8">He, 2016)</ref>. They represent each topic as a list of phrases, which are easy to read for humans. For example, the topic represented in "grounding con- ductor, grounding wire, aluminum wiring, neutral ground, ..." is easier to read than the topic with words "ground, wire, use, power, cable, wires, ...", although they are both about the topic of house- hold electricity.</p><p>But when the number of topics grows, it's hard to review all the topics, even they are represented in phrases. The correlation structure is introduced by CTM ( <ref type="bibr" target="#b1">Blei and Lafferty, 2005</ref>) to figure out the correlated relationship between topics and group the similar topics together. And the correlated top- ics mined from the scientific papers <ref type="bibr" target="#b2">(Blei and Lafferty, 2007)</ref>, news corpus ( <ref type="bibr" target="#b7">He et al., 2017)</ref>, and social science data ( <ref type="bibr" target="#b15">Roberts et al., 2016)</ref>, showed their practical utility on grasping the semantic meaning of text documents.</p><p>However, it's nontrivial to apply CTM directly on phrases. The reasons are mainly due to two facts: (1) phrases are much less than words in each document; (2) similar to LDA <ref type="bibr" target="#b19">(Tang et al., 2014)</ref>, CTM doesn't perform well on short doc- uments. Therefore, CTM needs more contextual information to build a good enough model, rather than only using the extracted phrases.</p><p>To find out the correlated topics at phrase level, we take full advantage of contextual information about the phrases. Firstly, the topic of a phrase in a document is highly related to the topics of other words and phrases in the same document. Secondly, some phrases' meaning can be implied from their component words. Taking a document in <ref type="figure" target="#fig_0">Figure 1</ref> as an example, the phrase "orbital ve- hicle" shares the same topic as the word "DC- X" (a reusable spaceship), as well as its compo- nent words "orbital", and "vehicle", which are all about the topic of space exploration. The assump- tion that the words within the same phrase tend to have the same latent topic is directly used by PhraseLDA ( <ref type="bibr" target="#b6">El-Kishky et al., 2014</ref>). Note that not all the phrases always have the same topic as their component words (e.g., the newspaper Boston Globe) ( <ref type="bibr" target="#b14">Mikolov et al., 2013</ref>). It's difficult to distinguish the "orbital vehicle" type phrases from "Boston Globe" type phrases, but we can use the data-driven method to find out the semantically coherent ones by the NPMI metric <ref type="bibr" target="#b3">(Bouma, 2009)</ref>, and put them in Markov Random Fields <ref type="bibr" target="#b11">(Kindermann and Snell, 1980)</ref> to align the topics of phrases and their component words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20newsgroups/sci.space/62117</head><p>extracted phrases words … … … It will be tough to make DC-X succeed, and to turn it into an operational orbital vehicle. Doubtless it will fail to meet some of the promised goals. The reason people are so fond of it is that it's the only chance we have now, or will have for a ong time to come, to develop a launch vehicle with radically lower costs. … "orbital vehicle" "launch vehicle" Based on these two kinds of contextual informa- tion, we propose a novel topic model PhraseCTM and a two-stage method. In the first stage, we train PhraseCTM, which (1) double counts the phrases as two parts, one as the phrase itself, the other as the component words; (2) models the generation of words and phrases simultaneously by linking the phrases and component words within Markov Random Fields when they are semantically coher- ent; (3) uses the logistic normal distribution to rep- resent the correlation among the topics, like a pre- vious method CTM. In the second stage, we gen- erate the correlation of topics from PhraseCTM.</p><p>We evaluate our method on five datasets by a quantitative experiment and a human study, show- ing that the correlated topic modeling on phrases is a good way to interpret the underlying themes of a corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>There are two orthogonal lines of research stud- ies related to our work. (1) With the development of phrase extraction techniques <ref type="bibr" target="#b6">(El-Kishky et al., 2014;</ref><ref type="bibr" target="#b13">Liu et al., 2015;</ref><ref type="bibr" target="#b17">Shang et al., 2018)</ref>, several topic models based on extracted phrases are pro- posed to provide high-quality phrase-level topics, such as PhraseLDA ( <ref type="bibr" target="#b6">El-Kishky et al., 2014</ref>), and TPM <ref type="bibr" target="#b8">(He, 2016)</ref>. Because of the quality of ex- tracted phrases, PhraseLDA performs better than previous n-gram method TNG ( <ref type="bibr" target="#b21">Wang et al., 2007)</ref>, which combines phrase extraction and topic mod- eling together. (2) CTM (Blei and <ref type="bibr" target="#b1">Lafferty, 2005</ref>) uses the logistic normal distribution <ref type="bibr" target="#b0">(Aitchison, 1982)</ref> to replace the Dirichlet prior, so it can cap- ture the correlated structure of topics. And the ex- periments in the further works <ref type="bibr" target="#b15">Roberts et al., 2016;</ref><ref type="bibr" target="#b7">He et al., 2017)</ref> showed its usefulness in exploring the text corpus by using the correlated word-level topics. Note that our work is not a simple combination of these two methods, because the existing topic models on phrases lack the ability to capture the correlation structure while CTM cannot be directly applied on phrases due to the sparseness of phrases in each document. And as we used Markov Random Fields ( <ref type="bibr" target="#b11">Kindermann and Snell, 1980)</ref>, our work is different from previous ones <ref type="bibr" target="#b5">(Daume III, 2009;</ref><ref type="bibr" target="#b18">Sun et al., 2009;</ref><ref type="bibr" target="#b22">Xie et al., 2015)</ref> because we don't put all links into Markov Random Fields but only choose the semantic coherent links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The proposed method</head><p>By preparing data as described in the subsection 3.1, our method is carried out in two stages, shown in the subsection 3.2, and 3.3 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semantically Coherent Links for MRF</head><p>Given the input data in the form of raw text doc- uments, we transform each document into the format as "words, phrases, semantically coherent links between phrases and component words". Ex- tracting words is trivial, and extracting phrases can be conducted by using an existing tool, e.g., <ref type="bibr">AutoPhrase (Shang et al., 2018)</ref>. In this process, each extracted phrase is counted twice, one as the phrase itself (represented in the phrase vocabu- lary), the other is divided into component words (represented in the word vocabulary).</p><p>In a given document, we denote the i-th phrase as w (P) i , and its component words as w l(i) . We use the Equation (1) to determine the semantic coherent score between w (P) i and w l(i) by utiliz- ing NPMI <ref type="bibr" target="#b3">(Bouma, 2009)</ref> . The NPMI metric is defined upon two word types as NPMI(x, y) = log p(x,y) p(x)p(y) /(− log p(x, y)), where p(x) is esti- mated by the document frequency |d(x)| D , and</p><formula xml:id="formula_0">p(x, y) = |d(x)∩d(y)| D . s(w (P) i , w l(i) ) = min j,k∈l(i) {NPMI(w j , w k )} (1)</formula><p>Bouma (2009) pointed out that NPMI has the advantage that it ranges within the fixed interval. Inherited from NPMI, the semantic coherent score also ranges from -1 to 1. A negative semantic co- herent score means the phrase does not share the same topic with its component words in the corpus level (e.g., long run, the newspaper Boston Globe). A positive score means the opposite, and the score 1 suggests that the phrase and its component words should be aligned to the same topic in whole cor- pus. By a reasonable threshold τ , we can add the semantically coherent link for w = z j )/|l(i)|}, where κ is the weight to adjust how much the link is introduced to con- straint the topics to be same. In the following ex- periment, κ is set to be 10 −3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PhraseCTM</head><p>In the first stage, when given the prepared data as described in subsection 3.1, we are going to train better correlated phrase-level topics β (P) . The contextual information of phrases include (i) words in the same document, and (ii) their com- ponent words within semantically coherent links. Part (ii) has been modeled in the previous subsec- tion. For part (i), we let the phrases and words in a same document d share the topic parameter η d , which is a K-dimension vector sampled from a Gaussian distribution N (µ, Σ). Like CTM, Σ is the covariance matrix, modeling the correlation between topics.</p><p>As a part of MRF, the unary potential on the topic node z</p><formula xml:id="formula_1">(P) d,i or z d,j is defined by a logistic- normal distribution like CTM p(z d,j = k|η d ) = exp η d,k / ∑ k exp η d,k</formula><p>. Therefore, the joint distri- bution of topics over the phrases and the words in document d are defined as the following equa- tion, where A d (η d ) is used for normalization, and N L d is the number of semantically coherent links in document d.</p><formula xml:id="formula_2">p(z d , z (P) d |η d ) = 1 A d (η d ) N d ∏ m=1 p(z d,m |η d ) · N (P) d ∏ i=1 p(z (P) d,i |η d ) · exp{ N L d ∑ i=1 ( κ |l(d, i)| ∑ j∈l(d,i) 1(z (P) d,i = z d,j ))}</formula><p>The whole generation process is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>(a). We train PhraseCTM by variational inference like CTM. The different part lies in the phrases and component words which are in seman- tically coherent links. For these phrases, we use</p><note type="other">Eq. (2) to update the variational parameters φ (P) d,i</note><p>for the latent topic z (P) d,i of the phrase w (P) d,i . For the phrases that are not in semantically coherent links, we use the Eq. (3), which is same as the original CTM's variational inference. In Eqs. <ref type="formula">(2)</ref> and <ref type="formula" target="#formula_3">(3)</ref></p><formula xml:id="formula_3">, λ d is the variational parameter for η d such that η d,k ∼ N (λ d,k , ν 2 d,k ). ˆ φ (P) d,i,k ∝ β (P) k,w i exp(λ d,k + κ |l(d, i)| ∑ j∈l(d,i) φ d,j,k ) (2) ˆ φ (P) d,i,k ∝ β (P) k,w i exp(λ d,k )<label>(3)</label></formula><p>Similarly, the variational parameters for the component words in semantically coherent links are updated by Eq. (4), while other words are up- dated by Eq. (5). In this way, PhraseCTM intro- duces the impact from words and phrases on each other by Markov Random Fields.</p><formula xml:id="formula_4">ˆ φ d,i,k ∝ β k,w i exp(λ d,k + κ |l(d, j)| φ (P) d,j,k ), i ∈ l(d, j) (4) ˆ φ d,i,k ∝ β k,w i exp(λ d,k ) (5) µ Σ η d z d,j w d,j β k z (P) d,i w (P) d,i β (P) k D N d N (P) d K (a)</formula><p>The first stage: training on our proposed model Phra- seCTM. When observed words W and phrases W (P) , we learn word topics β, and phrase topics β (P) .</p><formula xml:id="formula_5">µ (P) Σ (P) η (P) d z (P) d,i w (P) d,i β (P) k D N (P) d K (b)</formula><p>The second stage: inferring the phrase topics' correlation. When given the phrases W (P) , and the phrase topics β (P) learned from the first stage, we infer the phrase-level topics' covariance Σ (P) as the correlation result. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generation of Phrase Topics' Correlation</head><p>In the second stage, we aim to get the correla- tion for β (P) . It cannot be directly derived from Σ, which has also been learned in the first stage, because Σ consists the impact from word topics. Thus, given W (P) and β (P) , we use the variational inference again to learn Σ (P) as the illustration of <ref type="figure" target="#fig_2">Figure 2(b)</ref>. Finally, the correlation matrix can be computed by corr <ref type="bibr">(P)</ref>  </p><formula xml:id="formula_6">(i, j) = Σ (P) i,j √ Σ (P) i,i Σ (P) j,j . |V | |V (P) | |W | |W (P) | |D| |W |/|D| |W (P) |/|D| 20</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>PhraseCTM is supposed to get benefits from two aspects: (1) generating high-quality phrase-level topics; (2) providing the correlation among phrase topics to help users to understand the underlying themes of a corpus. To check the first claim, we compare PhraseCTM with existing topic models on phrases. To evaluate the second claim, we design a user study to compare PhraseCTM with standard CTM that runs only on words.</p><p>Datasets. We choose several public text cor- pora, including 20Newsgroups <ref type="bibr" target="#b12">(Lang, 1995)</ref>, sub- sets of English Wikipedia, a subset of PubMed Ab- stracts ( <ref type="bibr" target="#b20">Varmus et al., 1999</ref>). Due to efficiency problem, we do not test on the whole Wikipedia corpus. We construct the Mathematics, Chem- istry, and Argentina subsets of English Wikipedia as ( . For each corpus, we ex- tract the phrases by the implementation 1 of Au- toPhrase ( <ref type="bibr" target="#b17">Shang et al., 2018)</ref>, and build the se- mantically coherent links as subsection 3.1. More specifically, in phrase extraction process, we set the minimum support as 5, and leave other pa- rameters in AutoPhrase as its suggestion. In aver- age, each document of the resulted 20Newsgroups only contains 2.7 phrases while 72.3 words, show- ing that phrases are much less than words. More statistics about the datasets are shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Quantitative Result</head><p>Baselines. We compare with PhraseLDA ( <ref type="bibr">ElKishky et al., 2014</ref>), the state-of-the-art model on phrases. Besides that, we run plain LDA and plain CTM 2 directly on the extracted phrases (without considering the impact of words). To check the effectiveness of MRF, we run a variant version PhraseCTM(-) by removing all the semantically coherent links from PhraseCTM. We also run a n- gram based topic model TNG ( <ref type="bibr" target="#b21">Wang et al., 2007)</ref>, which has already been implemented in Mallet 3 . All the topic numbers are set to be 100. For plain LDA and PhraseLDA, we set β = 0.005. For plain CTM and TNG, we use the default settings in their existing implementations. Since TNG combines phrase extraction and topic modeling together, we run it on the raw datasets.</p><p>We use the NPMI metric <ref type="bibr" target="#b3">(Bouma, 2009)</ref> to eval- uate the semantic coherence of top-10 phrases in each topic (K=100), by taking the entire English Wikipedia as the reference corpus. <ref type="bibr">Roder (2015)</ref> has shown that the NPMI metric is highly corre- lated to human topic coherence ratings, so it's nat- ural to use it to show how PhraseCTM improves the quality of topics. Although we have already used NPMI in the semantic coherent score for finding semantically coherent links, it does not in- fluence the rationality of the metric used for topic evaluation, because the semantic coherent score is defined upon two word types while the NPMI score on topics is defined upon two phrase types.  The result is shown in <ref type="figure" target="#fig_4">Figure 3</ref>. Due to the computational costs, TNG cannot scale up to large datasets. Plain LDA and plain CTM performs not well on the datasets because of the sparsity of phrases in each document, while TNG per- forms better than them as it can utilize more words as its contextual information. PhraseCTM(- ) is comparable to PhraseLDA in the experiment. PhraseLDA also utilizes all the contextual infor- mation with the assumption that the words in a phrase have the same topic. But this assumption  is too strong, which can be adjusted by our intro- duced semantically coherent links in MRF. This experiment demonstrates that our method has gen- erated high-quality phrase-level topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Human Study</head><p>To compare the correlated topics at different level, we directly run CTM on words. Trained on Argentina@Wiki and Maths@Wiki by CTM and PhraseCTM respectively, we outputted the topics with top-10 words/phrases in each topic and the correlation of topics for 10 human annotators, and asked them to label the topics. The duration of consuming time for topic labeling is a quite use- ful metric to check whether the topics are easy to understand for human annotators. It's based on our following observation in the human study: the confused topic may consume more time to give it an appropriate label, while the good one is easy to understand for human and consumes less time.</p><p>There are 2 groups of annotators. The annotators in Group A got the CTM re- sult on Maths@Wiki and PhraseCTM's on Ar- gentina@Wiki. The annotators in Group B got the results in the opposite setting. The labeling pro- cess was logged to calculate the accumulated time. The labeling time to reach 50 accurate topics' la- bels on PhraseCTM is much less than the labeling time on CTM. In average, the annotators spent 7.1 minutes on PhraseCTM while 13.2 minutes on the others, which is listed in <ref type="table" target="#tab_3">Table 2</ref>. In <ref type="figure" target="#fig_4">Figure 3</ref>, it's easy to label the topics in top right corner as poli-  tics. And the edges in the figure illustrate the cor- relation between topics. As an example, the edge between the topic 71 and the topic 31 represents that the economics and the politics in Argentina is related, helping users to understand the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We provide a new topic model PhraseCTM to make the Correlated Topic Modeling available for phrase-level topics. PhraseCTM utilizes more contextual information of phrases, and put them within Markov Random Fields, so it can provide high-quality correlated topics at phrase level. The experiments show that the correlated topic model- ing on phrases is a practical tool to interpret the underlying themes of a corpus. In future, we will optimize the efficiency of PhraseCTM to scale it up to large datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of phrases' contextual information. The phrases and words marked gray are about the same topic. The arrows show the topics of the phrases and their component words tend to be same, which tendency are modeled within Markov Random Field.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>i</head><label></label><figDesc>, w l(i) ) &gt; τ . In practice, we set τ to 0.4. Assuming the topic of the phrase w (P) i is z (P) i and the topics of words w l(i) are z l(i) , when they have the above mentioned semantically co- herent link, we put z (P) i and z l(i) in a Markov Random Field. More specifically, for z (P) i and z j , j ∈ l(i), there's the edge potential function exp{κ·1(z (P) i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of two stages of our method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The quality of the learned topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A part of the topic graph (K=100) generated by our method on the Argentina-related Wikipedia pages, where each node shows the top-5 phrases in each topic, and edges connect correlated topics. The left-aligned numbers in the graph represent the topic ID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Human time consumption on topic label-
ing for correlated topics generated by CTM and 
PhraseCTM, measured in minutes. 

</table></figure>

			<note place="foot" n="1"> https://github.com/shangjingbo1226/ AutoPhrase 2 https://github.com/blei-lab/ctm-c</note>

			<note place="foot" n="3"> http://mallet.cs.umass.edu/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The statistical analysis of compositional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Aitchison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="177" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Correlated topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Neural Information Processing Systems, NIPS&apos;05</title>
		<meeting>the 18th International Conference on Neural Information Processing Systems, NIPS&apos;05</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A correlated topic model of science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="35" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Normalized (pointwise) mutual information in collocation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerlof</forename><surname>Bouma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the German Society for Computational Linguistics &amp; Language Technology</title>
		<meeting>the German Society for Computational Linguistics &amp; Language Technology</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scalable inference for logisticnormal topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2445" to="2453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Markov random topic fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-IJCNLP 2009 Conference Short Papers</title>
		<meeting>the ACL-IJCNLP 2009 Conference Short Papers</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="293" to="296" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable topical phrase mining from text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanglei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="305" to="316" />
		</imprint>
		<respStmt>
			<orgName>VLDB Endowment</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient correlated topic modeling with topic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD&apos;17</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD&apos;17</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="225" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extracting topical phrases from clinical documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI&apos;16</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence, AAAI&apos;16</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2957" to="2963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Category-level transfer learning from knowledge base to microblog stream for accurate event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengjiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Database Systems for Advanced Applications, DASFAA&apos;17</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="50" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Supervised n-gram topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriaki</forename><surname>Kawamae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM International Conference on Web Search and Data Mining, WSDM &apos;14</title>
		<meeting>the 7th ACM International Conference on Web Search and Data Mining, WSDM &apos;14</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="473" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Kindermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snell</surname></persName>
		</author>
		<title level="m">Markov random fields and their applications</title>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1980" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Newsweeder: Learning to filter netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Machine Learning</title>
		<meeting>the 12th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mining quality phrases from massive text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;15</title>
		<meeting>the 2015 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;15</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1729" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A model of text for experimentation in the social sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><forename type="middle">E</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><forename type="middle">M</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">515</biblScope>
			<biblScope unit="page" from="988" to="1003" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring the space of topic coherence measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Röder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Both</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hinneburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, WSDM &apos;15</title>
		<meeting>the Eighth ACM International Conference on Web Search and Data Mining, WSDM &apos;15</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="399" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automated phrase mining from massive text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">itopicmodel: Information network-integrated topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yintao</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth IEEE International Conference on Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="493" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding the limiting factors of topic modeling via posterior contraction analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoshi</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanlong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pubmed central: an nih-operated site for electronic distribution of life sciences research reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Varmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Topical n-grams: Phrase and topic discovery, with an application to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuerui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Seventh IEEE International Conference on Data Mining, ICDM &apos;07</title>
		<meeting>the 2007 Seventh IEEE International Conference on Data Mining, ICDM &apos;07</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Incorporating word correlation knowledge into topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="725" to="734" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
