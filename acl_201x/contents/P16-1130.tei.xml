<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Continuous Space Rule Selection Model for Syntax-based Statistical Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<addrLine>3-5Hikaridai</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Keihanna Science City</orgName>
								<address>
									<postCode>619-0289</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Graduate School of Information Science</orgName>
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<postCode>630-0192</postCode>
									<settlement>Takayama</settlement>
									<region>Ikoma, Nara</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<addrLine>3-5Hikaridai</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Keihanna Science City</orgName>
								<address>
									<postCode>619-0289</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichro</forename><surname>Sumita</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<addrLine>3-5Hikaridai</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Keihanna Science City</orgName>
								<address>
									<postCode>619-0289</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Graduate School of Information Science</orgName>
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<postCode>630-0192</postCode>
									<settlement>Takayama</settlement>
									<region>Ikoma, Nara</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Graduate School of Information Science</orgName>
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<postCode>630-0192</postCode>
									<settlement>Takayama</settlement>
									<region>Ikoma, Nara</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Continuous Space Rule Selection Model for Syntax-based Statistical Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1372" to="1381"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>One of the major challenges for statistical machine translation (SMT) is to choose the appropriate translation rules based on the sentence context. This paper proposes a continuous space rule selection (CSRS) model for syntax-based SMT to perform this context-dependent rule selection. In contrast to existing maximum en-tropy based rule selection (MERS) models , which use discrete representations of words as features, the CSRS model is learned by a feed-forward neural network and uses real-valued vector representations of words, allowing for better generalization. In addition, we propose a method to train the rule selection models only on minimal rules, which are more frequent and have richer training data compared to non-minimal rules. We tested our model on different translation tasks and the CSRS model outperformed a base-line without rule selection and the previous MERS model by up to 2.2 and 1.1 points of BLEU score respectively.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In syntax-based statistical machine translation (SMT), especially tree-to-string ( <ref type="bibr" target="#b13">Liu et al., 2006;</ref><ref type="bibr" target="#b9">Graehl and Knight, 2004</ref>) and forest-to-string ( <ref type="bibr" target="#b18">Mi et al., 2008</ref>) SMT, a source tree or forest is used as input and translated by a series of tree-based trans- lation rules into a target sentence. A tree-based translation rule can perform reordering and trans- lation jointly by projecting a source subtree into a target string, which can contain both terminals and nonterminals.</p><p>One of the difficulties in applying this model is the ambiguity existing in translation rules: a source subtree can have different target transla- tions extracted from the parallel corpus as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Selecting correct rules during decod- ing is a major challenge for SMT in general, and syntax-based models are no exception.</p><p>There have been several methods proposed to resolve this ambiguity. The most simple method, used in the first models of tree-to-string transla- tion ( <ref type="bibr" target="#b13">Liu et al., 2006</ref>), estimated the probability of a translation rule by relative frequencies. For ex- ample, in <ref type="figure" target="#fig_0">Figure 1</ref>, the rule that occurs more times in the training data will have a higher score. Later,  proposed a maximum entropy based rule selection (MERS, Section 2) model for syntax-based SMT, which used contextual infor- mation for rule selection, such as words surround- ing a rule and words covered by nonterminals in a rule. For example, to choose the correct rule from the two rules in <ref type="figure" target="#fig_0">Figure 1</ref> for decoding a par- ticular input sentence, if the source phrase cov- ered by "x1" is "a thief" and this child phrase has been seen in the training data, then the MERS model can use this information to determine that the first rule should be applied. However, if the source phrase covered by "x1" is a slightly differ- ent phrase, such as "a gunman", it will be hard for the MERS model to select the correct rule, because it treats "thief" and "gunman" as two different and unrelated words.</p><p>In this paper, we propose a continuous space rule selection (CSRS, Section 3) model, which is learned by a feed-forward neural network and re- places the discrete representations of words used in the MERS model with real-valued vector repre- sentations of words for better generalization. For example, the CSRS model can use the similarity of word representations for "gunman" and "thief" to infer that "a gunman" is more similar with "a thief" than "a cold".</p><p>In addition, we propose a new method, ap- plicable to both the MERS and CSRS models, to train rule selection models only on minimal rules. These minimal rules are more frequent and have richer training data compared to non-minimal rules, making it possible to further relieve the data sparsity problem.</p><p>In experiments (Section 4), we validate the proposed CSRS model and the minimal rule training method on English-to-German, English- to-French, English-to-Chinese and English-to- Japanese translation tasks.</p><p>2 Tree-to-String SMT and MERS 2.1 Tree-to-String SMT In tree-to-string SMT ( <ref type="bibr" target="#b13">Liu et al., 2006</ref>), a parse tree for the source sentence F is transformed into a target sentence E using translation rules R. Each tree-based translation rule r ∈ R translates a source subtree˜tsubtree˜ subtree˜t into a target string˜estring˜ string˜e, which can contain both terminals and nonterminals. During decoding, the translation system examines differ- ent derivations for each source sentence and out- puts the one with the highest probability,</p><formula xml:id="formula_0">ˆ E = arg max E,R Pr (E, R|F ) .<label>(1)</label></formula><p>For a translation E of a source sentence F with derivation R, the translation probability is calcu- lated as follows,</p><formula xml:id="formula_1">Pr (E, R|F ) ≈ exp K k=1 λ k h k (E, R, F ) E ,R exp K k=1 λ k h k (E , R , F ) .<label>(2)</label></formula><p>Here, h k are features used in the translation system and λ k are feature weights. Features used in <ref type="bibr" target="#b13">Liu et al. (2006)</ref>'s model contain a language model and simple features based on relative frequencies, which do not consider context information. One of the most important features used in this model is based on the log conditional probability of the target string given the input source subtree log Pr˜e|˜t Pr˜ Pr˜e|Pr˜e|˜ Pr˜e|˜t</p><p>. This allows the model to determine which target strings are more likely to be used in translation. However, as the correct translation of the rules may depend on context that is not di- rectly included in the rule, this simple context- independent estimate is inherently inaccurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Maximum Entropy Based Rule Selection</head><p>To perform context-dependent rule selection,  proposed the MERS model for syntax-based SMT. They built a maximum en- tropy classifier for each ambiguous source subtree˜t subtree˜ subtree˜t, which introduced contextual information C and estimated the conditional probability using a log- linear model as shown below,</p><formula xml:id="formula_2">Pr˜e|˜t Pr˜ Pr˜e|Pr˜e|˜ Pr˜e|˜t, C = exp K k=1 λ k h k (˜ e, C) ˜ e exp K k=1 λ k h k (˜ e , C)</formula><p>.</p><p>The target strings˜estrings˜ strings˜e are treated as different classes for the classifier. Supposing that,</p><formula xml:id="formula_4">• r covers source span [f ϕ , f ϑ ] and target span [e γ , e σ ], • ˜ t contains K nonterminals {X k |0 ≤ k ≤ K − 1}, • X k covers source span [f ϕ k , f ϑ k ] and target span [e γ k , e σ k ],</formula><p>the MERS model used 5 kinds of source-side fea- tures as follows,</p><p>1. Lexical features: words around a rule (e.g. f ϕ−1 ) and words covered by nonterminals in a rule (e.g. f ϕ 0 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Part-of-speech features:</head><p>part-of-speech (POS) of context words that are used as lexical features.</p><p>3. Span features: span lengths of source phrases covered by nonterminals in r.</p><p>4. Parent features: the parent node of˜tof˜ of˜t in the parse tree of the source sentence.</p><p>5. Sibling features: the siblings of the root of˜tof˜ of˜t.</p><p>Note that the MERS model does not use fea- tures of the source subtree˜tsubtree˜ subtree˜t, because the source subtree˜tsubtree˜ subtree˜t is fixed for each classifier.</p><p>The MERS model was integrated into the trans- lation system as two additional features in Equa- tion 2. Supposing that the derivation R contains M rules r 1 , ..., r M with ambiguous source sub- trees, then these two MERS features are as fol- lows,</p><formula xml:id="formula_5">h1 (E, R, F ) = M m=1 log Pr˜em|˜tm Pr˜ Pr˜em|Pr˜em|˜ Pr˜em|˜tm, Cm h2 (E, R, F ) = M,<label>(4)</label></formula><p>where˜twhere˜ where˜t m and˜eand˜ and˜e m are the source subtree and the target string contained in r m , and C m is the con- text of r m . h 1 is the MERS probability feature, and, h 2 is a penalty feature counting the number of predictions made by the MERS model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our CSRS Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling</head><p>The proposed CSRS model differs from the MERS model in three ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Instead of learning a single classifier for each</head><p>source subtree˜tsubtree˜ subtree˜t, it learns a single classifier for all rules.</p><p>2. Instead of hand-crafted features, it uses a feed-forward neural network to induce fea- tures from context words.</p><p>3. Instead of one-hot representations, it uses distributed representations to exploit similar- ities between words.</p><p>First, with regard to training, our CSRS model fol- lows <ref type="bibr">Zhang et al. (2015)</ref> in approximating the pos- terior probability by a binary classifier as follows,</p><formula xml:id="formula_6">Pr˜e|˜t Pr˜ Pr˜e|Pr˜e|˜ Pr˜e|˜t, C ≈ Pr v = 1|˜e1|˜e, ˜ t, C ,<label>(5)</label></formula><p>where v ∈ {0, 1} is an indicator of whether˜twhether˜ whether˜t is translated intõ e. This is in contrast to the MERS model, which treated the rule selection problem as a multi-class classification task. If instead we attempted to estimate output probabilities for all different˜edifferent˜ different˜e, the cost of estimating the normaliza- tion coefficient would be prohibitive, as the num- ber of unique output-side word strings˜estrings˜ strings˜e is large. There are a number of remedies to this, includ- ing noise contrastive estimation ( <ref type="bibr" target="#b26">Vaswani et al., 2013)</ref>, but the binary approximation method has been reported to have better performance <ref type="bibr">(Zhang et al., 2015)</ref>.</p><p>To learn this model, we use a feed-forward neu- ral network with structure similar to neural net- work language models ( <ref type="bibr" target="#b26">Vaswani et al., 2013</ref>). The input of the neural rule selection model is a vector representation for˜tfor˜ for˜t, another vector representation for˜efor˜ for˜e, and a set of ξ vector representations for both source-side and target-side context words of r:</p><formula xml:id="formula_7">C(r) = w1, ..., w ξ (6)</formula><p>In our model, C (r) is calculated differently depending on the number of nonterminals in- cluded in the rule. Specifically, Equation 7 de- fines C out (r, n) to be context words (n-grams) around r and C in (r, n, X k ) to be boundary words (n-grams) covered by nonterminal X k in r. 1</p><formula xml:id="formula_8">C out (r, n) = f ϕ−1 ϕ−n , f θ+n θ+1 , e γ−1 γ−n , e σ+n σ+1 C in (r, n, X k ) = f ϕ k +n−1 ϕ k , f θ k θ k −(n−1) , e γ k +n−1 γ k , e σ k σ k −(n−1) (7)</formula><p>The context words used for a translation rule r with K nonterminals are shown as below.</p><formula xml:id="formula_9">K C (r) = 0 Cout (r, 6) = 1 Cout (r, 4) , Cin (r, 2, X0) &gt; 1 Cout (r, 2) , Cin (r, 2, X0) , Cin (r, 2, X1)</formula><p>We can see that rules with different numbers of nonterminals K use different context words. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S</head><p>The blue rug on the floor of your apartment is really cute</p><formula xml:id="formula_10">DT JJ NN IN DT NN IN PRP NN VBZ JJ RB NP NP NP ADJP VP PP NP PP NP 你 公寓 地板 铺 在 蓝色 毛毯 很 上 的 可爱 PP IN NP on x0 在 x0 上 r:</formula><p>: area covered by r : area covered by x0 in r For example, if r does not contain nonterminals, then C in is not used. Besides, we use more con- text words surrounding the rule (C out (r, 6)) for rules with K = 0 than rules that contain non- terminals (C out (r, 4) for K = 1 and C out (r, 2) for K &gt; 1). This is based on the intuition that rules with K = 0 can only use the context words surrounding the rule as information for rule selec- tion, hence this information is more important than for other rules. <ref type="figure" target="#fig_1">Figure 2</ref> gives an example of con- text words when applying the rule r to the example sentence.</p><p>Note that we use target-side context because source-side context is not enough for selecting correct rules. Since it is not uncommon for one source sentence to have different correct transla- tions, a translation rule used in one correct deriva- tion may be incorrect for other derivations. In these cases, target-side context is useful for select- ing appropriate translation rules. <ref type="bibr">3</ref> The vector representations for˜tfor˜ for˜t, ˜ e and C are ob- tained by using a projection matrix to project each one-hot input into a real-valued embedding vector. This projection is another key advantage over the MERS model. Because the CSRS model learns one unified model for all rules and can share all training data to learn better vector representations of words and rules, and the similarities between vectors can be used to generalize in cases such as the "thief/gunman" example in the introduction.</p><p>After calculating the projections, two hidden layers are used to combine all inputs. Finally, the neural network has two outputs Pr v = 1|˜e1|˜e, ˜ t, C</p><p>and Pr v = 0|˜e0|˜e, ˜ t, C .</p><p>To train the CSRS model, we need both posi- tive and negative training examples. Positive ex- amples, ˜ e, ˜ t, C, 1</p><p>, can be extracted directly from the parallel corpus. </p><p>where, Count˜e Count˜ Count˜e, ˜ t is how many times˜ttimes˜ times˜t is trans- lated intõ e in the parallel corpus. During translating, following the MERS model, the CSRS model only calculates probabilities for rules with ambiguous source subtrees. These pre- dictions are converted into two CSRS features for the translation system similar to the two MERS features in Equation 4: one is the product of prob- abilities calculated by the CSRS model and the other one is a penalty feature that stands for how many rules with ambiguous source subtrees are contained in one translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Usage of Minimal Rules</head><p>Despite the fact that the CSRS model can share in- formation among instances using distributed word representations, it still poses an extremely sparse learning problem. Specifically, the numbers of unique subtrees˜tsubtrees˜ subtrees˜t and strings˜estrings˜ strings˜e are extremely large, and many may only appear a few times in the cor- pus. To reduce these problems of sparsity, we pro- pose another improvement to the model, specifi- cally through the use of minimal rules. Minimal rules ( <ref type="bibr" target="#b5">Galley et al., 2004</ref>) are trans- lation rules that cannot be split into two smaller rules. For example, in <ref type="figure" target="#fig_3">Figure 3</ref>, Rule2 is not a minimal rule, since Rule2 can be split into Rule1 and Rule3. In the same way, Rule4 and Rule6 are not minimal while Rule1, Rule3 and Rule5 are minimal.</p><p>Minimal rules are more frequent than non- minimal rules and have richer training data. Hence, we can expect that a rule selection model trained on minimal rules will suffer less from data sparsity problems. Besides, without non-minimal rules, the rule selection model will need less mem- ory and can be trained faster.</p><p>To take advantage of this fact, we train another version of the CSRS model (CSRS-MINI) over only minimal rules. The probability of a non- minimal rule is then calculated using the prod- uct of the probability of minimal rules contained therein.</p><p>Note that for both the standard CSRS and CSRS-MINI models, we use the same baseline translation system which can use non-minimal translation rules. The CSRS-MINI model will break translation rules used in translations down into minimal rules and multiply all probabilities to calculate the necessary features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setting</head><p>We evaluated the proposed approach for English- to-German (ED), English-to-French (EF), English-to-Chinese (EC) and English-to-Japanese (EJ) translation tasks. For the ED and EF tasks, the translation systems are trained on Europarl v7 parallel corpus and tested on the WMT 2015 translation task. <ref type="bibr">4</ref> The test sets for the WMT 2014 translation task were used as development sets in our experiments. For the EC and EJ tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011). <ref type="bibr">5</ref> The detailed statistics for training, development and test sets are given in <ref type="table">Table 1</ref>. The word segmentation was done by BaseSeg ( <ref type="bibr" target="#b29">Zhao et al., 2006</ref>) for Chinese and Mecab 6 for Japanese.</p><p>For each translation task, we used Travatar <ref type="bibr" target="#b19">(Neubig, 2013)</ref> to train a forest-to-string transla- tion system. GIZA++ <ref type="bibr" target="#b20">(Och and Ney, 2003)</ref> was used for word alignment. A 5-gram language model was trained on the target side of the train- ing corpus using the IRST-LM Toolkit 7 with mod- ified Kneser-Ney smoothing. Rule extraction was <ref type="bibr">4</ref> The WMT tasks provided other training corpora. We used only the Europarl corpus, because training a large-scale system on the whole data set requires large amounts of time and computational resources. <ref type="bibr">5</ref> Note that NTCIR-9 only contained a Chinese-to-English translation task. Because we want to test the proposed ap- proach with a similarly accurate parsing model across our tasks, we used English as the source language in our experi- ments. In NTCIR-9, the development and test sets were both provided for the CE task while only the test set was provided for the EJ task. Therefore, we used the sentences from the NTCIR-8 EJ and JE test sets as the development set in our experiments. <ref type="bibr">6</ref>  performed using the GHKM algorithm ( <ref type="bibr" target="#b6">Galley et al., 2006</ref>) and the maximum numbers of nontermi- nals and terminals contained in one rule were set to 2 and 10 respectively. Note that when extracting minimal rules, we release this limit. The decoding algorithm is the bottom-up forest-to-string decod- ing algorithm of <ref type="bibr" target="#b18">Mi et al. (2008)</ref>. For English parsing, we used Egret 8 , which is able to output packed forests for decoding. We trained the CSRS models (CSRS and CSRS- MINI) on translation rules extracted from the training set. Translation rules extracted from the development set were used as validation data for model training to avoid over-fitting. For different training epochs, we resample negative examples for each positive example to make use of differ- ent negative examples. The embedding dimension was set to be 50 and the number of hidden nodes was 100. The initial learning rate was set to be 0.1. The learning rate was halved each time the valida- tion likelihood decreased. The number of epoches was set to be 20. A model was saved after each epoch and the model with highest validation like- lihood was used in the translation system.</p><p>We   ing instances for their model were extracted from the training set. Following their work, the iteration number was set to be 100 and the Gaussian prior was set to be 1. We also compared the original MERS model and the MERS model trained only on minimal rules (MERS-MINI) to test the benefit of using minimal rules for model training.</p><p>The MERS and CSRS models were both used to calculate features used to rerank unique 1,000- best outputs of the baseline system. Tuning is per- formed to maximize BLEU score using minimum error rate training <ref type="bibr" target="#b21">(Och, 2003)</ref>. <ref type="table" target="#tab_2">Table 2</ref> shows the translation results and <ref type="table" target="#tab_3">Table 3</ref> shows significance test results using bootstrap re- sampling <ref type="bibr" target="#b12">(Koehn, 2004</ref>): "Base" stands for the baseline system without any; "MERS", "CSRS", "MERS-MINI" and "CSRS-MINI" means the out- puts of the baseline system were reranked using features from the MERS, CSRS, MERS-MINI and CSRS-MINI models respectively. Generally, the CSRS model outperformed the MERS model and the CSRS-MINI model outperformed the MERS- MINI model on different translation tasks. In ad- dition, using minimal rules for model training ben- efitted both the MERS and CSRS models. <ref type="table" target="#tab_4">Table 4</ref> shows translation examples in the EC task to demonstrate the reason why our ap- proach improved accuracy. Among all transla- tions, T CSRS−M IN I is basically the same as the reference with only a few paraphrases that do not alter the meaning of the sentence. In con-Source typical dynamic response rate of an optical gap sensor as described above is approximately 2 khz , or 0.5 milliseconds .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head><p>(described above) (optical) (gap) (sensor) (typical) (dynamic) R1: TMERS&amp;TCSRS PP ( IN ( "of" ) NP ( NP ( DT ( "an" ) NP' ( JJ ( "optical" ) x0:NN ) ) x1:NP' ) ) → "(optical)" x1 x0 "" R2: TMERS−MINI PP ( IN ( "of" ) NP ( NP ( DT ( "an" ) NP' ( JJ ( "optical" ) x0:NP' ) ) x1:SBAR ) ) → x0 "" "(optical)" x1 "" R3 : TCSRS−MINI NP' ( JJ ( "optical" ) x0:NP' ) → "(optical)" x0 <ref type="table">Table 5</ref>: Rules used to translate the source word "optical" in different translations. Shadows (R 3 ) stand for ambiguous rules.</p><formula xml:id="formula_12">(response) (rate) (approximately) (is) 2KHz (or) 0.5 (milliseconds) TBase (typical) (dynamic) (response) (rate) (gap) (sensor) (optical) (above) (described) (is) (approximately) 2 (khz) (or) 0.5 (milliseconds) TMERS (typical) (dynamic) (response) (rate) (optical) (sensor) (as) (above) (described) (gap) (approximately) 2 (khz) (or) 0.5 (milliseconds) TCSRS (optical) (sensor) (as) (above) (described) (gap) (typical) (dynamic) (response) (rate) (is) (approximately) 2 (khz) (or) 0.5 (milliseconds) TMERS−MINI (typical) (dynamic) (response) (rate) (gap) (sensor) (optical) (above) (described) (is) (approximately) 2 (khz) (or) 0.5 (milliseconds) TCSRS−MINI (above) (described) (optical) (gap) (sensor) (typical) (dynamic) (response) (rate) (is) (approximately) 2 (khz) (or) 0.5 (milliseconds)</formula><p>trast, T Base , T M ERS , T CSRS and T M ERS−M IN I all contain apparent mistakes. For example, the source phrase "optical gap sensor" (covered by gray shadows in <ref type="table" target="#tab_4">Table 4</ref>) is wrongly translated in T Base , T M ERS , T CSRS and T M ERS−M IN I due to incorrect reorderings. <ref type="table">Table 5</ref> shows rules used to translate the source word "optical" in different translations: R 1 is used in T M ERS and T CSRS ; R 2 is used in</p><formula xml:id="formula_13">T M ERS−M IN I ; R 3 is used in T CSRS−M IN I .</formula><p>Al- though the source word "optical" is translated to the correct translation "(optical)" in all trans- lations, R 1 , R 2 and R 3 cause different reorderings for the source phrase "optical gap sensor". R 3 re- orders this source phrase correctly while R 1 and R 2 cause wrong reorderings for this source phrase.</p><p>We can see that R 1 is umambiguous, so the MERS and CSRS models will give probability 1 to R 1 , which could make the MERS and CSRS models prefer T M ERS and T CSRS . This is a typ- ical translation error caused by sparse rules since the source subtree in R 1 does not have other trans- lations in the training corpus.</p><p>To compare the MERS-MINI and CSRS-MINI models, <ref type="table" target="#tab_5">Table 6</ref> shows minimal rules (R 2a , R 2b , R 3a and R 3b ) contained in R 2 and R 3 . <ref type="table">Table 7</ref> shows probabilities of these minimal rules calcu- lated by the MERS-MINI and CSRS-MINI mod- els respectively. We can see that the CSRS- MINI model gave higher scores for the correct translation rules R 3a and R 3b than the MERS- MINI model, while the MERS-MINI model gave a higher score to the incorrect rule R 2b than the CSRS-MINI model.</p><p>Note that R 2b and R 3b are the same rule, but the target-side context in T M ERS−M IN I and T CSRS−M IN I is different. The CSRS-MINI model will give R 2b and R 3b different scores be- cause the CSRS-MINI model used target-side con- text. However, the MERS-MINI model only used source-side features and gave R 2b and R 3b the same score. The fact that the CSRS-MINI model  <ref type="figure">( IN ( "of" )</ref> NP ( NP ( DT ( "an" ) NP' ( x0:JJ x1:NP' ) ) x2:SBAR ) ) → x1 "" x0 x2 "" R 2b JJ ( "optical" ) → "(optical)"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R3a</head><p>NP' ( x0:JJ x1:NP' ) → x0 x1 R 3b JJ ( "optical" ) → "(optical)"  <ref type="table">Table 7</ref>: Scores of minimal rules.</p><p>gave a higher score for R 3b than R 2b means that the CSRS-MINI model predicted the target string in R 2b and R 3b is a good translation in the con- text of T CSRS−M IN I but not so good in the con- text of T M ERS−M IN I . As we can see, the tar- get phrase "(above) (described) (of) (optical) (gap) (sensor)" around "(optical)" in T CSRS−M IN I is a reasonable Chinese phrase while the target phrase "(gap) (sensor) (of) (optical) (above) (described) (of)" around " (optical)" in T M ERS−M IN I does not make sense. Namely, the CSRS model trained with target-side context can perform rule selection considering target sen- tence fluency, which is the reason why target-side context can help in the rule selection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>To analyze the influence of different features, we trained the MERS model using source-side and target-side n-gram lexical features similar to the CSRS model. When using this feature set, the performance of the MERS model dropped signifi- cantly. This indicates that the syntactic, POS and span features used in the original MERS model are important for their model, since these fea- tures can generalize better. Purely lexical features are less effective due to sparsity problems when training one maximum entropy based classifier for each ambiguous source subtree and training data for each classifier is quite limited. In contrast, the CSRS model is trained in a continuous space and does not split training data, which relieves the sparsity problem of lexical features. As a result, the CSRS model achieved better performance us- ing only lexical features compared to the MERS model. We also tried to use pre-trained word em- bedding features for the MERS model, but it did not improve the performance of the MERS model, which indicates that the log-linear model is not able to benefit from distributed representations as well as the neural network model.</p><p>We also tried reranking with both the CSRS and MERS models added as features, but it did not achieve further improvement compared to only us- ing the CSRS model. This indicates that although these two models use different type of features, the information contained in these features are similar. For example, the POS features used in the MERS model and the distributed representations used in the CSRS model are both used for better general- ization.</p><p>In addition, using both the CSRS and CSRS- MINI models did not improve over using only the CSRS-MINI model in our experiments. There are two main differences between the CSRS and CSRS-MINI models. First, minimal rules are more frequent and have more training data than non-minimal rules, which is why the CSRS-MINI model is more robust than the CSRS model. Sec- ond, non-minimal rules contain more informa- tion than minimal rules. For example, in <ref type="figure">Fig- ure</ref> 3, Rule4 contains more information than Rule1, which could be an advantage for rule selec- tion. However, the information contained in Rule4 will be considered as context features for Rule1. Therefore, this is no longer an advantage for the CSRS model as long as we use rich enough con- text features, which could be the reason why using both the CSRS and CSRS-MINI models cannot further improve the translation quality compared to using only the CSRS-MINI model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>The rule selection problem for syntax-based SMT has received much attention.  proposed a lexicalized rule selection model to per- form context-sensitive rule selection for hierarchi- cal phrase-base translation. <ref type="bibr" target="#b1">Cui et al. (2010)</ref> in- troduced a joint rule selection model for hierarchi- cal phrase-based translation, which also approxi- mated the rule selection problem by a binary clas- sification problem like our approach. However, these two models adopted linear classifiers simi- lar to those used in the MERS model ( , which suffers more from the data sparsity problem compared to the CSRS model. There are also existing works that exploited neural networks to learn translation probabili- ties for translation rules used in the phrase-based translation model. Namely, these methods esti- mated translation probabilities for phrase pairs ex- tracted from the parallel corpus. <ref type="bibr" target="#b22">Schwenk (2012)</ref> proposed a continuous space translation model, which calculated the translation probability for each word in the target phrase and then multi- plied the probabilities together as the translation probability of the phrase pair. <ref type="bibr" target="#b7">Gao et al. (2014)</ref> and <ref type="bibr" target="#b27">Zhang et al. (2014)</ref> proposed methods to learn continuous space phrase representations and use the similarity between the source and tar- get phrases as translation probabilities for phrase pairs. All these three methods can only be used for the phrase-based translation model, not for syntax- based translation models.</p><p>There are also works that used minimal rules for modeling. <ref type="bibr" target="#b25">Vaswani et al. (2011)</ref> proposed a rule Markov model using minimal rules for both train- ing and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. <ref type="bibr" target="#b3">Durrani et al. (2013)</ref> proposed a method to model with minimal trans- lation units and decode with phrases for phrase- based SMT to improve translation performances. Both of these two methods do not use distributed representations as used in our model for better generalization.</p><p>In addition, neural machine translation (NMT) has shown promising results recently <ref type="bibr" target="#b24">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b15">Luong et al., 2015a;</ref><ref type="bibr" target="#b11">Jean et al., 2015;</ref><ref type="bibr" target="#b16">Luong et al., 2015b</ref>). NMT uses a recurrent neural network to encode the whole source sentence and then produce the target words one by one. These models can be trained on parallel corpora and do not need word alignments to be learned in advance. There are also neural translation models that are trained on word-aligned parallel corpus <ref type="bibr" target="#b2">(Devlin et al., 2014;</ref><ref type="bibr" target="#b17">Meng et al., 2015;</ref><ref type="bibr">Zhang et al., 2015;</ref><ref type="bibr" target="#b23">Setiawan et al., 2015)</ref>, which use the alignment information to decide which parts of the source sentence are more important for predicting one particular target word. All these models are trained on plain source and target sentences without considering any syn- tactic information while our neural model learns rule selection for tree-based translation rules and makes use of the tree structure of natural language for better translation. There is also a new syn- tactic NMT model ( <ref type="bibr" target="#b4">Eriguchi et al., 2016)</ref>, which extends the original sequence-to-sequence NMT model with the source-side phrase structure. Al- though this model takes source-side syntax into consideration, it still produces target words one by one as a sequence. In contrast, the tree-based translation rules used in our model can take advan- tage of the hierarchical structures of both source and target languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a CSRS model for syntax-based SMT, which is learned by a feed- forward neural network on a continuous space. Compared with the previous MERS model that used discrete representations of words as features, the CSRS model uses real-valued vector represen- tations of words and can exploit similarity infor- mation between words for better generalization. In addition, we propose to use only minimal rules for rule selection to further relieve the data spar- sity problem, since minimal rules are more fre- quent and have richer training data. In our exper- iments, the CSRS model outperformed the previ- ous MERS model and the usage of minimal rules benefitted both CSRS and MERS models on dif- ferent translation tasks.</p><p>For future work, we will explore more sophis- ticated features for the CSRS model, such as syn- tactic dependency relationships and head words, since only simple lexical features are used in the current incarnation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An ambiguous source subtree with different translations (English-to-Chinese).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Context word examples. The red words are contained in C out (r, 4) and the blue words are contained in C in (r, 2, X 0 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Rules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>implemented Liu et al. (2008)'s MERS model to compare with our approach. The train- 8 https://code.google.com/archive/p/egret-parser ED EF EC EJ Base 15.00 26.76 29.42 37.10 MERS 15.62 27.33 29.75 37.76 CSRS 16.15 28.05 30.12 37.83 MERS-MINI 15.77 28.13 30.53 38.14 CSRS-MINI 16.49 28.30 31.63 38.32</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Translation results. The bold numbers stand for the best systems.</head><label>2</label><figDesc></figDesc><table>ED 
EF 
EC 
EJ 
CSRS vs. MERS 
&gt;&gt; &gt;&gt; &gt; 
− 
CSRS-MINI vs. MERS-MINI &gt;&gt; − 
&gt;&gt; − 
MERS-MINI vs. MERS 
− 
&gt;&gt; &gt;&gt; &gt;&gt; 
CSRS-MINI vs. CSRS 
&gt; 
− 
&gt;&gt; &gt;&gt; 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Significance test results. The symbol &gt;&gt; 
(&gt;) represents a significant difference at the p &lt; 
0.01 (p &lt; 0.05) level and the symbol -represents 
no significant difference at the p &lt; 0.05 level. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 : Translation examples.</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Minimal rules contained in R 2 and R 3 . 
Shadows (R 2b , R 3a and R 3b ) stand for ambiguous 
rules. 

MERS-MINI CSRS-MINI 
R2a 1 
1 
R 2b 0.5441 
0.09632 
R3a 0.9943 
0.9987 
R 3b 0.5441 
0.7317 

</table></figure>

			<note place="foot" n="1"> Note that when extracting Cout, we use &quot;s&quot; and &quot;/s&quot; for context words that exceed the length of the sentence; When extracting Cin, we use &quot;non&quot; for context words that exceed the length of the nonterminal. Words that occur less than twice in the training data are replaced by &quot;unk&quot;. 2 In most cases, restrictions on extracted rules will ensure that rules will only contain two nonterminals. However, when using minimal rules as described in the next section, more than two nonterminals are possible, and in these cases, only contextual information covered by the first two nonterminals is used in the input. These cases are sufficiently rare, however, that we chose to consider only the first two.</note>

			<note place="foot" n="3"> It is also possible to consider target-side context in a framework like the MERS model, but we show in experiments that a linear model using the same features as the CSRS model did not improve accuracy.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A joint rule selection model for hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model with minimal translation units, but decode with phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Tree-to-sequence attentional neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06075</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What&apos;s in a translation rule?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable inference and training of context-rich syntactic translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Deneefe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Thayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="961" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations for translation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="699" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overview of the patent machine translation task at the NTCIR-9 workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><forename type="middle">Po</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin K</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NTCIR-9</title>
		<meeting>NTCIR-9</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="559" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Training tree transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving statistical machine translation using lexicalized rule selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Coling</title>
		<meeting>Coling</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Treeto-string alignment template for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maximum entropy based rule selection model for syntax-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="89" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Encoding source language with convolutional neural network for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACLIJCNLP</title>
		<meeting>ACLIJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="20" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Forestbased translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT-ACL</title>
		<meeting>HLT-ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="192" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Travatar: A forest-to-string machine translation engine based on tree transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="91" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Continuous space translation models for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1071" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Statistical machine translation features with multitask tensor networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="31" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rule markov models for fast treeto-string translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT-ACL</title>
		<meeting>HLT-ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="856" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoding with large-scale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1387" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bilingually-constrained phrase embeddings for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="111" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graham Neubig, and Satoshi Nakamura. 2015. A binarized neural network joint model for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<biblScope unit="page" from="2094" to="2099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An improved chinese word segmentation system with conditional random field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ning</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGHAN</title>
		<meeting>SIGHAN</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="162" to="165" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
