<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Conditional Variational Framework for Dialog Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Long</surname></persName>
						</author>
						<title level="a" type="main">A Conditional Variational Framework for Dialog Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="504" to="509"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2080</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Deep latent variable models have been shown to facilitate the response generation for open-domain dialog systems. However , these latent variables are highly ran-domized, leading to uncontrollable generated responses. In this paper, we propose a framework allowing conditional response generation based on specific attributes. These attributes can be either manually assigned or automatically detected. Moreover , the dialog states for both speakers are modeled separately in order to reflect personal features. We validate this framework on two different scenarios, where the attribute refers to genericness and sentiment states respectively. The experiment result testified the potential of our model, where meaningful responses can be generated in accordance with the specified attributes.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Seq2seq neural networks, ever since the success- ful application in machine translation <ref type="bibr" target="#b18">(Sutskever et al., 2014</ref>), have demonstrated impressive re- sults on dialog generation and spawned a great deal of variants ( <ref type="bibr" target="#b19">Vinyals and Le, 2015;</ref><ref type="bibr" target="#b21">Yao et al., 2015;</ref><ref type="bibr" target="#b17">Sordoni et al., 2015;</ref><ref type="bibr" target="#b15">Shang et al., 2015)</ref>. The vanilla seq2seq models suffer from the problem of generating too many generic re- sponses (generic denotes safe, commonplace re- sponses like "I don't know"). One major rea- son is that the element-wise prediction models stochastical variations only at the token level, se- ducing the system to gain immediate short re- wards and neglect the long-term structure. To * Authors contributed equally. Correspondence should be sent to H. Su (suhui15@iscas.ac.cn) and X. Shen (xshen@lsv.uni-saarland. <ref type="bibr">de)</ref> cope with this problem, ( <ref type="bibr" target="#b14">Serban et al., 2017</ref>) pro- posed a variational hierarchical encoder-decoder model (VHRED) that brought the idea of varia- tional auto-encoders (VAE) ( <ref type="bibr" target="#b7">Kingma and Welling, 2013;</ref>) into dialog genera- tion. For each utterance, VHRED samples a la- tent variable as a holistic representation so that the generative process will learn to maintain a coher- ent global sentence structure. However, the latent variable is learned purely in an unsupervised way and can only be explained vaguely as higher level decisions like topic or sentiment. Though effec- tive in generating utterances with more informa- tion content, it lacks the ability of explicitly con- trolling the generating process.</p><p>This paper presents a conditional variational framework for generating specific responses, in- spired by the semi-supervised deep generative model ( ). The principle idea is to generate the next response based on the dialog context, a stochastical latent variable and an exter- nal label. Furthermore, the dialog context for both speakers is modeled separately because they have different talking styles, personality and sentiment. The whole network structure functions like a con- ditional VAE ( <ref type="bibr" target="#b16">Sohn et al., 2015;</ref><ref type="bibr" target="#b20">Yan et al., 2016)</ref>. We test our framework on two scenarios. For the first scenario, the label serves as a signal to indi- cate whether the response is generic or not. By as- signing different values to the label, either generic or non-generic responses can be generated. For the second scenario, the label represents an imi- tated sentiment tag. Before generating the next re- sponse, the appropriate sentiment tag is predicted to direct the generating process.</p><p>Our framework is expressive and extendable. The generated responses agree with the predefined labels while maintaining meaningful. By chang- ing the definition of the label, our framework can be easily applied to other specific areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models</head><p>To provide a better dialog context, we build a hier- archical recurrent encoder-decoder with separated context models (SPHRED). This section first in- troduces the concept of SPHRED, then explains the conditional variational framework and two ap- plication scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SPHRED</head><p>We decomposes a dialog into two levels: se- quences of utterances and sub-sequences of words, as in ( . Let w 1 , . . . , w N be a dialog with N utterances, where w n = (w n,1 , . . . , w n,Mn ) is the n-th utterance. The prob- ability distribution of the utterance sequence fac- torizes as:</p><formula xml:id="formula_0">N n=1 Mn m=1 P θ (w m,n |w m,&lt;n , w &lt;n )<label>(1)</label></formula><p>where θ represents the model parameters and w &lt;n encodes the dialog context until step n.</p><p>If we model the dialog context through a single recurrent neural network (RNN), it can only rep- resent a general dialog state in common but fail to capture the respective status for different speakers. This is inapplicable when we want to infer implicit personal attributes from it and use them to influ- ence the sampling process of the latent variable, as we will see in Section 2.4. Therefore, we model the dialog status for both speakers separately. As displayed in <ref type="figure" target="#fig_0">Figure 1</ref>, SPHRED contains an en- coder RNN of tokens and two status RNNs of ut- terances, each for one speaker. When modeling turn k in a dialog, each status RNN takes as in- put the last encoder RNN state of turn k − 2. The ) and P θ (w n | y n , z n , w n−1 1 ). When y t+1 is known, there exists an additional link from y t+1 to z (dashed line). C t encodes context information up to time t. Dotted lines are posterior approximation Q φ (z n |y n , w n 1 ).</p><p>higher-level context vector is the concatenation of both status vectors. We will show later that SPHRED not only well keeps individual features, but also provides a bet- ter holistic representation for the response decoder than normal HRED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Conditional Variational Framework</head><p>VAEs have been used for text generation in <ref type="bibr" target="#b1">(Bowman et al., 2015;</ref><ref type="bibr" target="#b12">Semeniuta et al., 2017)</ref>, where texts are synthesized from latent variables. Start- ing from this idea, we assume every utterance w n comes with a corresponding label y n and la- tent variable z n . The generation of z n and w n are conditioned on the dialog context provided by SPHRED, and this additional class label y n . This includes 2 situations, where the label of the next sequence is known (like for Scenario 1 in Section 2.3) or not (Section 2.4). For each utterance, the latent variable z n is first sampled from a prior dis- tribution. The whole dialog can be explained by the generative process:</p><formula xml:id="formula_1">P θ (z n |y n , w n−1 1 ) = N (µ prior , Σ prior ) (2) P θ (w n | y n , z n , w n−1 1 ) = Mn m=1 P θ (w n,m | y n , z n , w n−1 1 , w n,m−1 n,1 )<label>(3)</label></formula><p>When the label y n is unknown, a suitable classi- fier is implemented to first predict it from the con- text vector. This classifier can be designed as, but not restricted to, multilayer perceptrons (MLP) or support vector machines (SVM). Similarly, the posterior distribution of z n is ap- proximated as in Equation 4, where the context and label of the next utterance is provided. The graphical model is depicted in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><formula xml:id="formula_2">Q φ (z n |y n , w n 1 ) = N (µ posterior , Σ posterior ) (4)</formula><p>The training objective is derived as in For- mula 5, which is a lower bound of the logarithm of the sequence probability. When the label is to be predicted (¯ y n ), an additional classification loss (first term) is added such that the distribution q φ (y n |w n−1 1 ) can be learned together with other parameters.</p><formula xml:id="formula_3">log P θ (w 1 , . . . , w N ) ≥ E p(wn,yn) q φ (y n |w n−1 1 ) − N n=1 KL Q ψ (z n | w n 1 , y n )||P θ (z n | w n−1 1 , ¯ y n ) + E Q ψ (zn|w n 1 ,yn) [log P θ (w n | z n , w n−1 1 , y n )]<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Scenario 1</head><p>A major focus in the current research is to avoid generating generic responses, so in the first sce- nario, we let the label y indicate whether the cor- responding sequence is a generic response, where y = 1 if the sequence is generic and y = 0 oth- erwise. To acquire these labels, we manually con- structed a list of generic phrases like "I have no idea", "I don't know", etc. Sequences containing any one of such phrases are defined as generic, which in total constitute around 2 percent of the whole corpus. At test time, if the label is fixed as 0, we expect the generated response should mostly belong to the non-generic class.</p><p>No prediction is needed, thus the training cost does not contain the first item in Formula 5. This scenario is designed to demonstrate our frame- work can explicitly control which class of re- sponses to generate by assigning corresponding values to the label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Scenario 2</head><p>In the second scenario, we experiment with as- signing imitated sentiment tags to generated re- sponses. The personal sentiment is simulated by appending :), :( or :P at the end of each utter- ance, representing positive, negative or neutral sentiment respectively. For example, if we ap- pend ":)" to the original "OK", the resulting "OK :)" becomes positive. The initial utterance of ev- ery speaker is randomly tagged. We consider two rules for the tags of next utterances. Rule 1 con- fines the sentiment tag to stay constant for both speakers. Rule 2 assigns the sentiment tag of next utterance as the average of the preceding two ones. Namely, if one is positive and the other is negative, the next response would be neutral.</p><p>The label y represents the sentiment tag, which is unknown at test time and needs to be predicted from the context. The probability q φ (y n |w n−1 1 ) is modeled by feedforward neural networks. This scenario is designed to demonstrate our frame- work can successfully learn the manually defined rules to predict the proper label and decode re- sponses conforming to this label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We conducted our experiments on the Ubuntu di- alog Corpus ( <ref type="bibr" target="#b9">Lowe et al., 2015)</ref>, which contains about 500,000 multi-turn dialogs. The vocabulary was set as the most frequent 20,000 words. All the letters are transferred to lowercase and the Out- of-Vocabulary (OOV) words were preprocessed as &lt;unk&gt; tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Procedures</head><p>Model hyperparameters were set the same as in VHRED model except that we reduced by half the context RNN dimension. The encoder, con- text and decoder RNNs all make use of the Gated Recurrent Unit (GRU) structure ( <ref type="bibr" target="#b2">Cho et al., 2014</ref>). Labels were mapped to embeddings with size 100 and word vectors were initialized with the pu- bic Word2Vec embeddings trained on the Google News Corpus 1 . Following <ref type="bibr" target="#b1">(Bowman et al., 2015</ref>), 25% of the words in the decoder were randomly dropped. We multiplied the KL divergence and classification error by a scalar which starts from zero and gradually increases so that the training would initially focus on the stochastic latent vari- ables. At test time, we outputted responses us- ing beam search with beam size set to 5 (Graves, 2012) and &lt;unk&gt; tokens were prevented from being generated. We implemented all the mod- els with the open-sourced Python library Tensor- flow ( <ref type="bibr" target="#b0">Abadi et al., 2016</ref>) and optimized using the Adam optimizer ( <ref type="bibr" target="#b5">Kingma and Ba, 2014</ref>). Dialogs are cut into set of slices with each slice containing 80 words then fed into the GPU memory. All mod- els were trained with batch size 128. We use the learning rate 0.0001 for our framework and 0.0002 for other models. Every model is tested on the val-idation dataset once every epoch and stops until it gains nothing more within 5 more epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation</head><p>Accurate automatic evaluation of dialog gener- ation is difficult ( <ref type="bibr" target="#b17">Galley et al., 2015;</ref><ref type="bibr" target="#b10">Pietquin and Hastie, 2013)</ref>. In our experiment, we con- ducted three embedding-based evaluations (aver- age, greedy and extrema) ( <ref type="bibr" target="#b8">Liu et al., 2016</ref>) on all our models, which map responses into vector space and compute the cosine similarity. Though not necessarily accurate, the embedding-based metrics can to a large extent measure the semantic similarity and test the ability of successfully gen- erating a response sharing a similar topic with the golden answer. The results of a GRU language model (LM), HRED and VHRED were also pro- vided for comparison. For the two scenarios of our framework, we further measured the percentage of generated responses matching the correct labels (accuracy). In ( <ref type="bibr" target="#b8">Liu et al., 2016)</ref>, current popular metrics are shown to be not well correlated with human judgements. Therefore, we also carried out a human evaluation. 100 examples were randomly sampled from the test dataset. The generated re- sponses from the models were shuffled and ran- domly distributed to 5 volunteers 2 . People were requested to give a binary score to the response from 3 aspects, grammaticality, coherence with history context and diversity. Every response was evaluated 3 times and the result agreed by most people was adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results of Metric-based Evaluation</head><p>As can be seen from <ref type="table">Table 1</ref>, SPHRED outper- forms both HRED and LM over all the three embedding-based metrics. This implies separating the single-line context RNN into two independent parts can actually lead to a better context represen- tation. It is worth mentioning the size of context RNN hidden states in SPHRED is only half of that in HRED, but it still behaves better with fewer pa- rameters. Hence it is reasonable to apply this con- text information to our framework.</p><p>The last 4 rows in <ref type="table">Table 1</ref> display the results of our framework applied in two scenarios men- tioned in Section 2.3 and 2.4. SCENE1-A and SCENE1-B correspond to Scenario 1 with the la- bel fixed as 1 and 0. 90.9% of generated responses in SCENE1-A are generic and 86.9% in SCENE1- B are non-generic according to the manually-built rule, which verified the proper effect of the label. SCENE2-A and SCENE2-B correspond to rule 1 and 2 in Scenario 2. Both successfully predict the sentiment with very minor mismatches (0.2% and 0.8%). The high accuracy further demon- strated SPHRED's capability of maintaining indi- vidual context information. We also experimented by substituting the encoder with a normal HRED, the resulting model cannot predict the correct sen- timent at all because the context information is highly mingled for both speakers. The embedding based scores of our framework are still compara- ble with SPHRED and even better than VHRED. Imposing an external label didn't bring any signif- icant quality decline.  <ref type="table">Table 1</ref>: Metric-based Evaluation. SCENE1-A is set to generate generic responses, so it makes no sense to measure it with embedding-based metrics</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results of Human Evaluation</head><p>We conducted human evaluations on VHRED and our framework <ref type="table">(Table 3</ref>). All models share similar scores, except SCENE1-A receiving lower scores with respect to coherence. This can be explained by the fact that SCENE1-A is trained to generate only generic responses, which limits its power of taking coherence into account. VHRED and Sce- nario 2 perform close to each other. Scenario 1, due to the effect of the label, receives extreme scores for diversity.</p><p>In general, the statistical results of human eval- uations on sentence quality are very similar be- tween the VHRED model and our framework. This agrees with the metric-based results and sup- ports the conclusion drawn in Section 3.3. Though the sample size is relatively small and human judgements can be inevitably disturbed by subjec- tive factors, we believe these results can shed some light on the understanding of our framework.</p><p>A snippet of the generated responses can be</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head><p>Response anybody in the house ????? eou how to change the default ubuntu wall paper ? eou how to change the default ubuntu wallpaper ? eou eot Is there an echo in your head ? Is there an echo in your head ? eou eot what do you mean ? eou eot Repeating = Bad . eou eot no body is answering me eou eot . LM: What do you want to do with it ? HRED: I don't know . SPHRED: If you want to change the default wallpaper , you can change the default theme How can I install seamonkey ? eou To save me the pastebin eou I am looking to install seamonkey , anyone ? eou eot http://www.seamonkey-project.org/ eou eot It i snot in the ubuntu repository any more eou eot .   <ref type="table">Table 3</ref>: Human Judgements, G refers to Gram- maticality and the last four columns is the confu- sion matrix with respect to coherence and diversity seen in <ref type="table" target="#tab_2">Table 2</ref>. Generally speaking, SPHRED better captures the intentions of both speakers, while HRED updates the common context state and the main topic might gradually vanish for the different talking styles of speakers. SCENE1-A and SCENE1-B are designed to reply to a given context in two different ways. We can see both re- sponses are reasonable and fit into the right class. The third and fourth rows are the same context with different appended sentiment tags and rules, both generate a suitable response and append the correct tag at the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and future work</head><p>In this work, we propose a conditional varia- tional framework for dialog generation and ver- ify it on two scenarios. To model the dialog state for both speakers separately, we first devised the SPHRED structure to provide the context vec- tor for our framework. Our evaluation results</p><p>show that SPHRED can itself provide a better con- text representation than HRED and help generate higher-quality responses. In both scenarios, our framework can successfully learn to generate re- sponses in accordance with the predefined labels.</p><p>Though with the restriction of an external label, the score of generated responses didn't signifi- cantly decreased, meaning that we can constrain the generation within a specific class while still maintaining the quality. The manually-defined rules, though primitive, represent two most common sentiment shift con- ditions in reality. The results demonstrated the potential of our model. To apply to real-world scenarios, we only need to adapt the classifier to detect more complex sentiments, which we leave for future research. External models can be used for detecting generic responses or classify- ing sentiment categories instead of rule or symbol- based approximations. We focused on the con- trolling ability of our framework, future research can also experiment with bringing external knowl- edge to improve the overall quality of generated responses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Computational graph for SPHRED structure. The status vector for Speaker A and Speaker B is modeled by separate RNNs then concatenated to represent the dialog context.</figDesc><graphic url="image-1.png" coords="2,60.66,62.81,240.95,91.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graphical model for the conditional variational framework. Solid lines denote generative model P θ (z n |y n , w n−1 1</figDesc><graphic url="image-2.png" coords="2,356.88,62.81,119.06,76.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Examples of context-response pairs for the neural network models. eou denotes end-of-
utterance and eot denotes end-of-turn token 

Model 
G 
CD C¬D ¬CD ¬C¬D 
VHRED 
97% 41% 12 % 24% 
23% 
SCENE1-A 96% 3% 37% 1% 
59% 
SCENE1-B 96% 47% 9% 40% 
4% 
SCENE2-A 97% 40% 14 % 23% 
23% 
SCENE2-B 95% 38% 20% 31% 
11% 

</table></figure>

			<note place="foot" n="1"> https://code.google.com/archive/p/ word2vec/</note>

			<note place="foot" n="2"> All volunteers are well-educated students who have received a Bachelor&apos;s degree on computer science or above.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgement</head><p>This work was supported by the National Natu-ral Science of China under Grant No. 61602451, 61672445 and JSPS KAKENHI Grant Numbers 15H02754, 16K12546.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06349</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06863</idno>
		<title level="m">Chris Quirk, Margaret Mitchell, Jianfeng Gao, and Bill Dolan. 2015. deltableu: A discriminative metric for generation tasks with intrinsically diverse targets</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<title level="m">Sequence transduction with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08023</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08909</idno>
		<title level="m">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey on metrics for the evaluation of user simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The knowledge engineering review</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="59" to="73" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<title level="m">Stochastic backpropagation and approximate inference in deep generative models</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A hybrid convolutional variational autoencoder for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislau</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhardt</forename><surname>Barth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02390</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02364</idno>
		<title level="m">Neural responding machine for short-text conversation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06714</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">A neural conversational model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="776" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.08565</idno>
		<title level="m">Attention with intention for a neural network conversation model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
