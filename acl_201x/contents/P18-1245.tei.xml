<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Structured Variational Autoencoder for Contextual Morphological Inflection</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 2631</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Wolf-Sonkin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><forename type="middle">J</forename><surname>Mielke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Structured Variational Autoencoder for Contextual Morphological Inflection</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2631" to="2641"/>
							<date type="published">July 15-20, 2018. 2018. 2631</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Statistical morphological inflectors are typically trained on fully supervised, type-level data. One remaining open research question is the following: How can we effectively exploit raw, token-level data to improve their performance? To this end, we introduce a novel generative latent-variable model for the semi-supervised learning of inflection generation. To enable posterior inference over the latent variables, we derive an efficient variational inference procedure based on the wake-sleep algorithm. We experiment on 23 languages, using the Universal Dependencies corpora in a simulated low-resource setting, and find improvements of over 10% absolute accuracy in some cases.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The majority of the world's languages overtly en- codes syntactic information on the word form it- self, a phenomenon termed inflectional morphology <ref type="bibr" target="#b12">(Dryer et al., 2005</ref>). In English, for example, the verbal lexeme with lemma talk has the four forms:</p><p>talk, talks, talked and talking. Other languages, such as Archi <ref type="bibr" target="#b22">(Kibrik, 1998)</ref>, distinguish more than a thousand verbal forms. Despite the cornucopia of unique variants a single lexeme may mutate into, native speakers can flawlessly predict the correct variant that the lexeme's syntactic context dictates. Thus, in computational linguistics, a natural ques- tion is the following: Can we estimate a probability model that can do the same?</p><p>The topic of inflection generation has been the focus of a flurry of individual attention of late and, moreover, has been the subject of two shared tasks * All authors contributed equally.  (1) and overlayed with example values of the random variables in the sequence. We highlight that all the conditionals in the Bayesian network are recurrent neural networks, e.g., we note that mi depends on m&lt;i because we employ a recurrent neural network to model the morphological tag sequence.</p><p>( <ref type="bibr" target="#b8">Cotterell et al., 2016</ref><ref type="bibr" target="#b6">Cotterell et al., , 2017</ref>). Most work, however, has focused on the fully supervised case-a source lemma and the morpho-syntactic properties are fed into a model, which is asked to produce the desired inflection. In contrast, our work focuses on the semi-supervised case, where we wish to make use of unannotated raw text, i.e., a sequence of inflected tokens.</p><p>Concretely, we develop a generative directed graphical model of inflected forms in context. A contextual inflection model works as follows: Rather than just generating the proper inflection for a single given word form out of context (for exam- ple walking as the gerund of walk), our generative model is actually a fully-fledged language model. In other words, it generates sequences of inflected words. The graphical model is displayed in <ref type="figure" target="#fig_0">Fig. 1</ref> and examples of words it may generate are pasted on top of the graphical model notation. That our model is a language model enables it to exploit both inflected lexicons and unlabeled raw text in a prin- cipled semi-supervised way. In order to train using raw-text corpora (which is useful when we have less annotated data), we marginalize out the unob- served lemmata and morpho-syntactic annotation from unlabeled data. In terms of <ref type="figure" target="#fig_0">Fig. 1</ref>, this refers to marginalizing out m 1 , . . . , m 4 and 1 , . . . , <ref type="bibr">4</ref> . As this marginalization is intractable, we derive a variational inference procedure that allows for efficient approximate inference. Specifically, we modify the wake-sleep procedure of . It is the inclusion of raw text in this fashion that makes our model token level, a novelty in the camp of inflection generation, as much recent work in inflection generation ( <ref type="bibr" target="#b11">Dreyer et al., 2008;</ref><ref type="bibr" target="#b13">Durrett and DeNero, 2013;</ref><ref type="bibr" target="#b28">Nicolai et al., 2015;</ref><ref type="bibr" target="#b0">Ahlberg et al., 2015;</ref><ref type="bibr" target="#b14">Faruqui et al., 2016</ref>), trains a model on type-level lexicons.</p><p>We offer empirical validation of our model's utility with experiments on 23 languages from the Universal Dependencies corpus in a simulated low- resource setting. 1 Our semi-supervised scheme im- proves inflection generation by over 10% absolute accuracy in some cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Morphological Inflection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Inflectional Morphology</head><p>To properly discuss models of inflectional morphol- ogy, we require a formalization. We adopt the framework of word-based morphology <ref type="bibr" target="#b2">(Aronoff, 1976;</ref><ref type="bibr" target="#b32">Spencer, 1991)</ref>. Note in the present paper, we omit derivational morphology.</p><p>We define an inflected lexicon as a set of 4- tuples consisting of a part-of-speech tag, a lexeme, an inflectional slot, and a surface form. A lexeme is a discrete object that indexes the word's core meaning and part of speech. In place of such an abstract lexeme, lexicographers will often use a lemma, denoted by , which is a designated 2 sur- 1 We make our code and data available at: https:// github.com/lwolfsonkin/morph-svae.</p><p>2 A specific slot of the paradigm is chosen, depending on face form of the lexeme (such as the infinitive). For the remainder of this paper, we will use the lemma as a proxy for the lexeme, wherever convenient, although we note that lemmata may be ambiguous:</p><p>bank is the lemma for at least two distinct nouns and two distinct verbs. For inflection, this ambigu- ity will rarely 3 play a role-for instance, all senses of bank inflect in the same fashion. A part-of-speech (POS) tag, denoted t, is a coarse syntactic category such as VERB. Each POS tag allows some set of lexemes, and also al- lows some set of inflectional slots, denoted as σ, such as <ref type="bibr">TNS=PAST, PERSON=3</ref> . Each allowed tag, lexeme, slot triple is realized-in only one way-as an inflected surface form, a string over a fixed phonological or orthographic alphabet Σ.</p><p>(In this work, we take Σ to be an orthographic alphabet.) Additionally, we will define the term morphological tag, denoted by m, which we take to be the POS-slot pair m = t, σ. We will further define T as the set of all POS tags and M as the set of all morphological tags.</p><p>A paradigm π(t, ) is the mapping from tag t's slots to the surface forms that "fill" those slots for lexeme/lemma . For example, in the English paradigm π(VERB, talk), the past-tense slot is said to be filled by talked, meaning that the lexicon con- tains the tuple VERB, talk, PAST, talked.</p><p>A cheat sheet for the notation is provided in Tab. 2.</p><p>We will specifically work with the UniMorph annotation scheme <ref type="bibr" target="#b36">(Sylak-Glassman, 2016)</ref>. Here, each slot specifies a morpho-syntactic bundle of inflectional features such as tense, mood, person, number, and gender. For example, the German surface form Wörtern is listed in the lexicon with tag NOUN, lemma Wort, and a slot specifying the feature bundle NUM=PL, CASE=DAT . The full paradigms π(NOUN, Wort) and π(NOUN, Herr) are found in Tab. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Morphological Inflection</head><p>Now, we formulate the task of context-free mor- phological inflection using the notation developed in §2. Given a set of N form-tag-lemma triples</p><formula xml:id="formula_0">{{f i , m i , i } N i=1</formula><p>, the goal of morphological inflec- tion is to map the pair m i , i to the form f i . As the part-of-speech tag -all these terms are defined next.   the definition above indicates, the task is tradition- ally performed at the type level. In this work, how- ever, we focus on a generalization of the task to the token level-we seek to map a bisequence of lemma-tag pairs to the sequence of inflected forms in context. Formally, we will denote the lemma- morphological tag bisequence as , m and the form sequence as f . Foreshadowing, the primary motivation for this generalization is to enable the use of raw-text in a semi-supervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generating Sequences of Inflections</head><p>The primary contribution of this paper is a novel generative model over sequences of inflected words in their sentential context. Following the nota- tion laid out in §2.2, we seek to jointly learn a distribution over sequences of forms f , lemmata , and morphological tags m. The generative procedure is as follows: First, we sample a se- quence of tags m, each morphological tag com- ing from a language model over morphological tags: m i ∼ p θ (· | m &lt;i ). Next, we sample the se- quence of lemmata given the previously sampled sequence of tags m-these are sampled condi- tioned only on the corresponding morphological tag: i ∼ p θ (· | m i ). Finally, we sample the se- quence of inflected words f , where, again, each word is chosen conditionally independent of other elements of the sequence:</p><formula xml:id="formula_1">f i ∼ p θ (· | i , m i ). 4</formula><p>This yields the factorized joint distribution:</p><formula xml:id="formula_2">p θ (f , , m) = (1) |f | i=1 p θ (f i | i , m i ) morphological inflector 3 · p θ ( i | m i ) lemma generator 2 · p θ (m) m-tag LM 1</formula><p>We depict the corresponding directed graphical model in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Relation to Other Models in NLP. As the graphical model drawn in <ref type="figure" target="#fig_0">Fig. 1</ref> shows, our model is quite similar to a Hidden Markov Model (HMM) <ref type="bibr" target="#b30">(Rabiner, 1989)</ref>. There are two primary differences. First, we remark that an HMM directly emits a form f i conditioned on the tag m i . Our model, in contrast, emits a lemma i conditioned on the morphological tag m i and, then, conditioned on both the lemma i and the tag m i , we emit the in- flected form f i . In this sense, our model resembles the hierarchical HMM of <ref type="bibr" target="#b15">Fine et al. (1998)</ref> with the difference that we do not have interdependence between the lemmata i . The second difference is that our model is non-Markovian: we sample the i th morphological tag m i from a distribution that depends on all previous tags, using an LSTM language model ( §4.1). This yields richer interac- tions among the tags, which may be necessary for modeling long-distance agreement phenomena.</p><p>Why a Generative Model? What is our interest in a generative model of inflected forms? Eq. <ref type="formula">(1)</ref> is a syntax-only language model in that it only allows for interdependencies between the morpho- syntactic tags in p θ (m). However, given a tag sequence m, the individual lemmata and forms are conditionally independent. This prevents the model from learning notions such as semantic frames and topicality. So what is this model good for? Our chief interest is the ability to train a morphological inflector on unlabeled data, which is a boon in a low-resource setting. As the model is generative, we may consider the latent-variable model:</p><formula xml:id="formula_3">p θ (f ) = ,m p θ (f , , m),<label>(2)</label></formula><p>where we marginalize out the latent lemmata and morphological tags from raw text. The sum in Eq. <ref type="formula" target="#formula_3">(2)</ref> is unabashedly intractable-given a se- quence f , it involves consideration of an exponen- tial (in |f |) number of tag sequences and an infinite number of lemmata sequences. Thus, we will fall back on an approximation scheme (see §5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Recurrent Neural Parameterization</head><p>The graphical model from §3 specifies a family of models that obey the conditional independence assumptions dictated by the graph in <ref type="figure" target="#fig_0">Fig. 1</ref>. In this section we define a specific parameterization using long short-term memory (LSTM) recurrent neu- ral network <ref type="bibr" target="#b18">(Hochreiter and Schmidhuber, 1997</ref>) language models ( <ref type="bibr" target="#b34">Sundermeyer et al., 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">LSTM Language Models</head><p>Before proceeding, we review the modeling of sequences with LSTM language models. Given some alphabet ∆, the distribution over sequences x ∈ ∆ * can be defined as follows:</p><formula xml:id="formula_4">p(x) = |x| j=1 p(x j | x &lt;j ),<label>(3)</label></formula><p>where x &lt;j = x 1 , . . . , x j−1 . The prediction at time step j of a single element x j is then parametrized by a neural network:</p><formula xml:id="formula_5">p(x j | x &lt;j ) = softmax (W · h j + b) ,<label>(4)</label></formula><p>where W ∈ R |∆|×d and b ∈ R |∆| are learned pa- rameters (for some number of hidden units d) and the hidden state h j ∈ R d is defined through the recurrence given by Hochreiter and Schmidhuber (1997) from the previous hidden state and an em- bedding of the previous character (assuming some learned embedding function e : ∆ → R c for some number of dimensions c):</p><formula xml:id="formula_6">h j = LSTM h j−1 , e(x j−1 )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Our Conditional Distributions</head><p>We discuss each of the factors in Eq.</p><p>(1) in turn.</p><formula xml:id="formula_7">1 Morphological Tag Language Model: p θ (m).</formula><p>We define p θ (m) as an LSTM language model, as defined in §4.1, where we take ∆ = M, i.e., the elements of the sequence that are to be predicted are tags like <ref type="bibr">POS=V, TNS=GERUND</ref> . Note that the embedding function e does not treat them as atomic units, but breaks them up into individual attribute- value pairs that are embedded individually and then summed to yield the final vector representation. To be precise, each tag is first encoded by a multi-hot vector, where each component corresponds to a attribute-value pair in the slot, and then this multi- hot vector is multiplied with an embedding matrix.</p><p>character. Thusly, we obtain the new recurrence relation for the hidden state:</p><formula xml:id="formula_8">h j = LSTM h j−1 , e [ i ] j−1 ; e t i ,<label>(6)</label></formula><p>where [ i ] j denotes the j th character of the gener- ated lemma i and e : T → R c for some c is a learned embedding function for POS tags. Note that we embed only the POS tag, rather than the entire morphological tag, as we assume the lemma depends on the part of speech exclusively.</p><p>3 Morphological Inflector:</p><formula xml:id="formula_9">p θ (f i | i , m i ).</formula><p>The final conditional in our model is a morpho- logical inflector, which we parameterize as a neural recurrent sequence-to-sequence model <ref type="bibr" target="#b35">(Sutskever et al., 2014</ref>) with Luong dot-style attention <ref type="bibr" target="#b25">(Luong et al., 2015</ref>). Our particular model uses a single encoder-decoder architecture ( <ref type="bibr" target="#b21">Kann and Schütze, 2016</ref>) for all tag pairs within a language and we refer to reader to that paper for further details. Con- cretely, the encoder runs over a string consisting of the desired slot and all characters of the lemma that is to be inflected (e.g. &lt;w&gt; V PST t a l k &lt;/w&gt;), one LSTM running left-to-right, the other right-to-left. Concatenating the hidden states of both RNNs at each time step results in hidden states</p><formula xml:id="formula_10">h (enc) j</formula><p>. The decoder, again, takes the form of an LSTM language model (we take ∆ = Σ), pro- ducing the inflected form character by character, but at each time step not only the previous hidden state and the previously generated token are consid- ered, but attention (a convex combination) over all encoder hidden states h (enc) j , with the distribution given by another neural network; see <ref type="bibr" target="#b25">Luong et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Semi-Supervised Wake-Sleep</head><p>We train the model with the wake-sleep procedure, which requires us to perform posterior inference over the latent variables. However, the exact com- putation in the model is intractable-it involves a sum over all possible lemmatizations and tag- gings of the sentence, as shown in Eq. (2). Thus, we fall back on a variational approximation <ref type="bibr" target="#b20">(Jordan et al., 1999</ref>). We train an inference network q φ (, m | f ) that approximates the true poste- rior over the latent variables p θ (, m | f ). <ref type="bibr">5</ref> The variational family we choose in this work will be detailed in §5.5. We fit the distribution q φ using a semi-supervised extension of the wake-sleep al- gorithm <ref type="bibr" target="#b5">Bornschein and Bengio, 2014</ref>). We derive the al- gorithm in the following subsections and provide pseudo-code in Alg. 1.</p><p>Note that the wake-sleep algorithm shows struc- tural similarities to the expectation-maximization (EM) algorithm <ref type="bibr" target="#b10">(Dempster et al., 1977)</ref>, and, pre- saging the exposition, we note that the wake-sleep procedure is a type of variational EM <ref type="bibr" target="#b3">(Beal, 2003)</ref>.</p><p>The key difference is that the E-step minimizes an inclusive KL divergence, rather than the exclusive one typically found in variational EM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Requirements of Wake-Sleep</head><p>We emphasize again that we will train our model in a semi-supervised fashion. Thus, we will assume a set of labeled sentences, D labeled , represented as a set of triples f , , m, and a set of unlabeled sentences, D unlabeled , represented as a set of surface form sequences f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The Sleep Phase</head><p>Wake-sleep first dictates that we find an approxi- mate posterior distribution q φ that minimizes the KL divergences for all form sequences:</p><formula xml:id="formula_11">D KL p θ (·, ·, ·) full joint: Eq. (1) || q φ (·, · | ·) variational approximation<label>(7)</label></formula><p>with respect to the parameters φ, which control the variational approximation q φ . Because q φ is trained to be a variational approximation for any input f , it is called an inference network. In other words, it will return an approximate pos- terior over the latent variables for any observed sequence. Importantly, note that computation of Eq. <ref type="formula" target="#formula_11">(7)</ref> is still hard-it requires us to normalize the distribution p θ , which, in turn, involves a sum over all lemmatizations and taggings. However, it does lend itself to an efficient Monte Carlo ap- proximation. As our model is fully generative and directed, we may easily take samples from the com- plete joint. Specifically, we will take K samples˜f samples˜ samples˜f , ˜ , ˜ m ∼ p θ (·, ·, ·) by forward sampling and de- fine them as D sleep . We remark that we use a tilde to indicate that a form, lemmata or tag is sampled, rather than human annotated. Using K samples, we obtain the objective</p><formula xml:id="formula_12">S unsup = 1 /K · ˜ f , ˜ , ˜ m∈ D sleep log q φ ( ˜ , ˜ m | ˜ f ),<label>(8)</label></formula><p>which we could maximize by fitting the model q φ through backpropagation <ref type="bibr" target="#b31">(Rumelhart et al., 1986)</ref>, as one would during maximum likelihood estima- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The Wake Phase</head><p>Now, given our approximate posterior q φ (, m | f ), we are in a position to re-estimate the param- eters of the generative model p θ (f , , m). Given a set of unannotated sentences D unlabeled , we again first consider the objective</p><formula xml:id="formula_13">W unsup = 1 /M · f , ˜ , ˜ m∈ D wake log p θ (f , ˜ , ˜ m)<label>(9)</label></formula><p>where D wake is a set of triples f , ˜ , ˜ m with f ∈ D unlabeled and˜,˜mand˜and˜,and˜,˜ and˜,˜m ∼ q φ (·, · | f ), maxi- mizing with respect to the parameters θ (we may stochastically backprop through the expectation simply by backpropagating through this sum). Note that Eq. <ref type="formula" target="#formula_13">(9)</ref> is a Monte Carlo approximation of the inclusive divergence of the data distribution of D unlabeled times q φ with p θ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Adding Supervision to Wake-Sleep</head><p>So far we presented a purely unsupervised training method that makes no assumptions about the la- tent lemmata and morphological tags. In our case, however, we have a very clear idea what the latent variables should look like. For instance, we are quite certain that the lemma of talking is talk and that it is in fact a GERUND. And, indeed, we have access to annotated examples D labeled in the form of an annotated corpus. In the presence of these data, we optimize the supervised sleep phase objective,</p><formula xml:id="formula_14">S sup = 1 /N · f ,,m∈D labeled log q φ (, m | f ).<label>(10)</label></formula><p>which is a Monte Carlo approximation of D KL (D labeled || q φ ). Thus, when fitting our varia- tional approximation q φ , we will optimize a joint objective S = S sup + γ sleep · S unsup , where S sup , to repeat, uses actual annotated lemmata and mor- phological tags; we balance the two parts of the objective with a scaling parameter γ sleep . Note that on the first sleep phase iteration, we set γ sleep = 0 since taking samples from an untrained p θ (·, ·, ·) when we have available labeled data is of little util- ity. We will discuss the provenance of our data in §7.2. Likewise, in the wake phase we can neglect the approximation q φ in favor of the annotated latent  variables found in D labeled ; this leads to the follow- ing supervised objective</p><formula xml:id="formula_15">W sup = 1 /N · f ,,m∈D labeled log p θ (f , , m),<label>(11)</label></formula><p>which is a Monte Carlo approximation of D KL (D labeled || p θ ). As in the sleep phase, we will maximize W = W sup + γ wake · W unsup , where γ wake is, again, a scaling parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Our Variational Family</head><p>How do we choose the variational family q φ ? In terms of NLP nomenclature, q φ represents a joint morphological tagger and lemmatizer. The open- source tool <ref type="bibr">LEMMING (Müller et al., 2015</ref>) repre- sents such an object. LEMMING is a higher-order linear-chain conditional random field (CRF; <ref type="bibr" target="#b24">Lafferty et al., 2001)</ref>, that is an extension of the mor- phological tagger of <ref type="bibr">Müller et al. (2013)</ref>. Interest- ingly, LEMMING is a linear model that makes use of simple character n-gram feature templates. On both the tasks of morphological tagging and lemma- tization, neural models have supplanted linear mod- els in terms of performance in the high-resource case ( ). However, we are inter- ested in producing an accurate approximation to the posterior in the presence of minimal annotated examples and potentially noisy samples produced during the sleep phase, where linear models still outperform non-linear approaches <ref type="bibr" target="#b6">(Cotterell and Heigold, 2017)</ref>. We note that our variational ap- proximation is compatible with any family.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Interpretation as an Autoencoder</head><p>We may also view our model as an autoencoder, following <ref type="bibr" target="#b23">Kingma and Welling (2013)</ref>, who saw that a variational approximation to any generative model naturally has this interpretation. The crucial difference between <ref type="bibr" target="#b23">Kingma and Welling (2013)</ref> and this work is that our model is a structured variational autoencoder in the sense that the space of our latent code is structured: the inference net- work encodes a sentence into a pair of lemmata and morphological tags , m. This bisequence is then decoded back into the sequence of forms f through a morphological inflector. The reason the model is called an autoencoder is that we arrive at an auto-encoding-like objective if we combine the p θ and q φ as so:</p><formula xml:id="formula_16">p(f | ˆ f ) = ,m p θ (f | , m) · q φ (, m | ˆ f ) (12)</formula><p>wherê f is a copy of the original sentence f . Note that this choice of latent space sadly pre- cludes us from making use of the reparametrization trick that makes inference in VAEs particularly ef- ficient. In fact, our whole inference procedure is quite different as we do not perform gradient de- scent on both q φ and p θ jointly but alternatingly optimize both (using wake-sleep). We nevertheless call our model a VAE to uphold the distinction be- tween the VAE as a model (essentially a specific Helmholtz machine ( , justified by variational inference) and the end-to-end infer- ence procedure that is commonly used.</p><p>Another way of viewing this model is that it tries to force the words in the corpus through a syntactic bottleneck. Spiritually, our work is close to the conditional random field autoencoder of <ref type="bibr" target="#b1">Ammar et al. (2014)</ref>.</p><p>We remark that many other structured NLP tasks can be "autoencoded" in this way and, thus, trained by a similar wake-sleep procedure. For instance, any two tasks that effectively function as inverses, e.g., translation and backtranslation, or language generation and parsing, can be treated with a simi- lar variational autoencoder. While this work only focuses on the creation of an improved morpho- logical inflector p θ (f | , m), one could imagine a situation where the encoder was also a task of interest. That is, the goal would be to improve both the decoder (the generation model) and the encoder (the variational approximation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Closest to our work is <ref type="bibr" target="#b39">Zhou and Neubig (2017)</ref>, who describe an unstructured variational autoen- coder. However, the exact use case of our re- spective models is distinct. Our method models the syntactic dynamics with an LSTM language model over morphological tags. Thus, in the semi- supervised setting, we require token-level anno- tation. Additionally, our latent variables are in- terpretable as they correspond to well-understood linguistic quantities. In contrast, <ref type="bibr" target="#b39">Zhou and Neubig (2017)</ref> infer latent lemmata as real vectors. To the best of our knowledge, we are only the second at- tempt, after <ref type="bibr" target="#b39">Zhou and Neubig (2017)</ref>, to attempt to perform semi-supervised learning for a neural inflection generator. Other non-neural attempts at semi-supervised learning of morphological inflec- tors include <ref type="bibr" target="#b19">Hulden et al. (2014)</ref>. Models in this vein are non-neural and often focus on exploiting corpus statistics, e.g., token frequency, rather than explicitly modeling the forms in context. All of these approaches are designed to learn from a type- level lexicon, rendering direct comparison difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>While we estimate all the parameters in the gen- erative model, the purpose of this work is to im- prove the performance of morphological inflectors through semi-supervised learning with the incorpo- ration of unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Low-Resource Inflection Generation</head><p>The development of our method was primarily aimed at the low-resource scenario, where we ob- serve a limited number of annotated data points. Why low-resource? When we have access to a preponderance of data, morphological inflection is close to being a solved problem, as evinced in SIGMORPHON's 2016 shared task. However, the CoNLL-SIGMORPHON 2017 shared task showed there is much progress to be made in the low- resource case. Semi-supervision is a clear avenue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Data</head><p>As our model requires token-level morphological annotation, we perform our experiments on the Universal Dependencies (UD) dataset <ref type="bibr" target="#b29">(Nivre et al., 2017)</ref>. As this stands in contrast to most work on morphological inflection (which has used the UniMorph ( <ref type="bibr" target="#b37">Sylak-Glassman et al., 2015)</ref> 6 datasets), we use a converted version of UD data, in which the UD morphological tags have been deterministically converted into UniMorph tags.</p><p>For each of the treebanks in the UD dataset, we divide the training portion into three chunks consist- ing of the first 500, 1000 and 5000 tokens, respec- tively. These labeled chunks will constitute three unique sets D labeled . The remaining sentences in the training portion will be used as unlabeled data D unlabeled for each language, i.e., we will discard those labels. The development and test portions will be left untouched.</p><p>Languages. We explore a typologically diverse set of languages of various stocks: Indo-European, Afro-Asiatic, Turkic and Finno-Ugric, as well as the language isolate Basque. We have organized our experimental languages in Tab. 3 by genetic grouping, highlighting sub-families where possi- ble. The Indo-European languages mostly exhibit fusional morphologies of varying degrees of com- plexity. The Basque, Turkic, and Finno-Ugric lan- guages are agglutinative. Both of the Afro-Asiatic languages, Arabic and Hebrew, are Semitic and have templatic morphology with fusional affixes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Evaluation</head><p>The end product of our procedure is a morphologi- cal inflector, whose performance is to be improved through the incorporation of unlabeled data. Thus, we evaluate using the standard metric accuracy. We will evaluate at the type level, as is traditional in the morphological inflection literature, even though the UD treebanks on which we evaluate are token-level resources. Concretely, we compile an incomplete type-level morphological lexicon from the token- level resource. To create this resource, we gather all unique form-lemma-tag triples f, , m present in the UD test data. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Baselines</head><p>As mentioned before, most work on morphological inflection has considered the task of estimating sta- tistical inflectors from type-level lexicons. Here, in <ref type="bibr">6</ref> The two annotation schemes are similar. For a discussion, we refer the reader to http: //universaldependencies.org/v2/features. html; sadly there are differences that render all numbers reported in this work incomparable with previous work, see §7.4. 7 Some of these form-lemma-tag triples will overlap with those seen in the training data. contrast, we require token-level annotation to esti- mate our model. For this reason, there is neither a competing approach whose numbers we can make a fair comparison to nor is there an open-source system we could easily run in the token-level set- ting. This is why we treat our token-level data as a list of "types" 8 and then use two simple type-based baselines.</p><p>First, we consider the probabilistic finite-state transducer used as the baseline for the CoNLL- SIGMORPHON 2017 shared task. <ref type="bibr">9</ref> We consider this a relatively strong baseline, as we seek to gener- alize from a minimal amount of data. As described by <ref type="bibr" target="#b6">Cotterell et al. (2017)</ref>, the baseline performed quite competitively in the task's low-resource set- ting. Note that the finite-state machine is created by heuristically extracting prefixes and suffixes from the word forms, based on an unsupervised align- ment step. The second baseline is our neural in- flector p(f | , m) given in §4 without the semi- supervision; this model is state-of-the-art on the high-resource version of the task.</p><p>We will refer to our baselines as follows: FST is the probabilistic transducer, NN is the neu- ral sequence-to-sequence model without semi- supervision, and SVAE is the structured variational autoencoder, which is equivalent to NN but also trained using wake-sleep and unlabeled data.</p><p>8 Typical type-based inflection lexicons are likely not i.i.d. samples from natural utterances, but we have no other choice if we want to make use of only our token-level data and not additional resources like frequency and regularity of forms.</p><p>9 https://sites.google.com/view/ conll-sigmorphon2017/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Results</head><p>We ran the three models on 23 languages with the hyperparameters and experimental details de- scribed in App. A. We present our results in <ref type="figure" target="#fig_3">Fig. 2</ref> and in Tab. 3. We also provide sample output of the generative model created using the dream step in App. B. The high-level take-away is that on almost all languages we are able to exploit the unlabeled data to improve the sequence-to-sequence model using unlabeled data, i.e., SVAE outperforms the NN model on all languages across all training sce- narios. However, in many cases, the FST model is a better choice-the FST can sometimes generalize better from a handful of supervised examples than the neural network, even with semi-supervision (SVAE). We highlight three finer-grained observa- tions below.</p><p>Observation 1: FST Good in Low-Resource. As clearly evinced in <ref type="figure" target="#fig_3">Fig. 2</ref>, the baseline FST is still competitive with the NN, or even our SVAE when data is extremely scarce. Our neural architecture is quite general, and lacks the prior knowledge and inductive biases of the rule-based system, which become more pertinent in low-resource scenarios. Even though our semi-supervised strategy clearly improves the performance of NN, we cannot al- ways recommend SVAE for the case when we only have 500 annotated tokens, but on average it does slightly better. The SVAE surpasses the FST when moving up to 1000 annotated tokens, becoming even more pronounced at 5000 annotated tokens. 500 tokens 1000 tokens 5000 tokens  Observation 2: Agglutinative Languages. The next trend we remark upon is that languages of an agglutinating nature tend to benefit more from the semi-supervised learning. Why should this be? Since in our experimental set-up, every lan- guage sees the same number of tokens, it is natu- rally harder to generalize on languages that have more distinct morphological variants. Also, by the nature of agglutinative languages, relevant mor- phemes could be arbitrarily far from the edges of the string, making the (NN and) SVAE's ability to learn more generic rules even more valuable.</p><formula xml:id="formula_17">lang FST NN SVAE ∆ FST ∆ NN FST NN SVAE ∆ FST ∆ NN FST NN SVAE ∆ FST ∆ NN ca 81.</formula><p>Observation 3: Non-concatenative Morphology.</p><p>One interesting advantage that the neural models have over the FSTs is the ability to learn non- concatenative phenomena. The FST model is based on prefix and suffix rewrite rules and, naturally, struggles when the correctly reinflected form is more than the concatenation of these parts. Thus we see that for the two semitic language, the SVAE is the best method across all resource settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have presented a novel generative model for morphological inflection generation in context. The model allows us to exploit unlabeled data in the training of morphological inflectors. As the model's rich parameterization prevents tractable in- ference, we craft a variational inference procedure, based on the wake-sleep algorithm, to marginal- ize out the latent variables. Experimentally, we provide empirical validation on 23 languages. We find that, especially in the lower-resource condi- tions, our model improves by large margins over the baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A length-4 example of our generative model factorized as in Eq. (1) and overlayed with example values of the random variables in the sequence. We highlight that all the conditionals in the Bayesian network are recurrent neural networks, e.g., we note that mi depends on m&lt;i because we employ a recurrent neural network to model the morphological tag sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Violin plots showing the distribution over accuracies. The structured variational autoencoder (SVAE) always outperforms the neural network (NN), but only outperformed the FST-based approach when trained on 5000 annotated tokens. Thus, while semi-supervised training helps neural models reduce their sample complexity, roughly 5000 annotated tokens are still required to boost their performance above more symbolic baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Notational cheat sheet for the paper. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 : Type-level morphological inflection accuracy across different models, training scenarios, and languages</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="3"> One example of a paradigm where the lexeme, rather than the lemma, may influence inflection is hang. If one chooses the lexeme that licenses animate objects, the proper past tense is hanged, whereas it is hung for the lexeme that licenses inanimate objects.</note>

			<note place="foot" n="4"> Note that we denote all three distributions as p θ to simplify notation and emphasize the joint modeling aspect; context will always resolve the ambiguity in this paper. We will discuss their parameterization in §4.</note>

			<note place="foot" n="2"> Lemma Generator: p θ ( i | m i ). The next distribution in our model is a lemma generator which we define to be a conditional LSTM language model over characters (we take ∆ = Σ), i.e., each x i is a single (orthographic) character. The language model is conditioned on t i (the part-ofspeech information contained in the morphological tag m i = t i , σ i ), which we embed into a lowdimensional space and feed to the LSTM by concatenating its embedding with that of the current</note>

			<note place="foot" n="5"> Inference networks are also known as stochastic inverses (Stuhlmüller et al., 2013) or recognition models (Dayan et al., 1995).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Paradigm classification in supervised learning of morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malin</forename><surname>Ahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Forsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mans</forename><surname>Hulden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL</title>
		<meeting><address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1024" to="1029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Conditional random field autoencoders for unsupervised structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3311" to="3319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Word Formation in Generative Grammar. Number 1 in Linguistic Inquiry Monographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Aronoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Variational Algorithms for Approximate Bayesian Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew James</forename><surname>Beal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>University College London</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Bornschein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Reweighted wake-sleep. CoRR, abs/1406.2751</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Crosslingual character-level neural morphological tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="748" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Jason Eisner, and Mans Hulden. 2017. The CoNLL-SIGMORPHON 2017 shared task: Universal morphological reinflection in 52 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christo</forename><surname>Kirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sylak-Glassman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Géraldine</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vylomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL-SIGMORPHON 2017</title>
		<meeting>the CoNLL-SIGMORPHON 2017</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The SIGMORPHON 2016 shared taskmorphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christo</forename><surname>Kirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sylak-Glassman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</title>
		<meeting>the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="22" />
		</imprint>
	</monogr>
	<note>Jason Eisner, and Mans Hulden</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Helmholtz machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radford</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="889" to="904" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Latent-variable modeling of string transductions with finite-state methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1080" to="1089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The world atlas of language structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">S</forename><surname>Dryer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Comrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Schmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Supervised learning of complete morphological paradigms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1185" to="1195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Morphological inflection generation using character sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="634" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The hierarchical hidden Markov model: Analysis and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="41" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An extensive empirical evaluation of character-based morphological tagging for 14 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guenter</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="505" to="513" />
		</imprint>
	</monogr>
	<note>Long Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The &quot;wake-sleep&quot; algorithm for unsupervised neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radford</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="issue">5214</biblScope>
			<biblScope unit="page" from="1158" to="1161" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of morphological paradigms and lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Mans Hulden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malin</forename><surname>Forsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="569" to="578" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="183" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Singlemodel encoder-decoder with explicit morphological representation for reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="555" to="560" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Archi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aleksandr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kibrik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Handbook of Morphology</title>
		<editor>Andrew Spencer and Arnold M. Zwicky</editor>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="455" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational Bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint lemmatization and morphological tagging with Lemming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2268" to="2274" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient higher-order CRFs for morphological tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Inflection generation as discriminative string transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><surname>Nicolai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Universal dependencies 2.0. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Željko</forename><surname>Agi´cagi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ahrenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Jesus</forename><surname>Aranzabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitziber</forename><surname>Atutxa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kepa</forename><surname>Bengoetxea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riyaz</forename><forename type="middle">Ahmad</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eckhard</forename><surname>Bick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül¸sengül¸sen</forename><surname>Cebiro˘ Glu Eryi˘ Git</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><forename type="middle">G A</forename><surname>Celano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabricio</forename><surname>Chalub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Faculty of Mathematics and Physics</title>
		<meeting><address><addrLine>Miriam Connor, Elizabeth Davidson, MarieCatherine de Marneffe, Valeria de Paiva, Arantza Diaz de Ilarraza</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Charles University</orgName>
		</respStmt>
	</monogr>
	<note>Ça˘ grı Çöltekin</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="286" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
		<respStmt>
			<orgName>California Univ San Diego La Jolla Inst for Cognitive Science</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Morphological Theory: An Introduction to Word Structure in Generative Grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Spencer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>Wiley-Blackwell</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning stochastic inverses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stuhlmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3048" to="3056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">LSTM neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The composition and use of the universal morphological feature schema (Unimorph schema)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sylak-Glassman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A language-independent feature schema for inflectional morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sylak-Glassman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christo</forename><surname>Kirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Que</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="674" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>arXiv preprint:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multispace variational encoder-decoders for semisupervised labeled sequence transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="310" to="320" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
