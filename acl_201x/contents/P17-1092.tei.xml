<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Discourse Structure for Text Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
						</author>
						<title level="a" type="main">Neural Discourse Structure for Text Categorization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="996" to="1005"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1092</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Advances in text categorization have the poten- tial to improve systems for analyzing sentiment, inferring authorship or author attributes, making predictions, and many more. Several past re- searchers have noticed that methods that reason about the relative salience or importance of pas- sages within a text can lead to improvements <ref type="bibr" target="#b15">(Ko et al., 2004</ref>). Latent variables ( <ref type="bibr" target="#b34">Yessenalina et al., 2010)</ref>, structured-sparse regularizers <ref type="bibr" target="#b35">(Yogatama and Smith, 2014)</ref>, and neural attention models ( <ref type="bibr">Yang et al., 2016</ref>) have all been explored.</p><p>Discourse structure, which represents the or- ganization of a text as a tree (for an example, see <ref type="figure" target="#fig_0">Figure 1</ref>), might provide cues for the importance of different parts of a text. Some promising re- sults on sentiment classification tasks support this idea: <ref type="bibr" target="#b4">Bhatia et al. (2015)</ref> and <ref type="bibr" target="#b9">Hogenboom et al. (2015)</ref> applied hand-crafted weighting schemes to the sentences in a document, based on its discourse structure, and showed benefit to sentiment polarity classification.</p><p>In this paper, we investigate the value of discourse structure for text categorization more broadly, considering five tasks, through the use of a recursive neural network built on an automatically-derived document parse from a top- performing, open-source discourse parser, DPLP <ref type="bibr" target="#b10">(Ji and Eisenstein, 2014</ref>). Our models learn to weight the importance of a document's sentences, based on their positions and relations in the dis- course tree. We introduce a new, unnormalized attention mechanism to this end.</p><p>Experimental results show that variants of our model outperform prior work on four out of five tasks considered. Our method unsurprisingly un- derperforms on the fifth task, making predictions about legislative bills-a genre in which discourse conventions are quite different from those in the discourse parser's training data. Further experi- ments show the effect of discourse parse quality on text categorization performance, suggesting that future improvements to discourse parsing will pay off for text categorization, and validate our new attention mechanism.</p><p>Our implementation is available at https:// github.com/jiyfeng/disco4textcat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Rhetorical Structure Theory</head><p>Rhetorical Structure Theory (RST; <ref type="bibr" target="#b17">Mann and Thompson, 1988</ref>) is a theory of discourse that has enjoyed popularity in NLP. RST posits that a document can be represented by a tree whose leaves are elementary discourse units (EDUs, typ- ically clauses or sentences). Internal nodes in the tree correspond to spans of sentences that are con- nected via discourse relations such as CONTRAST and ELABORATION. In most cases, a discourse re- lation links adjacent spans denoted "nucleus" and "satellite," with the former more essential to the writer's purpose than the latter. <ref type="bibr">1</ref> An example of a manually constructed RST parse for a restaurant review is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The six EDUs are indexed from A to F ; the dis- course tree organizes them hierarchically into in- creasingly larger spans, with the last CONTRAST relation resulting in a span that covers the whole review. Within each relation, the RST tree in- dicates the nucleus pointed by an arrow from its satellite (e.g., in the ELABORATION relation, A is the nucleus and B is the satellite).</p><p>The information embedded in RST trees has motivated many applications in NLP research, in- cluding document summarization <ref type="bibr" target="#b18">(Marcu, 1999)</ref>, argumentation mining <ref type="bibr" target="#b0">(Azar, 1999)</ref>, and sentiment analysis ( <ref type="bibr" target="#b4">Bhatia et al., 2015)</ref>. In most applica- tions, RST trees are built by automatic discourse parsing, due to the expensive cost of manual an- notation. In this work, we use a state-of-the-art open-source RST-style discourse parser, DPLP <ref type="bibr" target="#b10">(Ji and Eisenstein, 2014</ref>). <ref type="bibr">2</ref> We follow recent work that suggests trans- forming the RST tree into a dependency struc- ture ( <ref type="bibr" target="#b36">Yoshida et al., 2014</ref>). 3 <ref type="figure">Figure 2</ref>(a) shows the corresponding dependency structure of the RST tree in <ref type="figure" target="#fig_0">Figure 1</ref>. It is clear that C is the root of the tree, and in fact this clause summarizes the re- view and suffices to categorize it as negative. This dependency representation of the RST tree offers a form of inductive bias for our neural model, help- ing it to discern the most salient parts of a text in order to assign it a label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Our model is a recursive neural network built on a discourse dependency tree. It includes a dis- tributed representation computed for each EDU, and a composition function that combines EDUs and partial trees into larger trees. At the top of the tree, the representation of the complete document is used to make a categorization decision. Our ap- proach is analogous to (and inspired by) the use of recursive neural networks on syntactic depen- dency trees, with word embeddings at the leaves ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Representation of Sentences</head><p>Let e be the distributed representation of an EDU. We use a bidirectional LSTM on the words' em- beddings within each EDU (details of word em- beddings are given in section 4), concatenating the last hidden state vector from the forward LSTM ( − → e ) with that of the backward LSTM ( ← − e ) to get e. There is extensive recent work on architectures for embedding representations of sentences and other short pieces of text, including, for example, (bi)recursive neural networks ( <ref type="bibr" target="#b23">Paulus et al., 2014</ref>) and convolutional neural networks ( <ref type="bibr" target="#b13">Kalchbrenner et al., 2014</ref>). Future work might consider alterna- tives; we chose the bidirectional LSTM due to its effectiveness in many settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Full Recursive Model</head><p>Given the discourse dependency tree for an input text, our recursive model builds a vector represen- tation through composition at each arc in the tree. Let v i denote the vector representation of EDU i and its descendants. For the base case where EDU i is a leaf in the tree, we let v i = tanh(e i ), which is the elementwise hyperbolic tangent function.</p><p>For an internal node i, the composition function considers a parent and all of its children, whose in- dices are denoted by children(i). In defining this composition function, we seek for (i.) the contri- bution of the parent node e i to be central; and (ii.) the contribution of each child node e j be deter- mined by its content as well as the discourse rela- tion it holds with the parent. We therefore define Elab.</p><p>Cont. Exp. Exp.</p><p>Cont.</p><p>(a) dependency structure</p><formula xml:id="formula_0">tanh(e C + j∈{A,D,E} α C,j W C,j v j ) tanh(e D ) tanh(e A + α A,B W A,B v B ) tanh(e E + α F,E W F,E v F ) tanh(e B ) tanh(e F ) W A,B W C,A W C,D W C,E W F,E</formula><p>(b) recursive neural network structure <ref type="figure">Figure 2</ref>: The dependency discourse tree derived from the example RST tree in <ref type="figure" target="#fig_0">Figure 1</ref> (a) and the corresponding recursive neural network model on the tree (b).</p><formula xml:id="formula_1">v i = tanh   e i + j∈children(i) α i,j W r i,j v j   ,</formula><p>(1) where W r i,j is a relation-specific composition matrix indexed by the relation between i and j, r i,j .</p><p>α i,j is an "attention" weight, defined as</p><formula xml:id="formula_2">α i,j = σ e i W α v j ,<label>(2)</label></formula><p>where σ is the elementwise sigmoid and W α contains attention parameters (these are relation- independent). Our attention mechanism differs from prior work ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, in which attention weights are normalized to sum to one across competing candidates for attention. Here, α i,j does not depend on node i's other children. This is motivated by RST, in which the presence of a node does not signify lesser importance to its siblings. Consider, for example, EDU D and text span E-F in <ref type="figure" target="#fig_0">Figure 1</ref>, which in parallel provide EXPLANATION for EDU C. This scenario dif- fers from machine translation, where attention is- used to implicitly and softly align output-language words to relatively few input-language words. It also differs from attention in composition func- tions used in syntactic parsing ( <ref type="bibr" target="#b16">Kuncoro et al., 2017)</ref>, where attention can mimic head rules that follow from an endocentricity hypothesis of syn- tactic phrase representation. Our recursive composition function, through the attention mechanism and the relation-specific weight matrices, is designed to learn how to dif- ferently weight EDUs for the categorization task. This idea of using a weighting scheme along with discourse structure is explored in prior works ( <ref type="bibr" target="#b4">Bhatia et al., 2015;</ref><ref type="bibr" target="#b9">Hogenboom et al., 2015)</ref>, al- though they are manually designed, rather than learned from training data.</p><p>Once we have v root of a text, the prediction of its category is given by softmax (W o v root + b).</p><p>We refer to this model as the FULL model, since it makes use of the entire discourse dependency tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unlabeled Model</head><p>The FULL model based on Equation 1 uses a de- pendency discourse tree with relations. Because alternate discourse relation labels have been pro- posed (e.g., <ref type="bibr" target="#b25">Prasad et al., 2008)</ref>, we seek to mea- sure the effect of these labels. We therefore con- sider an UNLABELED model based only on the tree structure, without the relations:</p><formula xml:id="formula_3">v i = tanh   e i + j∈children(i) α i,j v j   . (3)</formula><p>Here, only attention weights are used to compose the children nodes' representations, significantly reducing the number of model parameters. This UNLABELED model is similar to the depth weighting scheme introduced by <ref type="bibr" target="#b4">Bhatia et al. (2015)</ref>, which also uses an unlabeled discourse de- pendency tree, but our attention weights are com- puted by a function whose parameters are learned. This approach sits squarely between <ref type="bibr" target="#b4">Bhatia et al. (2015)</ref> and the flat document structure used by <ref type="bibr">Yang et al. (2016)</ref>; the UNLABELED model still uses discourse to bias the model toward some con- tent (that which is closer to the tree's root).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Simpler Variants</head><p>We consider two additional baselines that are even simpler. The first, ROOT, uses the discourse de- pendency structure only to select the root EDU, which is used to represent the entire text: v root = e root . No composition function is needed. This model variant is motivated by work on document summarization ( <ref type="bibr" target="#b36">Yoshida et al., 2014)</ref>, where the most central EDU is used to represent the whole text.</p><p>The second variant, ADDITIVE, uses all the EDUs with a simple composition function, and does not depend on discourse structure at all:</p><formula xml:id="formula_4">v root = 1 N N i=1 e i ,</formula><p>where N is the total number of EDUs. This serves as a baseline to test the bene- fits of discourse, controlling for other design deci- sions and implementation choices. Although sen- tence representations e i are built in a different way from the work of <ref type="bibr">Yang et al. (2016)</ref>, this model is quite similar to their HN-AVE model on building document representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation Details</head><p>The parameters of all components of our model (top-level classification, composition, and EDU representation) are learned end-to-end using stan- dard methods. We implement our learning pro- cedure with the DyNet package ( .</p><p>Preprocessing. For all datasets, we use the same preprocessing steps, mostly following recent work on language modeling (e.g., <ref type="bibr" target="#b20">Mikolov et al., 2010)</ref>. We lowercased all the tokens and removed tokens that contain only punctuation symbols. We re- placed numbers in the documents with a special number token. Low-frequency word types were replaced by UNK; we reduce the vocabulary for each dataset until approximately 5% of tokens are mapped to UNK. The vocabulary sizes after pre- processing are also shown in <ref type="table">Table 1</ref>.</p><p>Discourse parsing. Our model requires the dis- course structure for each document. We used DPLP, the RST parser from <ref type="bibr" target="#b10">Ji and Eisenstein (2014)</ref>, which is one of the best discourse parsers on the RST discourse treebank benchmark <ref type="bibr" target="#b7">(Carlson et al., 2001</ref>). It employs a greedy decoding algorithm for parsing, producing 2,000 parses per minute on average on a single CPU. DPLP pro- vides discourse segmentation, breaking a text into EDUs, typically clauses or sentences, based on syntactic parses provided by Stanford CoreNLP. RST trees are converted to dependencies follow- ing the method of <ref type="bibr" target="#b36">Yoshida et al. (2014)</ref>. DPLP as distributed is trained on 347 Wall Street Jour- nal articles from the Penn Treebank ( <ref type="bibr" target="#b19">Marcus et al., 1993)</ref>.</p><p>Word embeddings. In cases where there are 10,000 or fewer training examples, we used pretrained GloVe word embeddings <ref type="bibr" target="#b24">(Pennington et al., 2014)</ref>, following previous work on neu- ral discourse processing ( . For larger datasets, we randomly initialized word embeddings and trained them alongside other model parameters.</p><p>Learning and hyperparameters. Online learn- ing was performed with the optimization method and initial learning rate as hyperparameters. To avoid the exploding gradient problem, we used the norm clipping trick with a threshold of τ = 5.0. In addition, dropout rate 0.3 was used on both input and hidden layers to avoid overfitting. We performed grid search over the word vector repre- sentation dimensionality, the LSTM hidden state dimensionality (both {32, 48, 64, 128, 256}), the initial learning rate ({0.1, 0.01, 0.001}), and the update method <ref type="bibr">(SGD and Adam, Kingma and Ba, 2015)</ref>. For each corpus, the highest-accuracy com- bination of these hyperparameters is selected us- ing development data or ten-fold cross validation, which will be specified in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Datasets</head><p>We selected five datasets of different sizes and cor- responding to varying categorization tasks. Some information about these datasets is summarized in <ref type="table">Table 1</ref>.</p><p>Sentiment analysis on Yelp reviews. Originally from the Yelp Dataset Challenge in 2015, this dataset contains 1.5 million examples. We used the preprocessed dataset from <ref type="bibr" target="#b37">Zhang et al. (2015)</ref>, which has 650,000 training and 50,000 test exam- ples. The task is to predict an ordinal rating (1-5) from the text of the review. To select the best com- bination of hyperparameters, we randomly sam- pled 10% training examples as the development data. We compared with hierarchical attention net- works ( <ref type="bibr">Yang et al., 2016)</ref>, which use the normal- ized attention mechanism on both word and sen- tence layers with a flat document structure, and provide the state-of-the-art result on this corpus.   Congressional floor debates. The corpus was originally collected by <ref type="bibr" target="#b28">Thomas et al. (2006)</ref>, and the data split we used was constructed by <ref type="bibr" target="#b34">Yessenalina et al. (2010)</ref>. The goal is to predict the vote ("yea" or "nay") for the speaker of each speech segment. The most recent work on this corpus is from <ref type="bibr" target="#b35">Yogatama and Smith (2014)</ref>, which pro- posed structured regularization methods based on linguistic components, e.g., sentences, topics, and syntactic parses. Each regularization method in- duces a linguistic bias to improve text classifica- tion accuracy, where the best result we repeated here is from the model with sentence regularizers.</p><p>Movie reviews. This classic movie review cor- pus was constructed by <ref type="bibr" target="#b22">Pang and Lee (2004)</ref> and includes 1,000 positive and 1,000 negative re- views. On this corpus, we used the standard ten- fold data split for cross validation and reported the average accuracy across folds. We compared with the work from both Congressional bill corpus. This corpus, col- lected by <ref type="bibr" target="#b33">Yano et al. (2012)</ref>, includes 51,762 leg- islative bills from the 103rd to 111th U.S. Con- gresses. The task is to predict whether a bill will survive based on its content. We randomly sam- pled 10% training examples as development data to search for the best hyperparameters. To our knowledge, the best published results are due to <ref type="bibr" target="#b35">Yogatama and Smith (2014)</ref>, which is the same baseline as for the congressional floor debates cor- pus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We evaluated all variants of our model on the five datasets presented in section 5, comparing in each case to the published state of the art as well as the most relevant works.</p><p>Results. See <ref type="table" target="#tab_2">Table 2</ref>. On four out of five datasets, our UNLABELED model (line 8) outper- forms past methods. In the case of the very large Yelp dataset, our FULL model (line 9) gives even stronger performance, but not elsewhere, suggest- ing that it is overparameterized for the smaller datasets. Indeed, on the MFC and Movies tasks, the discourse-ignorant ADDITIVE outperforms the FULL model. On these datasets, the selected FULL model had nearly 20 times as many parameters as the UNLABELED model, which in turn had twice as many parameters as the ADDITIVE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yelp</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MFC Debates Movies Bills</head><p>Prior work 1. <ref type="bibr">Yang et al. (2016)</ref> 71.0 - - - - 2. <ref type="bibr" target="#b6">Card et al. (2016)</ref> - 56.8 - - - 3. <ref type="bibr" target="#b35">Yogatama and Smith (2014)</ref> - - 74.0 - 88.5 4. <ref type="bibr" target="#b4">Bhatia et al. (2015)</ref> - - - 82.9 - 5. <ref type="bibr" target="#b9">Hogenboom et al. (2015)</ref> -  This finding demonstrates the benefit of ex- plicit discourse structure-even the output from an imperfect parser-for text categorization in some genres. This benefit is supported by both UN- LABELED and FULL, since both of them use dis- course structures of texts. The advantage of us- ing discourse information varies on different gen- res and different corpus sizes. Even though the discourse parser is trained on news text, it still of- fers benefit to restaurant and movie reviews and to the genre of congressional debates. Even for news text, if the training dataset is small (e.g., MFC), a lighter-weight variant of discourse (UNLABELED) is preferred.</p><p>Legislative bills, which have technical legal content and highly specialized conventions (see the supplementary material for an example), are arguably the most distant genre from news among those we considered. On that task, we see dis- course working against accuracy. Note that the corpus of bills is more than ten times larger than three cases where our UNLABELED model outper- formed past methods, suggesting that the drop in performance is not due to lack of data.</p><p>It is also important to notice that the ROOT model performs quite poorly in all cases. This im- plies that discourse structure is not simply helping by finding a single EDU upon which to make the categorization decision.</p><p>Qualitative analysis. <ref type="figure">Figure 3</ref> shows some ex- ample texts from the Yelp Review corpus with their discourse structures produced by DPLP, where the weights were generated with the FULL model. <ref type="figure">Figures 3(a) and 3(b)</ref> are two successful examples of the FULL model. <ref type="figure">Figure 3(a)</ref> shows a simple case with respect to the discourse struc- ture. <ref type="figure">Figure 3(b)</ref> is slightly different-the text in this example may have more than one reasonable discourse structure, e.g., 2D could be a child of 2C instead of 2A. In both cases, discourse struc- tures help the FULL model bias to the important sentences. <ref type="figure">Figure 3(c)</ref>, on the other hand, presents a neg- ative example, where DPLP failed to identify the most salient sentence 3F . In addition, the weights produced by the FULL model do not make much sense, which we suspect the model was confused by the structure. <ref type="figure">Figure 3</ref>(c) also presents a manually-constructed discourse struc- ture on the same text for reference. A more accu- rate prediction is expected if we use this manually- constructed discourse structure, because it has the appropriate dependency between sentences. In ad- dition, the annotated discourse relations are able to select the right relation-specific composition ma- trices in FULL model, which are consistent with the training examples.</p><p>Effect of parsing performance. A natural ques- tion is whether further improvements to RST dis- course parsing would lead to even greater gains in text categorization. While advances in discourse parsing are beyond the scope of this paper, we can gain some insight by exploring degradation to the DPLP parser. An easy way to do this is to train it on subsets of the RST discourse tree- bank. We repeated the conditions described above for our FULL model, training DPLP on 25%, 50%, and 75% of the training set (randomly selected in   <ref type="figure">Figure 4</ref>: Varying the amount of training data for the discourse parser, we can see how parsing F 1 performance affects accuracy on the Yelp review task.</p><p>each case) before re-parsing the data for the sen- timent analysis task. We did not repeat the hy- perparameter search. In <ref type="figure">Figure 4</ref>, we plot accu- racy of the classifier (y-axis) against the F 1 perfor- mance of the discourse parser (x-axis). Unsurpris- ingly, lower parsing performance implies lower classification accuracy. Notably, if the RST dis- course treebank were reduced to 25% of its size, our method would underperform the discourse- ignorant model of <ref type="bibr">Yang et al. (2016)</ref>. While we cannot extrapolate with certainty, these findings suggest that further improvements to discourse parsing, through larger annotated datasets or im- proved models, could lead to greater gains.</p><p>Attention mechanism. In section 3, we con- trasted our new attention mechanism (Equation 2), which is inspired by RST's lack of "competi- tion" for salience among satellites, with the atten- tion mechanism used in machine translation (Bah- danau et al., 2015). We consider here a variant of our model with normalized attention:</p><formula xml:id="formula_5">α i = softmax         . . . v j . . .     j∈children(i) W α · e i     .<label>(4)</label></formula><p>The result here is a vector α i , with one element for each child node j ∈ children(i), and which sums to one.</p><p>On Yelp dateset, this variant of the FULL model achieves 70.3% accuracy (1.5% absolute behind our FULL model), giving empirical support to our theoretically-motivated design decision not to nor- malize attention. Of course, further architecture improvements may yet be possible.</p><p>Discussion. Our findings in this work show the benefit of using discourse structure for text cate- gorization. Although discourse structure strongly improves the performance on most of corpora in our experiments, its benefit is limited particularly by two factors: (1) the state-of-the-art perfor- mance on RST discourse parsing; and (2) domain mismatch between the training corpus for a dis- course parser and the domain where the discourse parser is used. For the first factor, discourse pars- ing is still an active research topic in NLP, and may yet improve. The second factor suggests explor- ing domain adaptation methods or even direct dis- course annotation for genres of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Early work on text categorization often treated text as a bag of words (e.g., <ref type="bibr" target="#b12">Joachims, 1998;</ref><ref type="bibr" target="#b31">Yang and Pedersen, 1997</ref>). Representation learning, for ex- ample through matrix decomposition <ref type="bibr" target="#b8">(Deerwester et al., 1990</ref>) or latent topic variables ( <ref type="bibr" target="#b26">Ramage et al., 2009)</ref>, has been considered to avoid over- fitting in the face of sparse data.</p><p>The assumption that all parts of a text should influence categorization equally persists even as more powerful representation learners are consid- ered. <ref type="bibr" target="#b37">Zhang et al. (2015)</ref> treat a text as a sequence of characters, proposing to a deep convolutional neural network to build text representation. <ref type="bibr" target="#b29">Xiao and Cho (2016)</ref> extended that architecture by in- serting a recurrent neural network layer between the convolutional layer and the classification layer.</p><p>In contrast, our contributions follow <ref type="bibr" target="#b15">Ko et al. (2004)</ref>, who sought to weight the influence of dif- ferent parts of an input text on the task. Two works that sought to learn the importance of sentences in a document are <ref type="bibr" target="#b34">Yessenalina et al. (2010)</ref> and <ref type="bibr">Yang et al. (2016)</ref>. The former used a latent variable for the informativeness of each sentence, and the latter used a neural network to learn an attention function. Neither used any linguistic bias, relying only on task supervision to discover the latent vari- able distribution or attention function. Our work builds the neural network directly on a discourse dependency tree, favoring the most central EDUs over the others but giving the model the ability to overcome this bias.</p><p>Another way to use linguistic information was presented by <ref type="bibr" target="#b35">Yogatama and Smith (2014)</ref>, who used a bag-of-words model. The novelty in their approach was a data-driven regularization method that encouraged the model to collectively ignore groups of features found to coocur. Most related to our work is their "sentence regularizer," which encouraged the model to try to ignore training-set sentences that were not informative for the task. Discourse structure was not considered.</p><p>Discourse for sentiment analysis. Recently, discourse structure has been considered for sen- timent analysis, which can be cast as a text cate- gorization problem. <ref type="bibr" target="#b4">Bhatia et al. (2015)</ref> proposed two discourse-motivated models for sentiment po- larity prediction. One of the models is also based on discourse dependency trees, but using a hand- crafted weighting scheme. Our method's attention mechanism automates the weighting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We conclude that automatically-derived discourse structure can be helpful to text categorization, and the benefit increases with the accuracy of dis- course parsing. We did not see a benefit for categorizing legislative bills, a text genre whose discourse structure diverges from that of news. These findings motivate further improvements to discourse parsing, especially for new genres.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A manually constructed example of the RST (Mann and Thompson, 1988) discourse structure on a text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Framing dimensions in news articles.</head><label></label><figDesc>The Me- dia Frames Corpus (MFC; Card et al., 2015) in- cludes around 4,200 news articles about immi- gration from 13 U.S. newspapers over the years 1980-2012. The annotations of these articles are in terms of a set of 15 general-purpose labels, such as ECONOMICS and MORALITY, designed to categorize the emphasis framing applied to the Number of docs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Dataset</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Bhatia et al.</head><label></label><figDesc>(2015) and Hogenboom et al. (2015), which are two recent works on discourse for sentiment analysis. Bha- tia et al. (2015) used a hand-crafted weighting scheme to bias the bag-of-word representations on sentences. Hogenboom et al. (2015) also consid- ered manually-designed weighting schemes and a lexicon-based model as classifier, achieving per- formance inferior to fully-supervised methods like Bhatia et al. (2015) and ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>[From[</head><label></label><figDesc>This store is somewhat convenient but I can never find any workers,] 1A [it drives me crazy.] 1B [I never come here on the weekends or around holidays anymore.] 1C (a) true label: 2, predicted label: 2I love these people.] 2A [They are very friendly and always ask about my life.] 2B [They remember things I tell them then ask about it the next time I'm in.] 2C [Patrick and Lily are the best but everyone there is wonderful in their own ways.] 2D (b) true label: 5, predicted label: 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Task Classes Total Training Development Test Vocab. size</head><label></label><figDesc></figDesc><table>Yelp 
Sentiment 
5 700K 
650K 
-50K 
10K 
MFC 
Frames 
15 4.2K 
-
-
-
7.5K 
Debates 
Vote 
2 1.6K 
1,135 
105 403 
5K 
Movies 
Sentiment 
2 2.0K 
-
-
-
5K 
Bills 
Survival 
2 
52K 
46K 
-
6K 
10K 

Table 1: Information about the five datasets used in our experiments. To compare with prior work, we 
use different experimental settings. For Yelp and Bill corpora, we use 10% of the training examples as 
development data. For MFC and Movies corpora, we use 10-fold cross validation and report averages 
across all folds. 

immigration issue within the articles. We fo-
cused on predicting the single primary frame of 
each article. The state-of-the-art result on this 
corpus is from Card et al. (2016), where they 
used logistic regression together with unigrams, 
bigrams and Bamman-style personas (Bamman 
et al., 2014) as features. The best feature com-
bination in their model alongside other hyperpa-
rameters was identified by a Bayesian optimiza-
tion method (Bergstra et al., 2015). To select hy-
perparameters, we used a small set of examples 
from the corpus as a development set. Then, we 
report average accuracy across 10-fold cross vali-
dation as in (Card et al., 2016). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Test-set accuracy across five datasets. Results from prior work are reprinted from the corre- sponding publications. Boldface marks performance stronger than the previous state of the art.</figDesc><table></table></figure>

			<note place="foot" n="1"> There are also a few exceptions in which a relation can be realized with multiple nuclei. 2 https://github.com/jiyfeng/DPLP 3 The transformation is trivial and deterministic given the nucleus-satellite mapping for each relation. The procedure is analogous to the transformation of a headed phrase-structure parse in syntax into a dependency tree (e.g., Yamada and Matsumoto, 2003).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank anonymous reviewers and members of Noah's ARK for helpful feedback on this work. We thank Dallas Card and Jesse Dodge for helping prepare the Media Frames Corpus and the Con-gressional bill corpus. This work was made pos-sible by a University of Washington Innovation Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Argumentative text as rhetorical structure: An application of rhetorical structure theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Argumentation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="114" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning latent personas of film characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hyperopt: a Python library for model selection and hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><surname>Komer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Science &amp; Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Better document-level sentiment analysis from RST discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parminder</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Media Frames Corpus: Annotations of frames across issues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dallas</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amber</forename><forename type="middle">E</forename><surname>Boydstun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analyzing framing through the casts of characters in the news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dallas</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amber</forename><forename type="middle">E</forename><surname>Boydstun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Second SIGdial Workshop on Discourse and Dialogue</title>
		<meeting>Second SIGdial Workshop on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">391</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using rhetorical structure in sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hogenboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavius</forename><surname>Frasincar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Franciska De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uzay</forename><surname>Kaymak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="69" to="77" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Representation learning for document-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">One vector is not enough: Entity-augmented distributed semantics for discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="329" to="344" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>ArXiv:1404.2188</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving text categorization using the importance of sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoong</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungyun</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="79" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What do recurrent neural network grammars learn about syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rhetorical Structure Theory: Toward a functional theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discourse trees are good indicators of importance in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Automatic Text Summarization</title>
		<editor>Inderjeet Mani and Mark T. Maybury</editor>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="123" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno>ArXiv:1701.03980</idno>
		<title level="m">Dynet: The dynamic neural network toolkit</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd annual meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">271</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<title level="m">Global belief recursive neural networks. In NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The Penn Discourse Treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Labeled lda: A supervised topic model for credit attribution in multilabeled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Get out the vote: Determining support or opposition from Congressional floor-debate transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Efficient character-level document classification by combining convolution and recurrent layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>ArXiv:1602.00367</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Statistical dependency analysis with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWPT</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A comparative study on feature selection in text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Textual predictors of bill survival in congressional committees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae</forename><surname>Yano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Wilkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-level structured models for document sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainur</forename><surname>Yessenalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Linguistic structured sparsity in text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dependency-based discourse parser for single-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhisa</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
