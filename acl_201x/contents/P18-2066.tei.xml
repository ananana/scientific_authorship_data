<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
							<email>zhaoyue@software.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computer and Control Engineering</orgName>
								<orgName type="laboratory">CAS Key Laboratory of Network Data Science and Technology</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Science (CAS)</orgName>
								<orgName type="institution" key="instit2">The University of CAS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computer and Control Engineering</orgName>
								<orgName type="laboratory">CAS Key Laboratory of Network Data Science and Technology</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Science (CAS)</orgName>
								<orgName type="institution" key="instit2">The University of CAS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computer and Control Engineering</orgName>
								<orgName type="laboratory">CAS Key Laboratory of Network Data Science and Technology</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Science (CAS)</orgName>
								<orgName type="institution" key="instit2">The University of CAS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">School of Computer and Control Engineering</orgName>
								<orgName type="laboratory">CAS Key Laboratory of Network Data Science and Technology</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Science (CAS)</orgName>
								<orgName type="institution" key="instit2">The University of CAS</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="414" to="419"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>414</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Document-level information is very important for event detection even at sentence level. In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, to detect events in sentences. This model first learns event detection oriented embed-dings of documents through a hierarchical and supervised attention based RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events. It then uses the learned document embedding to enhance another bidirectional RNN model to identify event triggers and their types in sentences. Through experiments on the ACE-2005 dataset, we demonstrate the effectiveness and merits of the proposed DEEB-RNN model via comparison with state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Event Detection (ED) is an important subtask of event extraction. It extracts event triggers from in- dividual sentences and further identifies the type of the corresponding events. For instance, accord- ing to the ACE-2005 annotation guideline, in the sentence "Jane and John are married", an ED sys- tem should be able to identify the word "married" as a trigger of the event "Marry". However, it may be difficult to identify events from isolated sen- tences, because the same event trigger might rep- resent different event types in different contexts.</p><p>Existing ED methods can mainly be categorized into two classes, namely, feature-based methods (e.g., <ref type="bibr" target="#b15">(McClosky et al., 2011;</ref><ref type="bibr" target="#b5">Hong et al., 2011;</ref><ref type="bibr" target="#b9">Li et al., 2014)</ref>) and representation-based methods (e.g., <ref type="bibr" target="#b18">(Nguyen and Grishman, 2015;</ref><ref type="bibr" target="#b1">Chen et al., 2015</ref>; <ref type="bibr" target="#b12">Liu et al., 2016a;</ref>). The former mainly rely on a set of hand-designed fea- tures, while the latter employ distributed repre- sentation to capture meaningful semantic informa- tion. In general, most of these existing methods mainly exploit sentence-level contextual informa- tion. However, document-level information is also important for ED, because the sentences in the same document, although they may contain differ- ent types of events, are often correlated with re- spect to the theme of the document. For example, there are the following sentences in <ref type="bibr">ACE-2005:</ref> ... I knew it was time to leave. Isn't that a great argument for term limits? ... If we only examine the first sentence, it is hard to determine whether the trigger "leave" indicates a "Transport" event meaning that he wants to leave the current place, or an "End-Position" event in- dicating that he will stop working for his current organization. However, if we can capture the con- textual information of this sentence, it is more confident for us to label "leave" as the trigger of an "End-Position" event. Upon such observation, there have been some feature-based studies <ref type="bibr" target="#b7">(Ji and Grishman, 2008;</ref><ref type="bibr" target="#b11">Liao and Grishman, 2010;</ref><ref type="bibr" target="#b6">Huang and Riloff, 2012</ref>) that construct rules to capture document-level information for improving sentence-level ED. However, they suffer from two major limitations. First, the features used therein often need to be manually designed and may in- volve error propagation due to natural language processing; Second, they discover inter-event in- formation at document level by constructing infer- ence rules, which is time-consuming and is hard to make the rule set as complete as possible. Besides, a representation-based study has been presented in ( <ref type="bibr" target="#b2">Duan et al., 2017)</ref>, which employs the PV-DM model to train document embeddings and further uses it in a RNN-based event classifier. How- ever, as being limited by the unsupervised training In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, for ED at sentence level. This model first learns ED oriented embeddings of documents through a hierarchical and supervised attention based bidirectional RNN, which pays word-level attention to event triggers and sentence-level at- tention to those sentences containing events. It then uses the learned document embeddings to fa- cilitate another bidirectional RNN model to iden- tify event triggers and their types in individual sentences. This learning process is guided by a general loss function where the loss correspond- ing to attention at both word and sentence levels and that of event type identification are integrated. It should be mentioned that although the atten- tion mechanism has recently been applied effec- tively in various tasks, including machine transla- tion ( , question answering ( <ref type="bibr" target="#b4">Hao et al., 2017)</ref>, document summarization ( <ref type="bibr" target="#b21">Tan et al., 2017)</ref>, etc., this is the first study, to the best of our knowledge, which adopts a hierarchical and supervised attention mechanism to learn ED ori- ented embeddings of documents.</p><p>We evaluate the developed DEEB-RNN model on the benchmark dataset, ACE-2005, and sys- tematically investigate the impacts of differ- ent supervised attention strategies on its perfor- mance. Experimental results show that the DEEB- RNN model outperforms both feature-based and representation-based state-of-the-art methods in terms of recall and F1-measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposed Model</head><p>We formalize ED as a multi-class classification problem. Given a sentence, we treat every word in it as a trigger candidate, and classify each can- didate to a certain event type. In the ACE-2005 dataset, there are 8 event types, further being di- vided into 33 subtypes, and a "Not Applicable (NA)" type. Without loss of generality, in this pa- per we regard the 33 subtypes as 33 event types. <ref type="figure" target="#fig_0">Figure 1</ref> presents the schematic diagram of the proposed DEEB-RNN model, which contains two main modules:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">The ED Oriented Document Embedding</head><p>Learning (EDODEL) module, which learns the distributed representations of documents from both word and sentence levels via the well-designed hierarchical and supervised at- tention mechanism.</p><p>2. The Document-level Enhanced Event Detec- tor (DEED) module, which tags each trigger candidate with an event type based on the learned embedding of documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The EDODEL Module</head><p>To learn the ED oriented embedding of a docu- ment, we apply the hierarchical and supervised at- tention network presented in <ref type="figure" target="#fig_0">Figure 1</ref>, which con- sists of a word-level Bi-GRU ( <ref type="bibr" target="#b20">Schuster and Paliwal, 2002</ref>) encoder with attention on event triggers and a sentence-level Bi-GRU encoder with atten- tion on sentences with events. Given a document with L sentences, DEEB-RNN learns its embed- ding for detecting events in all sentences.</p><p>Word-level embeddings Given a sen- tence s i (i = 1, 2, ..., L) consisting of words {w it |t = 1, 2, ..., T }. For each word w it , we first concatenate its embedding w it and its entity type embedding 1 e it <ref type="bibr" target="#b18">(Nguyen and Grishman, 2015)</ref> as the input g it of a Bi-GRU and thus obtain the bidirectional hidden state h it :</p><formula xml:id="formula_0">h it = [ − −−− → GRU w (g it ), ← −−− − GRU w (g it )].<label>(1)</label></formula><p>We then feed h it to a perceptron with no bias to get u it = tanh(W w h it ) as a hidden representa- tion of h it and also obtain an attention weight α it = u T it c w , which should be normalized through a softmax function. Here, similar to that in ( <ref type="bibr" target="#b22">Yang et al., 2016)</ref>, c w is a vector representing the word- level context of w it , which is initialized at random. Finally, the embedding of the sentence s i can be obtained by summing up h it with their weights:</p><formula xml:id="formula_1">s i = T ∑ t=1 α it h it .<label>(2)</label></formula><p>To pay more attention to trigger words than other words, we construct the gold word-level attention signals α * i for the sentence s i , as illustrated in <ref type="figure" target="#fig_1">Fig- ure 2a</ref>. We can then take the square error as the general loss of the attention at word level to super- vise the learning process:</p><formula xml:id="formula_2">E w (α * , α) = L ∑ i=1 T ∑ t=1 (α * it − α it ) 2 .<label>(3)</label></formula><p>Sentence-level embeddings Given the sentence embeddings {s i |i = 1, 2, ..., L}, we first get the hidden state q i via a Bi-GRU:</p><formula xml:id="formula_3">q i = [ −−−→ GRU s (s i ), ←−−− GRU s (s i )].<label>(4)</label></formula><p>Then we feed q i to a perceptron with no bias to get the hidden representation t i = tanh(W s q i ) and also obtain an attention weight β i = t T i c s to be normalized via softmax. Similarly, c s represents the sentence-level context of s i to be randomly ini- tialized. We eventually obtain the document em- bedding d as:</p><formula xml:id="formula_4">d = L ∑ i=1 β i s i .<label>(5)</label></formula><p>We also think that the sentences containing event should obtain more attention than other ones. Therefore, similar to the case at word level, we construct the gold sentence-level attention signals β * for the document d, as illustrated in <ref type="figure" target="#fig_1">Figure 2b</ref>, and further take the square error as the general loss of the attention at sentence level to supervise the learning process:</p><formula xml:id="formula_5">E s (β * , β) = L ∑ i=1 (β * i − β i ) 2 .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The DEED Module</head><p>We employ another Bi-GRU encoder and a soft- max output layer to model the ED task, which can handle event triggers with multiple words. Specif- ically, given a sentence s j (j = 1, 2, ..., L) in doc- ument d, for each of its word w jt (t = 1, 2, ..., T ), we concatenate its word embedding w jt and entity type embedding e jt with the corresponding docu- ment embedding d as the input r jt of the Bi-GRU and thus obtain the hidden state f jt :</p><formula xml:id="formula_6">f jt = [ −−−→ GRU e (r jt ), ←−−− GRU e (r jt )].<label>(7)</label></formula><p>Finally, we get the probability vector o jt with K dimensions through a softmax layer for w jt , where the k-th element, o</p><p>jt , of o jt indicates the proba- bility of classifying w jt to the k-th event type. The loss function, J(y, o), can thus be defined in terms of the cross-entropy error of the real event type y jt and the predicted probability o (k) jt as follows:</p><formula xml:id="formula_8">J(y, o) = − L ∑ j=1 T ∑ t=1 K ∑ k=1 I(y jt = k)log o (k) jt , (8)</formula><p>where I(·) is the indicator function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Joint Training of the DEEB-RNN model</head><p>In the DEEB-RNN model, the above two modules are jointly trained. For this purpose, we define the joint loss function in the training process upon the losses specified for different modules as follows:</p><formula xml:id="formula_9">J(θ) = ∑ ∀d∈ϕ (J(y, o)+λE w (α * , α)+µE s (β * , β)),<label>(9)</label></formula><p>where θ denotes, as a whole, the parameters used in DEEB-RNN, ϕ is the training document set, and λ and µ are hyper-parameters for striking a bal- ance among J(y, o), E w (α * , α) and E s (β * , β).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Settings</head><p>We validate the proposed model through compar- ison with state-of-the-art methods on the ACE- 2005 dataset. In the experiments, the validation set has 30 documents from different genres, the test set has 40 documents and the training set con- tains the remaining 529 documents. All the data preprocessing and evaluation criteria follow those in ( <ref type="bibr" target="#b3">Ghaeini et al., 2016)</ref>.</p><p>Hyper-parameters are tuned on the validation set. We set the dimension of the hidden layers cor- responding to GRU w , GRU s , and GRU e to 300, 200, and 300, respectively, the output size of W w and W s to 600 and 400, respectively, the dimen- sion of entity type embeddings to 50, the batch size to 25, the dropout rate to 0.5. In addition, we utilize the pre-trained word embeddings with 300 dimensions from ( <ref type="bibr" target="#b16">Mikolov et al., 2013</ref>) for initialization. For entity types, their embeddings are randomly initialized. We train the model using Stochastic Gradient Descent (SGD) over shuffled mini-batches and using dropout ( <ref type="bibr" target="#b8">Krizhevsky et al., 2012</ref>) for regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baseline Models</head><p>In order to validate the proposed DEEB-RNN model through experimental comparison, we choose the following typical models as the base- lines.</p><p>Sentence-level is a feature-based model pro- posed in <ref type="bibr" target="#b5">(Hong et al., 2011</ref>), which regards entity- type consistency as a key feature to predict event mentions.</p><p>Joint Local is a feature-based model developed in ( <ref type="bibr" target="#b10">Li et al., 2013)</ref>, which incorporates such fea- tures that explicitly capture the dependency among multiple triggers and arguments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>λ µ P R F1 Bi-GRU - -66.2 72.3 69.1 DEEB-RNN 0 0 69.3 75.2 72.1 DEEB-RNN1 1 0 70.9 76.7 73.7 DEEB-RNN2 0 1 72.3 74.5 73.4 DEEB-RNN3 1 1 72.3 75.8 74.0 <ref type="table">Table 1</ref>: Experimental results with different atten- tion strategies.</p><p>JRNN is a representation-based model pro- posed in , which exploits the inter-dependency between event triggers and argu- ment roles via discrete structures.</p><p>Skip-CNN is a representation-based model pre- sented in , which proposes a novel convolution to exploit non- consecutive k-grams for event detection.</p><p>ANN-S2 is a representation-based model devel- oped in ( , which explicitly exploits argument information for event detection via su- pervised attention mechanisms.</p><p>Cross-event is a feature-based model proposed in ( <ref type="bibr" target="#b11">Liao and Grishman, 2010)</ref>, which learns rela- tions among event types from training corpus and futher helps predict the occurrence of events.</p><p>PSL is a feature-based model developed in ( <ref type="bibr" target="#b14">Liu et al., 2016b</ref>), which encods global information such as event-event association in the form of logic using the probabilistic soft logic model. DLRNN is a representation-based model pro- posed in <ref type="bibr" target="#b2">(Duan et al., 2017)</ref>, which automatically extracts cross-sentence clues to improve sentence- level event detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Impacts of Different Attention Strategies</head><p>In this section, we conduct experiments on the ACE-2005 dataset to demonstrate the effective- ness of different attention strategies.</p><p>Bi-GRU is the basic ED model, which does not employ document-level embeddings.</p><p>DEEB-RNN uses the document embeddings and computes attentions without supervision, in which hyper-parameters λ and µ are set to 0. DEEB-RNN1/2/3 means they uses the gold at- tention signals as supervision information. Specif- ically, DEEB-RNN1 uses only the gold word-level attention signal (λ = 1 and µ = 0), DEEB-RNN2 uses only the gold sentence-level attention signal (λ = 0 and µ = 1), whilst DEEB-RNN3 employs the gold attention signals at both word and sen-Methods P R F1 Sentence-level (2011) 67.6 53.5 59.7</p><p>Joint Local (2013) 73.7 59.3 65.7 JRNN <ref type="formula" target="#formula_0">(2016)</ref> 66.0 73.0 69.3 Skip-CNN <ref type="formula" target="#formula_0">(2016)</ref> N/A N/A 71.3 ANN-S2 <ref type="formula" target="#formula_0">(2017)</ref> 78.0 66.3 71.7 Cross-event <ref type="formula" target="#formula_0">(2010)</ref>   tence levels (λ = 1 and µ = 1). <ref type="table">Table 1</ref> compares these methods, where we can observe that the methods with document em- beddings (i.e., the last four) significantly outper- form the pure Bi-GRU method, which suggests that document-level information is very benefi- cial for ED. An interesting phenomenon is that, as compared to DEEB-RNN, DEEB-RNN2 changes the precision-recall balance. This is because of the following reasons. On one hand, as com- pared to DEEB-RNN, DEEB-RNN2 uses the gold sentence-level attention signal, indicating that it pays special attention to the sentences containing events with event triggers. In this way, the Bi- RNN model for learning document embeddings will filter out the sentences containing events but without explicit event triggers. That means the events detected by DEEB-RNN2 are basically the ones with explicit event triggers. Therefore, as compared to DEEB-RNN, the precision of DEEB- RNN2 is improved; On the other hand, the above strategy may result in less learning of words, which are event triggers but do not appear in the training dataset. Therefore, those sentences with such event triggers cannot be detected. The recall of DEEB-RNN2 is thus lowered, as compared to DEEB-RNN. Moreover, DEEB-RNN3 shows the best performance, indicating that the gold atten- tion signals at both word and sentence levels are useful for ED. <ref type="table" target="#tab_1">Table 2</ref> presents the overall performance of all methods on ACE-2005. We can see that dif- ferent versions of DEEB-RNN consistently out- perform the existing state-of-the-art methods in terms of both recall and F1-measure, while their precision is comparable to that of others. The better performance of DEEB-RNN can be ex- plained by the following reasons: (1) Compared with feature-based methods, including Sentence- level, Joint Local, and representation-based meth- ods, including JRNN, Skip-CNN and ANN-S2, our method exploits document-level information (i.e., the ED oriented document embeddings) from both word and sentence levels in a document by the supervised attention mechanism, which en- hance the ability of identifying trigger words; (2) Compared with feature-based methods using document-level information, such as Cross-event, PSL, our method can automatically capture event types in documents via a end-to-end Bi-RNN based model without manually designed rules; (3) Compared with representation-based methods us- ing document-level information, such as DLRNN, our method can learn event detection oriented em- beddings of documents through the hierarchical and supervised attention based Bi-RNN network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Performance Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>In this study, we proposed a hierarchical and su- pervised attention based and document embedding enhanced Bi-RNN method, called DEEB-RNN, for event detection. We explored different strate- gies to construct gold word-and sentence-level at- tentions to focus on event information. Experi- ments on the ACE-2005 dataset demonstrate that DEEB-RNN achieves better performance as com- pared to the state-of-the-art methods in terms of both recall and F1-measure. In this paper, we can strike a balance between sentence and document embeddings by adjusting their dimensions. In the future, we may improve the DEEB-RNN model to automatically determine the weights of sentence and document embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The schematic diagram of the DEEB-RNN model for ED at sentence level.</figDesc><graphic url="image-1.png" coords="2,75.40,62.81,446.75,212.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of the gold word-and sentence-level attention without normalization. (a) Word-level attention. "Indicated" is a candidate trigger; (b) Sentence-level attention. The sentences in purple contain trigger words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison between different methods. 
 † indicates that the corresponding ED method uses 

information at both sentence and document levels. 

</table></figure>

			<note place="foot" n="1"> The words in the ACE-2005 dataset are annotated with their entity types (annotated as &quot;NA&quot; if they are not an entity).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by National Key Re-search and Development Program of China under grants 2016YFB1000902 and 2017YFC0820404, and National Natural Science Foundation of China under grants 61772501, 61572473, 61572469, and 91646120. We are grateful to Dr. Liu Kang of the Institute of Automation, Chinese Academy of Sci-ences for very helpful discussion on event detec-tion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatically labeled data generation for large scale event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="409" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multipooling convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Daojian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting document level information to improve event detection via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifang</forename><surname>Shaoyang Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenli</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Event nugget detection with forward-backward recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Ghaeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="369" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An endto-end model for question answering over knowledge base with cross-attention combining global knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using cross-entity inference to improve event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1127" to="1136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling textual cohesion for event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1664" to="1670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Refining event extraction through cross-document inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="254" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>Hinton</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Constructing information networks using one single model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1846" to="1851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using document level cross-event inference to improve event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Leveraging framenet to improve automatic event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2134" to="2143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploiting argument information to improve event detection via supervised attention mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1789" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A probabilistic soft logic based approach to exploiting latent and global information in event classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2993" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Event extraction as dependency parsing for bionlp 2011</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="41" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint event extraction via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="300" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Event detection and domain adaptation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCNLP</title>
		<imprint>
			<biblScope unit="page" from="365" to="371" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling skip-grams for event detection with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="886" to="891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Abstractive document summarization with a graphbased attentional neural model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1171" to="1181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Incorporating word reordering knowledge into attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
