<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Varying Linguistic Purposes of Emoji in (Twitter) Context</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Na&amp;apos;aman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Provenza</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orion</forename><surname>Montoya</surname></persName>
						</author>
						<title level="a" type="main">Varying Linguistic Purposes of Emoji in (Twitter) Context</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2017, Student Research Workshop</title>
						<meeting>ACL 2017, Student Research Workshop <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="136" to="141"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-3022</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Early research into emoji in textual communication has focused largely on high-frequency usages and ambiguity of interpretations. Investigation of a wide range of emoji usage shows these glyphs serving at least two very different purposes: as content and function words, or as multimodal affective markers. Identifying where an emoji is replacing textual content allows NLP tools the possibility of parsing them as any other word or phrase. Recognizing the import of non-content emoji can be a a significant part of understanding a message as well. We report on an annotation task on En-glish Twitter data with the goal of classifying emoji uses by these categories, and on the effectiveness of a classifier trained on these annotations. We find that it is possible to train a classifier to tell the difference between those emoji used as linguistic content words and those used as par-alinguistic or affective multimodal markers even with a small amount of training data, but that accurate sub-classification of these multimodal emoji into specific classes like attitude, topic, or gesture will require more data and more feature engineering. 1 Background Emoji characters were first offered on Japanese mobile phones around the turn of the 21st century. These pictographic elements reached global language communities after being added to Uni-code 6.0 in 2010, and then being offered within software keyboards on smartphones. In the ensuing half-decade, digitally-mediated language users have evolved diverse and novel linguistic uses for emoji. The expressive richness of emoji communication would, on its own, be sufficient reason to seek a nuanced understanding of its usage. But our initial survey of emoji on Twitter reveals many cases where emoji serve direct semantic functions in a tweet or where they are used as a grammatical function such as a preposition or punctuation. Early work on Twitter emoticons (Schnoe-belen, 2012) pre-dated the wide spread of Uni-code emoji on mobile and desktop devices. Recent work (Miller et al., 2016) has explored the cross-platform ambiguity of emoji renderings; (Eis-ner et al., 2016) created word embeddings that performed competitively on emoji analogy tasks; (Ljubešic and Fišer, 2016) mapped global emoji distributions by frequency; (Barbieri et al., 2017) used LSTMs to predict them in context. We feel that a lexical semantics of emoji characters is implied in these studies without being directly addressed. Words are not used randomly, and neither are emoji. But even when they replace a word, emoji are used for different purposes than words. We believe that work on emoji would be better informed if there were an explicit typology of the linguistic functions that emoji can serve in expressive text. The current project offered anno-tators a framework and heuristics to classify uses of emoji by linguistic and discursive function. We then used a model based on this corpus to predict the grammatical function of emoji characters in novel contexts. 2 Annotation task Although recognizing the presence of emoji characters is trivial, the linguistic distinctions we sought to annotate were ambiguous and seemed prone to disagreement. Therefore in our annotation guidelines we structured the process to minimize cognitive load and lead the annotators to in-136 tuitive decisions. This was aided somewhat by the observation that emoji are often used in contexts that make them graphical replacements for existing lexical units, and that such uses are therefore straightforward to interpret. Taking advantage of such uses, our flow presented annotators with a few simple questions at each step, to determine whether to assign a label or to move on to the next category. 2.1 Categories and subtypes The high-level labels we defined for emoji uses were: • Function (func): stand-ins for a function word in an utterance. These had a type attribute with values prep, aux, conj, dt, punc, other. An example from our data: &quot;I like u&quot;. • Content (cont): stand-ins for lexical words or phrases that are part of the main informative content of the sentence. These have natural parts of speech, which anno-tators could subtype as: noun, verb, adj, adv, other. &quot;The to success is &quot; • Multimodal (mm): characters that enrich a grammatically-complete text with markers of affect or stance, whether to express an attitude (&quot;Let my work disrespect me one more time... &quot;), to echo the topic with an iconic repetition (&quot;Mean girls &quot;, or to express a gesture that might have accompanied the utterance in face-to-face speech (&quot;Omg why is my mom screaming so early &quot;). Sub-types: attitude, topic, gesture, other. The POS tags we chose were deliberately coarse-grained and did not include distinctions such as noun sub-types. We wanted to capture important differences while knowing that we would have fewer instances for the function and content labels. For all three labels, annotators were asked to provide a replacement: a word or phrase that could replace the emoji. For func and cont, replacements were a criterion for choosing the label ; for mm there was room for interpretation. 2.2 Data Collection Tweets were pulled from the public Twitter streaming API using the tweepy Python package. The collected tweets were automatically filtered to include: only tweets with characters from the Emoji Unicode ranges (i.e. generally U+1FXXX, U+26XX-U+27BF); only tweets labeled as being in English. We excluded tweets with embedded images or links. Redun-dant/duplicate tweets were filtered by comparing tweet texts after removal of hashtags and @men-tions; this left only a small number of cloned duplicates. After that, tweets were hand-selected to get a wide variety of emojis and context in a small sample size-therefore, our corpus does not reflect the true distribution of emoji uses or context types. 2.3 Guidelines Our guidelines gave annotators cursory background about emoji and their uses in social media, assuming no particular familiarity with the range of creative uses of emoji. In hindsight we noticed our assumption that annotators would have a fair degree of familiarity with modes of discourse on Twitter. The short-message social platform has many distinctive cultural and communicative codes of its own, not to mention sub-cultures, and continuously evolving trends combined with a long memory. As two of the authors are active and engaged users of Twitter, we unfortunately took it for granted that our annotators would be able to decipher emoji in contexts that required nuanced knowledge of InterNet language and Twitter norms. This left annotators occasionally bewildered: by random users begging celebrities to follow them, by dialogue-formatted tweets, and by other epigrammatic subgenres of the short-text form. The analytical steps we prescribed were: • Identifying each emoji in the tweet • Deciding whether multiple contiguous emoji should be considered separately or as a group • Choosing the best tag for the emoji (or sequence) • Providing a translation or interpretation for each tagged span. Eliciting an interpretation serves two goals: first, as a coercive prompt for the user to bias them toward a linguistic interpretation. A replaceable phrase that fits with the grammar of the sentence is a different proposition than a marker that amounts to a standalone utterance such as &quot;I am laughing&quot; 137 or &quot;I am sad&quot;. Secondly, one of the eventual applications of annotated corpus may be emoji-sense disambiguation (ESD), and mapping to a lexical-ized expression would be useful grounding for future ESD tasks. The text field was very helpful during the adjudication process, clarifying the an-notators&apos; judgments and understanding of the task. For each tweet, annotators first read without annotating anything, to get a sense of the general message of the tweet and to think about the relationship between the emoji and the text. On subsequent readings, they are asked to determine whether the emoji is serving as punctuation or a function word; then if it is a content word; and if it is neither of those, then to examine it as a multi-modal emoji. A key test, in our opinion, was asking annotators to simulate reading the message of the tweet aloud to another person. If a listener&apos;s comprehension of the core message seemed to require a word or phrase to be spoken in place of an emoji, then that would be a compelling sign that it should be tagged as function or content. For each step we provided examples of tweets and emoji uses that clearly belong in each category. These examples were not included in the data set. Uses that failed the first two tests were assigned the multimodal category. We provided guidance and examples for deciding between &apos;topic&apos;, &apos;attitude&apos; or &apos;gesture&apos; as subtypes of the multimodal category. 2.4 Inter-annotator agreement Four annotators, all computational linguistics grad students, were given 567 tweets with 878 total occurrences of emoji characters; in the gold standard these amounted to 775 tagged emoji spans. For the first 200 tweets annotated (&apos;Set 1&apos; and &apos;Set 2&apos; in Table 1), each was marked by four annotators. After establishing some facility with the task we divided annotators into two groups and had only two annotators per tweet for the remaining 367. There are two separate aspects of annotation for which IAA was relevant; the first, and less interesting , was the marking of the extent of emoji spans. Since emoji are unambiguously visible, we anticipated strong agreement. The one confounding aspect was that annotators were encouraged to group multiple emoji in a single span if they were a semantic/functional unit. The overall Krippen-dorff&apos;s α for extent markings was around 0.9. The more significant place to look at IAA is the labeling of the emoji&apos;s functions. Because we were categorizing tokens, and because these categories are not ordered and we presented more than two labels, we used Fleiss&apos;s κ. But Fleiss&apos;s κ requires that annotators have annotated the same things, and in some cases annotators did not complete the dataset or missed an individual emoji character in a tweet. In order to calculate the statistics on actual agreement, rather than impute disagreement in the case of an &apos;abstention&apos;, we removed from our IAA-calculation counts any spans that were not marked by all annotators. There are many of these in the first dataset, and progressively fewer in each subsequent dataset as the annotators become more experienced. A total of 150 spans were excluded from Fleiss&apos; kappa calculations for this reason. 2.5 Agreement/disagreement analysis Content words. Part-of-speech identification is a skill familiar to most of our annotators, so we were not surprised to see excellent levels of agreement among words tagged for part of speech. These content words, however, were a very small proportion of the data (51 out of 775 emoji spans) which may be problematically small. For datasets 3B and 4B, annotators were in perfect agreement. Multimodal. Agreement on multimodal sub-labels was much lower, and did not improve as annotation progressed. Multimodal emoji may be inherently ambiguous, and we need a labeling system that can account for this. A smiley face might be interpreted as a gesture (a smile), an attitude (joy), or a topic (for example, if the tweet is about what a good day the author is having)-and any of these would be a valid interpretation of a single tweet. A clearer typology of multimodal emojis, and, if possible, a more deter-ministic procedure for labeling emoji with these subtypes, may be one approach. Worst overall cross-label agreement scores were for week one, but all following datasets improved on that baseline after the annotation guidelines were refined.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Background</head><p>Emoji characters were first offered on Japanese mobile phones around the turn of the 21st cen- tury. These pictographic elements reached global language communities after being added to Uni- code 6.0 in 2010, and then being offered within software keyboards on smartphones. In the ensu- ing half-decade, digitally-mediated language users have evolved diverse and novel linguistic uses for emoji.</p><p>The expressive richness of emoji communica- tion would, on its own, be sufficient reason to seek a nuanced understanding of its usage. But our initial survey of emoji on Twitter reveals many cases where emoji serve direct semantic functions in a tweet or where they are used as a grammat- ical function such as a preposition or punctua- tion. Early work on Twitter emoticons <ref type="bibr" target="#b6">(Schnoebelen, 2012</ref>) pre-dated the wide spread of Uni- code emoji on mobile and desktop devices. Recent work ( <ref type="bibr" target="#b4">Miller et al., 2016</ref>) has explored the cross- platform ambiguity of emoji renderings; <ref type="bibr" target="#b2">(Eisner et al., 2016</ref>) created word embeddings that performed competitively on emoji analogy tasks; <ref type="bibr" target="#b3">(Ljubešic and Fišer, 2016</ref>) mapped global emoji distributions by frequency; ( <ref type="bibr" target="#b0">Barbieri et al., 2017)</ref> used LSTMs to predict them in context.</p><p>We feel that a lexical semantics of emoji char- acters is implied in these studies without being di- rectly addressed. Words are not used randomly, and neither are emoji. But even when they replace a word, emoji are used for different purposes than words. We believe that work on emoji would be better informed if there were an explicit typology of the linguistic functions that emoji can serve in expressive text. The current project offered anno- tators a framework and heuristics to classify uses of emoji by linguistic and discursive function. We then used a model based on this corpus to pre- dict the grammatical function of emoji characters in novel contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Annotation task</head><p>Although recognizing the presence of emoji char- acters is trivial, the linguistic distinctions we sought to annotate were ambiguous and seemed prone to disagreement. Therefore in our annota- tion guidelines we structured the process to mini- mize cognitive load and lead the annotators to in-tuitive decisions. This was aided somewhat by the observation that emoji are often used in contexts that make them graphical replacements for exist- ing lexical units, and that such uses are therefore straightforward to interpret. Taking advantage of such uses, our flow presented annotators with a few simple questions at each step, to determine whether to assign a label or to move on to the next category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Categories and subtypes</head><p>The high-level labels we defined for emoji uses were:</p><p>• Function (func): stand-ins for a function word in an utterance. These had a type attribute with values prep, aux, conj, dt, punc, other. An example from our data: "I like u".</p><p>• Content (cont): stand-ins for lexical words or phrases that are part of the main informative content of the sentence. These have natural parts of speech, which anno- tators could subtype as: noun, verb, adj, adv, other. "The to success is "</p><p>• Multimodal (mm): characters that enrich a grammatically-complete text with markers of affect or stance, whether to express an atti- tude ("Let my work disrespect me one more time... "), to echo the topic with an iconic repetition ("Mean girls ", or to express a gesture that might have accompanied the ut- terance in face-to-face speech ("Omg why is my mom screaming so early "). Sub- types: attitude, topic, gesture, other.</p><p>The POS tags we chose were deliberately coarse-grained and did not include distinctions such as noun sub-types. We wanted to capture im- portant differences while knowing that we would have fewer instances for the function and content labels. For all three labels, annotators were asked to provide a replacement: a word or phrase that could replace the emoji. For func and cont, replacements were a criterion for choosing the la- bel; for mm there was room for interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Collection</head><p>Tweets were pulled from the public Twitter streaming API using the tweepy Python pack- age. The collected tweets were automatically filtered to include: only tweets with characters from the Emoji Unicode ranges (i.e. gener- ally U+1FXXX, U+26XX-U+27BF); only tweets labeled as being in English.</p><p>We excluded tweets with embedded images or links. Redun- dant/duplicate tweets were filtered by comparing tweet texts after removal of hashtags and @men- tions; this left only a small number of cloned du- plicates. After that, tweets were hand-selected to get a wide variety of emojis and context in a small sample size -therefore, our corpus does not re- flect the true distribution of emoji uses or context types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Guidelines</head><p>Our guidelines gave annotators cursory back- ground about emoji and their uses in social media, assuming no particular familiarity with the range of creative uses of emoji. In hindsight we no- ticed our assumption that annotators would have a fair degree of familiarity with modes of dis- course on Twitter. The short-message social plat- form has many distinctive cultural and commu- nicative codes of its own, not to mention sub- cultures, and continuously evolving trends com- bined with a long memory. As two of the authors are active and engaged users of Twitter, we un- fortunately took it for granted that our annotators would be able to decipher emoji in contexts that required nuanced knowledge of InterNet language and Twitter norms. This left annotators occasion- ally bewildered: by random users begging celebri- ties to follow them, by dialogue-formatted tweets, and by other epigrammatic subgenres of the short- text form.</p><p>The analytical steps we prescribed were:</p><p>• Identifying each emoji in the tweet</p><p>• Deciding whether multiple contiguous emoji should be considered separately or as a group</p><p>• Choosing the best tag for the emoji (or se- quence)</p><p>• Providing a translation or interpretation for each tagged span.</p><p>Eliciting an interpretation serves two goals: first, as a coercive prompt for the user to bias them toward a linguistic interpretation. A replaceable phrase that fits with the grammar of the sentence is a different proposition than a marker that amounts to a standalone utterance such as "I am laughing" or "I am sad". Secondly, one of the eventual ap- plications of annotated corpus may be emoji-sense disambiguation (ESD), and mapping to a lexical- ized expression would be useful grounding for fu- ture ESD tasks. The text field was very helpful during the adjudication process, clarifying the an- notators' judgments and understanding of the task.</p><p>For each tweet, annotators first read without an- notating anything, to get a sense of the general message of the tweet and to think about the re- lationship between the emoji and the text. On subsequent readings, they are asked to determine whether the emoji is serving as punctuation or a function word; then if it is a content word; and if it is neither of those, then to examine it as a multi- modal emoji. A key test, in our opinion, was ask- ing annotators to simulate reading the message of the tweet aloud to another person. If a listener's comprehension of the core message seemed to re- quire a word or phrase to be spoken in place of an emoji, then that would be a compelling sign that it should be tagged as function or content.</p><p>For each step we provided examples of tweets and emoji uses that clearly belong in each cat- egory. These examples were not included in the data set. Uses that failed the first two tests were assigned the multimodal category. We pro- vided guidance and examples for deciding be- tween 'topic', 'attitude' or 'gesture' as subtypes of the multimodal category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Inter-annotator agreement</head><p>Four annotators, all computational linguistics grad students, were given 567 tweets with 878 total oc- currences of emoji characters; in the gold standard these amounted to 775 tagged emoji spans. For the first 200 tweets annotated ('Set 1' and 'Set 2' in <ref type="table">Table 1</ref>), each was marked by four annotators. After establishing some facility with the task we divided annotators into two groups and had only two annotators per tweet for the remaining 367.</p><p>There are two separate aspects of annotation for which IAA was relevant; the first, and less in- teresting, was the marking of the extent of emoji spans. Since emoji are unambiguously visible, we anticipated strong agreement. The one confound- ing aspect was that annotators were encouraged to group multiple emoji in a single span if they were a semantic/functional unit. The overall Krippen- dorff's α for extent markings was around 0.9.</p><p>The more significant place to look at IAA is the labeling of the emoji's functions. Because we were categorizing tokens, and because these cat- egories are not ordered and we presented more than two labels, we used Fleiss's κ. But Fleiss's κ requires that annotators have annotated the same things, and in some cases annotators did not com- plete the dataset or missed an individual emoji character in a tweet. In order to calculate the statis- tics on actual agreement, rather than impute dis- agreement in the case of an 'abstention', we re- moved from our IAA-calculation counts any spans that were not marked by all annotators. There are many of these in the first dataset, and progressively fewer in each subsequent dataset as the annotators become more experienced. A total of 150 spans were excluded from Fleiss' kappa calculations for this reason.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Agreement/disagreement analysis</head><p>Content words. Part-of-speech identification is a skill familiar to most of our annotators, so we were not surprised to see excellent levels of agreement among words tagged for part of speech. These content words, however, were a very small propor- tion of the data (51 out of 775 emoji spans) which may be problematically small. For datasets 3B and 4B, annotators were in perfect agreement.</p><p>Multimodal. Agreement on multimodal sub- labels was much lower, and did not improve as annotation progressed. Multimodal emoji may be inherently ambiguous, and we need a labeling sys- tem that can account for this. A smiley face might be interpreted as a gesture (a smile), an attitude (joy), or a topic (for example, if the tweet is about what a good day the author is hav- ing) -and any of these would be a valid inter- pretation of a single tweet. A clearer typology of multimodal emojis, and, if possible, a more deter- ministic procedure for labeling emoji with these subtypes, may be one approach.</p><p>Worst overall cross-label agreement scores were for week one, but all following datasets improved on that baseline after the annotation guidelines were refined.   <ref type="table">Table 2</ref>: Label counts and subtypes in gold- standard data bels assigned, we narrowed the focus of our clas- sification task to simply categorizing things cor- rectly as either mm or cont/func. After one iter- ation, we saw that the low number of func tokens was preventing us from finding any func emoji, so we combined the cont and func tokens into a single label of cont. Therefore our sequence tag- ger needed simply to decide whether a token was serving as a substitute for a textual word, or was a multimodal marker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature engineering</head><p>For reasons described above, we had a small and arbitrary sample of emoji usage available to study. After annotating 775 spans in 567 tweets, we had tagged 300 distinct emoji, 135 of which occurred only once. Given that our task is sequence tag- ging and our features are complex and indepen- dent, Conditional Random Fields seemed a good choice for our task. We used CRFSuite <ref type="bibr" target="#b5">(Okazaki, 2007)</ref> and, after experimenting with the training algorithms available, found that training with av- eraged perceptron <ref type="bibr" target="#b1">(Collins, 2002</ref>) yielded the best predictive results. Results for several iterations of features are given in <ref type="table">Table 3</ref>, generally in order of increasing improvement until "prev +emo class".</p><p>• The emoji span itself, here called 'character' although it may span multiple characters.</p><p>• 'emo?' is a binary feature indicating whether the token contains emoji characters (emo), or is purely word characters (txt).</p><p>• 'POS', a part-of-speech tag assigned by nltk.pos tag, which did apply part-of- speech labels to some emoji characters, and sometimes even correct ones.</p><p>• 'position' was a set of three positional fea- tures: an integer 0-9 indicating a token's po- sition in tenths of the way through the tweet; a three-class BEGIN/MID/END to indicate tokens at the beginning or end of a tweet (dif- ferent from the 0-9 feature in that multiple tokens may get 0 or 9, but only one token will get BEGIN or END); and the number of char- acters in the token.</p><p>• The 'contexty' feature is another set of three features, this time related to context: A boolean preceded by determiner aimed at catching noun emoji; and two fea- tures to record the pairing of the preceding and following part of speech with the present token type (i.e. emo/txt);</p><p>• Unicode blocks, which inhere in the order- ing of emoji characters. Thus far, emoji have been added in semantically-related groups that tend to be contiguous. So there is a block of smiley faces and other 'emoticons'; a block of transport images; blocks of food, sports, animals, clothing; a whole block of hearts of different colors and elaborations; office-related, clocks, weather, hands, plants, and celebratory characters. These provide  <ref type="table">Table 3</ref>: Performance of feature iterations. Only the F1 score is given for word and mm labels because precision and recall were pretty consistent. cont labels are broken down by precision, recall and F1 because they varied in interesting ways.</p><formula xml:id="formula_0">feature F1 word F1 mm P cont R cont F1 cont Macro-avg F1</formula><p>a very inexpensive proxy to semantics, and the 'emo class' feature yielded a marked im- provement in both precision and recall on content words, although the small number of cases in the test data make it hard to be sure of their true contribution.</p><p>We did a few other experiments to explore our features. 'best − character' showed that ignoring the character actually improved recall on content words, at the expense of precision. 'best − con- texty' removed the 'contexty' feature, since it had actually slightly worsened several metrics, but re- moving it from the final '(best)' feature set also worsened several metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Full-feature performance</head><p>The results in <ref type="table">Table 3</ref> show what we could reliably label with coarse-grained labels given the small size of our data set: 511 training tweets, 56 test tweets. But given that we annotated with finer- grained labels as well, it is worth looking at the performance on that task so far; results are shown in <ref type="table">Table 3</ref>. Our test set had only two of each of the verbal content words -content verb and func aux -and didn't catch either of them, nor label anything else with either label. In fact, the only two func aux in our dataset were in the test set, so they never actually got trained on. We saw fairly reasonable recall on the mm topic and mm attitude labels, but given that those are the most frequent labels in the entire data set, it is more relevant that our precision was low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Future directions</head><p>89 examples of content and functional uses of emoji is not enough to reliably model the behav- ior of these categories. More annotation may yield much richer models of the variety of purposes of emoji, and will help get a better handle on the range of emoji polysemy. Clustering of contexts based on observed features may induce more em- pirically valid subtypes than the ones defined by our specification.</p><p>Anglophone Twitter users use emoji in their tweets for a wide range of purposes, and a given emoji character means different things in different contexts. Every emoji linguist notes the fascinat- ing range of pragmatic and multimodal effects that emoji can have in electronic communication. If these effects are to be given lexicographical treat- ment and categorization, they must also be orga- nized into functional and pragmatic categories that are not part of the typical range of classes used to talk about printed words.</p><p>We have mentioned the notion of emoji-sense disambiguation (ESD). ESD in the model of tra- ditional WSD would seem to require an empirical inventory of emoji senses. Even our small sample has shown a number of characters that are used both as content words and as topical or gestural cues. Our data included "Mean girls ", i.e. 'I am watching the movie Mean Girls', which has no propositional content in common with (unat- tested in our data set) "Mean girls ", i.e. 'girls who are mean upset me'. There are a number of flower emoji: sometimes they are used to decorate a message about flowers themselves, and some- times they add sentiment to a message-and, just as in culture away from keyboards, a rose is always marked as conveying a message of 'love', while a cherry blossom is consistently associ- ated with 'beauty'.  <ref type="table">labeled true precision  recall  F1  mm topic  38  53  44  0.7170 0.8636 0.7835  mm attitude  11  26  16  0.4231 0.6875 0.5238  content noun  6  11  11  0.5455 0.5455 0.5455  mm gesture  2  2  8</ref> 1.0000 0.2500 0.4000 content verb 0 0 2 0.0000 0.0000 0.0000 func aux 0 0 2 0.0000 0.0000 0.0000 <ref type="table">Table 4</ref>: performance of best model on subtype labels</p><p>There can be little question that individuals use emoji differently, and this will certainly confound the study of emoji semantics in the immediate term. The study of community dialects will be es- sential to emoji semantics, and there is certain also to be strong variation on the level of idiolect. The categorizations may need refinement, but the phe- nomenon is undeniably worthy of further study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Dataset</head><label></label><figDesc></figDesc></figure>

			<note place="foot" n="3"> Tag prediction experiment We trained a sequence tagger to assign the correct linguistic-function label to an emoji character. Our annotators had assigned labels and subtypes, but due to the low agreement on multimodal (mm) labels, and the small number of cont and func la</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">105</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the ACL-02 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">emoji2vec: Learning emoji representations from their description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matko</forename><surname>Bošnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A global analysis of emoji usage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Ljubešic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darja</forename><surname>Fišer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">82</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Blissfully happy&quot; or &quot;ready to fight&quot;: Varying interpretations of emoji</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Thebault-Spieker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loren</forename><surname>Terveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><surname>Hecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Web and Social Media, ICWSM 2016</title>
		<meeting>the Tenth International Conference on Web and Social Media, ICWSM 2016<address><addrLine>Cologne, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1720" />
		</imprint>
	</monogr>
	<note>Association for the Advancement of Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">CRFsuite: a fast implementation of Conditional Random Fields (CRFs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Do you smile with your nose? Stylistic variation in Twitter emoticons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Schnoebelen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">University of Pennsylvania Working Papers in Linguistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="117" to="125" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
